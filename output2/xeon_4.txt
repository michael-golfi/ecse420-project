Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
WARNING:tensorflow:From cifar10_train.py:81 in train.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2016-03-02.
Instructions for updating:
Please use tf.global_variables instead.
2016-12-08 07:21:24.512520: step 0, loss = 4.68 (27.2 examples/sec; 4.700 sec/batch)
2016-12-08 07:21:30.992805: step 10, loss = 4.61 (286.6 examples/sec; 0.447 sec/batch)
2016-12-08 07:21:36.285072: step 20, loss = 4.62 (240.4 examples/sec; 0.532 sec/batch)
2016-12-08 07:21:41.617472: step 30, loss = 4.52 (294.1 examples/sec; 0.435 sec/batch)
2016-12-08 07:21:47.052600: step 40, loss = 4.44 (230.6 examples/sec; 0.555 sec/batch)
2016-12-08 07:21:52.626170: step 50, loss = 4.31 (228.7 examples/sec; 0.560 sec/batch)
2016-12-08 07:21:58.259886: step 60, loss = 4.26 (254.4 examples/sec; 0.503 sec/batch)
2016-12-08 07:22:03.475957: step 70, loss = 4.15 (247.1 examples/sec; 0.518 sec/batch)
2016-12-08 07:22:08.930282: step 80, loss = 4.25 (211.0 examples/sec; 0.607 sec/batch)
2016-12-08 07:22:14.261619: step 90, loss = 4.07 (227.3 examples/sec; 0.563 sec/batch)
2016-12-08 07:22:19.957181: step 100, loss = 4.13 (237.1 examples/sec; 0.540 sec/batch)
2016-12-08 07:22:25.849835: step 110, loss = 4.10 (246.2 examples/sec; 0.520 sec/batch)
2016-12-08 07:22:31.258515: step 120, loss = 4.13 (223.3 examples/sec; 0.573 sec/batch)
2016-12-08 07:22:36.629481: step 130, loss = 4.06 (223.7 examples/sec; 0.572 sec/batch)
2016-12-08 07:22:42.155930: step 140, loss = 3.98 (224.7 examples/sec; 0.570 sec/batch)
2016-12-08 07:22:47.670180: step 150, loss = 3.97 (204.2 examples/sec; 0.627 sec/batch)
2016-12-08 07:22:53.359941: step 160, loss = 3.98 (221.7 examples/sec; 0.577 sec/batch)
2016-12-08 07:22:58.722639: step 170, loss = 3.89 (218.5 examples/sec; 0.586 sec/batch)
2016-12-08 07:23:04.296297: step 180, loss = 3.83 (215.2 examples/sec; 0.595 sec/batch)
2016-12-08 07:23:09.819093: step 190, loss = 3.88 (208.1 examples/sec; 0.615 sec/batch)
2016-12-08 07:23:15.042172: step 200, loss = 3.91 (235.9 examples/sec; 0.543 sec/batch)
2016-12-08 07:23:21.181020: step 210, loss = 3.69 (220.4 examples/sec; 0.581 sec/batch)
2016-12-08 07:23:26.445462: step 220, loss = 3.78 (243.9 examples/sec; 0.525 sec/batch)
2016-12-08 07:23:31.705853: step 230, loss = 3.79 (244.7 examples/sec; 0.523 sec/batch)
2016-12-08 07:23:37.115230: step 240, loss = 3.68 (257.6 examples/sec; 0.497 sec/batch)
2016-12-08 07:23:42.485876: step 250, loss = 3.81 (229.6 examples/sec; 0.558 sec/batch)
2016-12-08 07:23:47.824873: step 260, loss = 3.65 (222.5 examples/sec; 0.575 sec/batch)
2016-12-08 07:23:53.115277: step 270, loss = 3.67 (249.2 examples/sec; 0.514 sec/batch)
2016-12-08 07:23:58.425319: step 280, loss = 3.63 (267.4 examples/sec; 0.479 sec/batch)
2016-12-08 07:24:03.711942: step 290, loss = 3.61 (253.0 examples/sec; 0.506 sec/batch)
2016-12-08 07:24:09.156923: step 300, loss = 3.73 (245.0 examples/sec; 0.522 sec/batch)
2016-12-08 07:24:14.859818: step 310, loss = 3.59 (276.8 examples/sec; 0.462 sec/batch)
2016-12-08 07:24:19.666943: step 320, loss = 3.47 (327.1 examples/sec; 0.391 sec/batch)
2016-12-08 07:24:24.945342: step 330, loss = 3.42 (232.0 examples/sec; 0.552 sec/batch)
2016-12-08 07:24:30.189522: step 340, loss = 3.55 (239.4 examples/sec; 0.535 sec/batch)
2016-12-08 07:24:35.565513: step 350, loss = 3.37 (268.6 examples/sec; 0.477 sec/batch)
2016-12-08 07:24:40.800112: step 360, loss = 3.45 (217.5 examples/sec; 0.588 sec/batch)
2016-12-08 07:24:46.003471: step 370, loss = 3.45 (211.8 examples/sec; 0.604 sec/batch)
2016-12-08 07:24:51.221780: step 380, loss = 3.22 (254.8 examples/sec; 0.502 sec/batch)
2016-12-08 07:24:56.312406: step 390, loss = 3.48 (264.4 examples/sec; 0.484 sec/batch)
2016-12-08 07:25:01.577556: step 400, loss = 3.30 (217.2 examples/sec; 0.589 sec/batch)
2016-12-08 07:25:07.203575: step 410, loss = 3.35 (299.3 examples/sec; 0.428 sec/batch)
2016-12-08 07:25:12.401338: step 420, loss = 3.49 (229.1 examples/sec; 0.559 sec/batch)
2016-12-08 07:25:17.856769: step 430, loss = 3.22 (209.1 examples/sec; 0.612 sec/batch)
2016-12-08 07:25:23.146716: step 440, loss = 3.40 (201.7 examples/sec; 0.635 sec/batch)
2016-12-08 07:25:28.377230: step 450, loss = 3.39 (216.5 examples/sec; 0.591 sec/batch)
2016-12-08 07:25:33.495513: step 460, loss = 3.19 (259.2 examples/sec; 0.494 sec/batch)
2016-12-08 07:25:38.818005: step 470, loss = 3.25 (252.8 examples/sec; 0.506 sec/batch)
2016-12-08 07:25:43.914832: step 480, loss = 3.08 (246.2 examples/sec; 0.520 sec/batch)
2016-12-08 07:25:49.373356: step 490, loss = 3.15 (211.1 examples/sec; 0.606 sec/batch)
2016-12-08 07:25:54.705061: step 500, loss = 3.11 (233.3 examples/sec; 0.549 sec/batch)
2016-12-08 07:26:00.697642: step 510, loss = 3.22 (322.8 examples/sec; 0.397 sec/batch)
2016-12-08 07:26:06.061694: step 520, loss = 3.14 (240.5 examples/sec; 0.532 sec/batch)
2016-12-08 07:26:11.602340: step 530, loss = 3.12 (255.3 examples/sec; 0.501 sec/batch)
2016-12-08 07:26:16.834953: step 540, loss = 3.08 (223.7 examples/sec; 0.572 sec/batch)
2016-12-08 07:26:22.014291: step 550, loss = 3.31 (219.4 examples/sec; 0.583 sec/batch)
2016-12-08 07:26:27.243990: step 560, loss = 3.16 (292.3 examples/sec; 0.438 sec/batch)
2016-12-08 07:26:32.576831: step 570, loss = 3.24 (212.8 examples/sec; 0.601 sec/batch)
2016-12-08 07:26:37.517821: step 580, loss = 3.20 (313.9 examples/sec; 0.408 sec/batch)
2016-12-08 07:26:42.956042: step 590, loss = 3.09 (247.4 examples/sec; 0.517 sec/batch)
2016-12-08 07:26:48.259346: step 600, loss = 3.36 (248.5 examples/sec; 0.515 sec/batch)
2016-12-08 07:26:54.142921: step 610, loss = 2.99 (248.9 examples/sec; 0.514 sec/batch)
2016-12-08 07:26:59.420086: step 620, loss = 3.05 (226.3 examples/sec; 0.566 sec/batch)
2016-12-08 07:27:04.751537: step 630, loss = 3.00 (236.7 examples/sec; 0.541 sec/batch)
2016-12-08 07:27:10.172360: step 640, loss = 2.89 (204.6 examples/sec; 0.626 sec/batch)
2016-12-08 07:27:15.200374: step 650, loss = 2.98 (261.1 examples/sec; 0.490 sec/batch)
2016-12-08 07:27:20.529549: step 660, loss = 2.99 (263.8 examples/sec; 0.485 sec/batch)
2016-12-08 07:27:25.823476: step 670, loss = 3.03 (249.8 examples/sec; 0.512 sec/batch)
2016-12-08 07:27:31.132208: step 680, loss = 2.92 (242.3 examples/sec; 0.528 sec/batch)
2016-12-08 07:27:36.442213: step 690, loss = 3.04 (267.7 examples/sec; 0.478 sec/batch)
2016-12-08 07:27:41.655856: step 700, loss = 2.91 (289.0 examples/sec; 0.443 sec/batch)
2016-12-08 07:27:47.388820: step 710, loss = 2.80 (233.6 examples/sec; 0.548 sec/batch)
2016-12-08 07:27:52.695825: step 720, loss = 2.96 (218.7 examples/sec; 0.585 sec/batch)
2016-12-08 07:27:58.055592: step 730, loss = 2.65 (232.7 examples/sec; 0.550 sec/batch)
2016-12-08 07:28:02.978563: step 740, loss = 3.00 (241.8 examples/sec; 0.529 sec/batch)
2016-12-08 07:28:08.330110: step 750, loss = 2.64 (255.6 examples/sec; 0.501 sec/batch)
2016-12-08 07:28:13.511248: step 760, loss = 2.94 (257.4 examples/sec; 0.497 sec/batch)
2016-12-08 07:28:18.761002: step 770, loss = 2.59 (264.9 examples/sec; 0.483 sec/batch)
2016-12-08 07:28:23.917320: step 780, loss = 2.92 (235.4 examples/sec; 0.544 sec/batch)
2016-12-08 07:28:28.836979: step 790, loss = 2.51 (305.8 examples/sec; 0.419 sec/batch)
2016-12-08 07:28:34.267435: step 800, loss = 2.63 (230.1 examples/sec; 0.556 sec/batch)
2016-12-08 07:28:39.974283: step 810, loss = 3.03 (274.7 examples/sec; 0.466 sec/batch)
2016-12-08 07:28:45.604115: step 820, loss = 2.63 (233.3 examples/sec; 0.549 sec/batch)
2016-12-08 07:28:50.989776: step 830, loss = 2.73 (221.9 examples/sec; 0.577 sec/batch)
2016-12-08 07:28:56.395798: step 840, loss = 2.85 (244.0 examples/sec; 0.525 sec/batch)
2016-12-08 07:29:01.641837: step 850, loss = 2.69 (275.0 examples/sec; 0.466 sec/batch)
2016-12-08 07:29:06.781097: step 860, loss = 2.67 (234.4 examples/sec; 0.546 sec/batch)
2016-12-08 07:29:12.420140: step 870, loss = 2.70 (239.9 examples/sec; 0.533 sec/batch)
2016-12-08 07:29:18.028824: step 880, loss = 2.49 (214.7 examples/sec; 0.596 sec/batch)
2016-12-08 07:29:23.169319: step 890, loss = 2.48 (218.7 examples/sec; 0.585 sec/batch)
2016-12-08 07:29:28.416372: step 900, loss = 2.66 (216.3 examples/sec; 0.592 sec/batch)
2016-12-08 07:29:34.237610: step 910, loss = 2.57 (234.9 examples/sec; 0.545 sec/batch)
2016-12-08 07:29:39.461350: step 920, loss = 2.69 (241.9 examples/sec; 0.529 sec/batch)
2016-12-08 07:29:44.645208: step 930, loss = 2.34 (251.5 examples/sec; 0.509 sec/batch)
2016-12-08 07:29:50.210850: step 940, loss = 2.41 (231.0 examples/sec; 0.554 sec/batch)
2016-12-08 07:29:55.389250: step 950, loss = 2.50 (233.8 examples/sec; 0.547 sec/batch)
2016-12-08 07:30:00.352961: step 960, loss = 2.46 (223.2 examples/sec; 0.574 sec/batch)
2016-12-08 07:30:05.496860: step 970, loss = 2.48 (217.2 examples/sec; 0.589 sec/batch)
2016-12-08 07:30:10.579078: step 980, loss = 2.17 (266.1 examples/sec; 0.481 sec/batch)
2016-12-08 07:30:16.015802: step 990, loss = 2.52 (241.8 examples/sec; 0.529 sec/batch)
2016-12-08 07:30:21.342655: step 1000, loss = 2.55 (264.1 examples/sec; 0.485 sec/batch)
2016-12-08 07:30:27.848329: step 1010, loss = 2.49 (278.2 examples/sec; 0.460 sec/batch)
2016-12-08 07:30:33.162875: step 1020, loss = 2.61 (266.5 examples/sec; 0.480 sec/batch)
2016-12-08 07:30:38.503622: step 1030, loss = 2.42 (240.7 examples/sec; 0.532 sec/batch)
2016-12-08 07:30:43.843746: step 1040, loss = 2.38 (212.6 examples/sec; 0.602 sec/batch)
2016-12-08 07:30:49.030486: step 1050, loss = 2.42 (237.7 examples/sec; 0.538 sec/batch)
2016-12-08 07:30:54.140793: step 1060, loss = 2.25 (244.3 examples/sec; 0.524 sec/batch)
2016-12-08 07:30:59.587410: step 1070, loss = 2.31 (239.4 examples/sec; 0.535 sec/batch)
2016-12-08 07:31:05.202256: step 1080, loss = 2.37 (226.1 examples/sec; 0.566 sec/batch)
2016-12-08 07:31:10.680331: step 1090, loss = 2.42 (244.9 examples/sec; 0.523 sec/batch)
2016-12-08 07:31:16.230465: step 1100, loss = 2.48 (256.4 examples/sec; 0.499 sec/batch)
2016-12-08 07:31:21.797416: step 1110, loss = 2.47 (218.6 examples/sec; 0.586 sec/batch)
2016-12-08 07:31:27.342556: step 1120, loss = 2.23 (226.6 examples/sec; 0.565 sec/batch)
2016-12-08 07:31:32.323457: step 1130, loss = 2.45 (298.0 examples/sec; 0.430 sec/batch)
2016-12-08 07:31:37.617999: step 1140, loss = 2.38 (269.6 examples/sec; 0.475 sec/batch)
2016-12-08 07:31:42.993296: step 1150, loss = 2.20 (249.2 examples/sec; 0.514 sec/batch)
2016-12-08 07:31:48.367610: step 1160, loss = 2.44 (239.6 examples/sec; 0.534 sec/batch)
2016-12-08 07:31:53.611012: step 1170, loss = 2.21 (273.5 examples/sec; 0.468 sec/batch)
2016-12-08 07:31:59.419933: step 1180, loss = 2.25 (193.4 examples/sec; 0.662 sec/batch)
2016-12-08 07:32:04.943280: step 1190, loss = 2.19 (258.1 examples/sec; 0.496 sec/batch)
2016-12-08 07:32:10.239613: step 1200, loss = 2.39 (210.9 examples/sec; 0.607 sec/batch)
2016-12-08 07:32:15.842680: step 1210, loss = 2.21 (210.3 examples/sec; 0.609 sec/batch)
2016-12-08 07:32:21.082106: step 1220, loss = 2.13 (269.2 examples/sec; 0.476 sec/batch)
2016-12-08 07:32:26.487120: step 1230, loss = 2.18 (214.3 examples/sec; 0.597 sec/batch)
2016-12-08 07:32:31.782928: step 1240, loss = 2.21 (261.3 examples/sec; 0.490 sec/batch)
2016-12-08 07:32:36.945158: step 1250, loss = 2.23 (247.1 examples/sec; 0.518 sec/batch)
2016-12-08 07:32:42.348236: step 1260, loss = 2.06 (232.3 examples/sec; 0.551 sec/batch)
2016-12-08 07:32:47.712212: step 1270, loss = 2.49 (261.2 examples/sec; 0.490 sec/batch)
2016-12-08 07:32:52.733878: step 1280, loss = 1.98 (272.6 examples/sec; 0.470 sec/batch)
2016-12-08 07:32:58.213327: step 1290, loss = 2.00 (231.5 examples/sec; 0.553 sec/batch)
2016-12-08 07:33:03.479218: step 1300, loss = 2.16 (223.1 examples/sec; 0.574 sec/batch)
2016-12-08 07:33:09.212875: step 1310, loss = 2.15 (280.4 examples/sec; 0.456 sec/batch)
2016-12-08 07:33:14.389091: step 1320, loss = 2.11 (205.4 examples/sec; 0.623 sec/batch)
2016-12-08 07:33:19.898106: step 1330, loss = 2.10 (260.3 examples/sec; 0.492 sec/batch)
2016-12-08 07:33:25.235286: step 1340, loss = 2.12 (262.6 examples/sec; 0.488 sec/batch)
2016-12-08 07:33:30.342581: step 1350, loss = 2.23 (261.4 examples/sec; 0.490 sec/batch)
2016-12-08 07:33:35.681904: step 1360, loss = 1.98 (255.2 examples/sec; 0.502 sec/batch)
2016-12-08 07:33:40.739615: step 1370, loss = 2.00 (240.3 examples/sec; 0.533 sec/batch)
2016-12-08 07:33:46.376314: step 1380, loss = 2.08 (239.9 examples/sec; 0.534 sec/batch)
2016-12-08 07:33:51.682909: step 1390, loss = 2.20 (224.4 examples/sec; 0.570 sec/batch)
2016-12-08 07:33:57.030995: step 1400, loss = 1.91 (227.1 examples/sec; 0.564 sec/batch)
2016-12-08 07:34:02.784314: step 1410, loss = 1.99 (254.5 examples/sec; 0.503 sec/batch)
2016-12-08 07:34:08.249889: step 1420, loss = 1.92 (267.3 examples/sec; 0.479 sec/batch)
2016-12-08 07:34:13.451577: step 1430, loss = 2.21 (212.3 examples/sec; 0.603 sec/batch)
2016-12-08 07:34:18.890719: step 1440, loss = 1.97 (225.6 examples/sec; 0.567 sec/batch)
2016-12-08 07:34:24.115512: step 1450, loss = 2.06 (230.4 examples/sec; 0.556 sec/batch)
2016-12-08 07:34:29.442139: step 1460, loss = 2.11 (254.4 examples/sec; 0.503 sec/batch)
2016-12-08 07:34:34.831112: step 1470, loss = 1.87 (241.7 examples/sec; 0.529 sec/batch)
2016-12-08 07:34:40.227892: step 1480, loss = 1.74 (225.4 examples/sec; 0.568 sec/batch)
2016-12-08 07:34:45.623091: step 1490, loss = 1.91 (254.3 examples/sec; 0.503 sec/batch)
2016-12-08 07:34:51.329312: step 1500, loss = 2.11 (238.0 examples/sec; 0.538 sec/batch)
2016-12-08 07:34:56.934206: step 1510, loss = 1.96 (263.1 examples/sec; 0.487 sec/batch)
2016-12-08 07:35:01.975980: step 1520, loss = 2.06 (237.1 examples/sec; 0.540 sec/batch)
2016-12-08 07:35:07.099901: step 1530, loss = 1.77 (197.3 examples/sec; 0.649 sec/batch)
2016-12-08 07:35:12.309573: step 1540, loss = 1.85 (274.0 examples/sec; 0.467 sec/batch)
2016-12-08 07:35:17.340564: step 1550, loss = 2.02 (281.4 examples/sec; 0.455 sec/batch)
2016-12-08 07:35:22.865606: step 1560, loss = 2.03 (218.5 examples/sec; 0.586 sec/batch)
2016-12-08 07:35:28.124449: step 1570, loss = 1.85 (259.2 examples/sec; 0.494 sec/batch)
2016-12-08 07:35:33.229740: step 1580, loss = 1.70 (223.0 examples/sec; 0.574 sec/batch)
2016-12-08 07:35:38.188786: step 1590, loss = 1.93 (255.4 examples/sec; 0.501 sec/batch)
2016-12-08 07:35:43.477341: step 1600, loss = 2.05 (234.3 examples/sec; 0.546 sec/batch)
2016-12-08 07:35:49.378303: step 1610, loss = 1.74 (229.3 examples/sec; 0.558 sec/batch)
2016-12-08 07:35:54.516474: step 1620, loss = 1.77 (214.5 examples/sec; 0.597 sec/batch)
2016-12-08 07:35:59.913185: step 1630, loss = 1.92 (224.2 examples/sec; 0.571 sec/batch)
2016-12-08 07:36:04.902399: step 1640, loss = 1.76 (258.0 examples/sec; 0.496 sec/batch)
2016-12-08 07:36:09.896055: step 1650, loss = 1.97 (248.9 examples/sec; 0.514 sec/batch)
2016-12-08 07:36:15.082640: step 1660, loss = 1.87 (255.7 examples/sec; 0.501 sec/batch)
2016-12-08 07:36:20.023346: step 1670, loss = 1.57 (237.8 examples/sec; 0.538 sec/batch)
2016-12-08 07:36:25.534406: step 1680, loss = 1.76 (224.1 examples/sec; 0.571 sec/batch)
2016-12-08 07:36:30.882926: step 1690, loss = 1.88 (255.5 examples/sec; 0.501 sec/batch)
2016-12-08 07:36:36.288041: step 1700, loss = 2.00 (232.1 examples/sec; 0.551 sec/batch)
2016-12-08 07:36:42.274835: step 1710, loss = 1.86 (256.4 examples/sec; 0.499 sec/batch)
2016-12-08 07:36:47.570634: step 1720, loss = 1.87 (249.7 examples/sec; 0.513 sec/batch)
2016-12-08 07:36:53.170462: step 1730, loss = 1.89 (224.4 examples/sec; 0.571 sec/batch)
2016-12-08 07:36:58.345661: step 1740, loss = 1.67 (238.1 examples/sec; 0.538 sec/batch)
2016-12-08 07:37:03.603731: step 1750, loss = 1.92 (279.3 examples/sec; 0.458 sec/batch)
2016-12-08 07:37:08.822210: step 1760, loss = 1.82 (222.8 examples/sec; 0.574 sec/batch)
2016-12-08 07:37:14.123028: step 1770, loss = 1.89 (253.3 examples/sec; 0.505 sec/batch)
2016-12-08 07:37:19.471243: step 1780, loss = 1.89 (199.4 examples/sec; 0.642 sec/batch)
2016-12-08 07:37:24.844432: step 1790, loss = 1.98 (292.9 examples/sec; 0.437 sec/batch)
2016-12-08 07:37:30.000528: step 1800, loss = 1.69 (236.2 examples/sec; 0.542 sec/batch)
2016-12-08 07:37:35.606795: step 1810, loss = 1.66 (219.1 examples/sec; 0.584 sec/batch)
2016-12-08 07:37:40.996543: step 1820, loss = 1.70 (238.7 examples/sec; 0.536 sec/batch)
2016-12-08 07:37:45.892037: step 1830, loss = 1.75 (200.9 examples/sec; 0.637 sec/batch)
2016-12-08 07:37:51.227739: step 1840, loss = 1.87 (229.3 examples/sec; 0.558 sec/batch)
2016-12-08 07:37:56.278472: step 1850, loss = 1.71 (293.1 examples/sec; 0.437 sec/batch)
2016-12-08 07:38:01.911984: step 1860, loss = 1.68 (217.2 examples/sec; 0.589 sec/batch)
2016-12-08 07:38:07.324869: step 1870, loss = 1.68 (232.4 examples/sec; 0.551 sec/batch)
2016-12-08 07:38:12.587433: step 1880, loss = 1.71 (268.9 examples/sec; 0.476 sec/batch)
2016-12-08 07:38:18.186257: step 1890, loss = 1.67 (218.9 examples/sec; 0.585 sec/batch)
2016-12-08 07:38:23.427329: step 1900, loss = 1.69 (270.1 examples/sec; 0.474 sec/batch)
2016-12-08 07:38:29.341813: step 1910, loss = 1.64 (227.6 examples/sec; 0.562 sec/batch)
2016-12-08 07:38:34.663705: step 1920, loss = 1.54 (228.4 examples/sec; 0.560 sec/batch)
2016-12-08 07:38:39.870925: step 1930, loss = 1.93 (240.9 examples/sec; 0.531 sec/batch)
2016-12-08 07:38:44.927838: step 1940, loss = 1.68 (223.1 examples/sec; 0.574 sec/batch)
2016-12-08 07:38:50.150098: step 1950, loss = 1.64 (238.4 examples/sec; 0.537 sec/batch)
2016-12-08 07:38:55.302345: step 1960, loss = 1.60 (317.0 examples/sec; 0.404 sec/batch)
2016-12-08 07:39:00.357333: step 1970, loss = 1.88 (224.0 examples/sec; 0.571 sec/batch)
2016-12-08 07:39:05.642230: step 1980, loss = 1.69 (243.7 examples/sec; 0.525 sec/batch)
2016-12-08 07:39:10.726707: step 1990, loss = 1.63 (274.8 examples/sec; 0.466 sec/batch)
2016-12-08 07:39:15.911530: step 2000, loss = 1.48 (261.1 examples/sec; 0.490 sec/batch)
2016-12-08 07:39:22.649347: step 2010, loss = 1.61 (280.2 examples/sec; 0.457 sec/batch)
2016-12-08 07:39:27.681197: step 2020, loss = 1.63 (238.6 examples/sec; 0.537 sec/batch)
2016-12-08 07:39:33.254613: step 2030, loss = 1.67 (261.0 examples/sec; 0.490 sec/batch)
2016-12-08 07:39:38.440697: step 2040, loss = 1.46 (235.0 examples/sec; 0.545 sec/batch)
2016-12-08 07:39:43.810655: step 2050, loss = 1.75 (225.1 examples/sec; 0.569 sec/batch)
2016-12-08 07:39:49.210053: step 2060, loss = 1.58 (225.4 examples/sec; 0.568 sec/batch)
2016-12-08 07:39:54.408828: step 2070, loss = 1.56 (220.8 examples/sec; 0.580 sec/batch)
2016-12-08 07:39:59.733811: step 2080, loss = 1.61 (242.7 examples/sec; 0.527 sec/batch)
2016-12-08 07:40:04.952529: step 2090, loss = 1.60 (233.1 examples/sec; 0.549 sec/batch)
2016-12-08 07:40:10.169447: step 2100, loss = 1.53 (224.8 examples/sec; 0.569 sec/batch)
2016-12-08 07:40:16.200832: step 2110, loss = 1.32 (241.1 examples/sec; 0.531 sec/batch)
2016-12-08 07:40:21.463050: step 2120, loss = 1.59 (230.4 examples/sec; 0.556 sec/batch)
2016-12-08 07:40:27.048840: step 2130, loss = 1.74 (217.8 examples/sec; 0.588 sec/batch)
2016-12-08 07:40:32.208708: step 2140, loss = 1.69 (227.8 examples/sec; 0.562 sec/batch)
2016-12-08 07:40:37.514371: step 2150, loss = 1.80 (222.5 examples/sec; 0.575 sec/batch)
2016-12-08 07:40:42.746196: step 2160, loss = 1.71 (253.6 examples/sec; 0.505 sec/batch)
2016-12-08 07:40:47.918580: step 2170, loss = 1.57 (210.6 examples/sec; 0.608 sec/batch)
2016-12-08 07:40:53.118652: step 2180, loss = 1.88 (233.9 examples/sec; 0.547 sec/batch)
2016-12-08 07:40:58.552674: step 2190, loss = 1.52 (251.3 examples/sec; 0.509 sec/batch)
2016-12-08 07:41:03.992118: step 2200, loss = 1.54 (229.2 examples/sec; 0.558 sec/batch)
2016-12-08 07:41:10.278791: step 2210, loss = 1.67 (225.3 examples/sec; 0.568 sec/batch)
2016-12-08 07:41:15.497851: step 2220, loss = 1.54 (247.0 examples/sec; 0.518 sec/batch)
2016-12-08 07:41:20.889140: step 2230, loss = 1.62 (208.8 examples/sec; 0.613 sec/batch)
2016-12-08 07:41:26.363823: step 2240, loss = 1.53 (204.3 examples/sec; 0.627 sec/batch)
2016-12-08 07:41:31.561416: step 2250, loss = 1.49 (261.2 examples/sec; 0.490 sec/batch)
2016-12-08 07:41:37.099978: step 2260, loss = 1.62 (241.2 examples/sec; 0.531 sec/batch)
2016-12-08 07:41:42.473890: step 2270, loss = 1.42 (221.4 examples/sec; 0.578 sec/batch)
2016-12-08 07:41:47.915920: step 2280, loss = 1.32 (201.5 examples/sec; 0.635 sec/batch)
2016-12-08 07:41:53.054122: step 2290, loss = 1.43 (254.3 examples/sec; 0.503 sec/batch)
2016-12-08 07:41:58.522810: step 2300, loss = 1.49 (214.9 examples/sec; 0.596 sec/batch)
2016-12-08 07:42:04.355822: step 2310, loss = 1.50 (264.6 examples/sec; 0.484 sec/batch)
2016-12-08 07:42:09.773623: step 2320, loss = 1.58 (227.1 examples/sec; 0.564 sec/batch)
2016-12-08 07:42:15.276759: step 2330, loss = 1.41 (310.6 examples/sec; 0.412 sec/batch)
2016-12-08 07:42:20.557614: step 2340, loss = 1.57 (251.2 examples/sec; 0.510 sec/batch)
2016-12-08 07:42:26.135519: step 2350, loss = 1.52 (218.8 examples/sec; 0.585 sec/batch)
2016-12-08 07:42:31.142206: step 2360, loss = 1.39 (249.9 examples/sec; 0.512 sec/batch)
2016-12-08 07:42:36.282256: step 2370, loss = 1.29 (275.7 examples/sec; 0.464 sec/batch)
2016-12-08 07:42:41.920469: step 2380, loss = 1.46 (267.3 examples/sec; 0.479 sec/batch)
2016-12-08 07:42:47.001605: step 2390, loss = 1.51 (213.9 examples/sec; 0.598 sec/batch)
2016-12-08 07:42:52.094804: step 2400, loss = 1.45 (246.1 examples/sec; 0.520 sec/batch)
2016-12-08 07:42:57.829727: step 2410, loss = 1.32 (267.9 examples/sec; 0.478 sec/batch)
2016-12-08 07:43:02.824101: step 2420, loss = 1.37 (268.5 examples/sec; 0.477 sec/batch)
2016-12-08 07:43:07.765197: step 2430, loss = 1.53 (234.8 examples/sec; 0.545 sec/batch)
2016-12-08 07:43:13.149288: step 2440, loss = 1.49 (222.3 examples/sec; 0.576 sec/batch)
2016-12-08 07:43:18.552503: step 2450, loss = 1.42 (219.4 examples/sec; 0.583 sec/batch)
2016-12-08 07:43:23.685884: step 2460, loss = 1.41 (221.8 examples/sec; 0.577 sec/batch)
2016-12-08 07:43:28.989861: step 2470, loss = 1.29 (267.6 examples/sec; 0.478 sec/batch)
2016-12-08 07:43:34.564921: step 2480, loss = 1.53 (227.4 examples/sec; 0.563 sec/batch)
2016-12-08 07:43:39.801946: step 2490, loss = 1.34 (271.0 examples/sec; 0.472 sec/batch)
2016-12-08 07:43:44.982663: step 2500, loss = 1.41 (238.7 examples/sec; 0.536 sec/batch)
2016-12-08 07:43:50.703890: step 2510, loss = 1.54 (221.5 examples/sec; 0.578 sec/batch)
2016-12-08 07:43:55.923466: step 2520, loss = 1.42 (251.4 examples/sec; 0.509 sec/batch)
2016-12-08 07:44:00.745692: step 2530, loss = 1.42 (295.6 examples/sec; 0.433 sec/batch)
2016-12-08 07:44:05.847808: step 2540, loss = 1.42 (238.4 examples/sec; 0.537 sec/batch)
2016-12-08 07:44:11.221896: step 2550, loss = 1.43 (254.5 examples/sec; 0.503 sec/batch)
2016-12-08 07:44:16.442119: step 2560, loss = 1.35 (262.2 examples/sec; 0.488 sec/batch)
2016-12-08 07:44:21.754962: step 2570, loss = 1.28 (239.4 examples/sec; 0.535 sec/batch)
2016-12-08 07:44:27.356837: step 2580, loss = 1.36 (227.3 examples/sec; 0.563 sec/batch)
2016-12-08 07:44:32.648178: step 2590, loss = 1.37 (295.7 examples/sec; 0.433 sec/batch)
2016-12-08 07:44:37.671418: step 2600, loss = 1.44 (291.6 examples/sec; 0.439 sec/batch)
2016-12-08 07:44:43.540077: step 2610, loss = 1.39 (207.1 examples/sec; 0.618 sec/batch)
2016-12-08 07:44:48.780128: step 2620, loss = 1.25 (250.0 examples/sec; 0.512 sec/batch)
2016-12-08 07:44:54.543713: step 2630, loss = 1.34 (218.9 examples/sec; 0.585 sec/batch)
2016-12-08 07:44:59.813422: step 2640, loss = 1.48 (250.4 examples/sec; 0.511 sec/batch)
2016-12-08 07:45:05.117309: step 2650, loss = 1.43 (255.5 examples/sec; 0.501 sec/batch)
2016-12-08 07:45:10.356883: step 2660, loss = 1.41 (282.2 examples/sec; 0.454 sec/batch)
2016-12-08 07:45:15.803536: step 2670, loss = 1.43 (217.4 examples/sec; 0.589 sec/batch)
2016-12-08 07:45:21.203988: step 2680, loss = 1.37 (211.1 examples/sec; 0.606 sec/batch)
2016-12-08 07:45:26.610756: step 2690, loss = 1.46 (227.9 examples/sec; 0.562 sec/batch)
2016-12-08 07:45:31.884039: step 2700, loss = 1.37 (251.7 examples/sec; 0.509 sec/batch)
2016-12-08 07:45:37.345261: step 2710, loss = 1.33 (261.9 examples/sec; 0.489 sec/batch)
2016-12-08 07:45:42.765609: step 2720, loss = 1.34 (235.1 examples/sec; 0.544 sec/batch)
2016-12-08 07:45:47.879490: step 2730, loss = 1.39 (254.5 examples/sec; 0.503 sec/batch)
2016-12-08 07:45:53.242648: step 2740, loss = 1.44 (259.4 examples/sec; 0.493 sec/batch)
2016-12-08 07:45:58.448434: step 2750, loss = 1.30 (280.4 examples/sec; 0.457 sec/batch)
2016-12-08 07:46:03.446951: step 2760, loss = 1.44 (296.9 examples/sec; 0.431 sec/batch)
2016-12-08 07:46:08.572189: step 2770, loss = 1.42 (274.1 examples/sec; 0.467 sec/batch)
2016-12-08 07:46:14.175742: step 2780, loss = 1.45 (221.3 examples/sec; 0.578 sec/batch)
2016-12-08 07:46:19.569917: step 2790, loss = 1.42 (227.3 examples/sec; 0.563 sec/batch)
2016-12-08 07:46:24.816463: step 2800, loss = 1.27 (270.9 examples/sec; 0.473 sec/batch)
2016-12-08 07:46:30.512831: step 2810, loss = 1.19 (251.7 examples/sec; 0.508 sec/batch)
2016-12-08 07:46:35.920952: step 2820, loss = 1.31 (219.8 examples/sec; 0.582 sec/batch)
2016-12-08 07:46:41.274699: step 2830, loss = 1.59 (239.4 examples/sec; 0.535 sec/batch)
2016-12-08 07:46:46.763923: step 2840, loss = 1.28 (235.8 examples/sec; 0.543 sec/batch)
2016-12-08 07:46:52.007075: step 2850, loss = 1.18 (218.1 examples/sec; 0.587 sec/batch)
2016-12-08 07:46:57.386983: step 2860, loss = 1.34 (201.9 examples/sec; 0.634 sec/batch)
2016-12-08 07:47:02.553041: step 2870, loss = 1.18 (237.2 examples/sec; 0.540 sec/batch)
2016-12-08 07:47:08.067068: step 2880, loss = 1.47 (220.6 examples/sec; 0.580 sec/batch)
2016-12-08 07:47:13.177879: step 2890, loss = 1.22 (252.8 examples/sec; 0.506 sec/batch)
2016-12-08 07:47:18.218753: step 2900, loss = 1.32 (266.5 examples/sec; 0.480 sec/batch)
2016-12-08 07:47:24.002947: step 2910, loss = 1.16 (239.4 examples/sec; 0.535 sec/batch)
2016-12-08 07:47:29.317469: step 2920, loss = 1.39 (270.0 examples/sec; 0.474 sec/batch)
2016-12-08 07:47:34.645235: step 2930, loss = 1.28 (217.3 examples/sec; 0.589 sec/batch)
2016-12-08 07:47:39.972061: step 2940, loss = 1.50 (267.9 examples/sec; 0.478 sec/batch)
2016-12-08 07:47:45.390096: step 2950, loss = 1.22 (258.6 examples/sec; 0.495 sec/batch)
2016-12-08 07:47:50.821231: step 2960, loss = 1.20 (250.7 examples/sec; 0.511 sec/batch)
2016-12-08 07:47:55.917265: step 2970, loss = 1.12 (256.2 examples/sec; 0.500 sec/batch)
2016-12-08 07:48:00.877006: step 2980, loss = 1.28 (234.9 examples/sec; 0.545 sec/batch)
2016-12-08 07:48:06.224175: step 2990, loss = 1.28 (217.5 examples/sec; 0.589 sec/batch)
2016-12-08 07:48:11.517105: step 3000, loss = 1.45 (225.3 examples/sec; 0.568 sec/batch)
2016-12-08 07:48:17.991246: step 3010, loss = 1.37 (238.9 examples/sec; 0.536 sec/batch)
2016-12-08 07:48:22.861107: step 3020, loss = 1.27 (271.4 examples/sec; 0.472 sec/batch)
2016-12-08 07:48:28.462183: step 3030, loss = 1.27 (239.0 examples/sec; 0.536 sec/batch)
2016-12-08 07:48:33.904610: step 3040, loss = 1.36 (243.2 examples/sec; 0.526 sec/batch)
2016-12-08 07:48:39.148558: step 3050, loss = 1.24 (216.4 examples/sec; 0.591 sec/batch)
2016-12-08 07:48:44.425500: step 3060, loss = 1.39 (248.4 examples/sec; 0.515 sec/batch)
2016-12-08 07:48:49.985064: step 3070, loss = 1.29 (219.8 examples/sec; 0.582 sec/batch)
2016-12-08 07:48:55.583046: step 3080, loss = 1.36 (237.7 examples/sec; 0.538 sec/batch)
2016-12-08 07:49:00.878257: step 3090, loss = 1.08 (242.0 examples/sec; 0.529 sec/batch)
2016-12-08 07:49:06.008991: step 3100, loss = 1.28 (251.5 examples/sec; 0.509 sec/batch)
2016-12-08 07:49:11.768890: step 3110, loss = 1.41 (230.9 examples/sec; 0.554 sec/batch)
2016-12-08 07:49:16.987796: step 3120, loss = 1.18 (262.5 examples/sec; 0.488 sec/batch)
2016-12-08 07:49:22.234403: step 3130, loss = 1.15 (227.6 examples/sec; 0.562 sec/batch)
2016-12-08 07:49:27.738298: step 3140, loss = 1.43 (208.4 examples/sec; 0.614 sec/batch)
2016-12-08 07:49:33.194730: step 3150, loss = 1.33 (238.2 examples/sec; 0.537 sec/batch)
2016-12-08 07:49:38.364519: step 3160, loss = 1.05 (251.6 examples/sec; 0.509 sec/batch)
2016-12-08 07:49:43.394881: step 3170, loss = 1.26 (214.2 examples/sec; 0.598 sec/batch)
2016-12-08 07:49:48.627868: step 3180, loss = 1.22 (259.8 examples/sec; 0.493 sec/batch)
2016-12-08 07:49:53.665323: step 3190, loss = 1.32 (272.4 examples/sec; 0.470 sec/batch)
2016-12-08 07:49:58.190612: step 3200, loss = 1.09 (270.1 examples/sec; 0.474 sec/batch)
2016-12-08 07:50:03.862522: step 3210, loss = 1.08 (262.2 examples/sec; 0.488 sec/batch)
2016-12-08 07:50:08.990724: step 3220, loss = 1.20 (231.7 examples/sec; 0.552 sec/batch)
2016-12-08 07:50:14.354261: step 3230, loss = 1.19 (221.6 examples/sec; 0.578 sec/batch)
2016-12-08 07:50:19.273934: step 3240, loss = 1.16 (257.5 examples/sec; 0.497 sec/batch)
2016-12-08 07:50:24.408924: step 3250, loss = 1.43 (244.7 examples/sec; 0.523 sec/batch)
2016-12-08 07:50:29.635366: step 3260, loss = 1.18 (221.1 examples/sec; 0.579 sec/batch)
2016-12-08 07:50:34.883226: step 3270, loss = 1.28 (233.9 examples/sec; 0.547 sec/batch)
2016-12-08 07:50:40.225435: step 3280, loss = 1.20 (219.3 examples/sec; 0.584 sec/batch)
2016-12-08 07:50:45.428642: step 3290, loss = 1.21 (247.0 examples/sec; 0.518 sec/batch)
2016-12-08 07:50:50.833195: step 3300, loss = 1.03 (211.1 examples/sec; 0.606 sec/batch)
2016-12-08 07:50:56.908510: step 3310, loss = 1.27 (222.3 examples/sec; 0.576 sec/batch)
2016-12-08 07:51:02.114729: step 3320, loss = 1.43 (259.4 examples/sec; 0.493 sec/batch)
2016-12-08 07:51:07.324136: step 3330, loss = 1.10 (240.6 examples/sec; 0.532 sec/batch)
2016-12-08 07:51:12.641491: step 3340, loss = 1.13 (235.0 examples/sec; 0.545 sec/batch)
2016-12-08 07:51:17.941328: step 3350, loss = 1.15 (227.0 examples/sec; 0.564 sec/batch)
2016-12-08 07:51:22.956556: step 3360, loss = 1.36 (246.3 examples/sec; 0.520 sec/batch)
2016-12-08 07:51:27.864328: step 3370, loss = 1.29 (256.4 examples/sec; 0.499 sec/batch)
2016-12-08 07:51:33.403376: step 3380, loss = 1.30 (217.5 examples/sec; 0.588 sec/batch)
2016-12-08 07:51:38.599452: step 3390, loss = 1.37 (208.9 examples/sec; 0.613 sec/batch)
2016-12-08 07:51:43.670297: step 3400, loss = 1.24 (261.7 examples/sec; 0.489 sec/batch)
2016-12-08 07:51:49.226101: step 3410, loss = 1.16 (240.5 examples/sec; 0.532 sec/batch)
2016-12-08 07:51:54.143595: step 3420, loss = 1.05 (249.6 examples/sec; 0.513 sec/batch)
2016-12-08 07:51:59.251606: step 3430, loss = 1.14 (280.5 examples/sec; 0.456 sec/batch)
2016-12-08 07:52:04.391499: step 3440, loss = 1.16 (249.5 examples/sec; 0.513 sec/batch)
2016-12-08 07:52:09.664648: step 3450, loss = 1.22 (241.3 examples/sec; 0.531 sec/batch)
2016-12-08 07:52:14.599159: step 3460, loss = 1.26 (242.8 examples/sec; 0.527 sec/batch)
2016-12-08 07:52:19.461733: step 3470, loss = 1.13 (259.1 examples/sec; 0.494 sec/batch)
2016-12-08 07:52:25.019563: step 3480, loss = 1.03 (237.1 examples/sec; 0.540 sec/batch)
2016-12-08 07:52:29.993525: step 3490, loss = 1.35 (233.1 examples/sec; 0.549 sec/batch)
2016-12-08 07:52:35.499711: step 3500, loss = 1.03 (235.0 examples/sec; 0.545 sec/batch)
2016-12-08 07:52:41.246647: step 3510, loss = 1.08 (234.7 examples/sec; 0.545 sec/batch)
2016-12-08 07:52:46.368474: step 3520, loss = 1.17 (292.4 examples/sec; 0.438 sec/batch)
2016-12-08 07:52:51.451827: step 3530, loss = 1.29 (241.0 examples/sec; 0.531 sec/batch)
2016-12-08 07:52:56.569198: step 3540, loss = 1.23 (248.0 examples/sec; 0.516 sec/batch)
2016-12-08 07:53:01.417530: step 3550, loss = 1.20 (282.5 examples/sec; 0.453 sec/batch)
2016-12-08 07:53:06.421617: step 3560, loss = 1.13 (234.7 examples/sec; 0.545 sec/batch)
2016-12-08 07:53:11.498534: step 3570, loss = 1.30 (251.9 examples/sec; 0.508 sec/batch)
2016-12-08 07:53:16.815686: step 3580, loss = 1.29 (226.8 examples/sec; 0.564 sec/batch)
2016-12-08 07:53:21.709010: step 3590, loss = 1.08 (278.8 examples/sec; 0.459 sec/batch)
2016-12-08 07:53:26.352713: step 3600, loss = 1.30 (282.5 examples/sec; 0.453 sec/batch)
2016-12-08 07:53:31.724521: step 3610, loss = 1.19 (223.8 examples/sec; 0.572 sec/batch)
2016-12-08 07:53:37.025735: step 3620, loss = 1.07 (212.3 examples/sec; 0.603 sec/batch)
2016-12-08 07:53:41.890698: step 3630, loss = 1.21 (245.0 examples/sec; 0.522 sec/batch)
2016-12-08 07:53:47.359589: step 3640, loss = 0.97 (267.9 examples/sec; 0.478 sec/batch)
2016-12-08 07:53:52.697980: step 3650, loss = 0.98 (230.3 examples/sec; 0.556 sec/batch)
2016-12-08 07:53:57.784876: step 3660, loss = 1.26 (287.9 examples/sec; 0.445 sec/batch)
2016-12-08 07:54:02.825723: step 3670, loss = 1.19 (289.7 examples/sec; 0.442 sec/batch)
2016-12-08 07:54:07.997549: step 3680, loss = 1.18 (227.8 examples/sec; 0.562 sec/batch)
2016-12-08 07:54:13.216067: step 3690, loss = 1.31 (236.7 examples/sec; 0.541 sec/batch)
2016-12-08 07:54:18.287046: step 3700, loss = 1.09 (259.4 examples/sec; 0.493 sec/batch)
2016-12-08 07:54:24.301885: step 3710, loss = 1.17 (240.7 examples/sec; 0.532 sec/batch)
2016-12-08 07:54:29.502533: step 3720, loss = 1.26 (278.9 examples/sec; 0.459 sec/batch)
2016-12-08 07:54:34.845213: step 3730, loss = 1.20 (223.5 examples/sec; 0.573 sec/batch)
2016-12-08 07:54:40.269713: step 3740, loss = 1.19 (263.9 examples/sec; 0.485 sec/batch)
2016-12-08 07:54:45.373981: step 3750, loss = 1.21 (260.1 examples/sec; 0.492 sec/batch)
2016-12-08 07:54:50.754114: step 3760, loss = 1.16 (237.0 examples/sec; 0.540 sec/batch)
2016-12-08 07:54:55.975668: step 3770, loss = 1.15 (221.3 examples/sec; 0.578 sec/batch)
2016-12-08 07:55:00.971238: step 3780, loss = 1.16 (237.8 examples/sec; 0.538 sec/batch)
2016-12-08 07:55:06.455439: step 3790, loss = 1.05 (217.6 examples/sec; 0.588 sec/batch)
2016-12-08 07:55:11.554656: step 3800, loss = 1.00 (261.6 examples/sec; 0.489 sec/batch)
2016-12-08 07:55:17.129453: step 3810, loss = 1.11 (233.7 examples/sec; 0.548 sec/batch)
2016-12-08 07:55:22.341790: step 3820, loss = 1.22 (265.9 examples/sec; 0.481 sec/batch)
2016-12-08 07:55:27.388947: step 3830, loss = 1.24 (266.9 examples/sec; 0.480 sec/batch)
2016-12-08 07:55:32.474120: step 3840, loss = 1.15 (233.5 examples/sec; 0.548 sec/batch)
2016-12-08 07:55:37.896052: step 3850, loss = 0.99 (215.5 examples/sec; 0.594 sec/batch)
2016-12-08 07:55:43.346346: step 3860, loss = 1.13 (240.6 examples/sec; 0.532 sec/batch)
2016-12-08 07:55:48.806451: step 3870, loss = 1.19 (236.6 examples/sec; 0.541 sec/batch)
2016-12-08 07:55:53.971742: step 3880, loss = 1.12 (281.2 examples/sec; 0.455 sec/batch)
2016-12-08 07:55:59.041308: step 3890, loss = 1.19 (244.6 examples/sec; 0.523 sec/batch)
2016-12-08 07:56:04.308180: step 3900, loss = 1.20 (256.8 examples/sec; 0.498 sec/batch)
2016-12-08 07:56:10.120321: step 3910, loss = 1.04 (249.3 examples/sec; 0.513 sec/batch)
2016-12-08 07:56:15.338197: step 3920, loss = 1.22 (262.0 examples/sec; 0.489 sec/batch)
2016-12-08 07:56:20.905226: step 3930, loss = 0.96 (203.0 examples/sec; 0.631 sec/batch)
2016-12-08 07:56:25.893282: step 3940, loss = 1.17 (266.4 examples/sec; 0.481 sec/batch)
2016-12-08 07:56:30.908876: step 3950, loss = 0.97 (238.7 examples/sec; 0.536 sec/batch)
2016-12-08 07:56:36.281497: step 3960, loss = 1.03 (224.2 examples/sec; 0.571 sec/batch)
2016-12-08 07:56:41.488650: step 3970, loss = 1.09 (265.7 examples/sec; 0.482 sec/batch)
2016-12-08 07:56:46.715960: step 3980, loss = 1.18 (247.0 examples/sec; 0.518 sec/batch)
2016-12-08 07:56:51.754251: step 3990, loss = 0.99 (230.4 examples/sec; 0.555 sec/batch)
2016-12-08 07:56:56.885482: step 4000, loss = 1.03 (234.4 examples/sec; 0.546 sec/batch)
2016-12-08 07:57:03.350651: step 4010, loss = 1.01 (254.8 examples/sec; 0.502 sec/batch)
2016-12-08 07:57:08.543217: step 4020, loss = 1.22 (220.6 examples/sec; 0.580 sec/batch)
2016-12-08 07:57:13.913008: step 4030, loss = 0.96 (265.1 examples/sec; 0.483 sec/batch)
2016-12-08 07:57:19.314859: step 4040, loss = 1.13 (243.8 examples/sec; 0.525 sec/batch)
2016-12-08 07:57:24.494696: step 4050, loss = 1.15 (224.5 examples/sec; 0.570 sec/batch)
2016-12-08 07:57:29.886245: step 4060, loss = 1.18 (231.5 examples/sec; 0.553 sec/batch)
2016-12-08 07:57:35.084853: step 4070, loss = 1.11 (292.7 examples/sec; 0.437 sec/batch)
2016-12-08 07:57:40.448051: step 4080, loss = 1.10 (245.1 examples/sec; 0.522 sec/batch)
2016-12-08 07:57:45.778206: step 4090, loss = 1.12 (240.2 examples/sec; 0.533 sec/batch)
2016-12-08 07:57:50.867261: step 4100, loss = 0.98 (276.9 examples/sec; 0.462 sec/batch)
2016-12-08 07:57:56.466700: step 4110, loss = 1.17 (234.0 examples/sec; 0.547 sec/batch)
2016-12-08 07:58:01.460766: step 4120, loss = 1.16 (310.9 examples/sec; 0.412 sec/batch)
2016-12-08 07:58:06.554914: step 4130, loss = 1.10 (245.3 examples/sec; 0.522 sec/batch)
2016-12-08 07:58:12.038443: step 4140, loss = 0.85 (240.5 examples/sec; 0.532 sec/batch)
2016-12-08 07:58:16.955511: step 4150, loss = 1.23 (224.7 examples/sec; 0.570 sec/batch)
2016-12-08 07:58:22.461714: step 4160, loss = 1.13 (231.4 examples/sec; 0.553 sec/batch)
2016-12-08 07:58:27.589396: step 4170, loss = 1.04 (241.3 examples/sec; 0.530 sec/batch)
2016-12-08 07:58:32.948809: step 4180, loss = 1.14 (246.5 examples/sec; 0.519 sec/batch)
2016-12-08 07:58:38.226498: step 4190, loss = 0.98 (233.2 examples/sec; 0.549 sec/batch)
2016-12-08 07:58:43.431447: step 4200, loss = 1.01 (247.6 examples/sec; 0.517 sec/batch)
2016-12-08 07:58:49.155188: step 4210, loss = 0.93 (272.1 examples/sec; 0.470 sec/batch)
2016-12-08 07:58:54.443337: step 4220, loss = 1.09 (296.5 examples/sec; 0.432 sec/batch)
2016-12-08 07:58:59.624359: step 4230, loss = 1.16 (269.8 examples/sec; 0.474 sec/batch)
2016-12-08 07:59:04.694501: step 4240, loss = 0.91 (210.2 examples/sec; 0.609 sec/batch)
2016-12-08 07:59:09.848311: step 4250, loss = 1.25 (247.1 examples/sec; 0.518 sec/batch)
2016-12-08 07:59:14.911220: step 4260, loss = 0.95 (240.5 examples/sec; 0.532 sec/batch)
2016-12-08 07:59:19.866313: step 4270, loss = 1.01 (264.6 examples/sec; 0.484 sec/batch)
2016-12-08 07:59:24.952079: step 4280, loss = 1.16 (294.2 examples/sec; 0.435 sec/batch)
2016-12-08 07:59:29.860513: step 4290, loss = 1.12 (230.5 examples/sec; 0.555 sec/batch)
2016-12-08 07:59:35.172662: step 4300, loss = 1.03 (248.8 examples/sec; 0.514 sec/batch)
2016-12-08 07:59:40.876680: step 4310, loss = 1.17 (279.9 examples/sec; 0.457 sec/batch)
2016-12-08 07:59:45.990976: step 4320, loss = 1.06 (266.0 examples/sec; 0.481 sec/batch)
2016-12-08 07:59:51.189225: step 4330, loss = 1.11 (229.9 examples/sec; 0.557 sec/batch)
2016-12-08 07:59:56.458801: step 4340, loss = 1.06 (250.4 examples/sec; 0.511 sec/batch)
2016-12-08 08:00:01.694440: step 4350, loss = 1.24 (250.3 examples/sec; 0.511 sec/batch)
2016-12-08 08:00:07.016121: step 4360, loss = 1.08 (255.3 examples/sec; 0.501 sec/batch)
2016-12-08 08:00:12.312172: step 4370, loss = 1.02 (215.1 examples/sec; 0.595 sec/batch)
2016-12-08 08:00:17.366050: step 4380, loss = 0.98 (223.0 examples/sec; 0.574 sec/batch)
2016-12-08 08:00:22.635274: step 4390, loss = 1.11 (223.7 examples/sec; 0.572 sec/batch)
2016-12-08 08:00:27.614171: step 4400, loss = 1.23 (277.6 examples/sec; 0.461 sec/batch)
2016-12-08 08:00:33.318288: step 4410, loss = 1.02 (251.1 examples/sec; 0.510 sec/batch)
2016-12-08 08:00:38.544861: step 4420, loss = 1.06 (235.9 examples/sec; 0.543 sec/batch)
2016-12-08 08:00:44.020677: step 4430, loss = 0.91 (256.9 examples/sec; 0.498 sec/batch)
2016-12-08 08:00:49.312724: step 4440, loss = 1.04 (256.9 examples/sec; 0.498 sec/batch)
2016-12-08 08:00:54.665227: step 4450, loss = 1.21 (247.7 examples/sec; 0.517 sec/batch)
2016-12-08 08:00:59.717570: step 4460, loss = 1.05 (291.2 examples/sec; 0.440 sec/batch)
2016-12-08 08:01:04.847215: step 4470, loss = 1.05 (264.1 examples/sec; 0.485 sec/batch)
2016-12-08 08:01:10.248999: step 4480, loss = 1.17 (235.4 examples/sec; 0.544 sec/batch)
2016-12-08 08:01:15.315723: step 4490, loss = 0.98 (268.5 examples/sec; 0.477 sec/batch)
2016-12-08 08:01:20.371044: step 4500, loss = 1.17 (262.8 examples/sec; 0.487 sec/batch)
2016-12-08 08:01:25.985914: step 4510, loss = 1.00 (230.6 examples/sec; 0.555 sec/batch)
2016-12-08 08:01:31.179179: step 4520, loss = 1.09 (259.9 examples/sec; 0.493 sec/batch)
2016-12-08 08:01:36.427810: step 4530, loss = 1.00 (229.9 examples/sec; 0.557 sec/batch)
2016-12-08 08:01:41.770914: step 4540, loss = 1.12 (274.5 examples/sec; 0.466 sec/batch)
2016-12-08 08:01:46.723012: step 4550, loss = 1.13 (251.9 examples/sec; 0.508 sec/batch)
2016-12-08 08:01:51.677164: step 4560, loss = 1.09 (308.3 examples/sec; 0.415 sec/batch)
2016-12-08 08:01:56.869352: step 4570, loss = 1.05 (241.2 examples/sec; 0.531 sec/batch)
2016-12-08 08:02:01.629024: step 4580, loss = 1.04 (242.2 examples/sec; 0.529 sec/batch)
2016-12-08 08:02:06.773823: step 4590, loss = 1.20 (239.6 examples/sec; 0.534 sec/batch)
2016-12-08 08:02:12.030133: step 4600, loss = 1.04 (230.7 examples/sec; 0.555 sec/batch)
2016-12-08 08:02:17.969499: step 4610, loss = 0.97 (249.8 examples/sec; 0.513 sec/batch)
2016-12-08 08:02:23.162900: step 4620, loss = 0.94 (229.8 examples/sec; 0.557 sec/batch)
2016-12-08 08:02:28.563826: step 4630, loss = 1.06 (239.4 examples/sec; 0.535 sec/batch)
2016-12-08 08:02:33.876220: step 4640, loss = 1.13 (271.5 examples/sec; 0.471 sec/batch)
2016-12-08 08:02:39.166627: step 4650, loss = 1.07 (215.5 examples/sec; 0.594 sec/batch)
2016-12-08 08:02:44.330990: step 4660, loss = 1.20 (256.1 examples/sec; 0.500 sec/batch)
2016-12-08 08:02:49.374449: step 4670, loss = 1.07 (274.4 examples/sec; 0.466 sec/batch)
2016-12-08 08:02:54.509287: step 4680, loss = 1.24 (297.7 examples/sec; 0.430 sec/batch)
2016-12-08 08:02:59.902869: step 4690, loss = 1.14 (224.4 examples/sec; 0.570 sec/batch)
2016-12-08 08:03:05.141057: step 4700, loss = 1.15 (263.7 examples/sec; 0.485 sec/batch)
2016-12-08 08:03:10.800493: step 4710, loss = 1.09 (234.0 examples/sec; 0.547 sec/batch)
2016-12-08 08:03:16.126313: step 4720, loss = 0.93 (227.3 examples/sec; 0.563 sec/batch)
2016-12-08 08:03:21.613954: step 4730, loss = 1.07 (266.3 examples/sec; 0.481 sec/batch)
2016-12-08 08:03:26.996932: step 4740, loss = 0.87 (233.6 examples/sec; 0.548 sec/batch)
2016-12-08 08:03:32.491399: step 4750, loss = 1.03 (244.0 examples/sec; 0.525 sec/batch)
2016-12-08 08:03:37.467994: step 4760, loss = 1.03 (248.9 examples/sec; 0.514 sec/batch)
2016-12-08 08:03:42.637010: step 4770, loss = 1.11 (248.8 examples/sec; 0.514 sec/batch)
2016-12-08 08:03:47.999831: step 4780, loss = 1.19 (238.7 examples/sec; 0.536 sec/batch)
2016-12-08 08:03:53.498652: step 4790, loss = 1.05 (209.1 examples/sec; 0.612 sec/batch)
2016-12-08 08:03:58.943834: step 4800, loss = 1.00 (250.0 examples/sec; 0.512 sec/batch)
2016-12-08 08:04:04.878302: step 4810, loss = 0.90 (230.6 examples/sec; 0.555 sec/batch)
2016-12-08 08:04:10.151005: step 4820, loss = 1.09 (242.1 examples/sec; 0.529 sec/batch)
2016-12-08 08:04:15.716057: step 4830, loss = 0.99 (228.6 examples/sec; 0.560 sec/batch)
2016-12-08 08:04:21.241151: step 4840, loss = 1.05 (257.3 examples/sec; 0.498 sec/batch)
2016-12-08 08:04:26.404668: step 4850, loss = 1.11 (255.6 examples/sec; 0.501 sec/batch)
2016-12-08 08:04:31.713098: step 4860, loss = 1.04 (243.5 examples/sec; 0.526 sec/batch)
2016-12-08 08:04:37.124072: step 4870, loss = 0.90 (266.1 examples/sec; 0.481 sec/batch)
2016-12-08 08:04:42.520500: step 4880, loss = 0.97 (228.6 examples/sec; 0.560 sec/batch)
2016-12-08 08:04:47.703463: step 4890, loss = 1.03 (235.4 examples/sec; 0.544 sec/batch)
2016-12-08 08:04:52.898996: step 4900, loss = 1.18 (278.0 examples/sec; 0.460 sec/batch)
2016-12-08 08:04:58.207794: step 4910, loss = 0.94 (253.8 examples/sec; 0.504 sec/batch)
2016-12-08 08:05:03.314053: step 4920, loss = 0.84 (243.4 examples/sec; 0.526 sec/batch)
2016-12-08 08:05:08.502914: step 4930, loss = 1.09 (242.0 examples/sec; 0.529 sec/batch)
2016-12-08 08:05:13.562848: step 4940, loss = 1.04 (276.9 examples/sec; 0.462 sec/batch)
2016-12-08 08:05:18.667186: step 4950, loss = 0.87 (248.0 examples/sec; 0.516 sec/batch)
2016-12-08 08:05:23.975409: step 4960, loss = 1.27 (211.6 examples/sec; 0.605 sec/batch)
2016-12-08 08:05:29.552511: step 4970, loss = 1.17 (208.6 examples/sec; 0.614 sec/batch)
2016-12-08 08:05:34.824920: step 4980, loss = 1.02 (229.9 examples/sec; 0.557 sec/batch)
2016-12-08 08:05:40.299237: step 4990, loss = 0.95 (223.1 examples/sec; 0.574 sec/batch)
2016-12-08 08:05:45.482879: step 5000, loss = 1.20 (235.1 examples/sec; 0.544 sec/batch)
2016-12-08 08:05:51.805250: step 5010, loss = 0.93 (238.6 examples/sec; 0.537 sec/batch)
2016-12-08 08:05:56.948962: step 5020, loss = 1.08 (241.0 examples/sec; 0.531 sec/batch)
2016-12-08 08:06:01.898639: step 5030, loss = 0.98 (233.3 examples/sec; 0.549 sec/batch)
2016-12-08 08:06:06.889897: step 5040, loss = 1.09 (222.8 examples/sec; 0.574 sec/batch)
2016-12-08 08:06:12.187326: step 5050, loss = 0.94 (276.7 examples/sec; 0.463 sec/batch)
2016-12-08 08:06:17.337645: step 5060, loss = 1.20 (211.5 examples/sec; 0.605 sec/batch)
2016-12-08 08:06:22.557689: step 5070, loss = 0.89 (243.4 examples/sec; 0.526 sec/batch)
2016-12-08 08:06:27.812643: step 5080, loss = 1.06 (298.3 examples/sec; 0.429 sec/batch)
2016-12-08 08:06:33.046213: step 5090, loss = 1.08 (249.6 examples/sec; 0.513 sec/batch)
2016-12-08 08:06:38.221390: step 5100, loss = 1.03 (208.2 examples/sec; 0.615 sec/batch)
2016-12-08 08:06:43.999961: step 5110, loss = 0.92 (232.7 examples/sec; 0.550 sec/batch)
2016-12-08 08:06:49.218628: step 5120, loss = 0.92 (259.0 examples/sec; 0.494 sec/batch)
2016-12-08 08:06:54.514783: step 5130, loss = 0.78 (256.9 examples/sec; 0.498 sec/batch)
2016-12-08 08:06:59.808288: step 5140, loss = 1.05 (259.2 examples/sec; 0.494 sec/batch)
2016-12-08 08:07:05.189538: step 5150, loss = 1.09 (240.3 examples/sec; 0.533 sec/batch)
2016-12-08 08:07:10.412374: step 5160, loss = 0.90 (219.8 examples/sec; 0.582 sec/batch)
2016-12-08 08:07:15.709219: step 5170, loss = 0.97 (236.2 examples/sec; 0.542 sec/batch)
2016-12-08 08:07:21.004574: step 5180, loss = 0.98 (263.4 examples/sec; 0.486 sec/batch)
2016-12-08 08:07:26.426666: step 5190, loss = 0.94 (251.0 examples/sec; 0.510 sec/batch)
2016-12-08 08:07:31.884997: step 5200, loss = 1.13 (216.7 examples/sec; 0.591 sec/batch)
2016-12-08 08:07:37.958315: step 5210, loss = 0.83 (215.8 examples/sec; 0.593 sec/batch)
2016-12-08 08:07:43.296010: step 5220, loss = 1.15 (250.4 examples/sec; 0.511 sec/batch)
2016-12-08 08:07:48.528091: step 5230, loss = 0.88 (228.8 examples/sec; 0.560 sec/batch)
2016-12-08 08:07:53.921657: step 5240, loss = 1.04 (245.6 examples/sec; 0.521 sec/batch)
2016-12-08 08:07:59.181575: step 5250, loss = 0.89 (247.4 examples/sec; 0.517 sec/batch)
2016-12-08 08:08:04.464094: step 5260, loss = 1.15 (245.1 examples/sec; 0.522 sec/batch)
2016-12-08 08:08:09.827530: step 5270, loss = 1.07 (226.5 examples/sec; 0.565 sec/batch)
2016-12-08 08:08:15.018962: step 5280, loss = 0.94 (250.0 examples/sec; 0.512 sec/batch)
2016-12-08 08:08:20.449974: step 5290, loss = 0.93 (243.9 examples/sec; 0.525 sec/batch)
2016-12-08 08:08:25.609864: step 5300, loss = 0.97 (264.1 examples/sec; 0.485 sec/batch)
2016-12-08 08:08:31.608989: step 5310, loss = 0.90 (211.8 examples/sec; 0.604 sec/batch)
2016-12-08 08:08:37.130836: step 5320, loss = 0.95 (215.6 examples/sec; 0.594 sec/batch)
2016-12-08 08:08:42.326302: step 5330, loss = 1.03 (292.8 examples/sec; 0.437 sec/batch)
2016-12-08 08:08:47.701274: step 5340, loss = 0.96 (226.4 examples/sec; 0.565 sec/batch)
2016-12-08 08:08:53.025044: step 5350, loss = 0.82 (242.4 examples/sec; 0.528 sec/batch)
2016-12-08 08:08:57.877822: step 5360, loss = 0.93 (232.4 examples/sec; 0.551 sec/batch)
2016-12-08 08:09:03.073337: step 5370, loss = 1.22 (309.6 examples/sec; 0.413 sec/batch)
2016-12-08 08:09:08.068676: step 5380, loss = 1.08 (236.9 examples/sec; 0.540 sec/batch)
2016-12-08 08:09:13.312826: step 5390, loss = 0.89 (235.4 examples/sec; 0.544 sec/batch)
2016-12-08 08:09:18.588712: step 5400, loss = 1.17 (246.9 examples/sec; 0.518 sec/batch)
2016-12-08 08:09:24.539483: step 5410, loss = 0.96 (261.5 examples/sec; 0.489 sec/batch)
2016-12-08 08:09:29.569831: step 5420, loss = 1.00 (235.0 examples/sec; 0.545 sec/batch)
2016-12-08 08:09:34.857945: step 5430, loss = 1.06 (212.7 examples/sec; 0.602 sec/batch)
2016-12-08 08:09:40.224385: step 5440, loss = 0.85 (205.7 examples/sec; 0.622 sec/batch)
2016-12-08 08:09:45.710578: step 5450, loss = 0.98 (249.0 examples/sec; 0.514 sec/batch)
2016-12-08 08:09:51.062365: step 5460, loss = 1.06 (240.1 examples/sec; 0.533 sec/batch)
2016-12-08 08:09:55.998704: step 5470, loss = 0.98 (242.2 examples/sec; 0.529 sec/batch)
2016-12-08 08:10:01.132349: step 5480, loss = 0.84 (244.4 examples/sec; 0.524 sec/batch)
2016-12-08 08:10:06.279296: step 5490, loss = 1.07 (313.2 examples/sec; 0.409 sec/batch)
2016-12-08 08:10:11.454723: step 5500, loss = 1.01 (228.5 examples/sec; 0.560 sec/batch)
2016-12-08 08:10:16.603275: step 5510, loss = 0.91 (302.2 examples/sec; 0.423 sec/batch)
2016-12-08 08:10:21.933627: step 5520, loss = 0.92 (261.4 examples/sec; 0.490 sec/batch)
2016-12-08 08:10:27.336070: step 5530, loss = 0.94 (236.4 examples/sec; 0.542 sec/batch)
2016-12-08 08:10:32.346251: step 5540, loss = 0.97 (239.0 examples/sec; 0.536 sec/batch)
2016-12-08 08:10:37.765550: step 5550, loss = 1.02 (231.6 examples/sec; 0.553 sec/batch)
2016-12-08 08:10:43.072388: step 5560, loss = 0.93 (254.0 examples/sec; 0.504 sec/batch)
2016-12-08 08:10:48.160672: step 5570, loss = 0.90 (222.7 examples/sec; 0.575 sec/batch)
2016-12-08 08:10:53.192630: step 5580, loss = 1.04 (224.2 examples/sec; 0.571 sec/batch)
2016-12-08 08:10:58.099427: step 5590, loss = 0.89 (239.6 examples/sec; 0.534 sec/batch)
2016-12-08 08:11:03.589594: step 5600, loss = 0.95 (226.2 examples/sec; 0.566 sec/batch)
2016-12-08 08:11:08.855424: step 5610, loss = 1.12 (235.4 examples/sec; 0.544 sec/batch)
2016-12-08 08:11:14.105183: step 5620, loss = 1.06 (245.0 examples/sec; 0.522 sec/batch)
2016-12-08 08:11:19.554866: step 5630, loss = 1.27 (220.4 examples/sec; 0.581 sec/batch)
2016-12-08 08:11:24.981794: step 5640, loss = 0.92 (246.0 examples/sec; 0.520 sec/batch)
2016-12-08 08:11:30.506283: step 5650, loss = 0.90 (232.4 examples/sec; 0.551 sec/batch)
2016-12-08 08:11:36.018338: step 5660, loss = 0.98 (274.0 examples/sec; 0.467 sec/batch)
2016-12-08 08:11:41.375044: step 5670, loss = 0.93 (229.3 examples/sec; 0.558 sec/batch)
2016-12-08 08:11:46.961092: step 5680, loss = 0.99 (236.3 examples/sec; 0.542 sec/batch)
2016-12-08 08:11:51.997570: step 5690, loss = 1.18 (224.0 examples/sec; 0.571 sec/batch)
2016-12-08 08:11:57.182888: step 5700, loss = 0.88 (245.9 examples/sec; 0.521 sec/batch)
2016-12-08 08:12:02.742614: step 5710, loss = 0.93 (242.1 examples/sec; 0.529 sec/batch)
2016-12-08 08:12:07.755174: step 5720, loss = 1.02 (270.8 examples/sec; 0.473 sec/batch)
2016-12-08 08:12:13.326040: step 5730, loss = 1.05 (229.8 examples/sec; 0.557 sec/batch)
2016-12-08 08:12:18.439306: step 5740, loss = 0.92 (250.6 examples/sec; 0.511 sec/batch)
2016-12-08 08:12:23.254492: step 5750, loss = 1.02 (294.5 examples/sec; 0.435 sec/batch)
2016-12-08 08:12:28.255305: step 5760, loss = 0.80 (296.0 examples/sec; 0.432 sec/batch)
2016-12-08 08:12:33.577165: step 5770, loss = 0.98 (265.6 examples/sec; 0.482 sec/batch)
2016-12-08 08:12:38.792691: step 5780, loss = 1.00 (253.4 examples/sec; 0.505 sec/batch)
2016-12-08 08:12:43.956367: step 5790, loss = 1.00 (236.9 examples/sec; 0.540 sec/batch)
2016-12-08 08:12:49.128622: step 5800, loss = 1.10 (245.9 examples/sec; 0.521 sec/batch)
2016-12-08 08:12:54.897742: step 5810, loss = 1.04 (258.5 examples/sec; 0.495 sec/batch)
2016-12-08 08:12:59.923901: step 5820, loss = 0.98 (237.9 examples/sec; 0.538 sec/batch)
2016-12-08 08:13:05.351064: step 5830, loss = 1.09 (273.7 examples/sec; 0.468 sec/batch)
2016-12-08 08:13:10.737021: step 5840, loss = 0.97 (245.1 examples/sec; 0.522 sec/batch)
2016-12-08 08:13:15.578318: step 5850, loss = 0.98 (300.9 examples/sec; 0.425 sec/batch)
2016-12-08 08:13:20.311308: step 5860, loss = 1.06 (246.3 examples/sec; 0.520 sec/batch)
2016-12-08 08:13:25.324407: step 5870, loss = 1.05 (276.6 examples/sec; 0.463 sec/batch)
2016-12-08 08:13:30.344646: step 5880, loss = 0.86 (279.3 examples/sec; 0.458 sec/batch)
2016-12-08 08:13:35.431290: step 5890, loss = 0.97 (301.2 examples/sec; 0.425 sec/batch)
2016-12-08 08:13:40.432029: step 5900, loss = 0.90 (300.1 examples/sec; 0.427 sec/batch)
2016-12-08 08:13:46.019445: step 5910, loss = 0.94 (261.5 examples/sec; 0.489 sec/batch)
2016-12-08 08:13:50.924610: step 5920, loss = 0.92 (234.9 examples/sec; 0.545 sec/batch)
2016-12-08 08:13:56.330629: step 5930, loss = 1.10 (233.1 examples/sec; 0.549 sec/batch)
2016-12-08 08:14:01.704872: step 5940, loss = 0.95 (249.9 examples/sec; 0.512 sec/batch)
2016-12-08 08:14:06.991157: step 5950, loss = 0.77 (219.8 examples/sec; 0.582 sec/batch)
2016-12-08 08:14:12.150872: step 5960, loss = 1.00 (275.1 examples/sec; 0.465 sec/batch)
2016-12-08 08:14:17.344767: step 5970, loss = 1.05 (262.7 examples/sec; 0.487 sec/batch)
2016-12-08 08:14:22.671206: step 5980, loss = 0.97 (250.4 examples/sec; 0.511 sec/batch)
2016-12-08 08:14:28.049199: step 5990, loss = 0.92 (249.8 examples/sec; 0.512 sec/batch)
2016-12-08 08:14:33.061883: step 6000, loss = 1.09 (264.7 examples/sec; 0.484 sec/batch)
2016-12-08 08:14:39.250374: step 6010, loss = 1.00 (234.9 examples/sec; 0.545 sec/batch)
2016-12-08 08:14:44.788233: step 6020, loss = 0.91 (237.3 examples/sec; 0.539 sec/batch)
2016-12-08 08:14:49.903198: step 6030, loss = 0.78 (239.7 examples/sec; 0.534 sec/batch)
2016-12-08 08:14:55.126588: step 6040, loss = 0.94 (229.9 examples/sec; 0.557 sec/batch)
2016-12-08 08:15:00.234579: step 6050, loss = 1.01 (224.0 examples/sec; 0.572 sec/batch)
2016-12-08 08:15:05.271503: step 6060, loss = 0.87 (248.9 examples/sec; 0.514 sec/batch)
2016-12-08 08:15:10.606890: step 6070, loss = 0.83 (218.3 examples/sec; 0.586 sec/batch)
2016-12-08 08:15:15.896297: step 6080, loss = 1.09 (229.7 examples/sec; 0.557 sec/batch)
2016-12-08 08:15:21.181717: step 6090, loss = 0.92 (258.5 examples/sec; 0.495 sec/batch)
2016-12-08 08:15:26.287068: step 6100, loss = 0.93 (310.3 examples/sec; 0.412 sec/batch)
2016-12-08 08:15:31.802807: step 6110, loss = 1.06 (239.1 examples/sec; 0.535 sec/batch)
2016-12-08 08:15:37.119990: step 6120, loss = 1.09 (211.4 examples/sec; 0.606 sec/batch)
2016-12-08 08:15:42.120936: step 6130, loss = 1.05 (233.2 examples/sec; 0.549 sec/batch)
2016-12-08 08:15:47.324976: step 6140, loss = 0.90 (232.7 examples/sec; 0.550 sec/batch)
2016-12-08 08:15:52.372310: step 6150, loss = 0.81 (252.2 examples/sec; 0.508 sec/batch)
2016-12-08 08:15:57.667355: step 6160, loss = 1.09 (274.6 examples/sec; 0.466 sec/batch)
2016-12-08 08:16:03.047376: step 6170, loss = 0.87 (238.3 examples/sec; 0.537 sec/batch)
2016-12-08 08:16:07.956379: step 6180, loss = 0.89 (287.5 examples/sec; 0.445 sec/batch)
2016-12-08 08:16:13.355738: step 6190, loss = 0.88 (268.0 examples/sec; 0.478 sec/batch)
2016-12-08 08:16:18.425990: step 6200, loss = 0.98 (273.1 examples/sec; 0.469 sec/batch)
2016-12-08 08:16:24.465114: step 6210, loss = 0.96 (230.2 examples/sec; 0.556 sec/batch)
2016-12-08 08:16:29.872414: step 6220, loss = 0.97 (229.2 examples/sec; 0.558 sec/batch)
2016-12-08 08:16:35.159899: step 6230, loss = 0.88 (233.4 examples/sec; 0.548 sec/batch)
2016-12-08 08:16:40.545899: step 6240, loss = 0.79 (245.3 examples/sec; 0.522 sec/batch)
2016-12-08 08:16:46.008359: step 6250, loss = 0.95 (236.9 examples/sec; 0.540 sec/batch)
2016-12-08 08:16:51.255005: step 6260, loss = 1.08 (220.1 examples/sec; 0.581 sec/batch)
2016-12-08 08:16:56.521274: step 6270, loss = 1.02 (242.5 examples/sec; 0.528 sec/batch)
2016-12-08 08:17:01.738454: step 6280, loss = 1.02 (228.1 examples/sec; 0.561 sec/batch)
2016-12-08 08:17:06.862309: step 6290, loss = 0.95 (255.5 examples/sec; 0.501 sec/batch)
2016-12-08 08:17:12.029353: step 6300, loss = 1.01 (288.0 examples/sec; 0.445 sec/batch)
2016-12-08 08:17:17.692388: step 6310, loss = 0.72 (252.5 examples/sec; 0.507 sec/batch)
2016-12-08 08:17:23.217764: step 6320, loss = 1.02 (234.6 examples/sec; 0.546 sec/batch)
2016-12-08 08:17:28.286649: step 6330, loss = 0.95 (242.1 examples/sec; 0.529 sec/batch)
2016-12-08 08:17:33.837741: step 6340, loss = 0.77 (237.2 examples/sec; 0.540 sec/batch)
2016-12-08 08:17:38.971971: step 6350, loss = 0.87 (239.1 examples/sec; 0.535 sec/batch)
2016-12-08 08:17:44.193336: step 6360, loss = 0.96 (225.0 examples/sec; 0.569 sec/batch)
2016-12-08 08:17:49.347458: step 6370, loss = 1.01 (243.7 examples/sec; 0.525 sec/batch)
2016-12-08 08:17:54.408233: step 6380, loss = 0.96 (222.9 examples/sec; 0.574 sec/batch)
2016-12-08 08:17:59.830306: step 6390, loss = 0.87 (238.0 examples/sec; 0.538 sec/batch)
2016-12-08 08:18:04.715085: step 6400, loss = 0.95 (280.6 examples/sec; 0.456 sec/batch)
2016-12-08 08:18:10.454459: step 6410, loss = 0.89 (254.0 examples/sec; 0.504 sec/batch)
2016-12-08 08:18:15.788072: step 6420, loss = 0.88 (247.8 examples/sec; 0.517 sec/batch)
2016-12-08 08:18:20.797201: step 6430, loss = 0.88 (269.1 examples/sec; 0.476 sec/batch)
2016-12-08 08:18:25.870801: step 6440, loss = 0.76 (304.3 examples/sec; 0.421 sec/batch)
2016-12-08 08:18:31.071802: step 6450, loss = 0.94 (250.8 examples/sec; 0.510 sec/batch)
2016-12-08 08:18:36.260061: step 6460, loss = 0.91 (257.6 examples/sec; 0.497 sec/batch)
2016-12-08 08:18:41.301523: step 6470, loss = 0.82 (272.1 examples/sec; 0.470 sec/batch)
2016-12-08 08:18:46.181540: step 6480, loss = 1.21 (246.3 examples/sec; 0.520 sec/batch)
2016-12-08 08:18:51.277448: step 6490, loss = 0.87 (281.8 examples/sec; 0.454 sec/batch)
2016-12-08 08:18:56.469787: step 6500, loss = 1.03 (261.3 examples/sec; 0.490 sec/batch)
2016-12-08 08:19:02.436699: step 6510, loss = 0.90 (251.5 examples/sec; 0.509 sec/batch)
2016-12-08 08:19:07.575595: step 6520, loss = 0.93 (288.5 examples/sec; 0.444 sec/batch)
2016-12-08 08:19:12.491907: step 6530, loss = 1.13 (270.5 examples/sec; 0.473 sec/batch)
2016-12-08 08:19:17.843848: step 6540, loss = 1.03 (239.7 examples/sec; 0.534 sec/batch)
2016-12-08 08:19:22.934306: step 6550, loss = 0.83 (246.4 examples/sec; 0.519 sec/batch)
2016-12-08 08:19:28.035354: step 6560, loss = 0.90 (231.1 examples/sec; 0.554 sec/batch)
2016-12-08 08:19:33.351719: step 6570, loss = 0.89 (243.0 examples/sec; 0.527 sec/batch)
2016-12-08 08:19:38.190633: step 6580, loss = 0.84 (234.8 examples/sec; 0.545 sec/batch)
2016-12-08 08:19:43.239717: step 6590, loss = 0.89 (241.8 examples/sec; 0.529 sec/batch)
2016-12-08 08:19:48.162388: step 6600, loss = 0.91 (225.7 examples/sec; 0.567 sec/batch)
2016-12-08 08:19:53.990185: step 6610, loss = 0.81 (232.6 examples/sec; 0.550 sec/batch)
2016-12-08 08:19:59.409267: step 6620, loss = 0.95 (241.0 examples/sec; 0.531 sec/batch)
2016-12-08 08:20:04.652511: step 6630, loss = 0.87 (275.8 examples/sec; 0.464 sec/batch)
2016-12-08 08:20:10.028261: step 6640, loss = 0.87 (267.8 examples/sec; 0.478 sec/batch)
2016-12-08 08:20:15.382829: step 6650, loss = 0.90 (250.1 examples/sec; 0.512 sec/batch)
2016-12-08 08:20:20.830808: step 6660, loss = 1.09 (203.1 examples/sec; 0.630 sec/batch)
2016-12-08 08:20:25.656718: step 6670, loss = 1.07 (236.8 examples/sec; 0.541 sec/batch)
2016-12-08 08:20:31.008419: step 6680, loss = 0.92 (246.7 examples/sec; 0.519 sec/batch)
2016-12-08 08:20:36.189748: step 6690, loss = 0.95 (258.5 examples/sec; 0.495 sec/batch)
2016-12-08 08:20:41.670837: step 6700, loss = 1.07 (243.2 examples/sec; 0.526 sec/batch)
2016-12-08 08:20:47.239319: step 6710, loss = 0.80 (275.0 examples/sec; 0.466 sec/batch)
2016-12-08 08:20:52.661804: step 6720, loss = 0.92 (239.5 examples/sec; 0.534 sec/batch)
2016-12-08 08:20:57.830695: step 6730, loss = 0.85 (271.8 examples/sec; 0.471 sec/batch)
2016-12-08 08:21:02.811461: step 6740, loss = 0.93 (272.3 examples/sec; 0.470 sec/batch)
2016-12-08 08:21:08.263535: step 6750, loss = 1.00 (217.7 examples/sec; 0.588 sec/batch)
2016-12-08 08:21:13.527076: step 6760, loss = 0.93 (244.3 examples/sec; 0.524 sec/batch)
