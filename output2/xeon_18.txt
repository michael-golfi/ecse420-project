Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
WARNING:tensorflow:From cifar10_train.py:81 in train.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2016-03-02.
Instructions for updating:
Please use tf.global_variables instead.
2016-12-08 17:36:46.845608: step 0, loss = 4.68 (27.7 examples/sec; 4.615 sec/batch)
2016-12-08 17:36:49.235428: step 10, loss = 4.62 (926.5 examples/sec; 0.138 sec/batch)
2016-12-08 17:36:50.603910: step 20, loss = 4.46 (937.9 examples/sec; 0.136 sec/batch)
2016-12-08 17:36:51.918503: step 30, loss = 4.46 (1004.7 examples/sec; 0.127 sec/batch)
2016-12-08 17:36:53.300293: step 40, loss = 4.44 (906.4 examples/sec; 0.141 sec/batch)
2016-12-08 17:36:54.688661: step 50, loss = 4.29 (753.3 examples/sec; 0.170 sec/batch)
2016-12-08 17:36:56.065508: step 60, loss = 4.17 (901.1 examples/sec; 0.142 sec/batch)
2016-12-08 17:36:57.543550: step 70, loss = 4.10 (787.6 examples/sec; 0.163 sec/batch)
2016-12-08 17:36:59.007151: step 80, loss = 4.25 (967.0 examples/sec; 0.132 sec/batch)
2016-12-08 17:37:00.362810: step 90, loss = 4.10 (962.6 examples/sec; 0.133 sec/batch)
2016-12-08 17:37:01.710932: step 100, loss = 4.45 (953.5 examples/sec; 0.134 sec/batch)
2016-12-08 17:37:03.352556: step 110, loss = 4.12 (898.2 examples/sec; 0.142 sec/batch)
2016-12-08 17:37:04.695259: step 120, loss = 3.97 (989.5 examples/sec; 0.129 sec/batch)
2016-12-08 17:37:06.058291: step 130, loss = 3.97 (968.9 examples/sec; 0.132 sec/batch)
2016-12-08 17:37:07.501758: step 140, loss = 3.99 (942.1 examples/sec; 0.136 sec/batch)
2016-12-08 17:37:08.867673: step 150, loss = 3.92 (964.8 examples/sec; 0.133 sec/batch)
2016-12-08 17:37:10.231995: step 160, loss = 3.85 (933.8 examples/sec; 0.137 sec/batch)
2016-12-08 17:37:11.620928: step 170, loss = 3.94 (951.5 examples/sec; 0.135 sec/batch)
2016-12-08 17:37:13.025064: step 180, loss = 3.88 (953.1 examples/sec; 0.134 sec/batch)
2016-12-08 17:37:14.401500: step 190, loss = 3.83 (964.2 examples/sec; 0.133 sec/batch)
2016-12-08 17:37:15.791079: step 200, loss = 3.75 (900.7 examples/sec; 0.142 sec/batch)
2016-12-08 17:37:17.346936: step 210, loss = 3.69 (905.5 examples/sec; 0.141 sec/batch)
2016-12-08 17:37:18.740753: step 220, loss = 3.77 (944.1 examples/sec; 0.136 sec/batch)
2016-12-08 17:37:20.243235: step 230, loss = 3.76 (805.0 examples/sec; 0.159 sec/batch)
2016-12-08 17:37:21.575676: step 240, loss = 3.78 (932.9 examples/sec; 0.137 sec/batch)
2016-12-08 17:37:22.949287: step 250, loss = 3.82 (942.7 examples/sec; 0.136 sec/batch)
2016-12-08 17:37:24.286898: step 260, loss = 3.78 (931.8 examples/sec; 0.137 sec/batch)
2016-12-08 17:37:25.652206: step 270, loss = 3.61 (992.2 examples/sec; 0.129 sec/batch)
2016-12-08 17:37:27.023650: step 280, loss = 3.88 (870.5 examples/sec; 0.147 sec/batch)
2016-12-08 17:37:28.399446: step 290, loss = 3.64 (1008.3 examples/sec; 0.127 sec/batch)
2016-12-08 17:37:29.749159: step 300, loss = 3.58 (841.6 examples/sec; 0.152 sec/batch)
2016-12-08 17:37:31.317269: step 310, loss = 3.65 (922.5 examples/sec; 0.139 sec/batch)
2016-12-08 17:37:32.678035: step 320, loss = 3.45 (983.9 examples/sec; 0.130 sec/batch)
2016-12-08 17:37:34.018637: step 330, loss = 3.38 (1011.2 examples/sec; 0.127 sec/batch)
2016-12-08 17:37:35.364561: step 340, loss = 3.43 (907.9 examples/sec; 0.141 sec/batch)
2016-12-08 17:37:36.727569: step 350, loss = 3.49 (985.4 examples/sec; 0.130 sec/batch)
2016-12-08 17:37:38.074517: step 360, loss = 3.38 (891.6 examples/sec; 0.144 sec/batch)
2016-12-08 17:37:39.471661: step 370, loss = 3.52 (873.2 examples/sec; 0.147 sec/batch)
2016-12-08 17:37:40.898222: step 380, loss = 3.38 (837.2 examples/sec; 0.153 sec/batch)
2016-12-08 17:37:42.276257: step 390, loss = 3.38 (941.3 examples/sec; 0.136 sec/batch)
2016-12-08 17:37:43.653720: step 400, loss = 3.24 (958.2 examples/sec; 0.134 sec/batch)
2016-12-08 17:37:45.205221: step 410, loss = 3.26 (956.8 examples/sec; 0.134 sec/batch)
2016-12-08 17:37:46.575574: step 420, loss = 3.22 (987.6 examples/sec; 0.130 sec/batch)
2016-12-08 17:37:47.923007: step 430, loss = 3.41 (970.2 examples/sec; 0.132 sec/batch)
2016-12-08 17:37:49.258445: step 440, loss = 3.32 (924.7 examples/sec; 0.138 sec/batch)
2016-12-08 17:37:50.617981: step 450, loss = 3.19 (928.1 examples/sec; 0.138 sec/batch)
2016-12-08 17:37:52.015492: step 460, loss = 3.42 (958.7 examples/sec; 0.134 sec/batch)
2016-12-08 17:37:53.407579: step 470, loss = 3.50 (920.3 examples/sec; 0.139 sec/batch)
2016-12-08 17:37:54.760901: step 480, loss = 3.04 (939.8 examples/sec; 0.136 sec/batch)
2016-12-08 17:37:56.151446: step 490, loss = 3.15 (889.8 examples/sec; 0.144 sec/batch)
2016-12-08 17:37:57.457787: step 500, loss = 3.13 (921.7 examples/sec; 0.139 sec/batch)
2016-12-08 17:37:58.961111: step 510, loss = 3.00 (968.5 examples/sec; 0.132 sec/batch)
2016-12-08 17:38:00.331292: step 520, loss = 3.00 (893.0 examples/sec; 0.143 sec/batch)
2016-12-08 17:38:01.661835: step 530, loss = 2.98 (994.1 examples/sec; 0.129 sec/batch)
2016-12-08 17:38:03.036238: step 540, loss = 3.35 (999.0 examples/sec; 0.128 sec/batch)
2016-12-08 17:38:04.437789: step 550, loss = 3.04 (961.5 examples/sec; 0.133 sec/batch)
2016-12-08 17:38:05.801863: step 560, loss = 3.07 (958.2 examples/sec; 0.134 sec/batch)
2016-12-08 17:38:07.147975: step 570, loss = 3.05 (1013.9 examples/sec; 0.126 sec/batch)
2016-12-08 17:38:08.484997: step 580, loss = 3.17 (947.2 examples/sec; 0.135 sec/batch)
2016-12-08 17:38:09.882260: step 590, loss = 3.23 (955.0 examples/sec; 0.134 sec/batch)
2016-12-08 17:38:11.229204: step 600, loss = 3.01 (997.8 examples/sec; 0.128 sec/batch)
2016-12-08 17:38:12.870338: step 610, loss = 2.93 (1018.0 examples/sec; 0.126 sec/batch)
2016-12-08 17:38:14.174640: step 620, loss = 2.95 (910.9 examples/sec; 0.141 sec/batch)
2016-12-08 17:38:15.558313: step 630, loss = 3.01 (947.2 examples/sec; 0.135 sec/batch)
2016-12-08 17:38:16.912595: step 640, loss = 2.79 (963.1 examples/sec; 0.133 sec/batch)
2016-12-08 17:38:18.263269: step 650, loss = 3.04 (949.1 examples/sec; 0.135 sec/batch)
2016-12-08 17:38:19.618809: step 660, loss = 2.88 (945.3 examples/sec; 0.135 sec/batch)
2016-12-08 17:38:20.979976: step 670, loss = 2.90 (922.3 examples/sec; 0.139 sec/batch)
2016-12-08 17:38:22.406499: step 680, loss = 2.91 (810.4 examples/sec; 0.158 sec/batch)
2016-12-08 17:38:23.776664: step 690, loss = 2.74 (930.0 examples/sec; 0.138 sec/batch)
2016-12-08 17:38:25.120604: step 700, loss = 2.80 (978.9 examples/sec; 0.131 sec/batch)
2016-12-08 17:38:26.667779: step 710, loss = 2.74 (979.0 examples/sec; 0.131 sec/batch)
2016-12-08 17:38:27.979893: step 720, loss = 2.67 (963.5 examples/sec; 0.133 sec/batch)
2016-12-08 17:38:29.312188: step 730, loss = 2.72 (950.9 examples/sec; 0.135 sec/batch)
2016-12-08 17:38:30.614850: step 740, loss = 2.56 (940.7 examples/sec; 0.136 sec/batch)
2016-12-08 17:38:31.946021: step 750, loss = 2.74 (929.2 examples/sec; 0.138 sec/batch)
2016-12-08 17:38:33.336098: step 760, loss = 2.64 (913.0 examples/sec; 0.140 sec/batch)
2016-12-08 17:38:34.701509: step 770, loss = 2.71 (871.5 examples/sec; 0.147 sec/batch)
2016-12-08 17:38:36.054653: step 780, loss = 2.60 (935.4 examples/sec; 0.137 sec/batch)
2016-12-08 17:38:37.437712: step 790, loss = 2.75 (938.6 examples/sec; 0.136 sec/batch)
2016-12-08 17:38:38.868083: step 800, loss = 2.57 (858.0 examples/sec; 0.149 sec/batch)
2016-12-08 17:38:40.534316: step 810, loss = 2.75 (956.1 examples/sec; 0.134 sec/batch)
2016-12-08 17:38:41.860994: step 820, loss = 2.79 (979.9 examples/sec; 0.131 sec/batch)
2016-12-08 17:38:43.221084: step 830, loss = 2.69 (922.0 examples/sec; 0.139 sec/batch)
2016-12-08 17:38:44.597760: step 840, loss = 2.50 (895.7 examples/sec; 0.143 sec/batch)
2016-12-08 17:38:45.963511: step 850, loss = 2.56 (891.7 examples/sec; 0.144 sec/batch)
2016-12-08 17:38:47.347734: step 860, loss = 2.75 (963.5 examples/sec; 0.133 sec/batch)
2016-12-08 17:38:48.725355: step 870, loss = 2.51 (1026.9 examples/sec; 0.125 sec/batch)
2016-12-08 17:38:50.063132: step 880, loss = 2.44 (952.1 examples/sec; 0.134 sec/batch)
2016-12-08 17:38:51.453494: step 890, loss = 2.69 (869.6 examples/sec; 0.147 sec/batch)
2016-12-08 17:38:52.767881: step 900, loss = 2.51 (1048.5 examples/sec; 0.122 sec/batch)
2016-12-08 17:38:54.399969: step 910, loss = 2.57 (924.5 examples/sec; 0.138 sec/batch)
2016-12-08 17:38:55.777300: step 920, loss = 2.70 (947.3 examples/sec; 0.135 sec/batch)
2016-12-08 17:38:57.161067: step 930, loss = 2.51 (1013.7 examples/sec; 0.126 sec/batch)
2016-12-08 17:38:58.595557: step 940, loss = 2.31 (961.8 examples/sec; 0.133 sec/batch)
2016-12-08 17:38:59.963391: step 950, loss = 2.36 (913.4 examples/sec; 0.140 sec/batch)
2016-12-08 17:39:01.344046: step 960, loss = 2.66 (871.0 examples/sec; 0.147 sec/batch)
2016-12-08 17:39:02.745824: step 970, loss = 2.46 (913.0 examples/sec; 0.140 sec/batch)
2016-12-08 17:39:04.114747: step 980, loss = 2.32 (961.3 examples/sec; 0.133 sec/batch)
2016-12-08 17:39:05.468906: step 990, loss = 2.63 (924.1 examples/sec; 0.139 sec/batch)
2016-12-08 17:39:06.827615: step 1000, loss = 2.41 (964.5 examples/sec; 0.133 sec/batch)
2016-12-08 17:39:09.018222: step 1010, loss = 2.47 (879.5 examples/sec; 0.146 sec/batch)
2016-12-08 17:39:10.411833: step 1020, loss = 2.27 (913.5 examples/sec; 0.140 sec/batch)
2016-12-08 17:39:11.884156: step 1030, loss = 2.24 (904.4 examples/sec; 0.142 sec/batch)
2016-12-08 17:39:13.366958: step 1040, loss = 2.38 (916.0 examples/sec; 0.140 sec/batch)
2016-12-08 17:39:14.740574: step 1050, loss = 2.46 (1023.9 examples/sec; 0.125 sec/batch)
2016-12-08 17:39:16.107127: step 1060, loss = 2.50 (946.2 examples/sec; 0.135 sec/batch)
2016-12-08 17:39:17.450540: step 1070, loss = 2.18 (971.6 examples/sec; 0.132 sec/batch)
2016-12-08 17:39:18.839548: step 1080, loss = 2.20 (908.0 examples/sec; 0.141 sec/batch)
