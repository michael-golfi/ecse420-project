Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
WARNING:tensorflow:From cifar10_train.py:81 in train.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2016-03-02.
Instructions for updating:
Please use tf.global_variables instead.
2016-12-08 05:21:24.474675: step 0, loss = 4.67 (26.3 examples/sec; 4.860 sec/batch)
2016-12-08 05:21:35.646349: step 10, loss = 4.57 (137.7 examples/sec; 0.929 sec/batch)
2016-12-08 05:21:45.606635: step 20, loss = 4.53 (126.6 examples/sec; 1.011 sec/batch)
2016-12-08 05:21:55.140746: step 30, loss = 4.40 (131.8 examples/sec; 0.972 sec/batch)
2016-12-08 05:22:04.889589: step 40, loss = 4.51 (115.3 examples/sec; 1.110 sec/batch)
2016-12-08 05:22:14.426879: step 50, loss = 4.23 (126.7 examples/sec; 1.010 sec/batch)
2016-12-08 05:22:24.163902: step 60, loss = 4.23 (130.6 examples/sec; 0.980 sec/batch)
2016-12-08 05:22:33.747298: step 70, loss = 4.25 (141.4 examples/sec; 0.905 sec/batch)
2016-12-08 05:22:43.622544: step 80, loss = 4.08 (135.4 examples/sec; 0.945 sec/batch)
2016-12-08 05:22:53.360798: step 90, loss = 4.17 (129.4 examples/sec; 0.989 sec/batch)
2016-12-08 05:23:02.929898: step 100, loss = 4.17 (124.6 examples/sec; 1.028 sec/batch)
2016-12-08 05:23:13.525798: step 110, loss = 4.06 (136.8 examples/sec; 0.936 sec/batch)
2016-12-08 05:23:23.026192: step 120, loss = 4.01 (120.9 examples/sec; 1.059 sec/batch)
2016-12-08 05:23:32.124117: step 130, loss = 3.98 (140.4 examples/sec; 0.912 sec/batch)
2016-12-08 05:23:41.563358: step 140, loss = 4.13 (157.7 examples/sec; 0.812 sec/batch)
2016-12-08 05:23:50.685621: step 150, loss = 4.00 (133.5 examples/sec; 0.959 sec/batch)
2016-12-08 05:23:59.954013: step 160, loss = 4.01 (160.9 examples/sec; 0.796 sec/batch)
2016-12-08 05:24:09.308763: step 170, loss = 3.92 (140.0 examples/sec; 0.914 sec/batch)
2016-12-08 05:24:18.900470: step 180, loss = 3.87 (129.8 examples/sec; 0.986 sec/batch)
2016-12-08 05:24:28.556818: step 190, loss = 3.84 (126.7 examples/sec; 1.010 sec/batch)
2016-12-08 05:24:38.347357: step 200, loss = 3.96 (122.0 examples/sec; 1.049 sec/batch)
2016-12-08 05:24:49.043259: step 210, loss = 3.85 (139.3 examples/sec; 0.919 sec/batch)
2016-12-08 05:24:58.702703: step 220, loss = 3.64 (122.9 examples/sec; 1.042 sec/batch)
2016-12-08 05:25:07.795559: step 230, loss = 3.79 (155.5 examples/sec; 0.823 sec/batch)
2016-12-08 05:25:17.062129: step 240, loss = 3.82 (151.1 examples/sec; 0.847 sec/batch)
2016-12-08 05:25:26.484531: step 250, loss = 3.71 (139.3 examples/sec; 0.919 sec/batch)
2016-12-08 05:25:35.753147: step 260, loss = 3.58 (148.9 examples/sec; 0.860 sec/batch)
2016-12-08 05:25:45.177302: step 270, loss = 3.68 (125.5 examples/sec; 1.020 sec/batch)
2016-12-08 05:25:54.744729: step 280, loss = 3.71 (121.2 examples/sec; 1.056 sec/batch)
2016-12-08 05:26:04.048916: step 290, loss = 3.89 (164.5 examples/sec; 0.778 sec/batch)
2016-12-08 05:26:13.138437: step 300, loss = 3.48 (149.0 examples/sec; 0.859 sec/batch)
2016-12-08 05:26:23.647436: step 310, loss = 3.44 (128.4 examples/sec; 0.997 sec/batch)
2016-12-08 05:26:33.207365: step 320, loss = 3.48 (135.5 examples/sec; 0.944 sec/batch)
2016-12-08 05:26:42.378583: step 330, loss = 3.55 (137.8 examples/sec; 0.929 sec/batch)
2016-12-08 05:26:51.667840: step 340, loss = 3.30 (132.5 examples/sec; 0.966 sec/batch)
2016-12-08 05:27:01.059944: step 350, loss = 3.56 (131.4 examples/sec; 0.974 sec/batch)
2016-12-08 05:27:09.757874: step 360, loss = 3.51 (135.4 examples/sec; 0.945 sec/batch)
2016-12-08 05:27:18.948374: step 370, loss = 3.22 (146.8 examples/sec; 0.872 sec/batch)
2016-12-08 05:27:28.094233: step 380, loss = 3.35 (125.7 examples/sec; 1.018 sec/batch)
2016-12-08 05:27:37.848039: step 390, loss = 3.46 (136.4 examples/sec; 0.939 sec/batch)
2016-12-08 05:27:47.203374: step 400, loss = 3.27 (132.8 examples/sec; 0.964 sec/batch)
2016-12-08 05:27:57.907846: step 410, loss = 3.29 (149.0 examples/sec; 0.859 sec/batch)
2016-12-08 05:28:07.029030: step 420, loss = 3.25 (150.8 examples/sec; 0.849 sec/batch)
2016-12-08 05:28:16.107848: step 430, loss = 3.38 (118.9 examples/sec; 1.076 sec/batch)
2016-12-08 05:28:25.666818: step 440, loss = 3.40 (119.8 examples/sec; 1.068 sec/batch)
2016-12-08 05:28:35.068277: step 450, loss = 3.26 (142.6 examples/sec; 0.897 sec/batch)
2016-12-08 05:28:44.233910: step 460, loss = 3.29 (153.9 examples/sec; 0.832 sec/batch)
2016-12-08 05:28:53.971081: step 470, loss = 3.30 (129.1 examples/sec; 0.992 sec/batch)
2016-12-08 05:29:03.808437: step 480, loss = 3.21 (114.7 examples/sec; 1.116 sec/batch)
2016-12-08 05:29:13.769609: step 490, loss = 3.18 (120.5 examples/sec; 1.062 sec/batch)
2016-12-08 05:29:23.296951: step 500, loss = 3.29 (121.7 examples/sec; 1.052 sec/batch)
2016-12-08 05:29:33.624331: step 510, loss = 3.27 (138.6 examples/sec; 0.923 sec/batch)
2016-12-08 05:29:43.636820: step 520, loss = 3.50 (131.9 examples/sec; 0.971 sec/batch)
2016-12-08 05:29:52.790537: step 530, loss = 3.21 (139.4 examples/sec; 0.918 sec/batch)
2016-12-08 05:30:02.016694: step 540, loss = 3.10 (161.6 examples/sec; 0.792 sec/batch)
2016-12-08 05:30:11.081223: step 550, loss = 3.31 (146.1 examples/sec; 0.876 sec/batch)
2016-12-08 05:30:20.272463: step 560, loss = 3.00 (150.4 examples/sec; 0.851 sec/batch)
2016-12-08 05:30:29.596490: step 570, loss = 3.04 (141.3 examples/sec; 0.906 sec/batch)
2016-12-08 05:30:38.942212: step 580, loss = 3.09 (142.0 examples/sec; 0.902 sec/batch)
2016-12-08 05:30:48.370538: step 590, loss = 3.09 (130.2 examples/sec; 0.983 sec/batch)
2016-12-08 05:30:57.821689: step 600, loss = 3.04 (162.8 examples/sec; 0.786 sec/batch)
2016-12-08 05:31:08.319660: step 610, loss = 2.93 (130.9 examples/sec; 0.978 sec/batch)
2016-12-08 05:31:17.521914: step 620, loss = 3.32 (139.2 examples/sec; 0.920 sec/batch)
2016-12-08 05:31:27.112891: step 630, loss = 2.89 (142.5 examples/sec; 0.898 sec/batch)
2016-12-08 05:31:37.151902: step 640, loss = 3.09 (110.0 examples/sec; 1.163 sec/batch)
2016-12-08 05:31:46.556929: step 650, loss = 3.10 (138.8 examples/sec; 0.922 sec/batch)
2016-12-08 05:31:55.739536: step 660, loss = 2.83 (147.8 examples/sec; 0.866 sec/batch)
2016-12-08 05:32:05.255953: step 670, loss = 2.66 (125.6 examples/sec; 1.019 sec/batch)
2016-12-08 05:32:15.051152: step 680, loss = 2.76 (141.9 examples/sec; 0.902 sec/batch)
2016-12-08 05:32:24.324316: step 690, loss = 2.86 (122.5 examples/sec; 1.045 sec/batch)
2016-12-08 05:32:33.989878: step 700, loss = 3.02 (133.0 examples/sec; 0.963 sec/batch)
2016-12-08 05:32:43.776327: step 710, loss = 2.90 (133.7 examples/sec; 0.957 sec/batch)
2016-12-08 05:32:52.909901: step 720, loss = 2.84 (135.9 examples/sec; 0.942 sec/batch)
2016-12-08 05:33:01.805786: step 730, loss = 2.70 (164.4 examples/sec; 0.779 sec/batch)
2016-12-08 05:33:10.913639: step 740, loss = 2.85 (139.7 examples/sec; 0.916 sec/batch)
2016-12-08 05:33:19.811577: step 750, loss = 2.87 (164.1 examples/sec; 0.780 sec/batch)
2016-12-08 05:33:29.787536: step 760, loss = 2.62 (137.1 examples/sec; 0.934 sec/batch)
2016-12-08 05:33:39.424766: step 770, loss = 2.64 (127.9 examples/sec; 1.001 sec/batch)
2016-12-08 05:33:48.856931: step 780, loss = 2.82 (137.2 examples/sec; 0.933 sec/batch)
2016-12-08 05:33:58.145257: step 790, loss = 2.77 (117.5 examples/sec; 1.089 sec/batch)
2016-12-08 05:34:08.015563: step 800, loss = 2.90 (135.8 examples/sec; 0.942 sec/batch)
2016-12-08 05:34:18.428764: step 810, loss = 2.48 (127.5 examples/sec; 1.004 sec/batch)
2016-12-08 05:34:27.692112: step 820, loss = 2.64 (148.1 examples/sec; 0.864 sec/batch)
2016-12-08 05:34:36.763004: step 830, loss = 2.59 (147.3 examples/sec; 0.869 sec/batch)
2016-12-08 05:34:45.922949: step 840, loss = 2.63 (137.1 examples/sec; 0.934 sec/batch)
2016-12-08 05:34:55.693169: step 850, loss = 2.75 (158.1 examples/sec; 0.810 sec/batch)
2016-12-08 05:35:05.065405: step 860, loss = 2.64 (162.4 examples/sec; 0.788 sec/batch)
2016-12-08 05:35:14.471073: step 870, loss = 2.94 (136.0 examples/sec; 0.941 sec/batch)
2016-12-08 05:35:23.368097: step 880, loss = 2.50 (137.9 examples/sec; 0.928 sec/batch)
2016-12-08 05:35:32.060066: step 890, loss = 2.60 (148.3 examples/sec; 0.863 sec/batch)
2016-12-08 05:35:41.578988: step 900, loss = 2.49 (136.6 examples/sec; 0.937 sec/batch)
2016-12-08 05:35:52.471609: step 910, loss = 2.52 (132.8 examples/sec; 0.964 sec/batch)
2016-12-08 05:36:01.531656: step 920, loss = 2.67 (149.4 examples/sec; 0.857 sec/batch)
2016-12-08 05:36:10.425555: step 930, loss = 2.41 (160.5 examples/sec; 0.797 sec/batch)
2016-12-08 05:36:20.443104: step 940, loss = 2.62 (119.5 examples/sec; 1.072 sec/batch)
2016-12-08 05:36:29.743282: step 950, loss = 2.61 (154.1 examples/sec; 0.831 sec/batch)
2016-12-08 05:36:38.733681: step 960, loss = 2.39 (141.1 examples/sec; 0.907 sec/batch)
2016-12-08 05:36:48.214202: step 970, loss = 2.48 (144.0 examples/sec; 0.889 sec/batch)
2016-12-08 05:36:57.480097: step 980, loss = 2.56 (129.0 examples/sec; 0.992 sec/batch)
2016-12-08 05:37:06.780955: step 990, loss = 2.70 (136.1 examples/sec; 0.941 sec/batch)
2016-12-08 05:37:15.741350: step 1000, loss = 2.35 (147.6 examples/sec; 0.867 sec/batch)
2016-12-08 05:37:25.985929: step 1010, loss = 2.41 (126.3 examples/sec; 1.014 sec/batch)
2016-12-08 05:37:35.134285: step 1020, loss = 2.57 (129.1 examples/sec; 0.992 sec/batch)
2016-12-08 05:37:44.989015: step 1030, loss = 2.54 (146.5 examples/sec; 0.874 sec/batch)
2016-12-08 05:37:54.500245: step 1040, loss = 2.38 (128.6 examples/sec; 0.996 sec/batch)
2016-12-08 05:38:03.680985: step 1050, loss = 2.27 (154.4 examples/sec; 0.829 sec/batch)
2016-12-08 05:38:13.352903: step 1060, loss = 2.50 (130.6 examples/sec; 0.980 sec/batch)
2016-12-08 05:38:22.410392: step 1070, loss = 2.39 (154.4 examples/sec; 0.829 sec/batch)
2016-12-08 05:38:31.588197: step 1080, loss = 2.43 (144.7 examples/sec; 0.885 sec/batch)
2016-12-08 05:38:40.392810: step 1090, loss = 2.45 (148.6 examples/sec; 0.862 sec/batch)
2016-12-08 05:38:49.644592: step 1100, loss = 2.41 (136.1 examples/sec; 0.940 sec/batch)
2016-12-08 05:38:59.760137: step 1110, loss = 2.33 (152.9 examples/sec; 0.837 sec/batch)
2016-12-08 05:39:09.121787: step 1120, loss = 2.28 (127.1 examples/sec; 1.007 sec/batch)
2016-12-08 05:39:18.656358: step 1130, loss = 2.29 (150.1 examples/sec; 0.853 sec/batch)
2016-12-08 05:39:27.408859: step 1140, loss = 2.55 (148.8 examples/sec; 0.860 sec/batch)
2016-12-08 05:39:36.750754: step 1150, loss = 2.40 (130.0 examples/sec; 0.984 sec/batch)
2016-12-08 05:39:45.948607: step 1160, loss = 2.30 (123.4 examples/sec; 1.037 sec/batch)
2016-12-08 05:39:54.948213: step 1170, loss = 2.34 (142.1 examples/sec; 0.901 sec/batch)
2016-12-08 05:40:04.357518: step 1180, loss = 2.13 (138.4 examples/sec; 0.925 sec/batch)
2016-12-08 05:40:13.368424: step 1190, loss = 2.42 (140.6 examples/sec; 0.910 sec/batch)
2016-12-08 05:40:22.581494: step 1200, loss = 2.36 (135.4 examples/sec; 0.945 sec/batch)
2016-12-08 05:40:32.976683: step 1210, loss = 1.99 (138.5 examples/sec; 0.924 sec/batch)
2016-12-08 05:40:42.554536: step 1220, loss = 2.19 (124.0 examples/sec; 1.032 sec/batch)
2016-12-08 05:40:51.460355: step 1230, loss = 2.03 (135.7 examples/sec; 0.943 sec/batch)
2016-12-08 05:41:00.810150: step 1240, loss = 2.12 (135.2 examples/sec; 0.947 sec/batch)
2016-12-08 05:41:09.506127: step 1250, loss = 2.27 (139.2 examples/sec; 0.919 sec/batch)
2016-12-08 05:41:18.581932: step 1260, loss = 1.91 (152.6 examples/sec; 0.839 sec/batch)
2016-12-08 05:41:27.964190: step 1270, loss = 2.28 (132.2 examples/sec; 0.968 sec/batch)
2016-12-08 05:41:37.698260: step 1280, loss = 2.14 (125.5 examples/sec; 1.020 sec/batch)
2016-12-08 05:41:47.204281: step 1290, loss = 2.12 (147.6 examples/sec; 0.867 sec/batch)
2016-12-08 05:41:56.628003: step 1300, loss = 2.22 (133.9 examples/sec; 0.956 sec/batch)
2016-12-08 05:42:06.852794: step 1310, loss = 2.07 (142.0 examples/sec; 0.902 sec/batch)
2016-12-08 05:42:16.276682: step 1320, loss = 2.11 (121.9 examples/sec; 1.050 sec/batch)
2016-12-08 05:42:25.347933: step 1330, loss = 2.28 (146.6 examples/sec; 0.873 sec/batch)
2016-12-08 05:42:34.745646: step 1340, loss = 2.01 (144.1 examples/sec; 0.888 sec/batch)
2016-12-08 05:42:44.120959: step 1350, loss = 2.16 (139.4 examples/sec; 0.918 sec/batch)
2016-12-08 05:42:53.203226: step 1360, loss = 1.98 (139.8 examples/sec; 0.915 sec/batch)
2016-12-08 05:43:02.788424: step 1370, loss = 2.13 (123.7 examples/sec; 1.035 sec/batch)
2016-12-08 05:43:11.972312: step 1380, loss = 1.84 (141.7 examples/sec; 0.903 sec/batch)
2016-12-08 05:43:21.000602: step 1390, loss = 2.03 (131.6 examples/sec; 0.973 sec/batch)
2016-12-08 05:43:30.838076: step 1400, loss = 2.30 (123.1 examples/sec; 1.039 sec/batch)
2016-12-08 05:43:40.909452: step 1410, loss = 1.82 (131.3 examples/sec; 0.975 sec/batch)
2016-12-08 05:43:49.904580: step 1420, loss = 2.27 (137.8 examples/sec; 0.929 sec/batch)
2016-12-08 05:43:59.545721: step 1430, loss = 2.16 (139.4 examples/sec; 0.918 sec/batch)
2016-12-08 05:44:09.472196: step 1440, loss = 1.89 (146.7 examples/sec; 0.872 sec/batch)
2016-12-08 05:44:18.843901: step 1450, loss = 1.98 (135.7 examples/sec; 0.943 sec/batch)
2016-12-08 05:44:28.557207: step 1460, loss = 1.97 (131.7 examples/sec; 0.972 sec/batch)
2016-12-08 05:44:37.510841: step 1470, loss = 1.81 (149.0 examples/sec; 0.859 sec/batch)
2016-12-08 05:44:46.654064: step 1480, loss = 1.87 (132.9 examples/sec; 0.963 sec/batch)
2016-12-08 05:44:55.522580: step 1490, loss = 2.14 (147.0 examples/sec; 0.870 sec/batch)
2016-12-08 05:45:04.678836: step 1500, loss = 2.20 (133.5 examples/sec; 0.959 sec/batch)
2016-12-08 05:45:15.027570: step 1510, loss = 1.95 (122.3 examples/sec; 1.046 sec/batch)
2016-12-08 05:45:24.280155: step 1520, loss = 1.90 (138.7 examples/sec; 0.923 sec/batch)
2016-12-08 05:45:33.048464: step 1530, loss = 1.90 (163.3 examples/sec; 0.784 sec/batch)
2016-12-08 05:45:41.974316: step 1540, loss = 2.11 (145.7 examples/sec; 0.879 sec/batch)
2016-12-08 05:45:51.421268: step 1550, loss = 1.74 (131.3 examples/sec; 0.975 sec/batch)
2016-12-08 05:46:00.519381: step 1560, loss = 2.08 (133.8 examples/sec; 0.956 sec/batch)
2016-12-08 05:46:09.903993: step 1570, loss = 1.89 (130.5 examples/sec; 0.980 sec/batch)
2016-12-08 05:46:18.813334: step 1580, loss = 1.92 (147.1 examples/sec; 0.870 sec/batch)
2016-12-08 05:46:27.902625: step 1590, loss = 1.90 (124.3 examples/sec; 1.030 sec/batch)
2016-12-08 05:46:37.980095: step 1600, loss = 2.05 (129.1 examples/sec; 0.991 sec/batch)
2016-12-08 05:46:48.362425: step 1610, loss = 1.86 (133.0 examples/sec; 0.962 sec/batch)
2016-12-08 05:46:57.102682: step 1620, loss = 1.71 (133.6 examples/sec; 0.958 sec/batch)
2016-12-08 05:47:06.245971: step 1630, loss = 1.77 (137.8 examples/sec; 0.929 sec/batch)
2016-12-08 05:47:15.452040: step 1640, loss = 1.78 (128.6 examples/sec; 0.995 sec/batch)
2016-12-08 05:47:24.635881: step 1650, loss = 1.84 (130.0 examples/sec; 0.984 sec/batch)
2016-12-08 05:47:33.822620: step 1660, loss = 1.68 (141.6 examples/sec; 0.904 sec/batch)
2016-12-08 05:47:43.258035: step 1670, loss = 1.87 (129.3 examples/sec; 0.990 sec/batch)
2016-12-08 05:47:52.569289: step 1680, loss = 1.83 (129.6 examples/sec; 0.988 sec/batch)
2016-12-08 05:48:02.105086: step 1690, loss = 1.92 (142.9 examples/sec; 0.896 sec/batch)
2016-12-08 05:48:11.552883: step 1700, loss = 2.00 (130.3 examples/sec; 0.982 sec/batch)
2016-12-08 05:48:21.797571: step 1710, loss = 1.87 (136.0 examples/sec; 0.941 sec/batch)
2016-12-08 05:48:31.711573: step 1720, loss = 2.03 (129.1 examples/sec; 0.991 sec/batch)
2016-12-08 05:48:41.249029: step 1730, loss = 1.79 (133.9 examples/sec; 0.956 sec/batch)
2016-12-08 05:48:50.586086: step 1740, loss = 1.68 (137.2 examples/sec; 0.933 sec/batch)
2016-12-08 05:49:00.049854: step 1750, loss = 1.62 (127.4 examples/sec; 1.005 sec/batch)
2016-12-08 05:49:09.708980: step 1760, loss = 1.86 (126.3 examples/sec; 1.014 sec/batch)
2016-12-08 05:49:19.178313: step 1770, loss = 1.92 (141.3 examples/sec; 0.906 sec/batch)
2016-12-08 05:49:28.142778: step 1780, loss = 1.74 (139.0 examples/sec; 0.921 sec/batch)
2016-12-08 05:49:36.970851: step 1790, loss = 1.57 (146.2 examples/sec; 0.876 sec/batch)
2016-12-08 05:49:46.170784: step 1800, loss = 1.85 (146.7 examples/sec; 0.872 sec/batch)
2016-12-08 05:49:55.769968: step 1810, loss = 1.77 (164.1 examples/sec; 0.780 sec/batch)
2016-12-08 05:50:04.935850: step 1820, loss = 1.81 (130.6 examples/sec; 0.980 sec/batch)
2016-12-08 05:50:14.052585: step 1830, loss = 1.77 (167.5 examples/sec; 0.764 sec/batch)
2016-12-08 05:50:23.335008: step 1840, loss = 1.65 (160.4 examples/sec; 0.798 sec/batch)
2016-12-08 05:50:32.168786: step 1850, loss = 1.82 (146.9 examples/sec; 0.872 sec/batch)
2016-12-08 05:50:41.518625: step 1860, loss = 1.72 (142.4 examples/sec; 0.899 sec/batch)
2016-12-08 05:50:51.241410: step 1870, loss = 1.58 (136.6 examples/sec; 0.937 sec/batch)
2016-12-08 05:51:00.308453: step 1880, loss = 1.83 (137.8 examples/sec; 0.929 sec/batch)
2016-12-08 05:51:09.286815: step 1890, loss = 1.71 (149.2 examples/sec; 0.858 sec/batch)
2016-12-08 05:51:18.220979: step 1900, loss = 1.66 (152.6 examples/sec; 0.839 sec/batch)
2016-12-08 05:51:28.273158: step 1910, loss = 1.93 (147.4 examples/sec; 0.868 sec/batch)
2016-12-08 05:51:37.357324: step 1920, loss = 1.77 (135.9 examples/sec; 0.942 sec/batch)
2016-12-08 05:51:46.497579: step 1930, loss = 1.71 (128.1 examples/sec; 1.000 sec/batch)
2016-12-08 05:51:56.080291: step 1940, loss = 1.59 (135.1 examples/sec; 0.947 sec/batch)
2016-12-08 05:52:05.204736: step 1950, loss = 1.57 (156.0 examples/sec; 0.820 sec/batch)
2016-12-08 05:52:14.495034: step 1960, loss = 1.58 (151.3 examples/sec; 0.846 sec/batch)
2016-12-08 05:52:23.470053: step 1970, loss = 1.69 (167.5 examples/sec; 0.764 sec/batch)
2016-12-08 05:52:32.198402: step 1980, loss = 1.66 (147.2 examples/sec; 0.869 sec/batch)
2016-12-08 05:52:41.549444: step 1990, loss = 1.80 (150.1 examples/sec; 0.853 sec/batch)
2016-12-08 05:52:50.754582: step 2000, loss = 1.60 (133.2 examples/sec; 0.961 sec/batch)
2016-12-08 05:53:02.132831: step 2010, loss = 1.60 (137.4 examples/sec; 0.932 sec/batch)
2016-12-08 05:53:11.406690: step 2020, loss = 1.81 (139.4 examples/sec; 0.918 sec/batch)
2016-12-08 05:53:20.682948: step 2030, loss = 1.56 (133.4 examples/sec; 0.959 sec/batch)
2016-12-08 05:53:30.324763: step 2040, loss = 1.57 (130.5 examples/sec; 0.981 sec/batch)
2016-12-08 05:53:39.648895: step 2050, loss = 1.51 (151.0 examples/sec; 0.847 sec/batch)
2016-12-08 05:53:48.902162: step 2060, loss = 1.48 (139.2 examples/sec; 0.919 sec/batch)
2016-12-08 05:53:58.322941: step 2070, loss = 1.49 (137.4 examples/sec; 0.931 sec/batch)
2016-12-08 05:54:07.358660: step 2080, loss = 1.74 (146.9 examples/sec; 0.871 sec/batch)
2016-12-08 05:54:16.656095: step 2090, loss = 1.45 (140.5 examples/sec; 0.911 sec/batch)
2016-12-08 05:54:25.905669: step 2100, loss = 1.61 (148.1 examples/sec; 0.864 sec/batch)
2016-12-08 05:54:35.942748: step 2110, loss = 1.48 (136.2 examples/sec; 0.939 sec/batch)
2016-12-08 05:54:45.005374: step 2120, loss = 1.39 (159.6 examples/sec; 0.802 sec/batch)
2016-12-08 05:54:54.328794: step 2130, loss = 1.68 (142.3 examples/sec; 0.900 sec/batch)
2016-12-08 05:55:03.563446: step 2140, loss = 1.57 (136.4 examples/sec; 0.939 sec/batch)
2016-12-08 05:55:12.751714: step 2150, loss = 1.46 (133.2 examples/sec; 0.961 sec/batch)
2016-12-08 05:55:22.015640: step 2160, loss = 1.57 (149.8 examples/sec; 0.855 sec/batch)
2016-12-08 05:55:31.040408: step 2170, loss = 1.63 (141.8 examples/sec; 0.903 sec/batch)
2016-12-08 05:55:40.421709: step 2180, loss = 1.64 (125.0 examples/sec; 1.024 sec/batch)
2016-12-08 05:55:49.575646: step 2190, loss = 1.39 (132.5 examples/sec; 0.966 sec/batch)
2016-12-08 05:55:59.130965: step 2200, loss = 1.48 (109.9 examples/sec; 1.164 sec/batch)
2016-12-08 05:56:09.002796: step 2210, loss = 1.54 (139.8 examples/sec; 0.915 sec/batch)
2016-12-08 05:56:18.147089: step 2220, loss = 1.54 (129.9 examples/sec; 0.985 sec/batch)
2016-12-08 05:56:27.609745: step 2230, loss = 1.57 (142.6 examples/sec; 0.898 sec/batch)
2016-12-08 05:56:36.814480: step 2240, loss = 1.48 (136.9 examples/sec; 0.935 sec/batch)
2016-12-08 05:56:46.276618: step 2250, loss = 1.39 (146.2 examples/sec; 0.875 sec/batch)
2016-12-08 05:56:55.713388: step 2260, loss = 1.57 (124.5 examples/sec; 1.028 sec/batch)
2016-12-08 05:57:04.837313: step 2270, loss = 1.46 (133.3 examples/sec; 0.960 sec/batch)
2016-12-08 05:57:14.196671: step 2280, loss = 1.49 (152.1 examples/sec; 0.841 sec/batch)
2016-12-08 05:57:23.523519: step 2290, loss = 1.56 (123.4 examples/sec; 1.037 sec/batch)
2016-12-08 05:57:32.667912: step 2300, loss = 1.45 (146.3 examples/sec; 0.875 sec/batch)
2016-12-08 05:57:43.291052: step 2310, loss = 1.37 (154.0 examples/sec; 0.831 sec/batch)
2016-12-08 05:57:52.426138: step 2320, loss = 1.80 (152.0 examples/sec; 0.842 sec/batch)
2016-12-08 05:58:01.439217: step 2330, loss = 1.47 (136.3 examples/sec; 0.939 sec/batch)
2016-12-08 05:58:10.507069: step 2340, loss = 1.43 (150.5 examples/sec; 0.850 sec/batch)
2016-12-08 05:58:19.465707: step 2350, loss = 1.62 (136.6 examples/sec; 0.937 sec/batch)
2016-12-08 05:58:28.804783: step 2360, loss = 1.50 (136.5 examples/sec; 0.937 sec/batch)
2016-12-08 05:58:37.992470: step 2370, loss = 1.34 (143.9 examples/sec; 0.890 sec/batch)
2016-12-08 05:58:46.890396: step 2380, loss = 1.43 (133.3 examples/sec; 0.960 sec/batch)
2016-12-08 05:58:55.804854: step 2390, loss = 1.25 (148.6 examples/sec; 0.861 sec/batch)
2016-12-08 05:59:04.562886: step 2400, loss = 1.56 (147.8 examples/sec; 0.866 sec/batch)
2016-12-08 05:59:14.757639: step 2410, loss = 1.40 (128.1 examples/sec; 1.000 sec/batch)
2016-12-08 05:59:24.054322: step 2420, loss = 1.46 (166.5 examples/sec; 0.769 sec/batch)
2016-12-08 05:59:32.555512: step 2430, loss = 1.35 (128.0 examples/sec; 1.000 sec/batch)
2016-12-08 05:59:41.384473: step 2440, loss = 1.59 (153.4 examples/sec; 0.834 sec/batch)
2016-12-08 05:59:50.366393: step 2450, loss = 1.53 (141.1 examples/sec; 0.907 sec/batch)
2016-12-08 05:59:59.714484: step 2460, loss = 1.35 (140.5 examples/sec; 0.911 sec/batch)
2016-12-08 06:00:08.867332: step 2470, loss = 1.44 (141.8 examples/sec; 0.902 sec/batch)
2016-12-08 06:00:18.088217: step 2480, loss = 1.31 (153.9 examples/sec; 0.831 sec/batch)
2016-12-08 06:00:27.076305: step 2490, loss = 1.30 (133.3 examples/sec; 0.960 sec/batch)
2016-12-08 06:00:36.409438: step 2500, loss = 1.57 (140.6 examples/sec; 0.911 sec/batch)
2016-12-08 06:00:46.141203: step 2510, loss = 1.22 (152.6 examples/sec; 0.839 sec/batch)
2016-12-08 06:00:55.257773: step 2520, loss = 1.38 (136.7 examples/sec; 0.936 sec/batch)
2016-12-08 06:01:04.536597: step 2530, loss = 1.37 (139.2 examples/sec; 0.920 sec/batch)
2016-12-08 06:01:14.009056: step 2540, loss = 1.24 (134.4 examples/sec; 0.952 sec/batch)
2016-12-08 06:01:23.061208: step 2550, loss = 1.31 (122.0 examples/sec; 1.049 sec/batch)
2016-12-08 06:01:32.451196: step 2560, loss = 1.33 (131.5 examples/sec; 0.973 sec/batch)
2016-12-08 06:01:41.480743: step 2570, loss = 1.43 (128.8 examples/sec; 0.994 sec/batch)
2016-12-08 06:01:50.990452: step 2580, loss = 1.31 (144.5 examples/sec; 0.886 sec/batch)
2016-12-08 06:02:00.063217: step 2590, loss = 1.52 (152.4 examples/sec; 0.840 sec/batch)
2016-12-08 06:02:09.796698: step 2600, loss = 1.46 (129.2 examples/sec; 0.991 sec/batch)
2016-12-08 06:02:20.341372: step 2610, loss = 1.28 (124.3 examples/sec; 1.030 sec/batch)
2016-12-08 06:02:30.152554: step 2620, loss = 1.39 (130.9 examples/sec; 0.978 sec/batch)
2016-12-08 06:02:39.179469: step 2630, loss = 1.37 (132.4 examples/sec; 0.967 sec/batch)
2016-12-08 06:02:48.667903: step 2640, loss = 1.37 (138.3 examples/sec; 0.925 sec/batch)
2016-12-08 06:02:58.314476: step 2650, loss = 1.28 (140.6 examples/sec; 0.910 sec/batch)
2016-12-08 06:03:07.797732: step 2660, loss = 1.35 (140.6 examples/sec; 0.910 sec/batch)
2016-12-08 06:03:16.533083: step 2670, loss = 1.40 (159.2 examples/sec; 0.804 sec/batch)
2016-12-08 06:03:25.696579: step 2680, loss = 1.29 (157.1 examples/sec; 0.815 sec/batch)
2016-12-08 06:03:35.061809: step 2690, loss = 1.25 (155.1 examples/sec; 0.825 sec/batch)
2016-12-08 06:03:44.234281: step 2700, loss = 1.43 (153.2 examples/sec; 0.836 sec/batch)
2016-12-08 06:03:54.206242: step 2710, loss = 1.36 (143.3 examples/sec; 0.893 sec/batch)
2016-12-08 06:04:03.522921: step 2720, loss = 1.36 (137.8 examples/sec; 0.929 sec/batch)
2016-12-08 06:04:12.504284: step 2730, loss = 1.49 (134.0 examples/sec; 0.955 sec/batch)
2016-12-08 06:04:21.676004: step 2740, loss = 1.26 (122.5 examples/sec; 1.045 sec/batch)
2016-12-08 06:04:30.799026: step 2750, loss = 1.35 (124.3 examples/sec; 1.030 sec/batch)
2016-12-08 06:04:40.009765: step 2760, loss = 1.21 (146.1 examples/sec; 0.876 sec/batch)
2016-12-08 06:04:49.358564: step 2770, loss = 1.11 (118.9 examples/sec; 1.077 sec/batch)
2016-12-08 06:04:58.186781: step 2780, loss = 1.29 (144.1 examples/sec; 0.888 sec/batch)
2016-12-08 06:05:07.247572: step 2790, loss = 1.41 (151.4 examples/sec; 0.845 sec/batch)
2016-12-08 06:05:16.668515: step 2800, loss = 1.32 (144.0 examples/sec; 0.889 sec/batch)
2016-12-08 06:05:27.096751: step 2810, loss = 1.60 (145.8 examples/sec; 0.878 sec/batch)
2016-12-08 06:05:36.268294: step 2820, loss = 1.14 (120.7 examples/sec; 1.060 sec/batch)
2016-12-08 06:05:45.382017: step 2830, loss = 1.34 (151.2 examples/sec; 0.846 sec/batch)
2016-12-08 06:05:54.282483: step 2840, loss = 1.23 (155.1 examples/sec; 0.825 sec/batch)
2016-12-08 06:06:03.639511: step 2850, loss = 1.42 (126.6 examples/sec; 1.011 sec/batch)
2016-12-08 06:06:13.281760: step 2860, loss = 1.43 (133.2 examples/sec; 0.961 sec/batch)
2016-12-08 06:06:22.616994: step 2870, loss = 1.10 (144.9 examples/sec; 0.883 sec/batch)
2016-12-08 06:06:32.016513: step 2880, loss = 1.31 (119.5 examples/sec; 1.071 sec/batch)
2016-12-08 06:06:41.371917: step 2890, loss = 1.42 (136.8 examples/sec; 0.936 sec/batch)
2016-12-08 06:06:51.049850: step 2900, loss = 1.33 (133.2 examples/sec; 0.961 sec/batch)
2016-12-08 06:07:01.269747: step 2910, loss = 1.30 (144.4 examples/sec; 0.886 sec/batch)
2016-12-08 06:07:10.707792: step 2920, loss = 1.55 (133.5 examples/sec; 0.959 sec/batch)
2016-12-08 06:07:19.906293: step 2930, loss = 1.35 (157.0 examples/sec; 0.816 sec/batch)
2016-12-08 06:07:29.382742: step 2940, loss = 1.08 (139.5 examples/sec; 0.918 sec/batch)
2016-12-08 06:07:38.030320: step 2950, loss = 1.26 (140.5 examples/sec; 0.911 sec/batch)
2016-12-08 06:07:46.881873: step 2960, loss = 1.31 (151.7 examples/sec; 0.844 sec/batch)
2016-12-08 06:07:55.827112: step 2970, loss = 1.26 (145.7 examples/sec; 0.878 sec/batch)
2016-12-08 06:08:05.482738: step 2980, loss = 1.18 (127.6 examples/sec; 1.003 sec/batch)
2016-12-08 06:08:15.005854: step 2990, loss = 1.31 (144.1 examples/sec; 0.888 sec/batch)
2016-12-08 06:08:24.810616: step 3000, loss = 1.26 (136.0 examples/sec; 0.941 sec/batch)
2016-12-08 06:08:35.267040: step 3010, loss = 1.17 (162.6 examples/sec; 0.787 sec/batch)
2016-12-08 06:08:44.317108: step 3020, loss = 1.33 (134.3 examples/sec; 0.953 sec/batch)
2016-12-08 06:08:53.474425: step 3030, loss = 1.12 (147.3 examples/sec; 0.869 sec/batch)
2016-12-08 06:09:03.266591: step 3040, loss = 1.21 (129.1 examples/sec; 0.991 sec/batch)
2016-12-08 06:09:12.557802: step 3050, loss = 1.19 (135.1 examples/sec; 0.948 sec/batch)
2016-12-08 06:09:21.930508: step 3060, loss = 1.40 (137.6 examples/sec; 0.930 sec/batch)
2016-12-08 06:09:31.325444: step 3070, loss = 1.40 (135.8 examples/sec; 0.942 sec/batch)
2016-12-08 06:09:40.650623: step 3080, loss = 1.26 (135.1 examples/sec; 0.948 sec/batch)
2016-12-08 06:09:49.851113: step 3090, loss = 1.46 (120.8 examples/sec; 1.060 sec/batch)
2016-12-08 06:09:58.921021: step 3100, loss = 1.44 (146.6 examples/sec; 0.873 sec/batch)
2016-12-08 06:10:09.471976: step 3110, loss = 1.23 (140.0 examples/sec; 0.914 sec/batch)
2016-12-08 06:10:18.722890: step 3120, loss = 1.29 (137.6 examples/sec; 0.930 sec/batch)
2016-12-08 06:10:27.860705: step 3130, loss = 1.25 (153.9 examples/sec; 0.832 sec/batch)
2016-12-08 06:10:36.880217: step 3140, loss = 1.31 (137.6 examples/sec; 0.930 sec/batch)
2016-12-08 06:10:46.062823: step 3150, loss = 1.22 (146.1 examples/sec; 0.876 sec/batch)
2016-12-08 06:10:55.319906: step 3160, loss = 1.28 (123.2 examples/sec; 1.039 sec/batch)
2016-12-08 06:11:04.833089: step 3170, loss = 1.24 (146.9 examples/sec; 0.871 sec/batch)
2016-12-08 06:11:13.786047: step 3180, loss = 1.23 (148.1 examples/sec; 0.864 sec/batch)
2016-12-08 06:11:22.710741: step 3190, loss = 1.46 (143.9 examples/sec; 0.889 sec/batch)
2016-12-08 06:11:31.876520: step 3200, loss = 1.21 (141.0 examples/sec; 0.908 sec/batch)
2016-12-08 06:11:41.547571: step 3210, loss = 1.29 (138.2 examples/sec; 0.926 sec/batch)
2016-12-08 06:11:50.958916: step 3220, loss = 1.44 (145.1 examples/sec; 0.882 sec/batch)
2016-12-08 06:12:00.282688: step 3230, loss = 1.36 (115.9 examples/sec; 1.104 sec/batch)
2016-12-08 06:12:09.561983: step 3240, loss = 1.15 (145.4 examples/sec; 0.880 sec/batch)
2016-12-08 06:12:18.814829: step 3250, loss = 1.29 (151.1 examples/sec; 0.847 sec/batch)
2016-12-08 06:12:27.509963: step 3260, loss = 1.25 (118.1 examples/sec; 1.084 sec/batch)
2016-12-08 06:12:36.493105: step 3270, loss = 1.33 (140.9 examples/sec; 0.908 sec/batch)
2016-12-08 06:12:46.007675: step 3280, loss = 1.27 (117.8 examples/sec; 1.086 sec/batch)
2016-12-08 06:12:55.309689: step 3290, loss = 1.19 (139.5 examples/sec; 0.917 sec/batch)
2016-12-08 06:13:04.584414: step 3300, loss = 1.37 (136.8 examples/sec; 0.935 sec/batch)
2016-12-08 06:13:15.203454: step 3310, loss = 1.22 (117.2 examples/sec; 1.092 sec/batch)
2016-12-08 06:13:24.542681: step 3320, loss = 1.17 (141.6 examples/sec; 0.904 sec/batch)
2016-12-08 06:13:33.772505: step 3330, loss = 1.36 (125.0 examples/sec; 1.024 sec/batch)
2016-12-08 06:13:43.265215: step 3340, loss = 1.36 (120.7 examples/sec; 1.060 sec/batch)
2016-12-08 06:13:52.628605: step 3350, loss = 1.19 (125.6 examples/sec; 1.019 sec/batch)
2016-12-08 06:14:02.345179: step 3360, loss = 1.08 (127.2 examples/sec; 1.006 sec/batch)
2016-12-08 06:14:11.816274: step 3370, loss = 1.22 (146.0 examples/sec; 0.877 sec/batch)
2016-12-08 06:14:20.602891: step 3380, loss = 1.20 (147.0 examples/sec; 0.871 sec/batch)
2016-12-08 06:14:29.586965: step 3390, loss = 1.11 (142.1 examples/sec; 0.901 sec/batch)
2016-12-08 06:14:38.891483: step 3400, loss = 1.19 (138.6 examples/sec; 0.924 sec/batch)
2016-12-08 06:14:49.155213: step 3410, loss = 1.24 (126.8 examples/sec; 1.010 sec/batch)
2016-12-08 06:14:58.090537: step 3420, loss = 1.15 (146.8 examples/sec; 0.872 sec/batch)
2016-12-08 06:15:07.386147: step 3430, loss = 1.26 (136.0 examples/sec; 0.941 sec/batch)
2016-12-08 06:15:17.024588: step 3440, loss = 1.02 (147.1 examples/sec; 0.870 sec/batch)
2016-12-08 06:15:26.138188: step 3450, loss = 1.14 (142.4 examples/sec; 0.899 sec/batch)
2016-12-08 06:15:35.426408: step 3460, loss = 1.01 (126.4 examples/sec; 1.013 sec/batch)
2016-12-08 06:15:44.941349: step 3470, loss = 1.23 (129.0 examples/sec; 0.992 sec/batch)
2016-12-08 06:15:53.966975: step 3480, loss = 1.14 (142.6 examples/sec; 0.898 sec/batch)
2016-12-08 06:16:02.890959: step 3490, loss = 1.28 (155.4 examples/sec; 0.824 sec/batch)
2016-12-08 06:16:12.099043: step 3500, loss = 1.28 (138.1 examples/sec; 0.927 sec/batch)
2016-12-08 06:16:22.056850: step 3510, loss = 1.04 (141.0 examples/sec; 0.908 sec/batch)
2016-12-08 06:16:31.208484: step 3520, loss = 1.02 (141.3 examples/sec; 0.906 sec/batch)
2016-12-08 06:16:40.763627: step 3530, loss = 1.09 (124.1 examples/sec; 1.031 sec/batch)
2016-12-08 06:16:50.320515: step 3540, loss = 1.28 (128.5 examples/sec; 0.996 sec/batch)
2016-12-08 06:16:59.513534: step 3550, loss = 1.09 (147.1 examples/sec; 0.870 sec/batch)
2016-12-08 06:17:08.974584: step 3560, loss = 1.08 (131.8 examples/sec; 0.971 sec/batch)
2016-12-08 06:17:18.353335: step 3570, loss = 1.18 (116.7 examples/sec; 1.097 sec/batch)
2016-12-08 06:17:27.603270: step 3580, loss = 1.18 (141.8 examples/sec; 0.903 sec/batch)
2016-12-08 06:17:36.881268: step 3590, loss = 1.30 (142.3 examples/sec; 0.900 sec/batch)
2016-12-08 06:17:45.957349: step 3600, loss = 1.09 (133.6 examples/sec; 0.958 sec/batch)
2016-12-08 06:17:56.204495: step 3610, loss = 1.25 (123.4 examples/sec; 1.037 sec/batch)
2016-12-08 06:18:05.293075: step 3620, loss = 1.02 (139.3 examples/sec; 0.919 sec/batch)
2016-12-08 06:18:14.128680: step 3630, loss = 0.92 (133.2 examples/sec; 0.961 sec/batch)
2016-12-08 06:18:23.294116: step 3640, loss = 1.07 (140.8 examples/sec; 0.909 sec/batch)
2016-12-08 06:18:32.337188: step 3650, loss = 1.09 (160.1 examples/sec; 0.800 sec/batch)
2016-12-08 06:18:41.281803: step 3660, loss = 1.17 (139.8 examples/sec; 0.916 sec/batch)
2016-12-08 06:18:50.492330: step 3670, loss = 1.01 (141.1 examples/sec; 0.907 sec/batch)
2016-12-08 06:19:00.019324: step 3680, loss = 1.13 (139.5 examples/sec; 0.918 sec/batch)
2016-12-08 06:19:09.181575: step 3690, loss = 1.10 (161.0 examples/sec; 0.795 sec/batch)
2016-12-08 06:19:18.534244: step 3700, loss = 1.08 (154.6 examples/sec; 0.828 sec/batch)
2016-12-08 06:19:28.305884: step 3710, loss = 1.02 (136.9 examples/sec; 0.935 sec/batch)
2016-12-08 06:19:38.150730: step 3720, loss = 1.15 (121.6 examples/sec; 1.053 sec/batch)
2016-12-08 06:19:47.401604: step 3730, loss = 1.06 (128.0 examples/sec; 1.000 sec/batch)
2016-12-08 06:19:56.381112: step 3740, loss = 1.35 (147.6 examples/sec; 0.867 sec/batch)
2016-12-08 06:20:05.659554: step 3750, loss = 1.14 (144.3 examples/sec; 0.887 sec/batch)
2016-12-08 06:20:14.602742: step 3760, loss = 1.06 (143.2 examples/sec; 0.894 sec/batch)
2016-12-08 06:20:23.702404: step 3770, loss = 1.24 (141.3 examples/sec; 0.906 sec/batch)
2016-12-08 06:20:32.633977: step 3780, loss = 1.37 (145.9 examples/sec; 0.877 sec/batch)
2016-12-08 06:20:41.414346: step 3790, loss = 1.27 (141.9 examples/sec; 0.902 sec/batch)
2016-12-08 06:20:51.016228: step 3800, loss = 1.22 (132.7 examples/sec; 0.964 sec/batch)
2016-12-08 06:21:01.069347: step 3810, loss = 1.21 (142.6 examples/sec; 0.897 sec/batch)
2016-12-08 06:21:09.884861: step 3820, loss = 1.10 (148.1 examples/sec; 0.864 sec/batch)
