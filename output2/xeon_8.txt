2016-12-08 03:16:34.978390: step 10, loss = 4.64 (542.7 examples/sec; 0.236 sec/batch)
2016-12-08 03:16:37.410572: step 20, loss = 4.51 (548.9 examples/sec; 0.233 sec/batch)
2016-12-08 03:16:39.671738: step 30, loss = 4.46 (623.1 examples/sec; 0.205 sec/batch)
2016-12-08 03:16:41.875875: step 40, loss = 4.36 (623.8 examples/sec; 0.205 sec/batch)
2016-12-08 03:16:44.089807: step 50, loss = 4.26 (525.4 examples/sec; 0.244 sec/batch)
2016-12-08 03:16:46.302487: step 60, loss = 4.30 (609.3 examples/sec; 0.210 sec/batch)
2016-12-08 03:16:48.537329: step 70, loss = 4.16 (546.0 examples/sec; 0.234 sec/batch)
2016-12-08 03:16:50.804059: step 80, loss = 4.07 (617.0 examples/sec; 0.207 sec/batch)
2016-12-08 03:16:52.967040: step 90, loss = 4.12 (609.9 examples/sec; 0.210 sec/batch)
2016-12-08 03:16:55.177709: step 100, loss = 4.08 (558.2 examples/sec; 0.229 sec/batch)
2016-12-08 03:16:57.566019: step 110, loss = 4.04 (571.8 examples/sec; 0.224 sec/batch)
2016-12-08 03:16:59.766955: step 120, loss = 4.01 (604.7 examples/sec; 0.212 sec/batch)
2016-12-08 03:17:01.939736: step 130, loss = 4.30 (600.6 examples/sec; 0.213 sec/batch)
2016-12-08 03:17:04.037108: step 140, loss = 3.87 (623.4 examples/sec; 0.205 sec/batch)
2016-12-08 03:17:06.206004: step 150, loss = 3.91 (546.7 examples/sec; 0.234 sec/batch)
2016-12-08 03:17:08.327873: step 160, loss = 3.81 (569.0 examples/sec; 0.225 sec/batch)
2016-12-08 03:17:10.425576: step 170, loss = 3.95 (642.8 examples/sec; 0.199 sec/batch)
2016-12-08 03:17:12.550128: step 180, loss = 3.84 (611.1 examples/sec; 0.209 sec/batch)
2016-12-08 03:17:14.664879: step 190, loss = 3.76 (660.4 examples/sec; 0.194 sec/batch)
2016-12-08 03:17:16.736841: step 200, loss = 3.82 (605.4 examples/sec; 0.211 sec/batch)
2016-12-08 03:17:19.079097: step 210, loss = 3.78 (644.4 examples/sec; 0.199 sec/batch)
2016-12-08 03:17:21.193066: step 220, loss = 3.98 (604.3 examples/sec; 0.212 sec/batch)
2016-12-08 03:17:23.283351: step 230, loss = 3.64 (570.3 examples/sec; 0.224 sec/batch)
2016-12-08 03:17:25.504481: step 240, loss = 3.77 (581.9 examples/sec; 0.220 sec/batch)
2016-12-08 03:17:27.658897: step 250, loss = 3.58 (599.9 examples/sec; 0.213 sec/batch)
2016-12-08 03:17:29.809758: step 260, loss = 3.80 (650.3 examples/sec; 0.197 sec/batch)
2016-12-08 03:17:31.954752: step 270, loss = 3.60 (579.0 examples/sec; 0.221 sec/batch)
2016-12-08 03:17:34.039834: step 280, loss = 3.58 (584.6 examples/sec; 0.219 sec/batch)
2016-12-08 03:17:36.183132: step 290, loss = 3.54 (630.3 examples/sec; 0.203 sec/batch)
2016-12-08 03:17:38.380888: step 300, loss = 3.54 (565.9 examples/sec; 0.226 sec/batch)
2016-12-08 03:17:40.756966: step 310, loss = 3.54 (623.4 examples/sec; 0.205 sec/batch)
2016-12-08 03:17:42.881436: step 320, loss = 3.43 (639.1 examples/sec; 0.200 sec/batch)
2016-12-08 03:17:45.049685: step 330, loss = 3.45 (597.1 examples/sec; 0.214 sec/batch)
2016-12-08 03:17:47.192517: step 340, loss = 3.37 (608.7 examples/sec; 0.210 sec/batch)
2016-12-08 03:17:49.266139: step 350, loss = 3.45 (633.9 examples/sec; 0.202 sec/batch)
2016-12-08 03:17:51.364705: step 360, loss = 3.37 (597.0 examples/sec; 0.214 sec/batch)
2016-12-08 03:17:53.542923: step 370, loss = 3.49 (533.3 examples/sec; 0.240 sec/batch)
2016-12-08 03:17:55.673851: step 380, loss = 3.57 (592.8 examples/sec; 0.216 sec/batch)
2016-12-08 03:17:57.791830: step 390, loss = 3.40 (585.9 examples/sec; 0.218 sec/batch)
2016-12-08 03:17:59.879464: step 400, loss = 3.35 (682.3 examples/sec; 0.188 sec/batch)
2016-12-08 03:18:02.241524: step 410, loss = 3.36 (569.0 examples/sec; 0.225 sec/batch)
2016-12-08 03:18:04.360263: step 420, loss = 3.21 (592.2 examples/sec; 0.216 sec/batch)
2016-12-08 03:18:06.478297: step 430, loss = 3.22 (604.9 examples/sec; 0.212 sec/batch)
2016-12-08 03:18:08.752645: step 440, loss = 3.38 (608.1 examples/sec; 0.211 sec/batch)
2016-12-08 03:18:10.828866: step 450, loss = 3.46 (582.7 examples/sec; 0.220 sec/batch)
2016-12-08 03:18:13.026242: step 460, loss = 3.23 (575.1 examples/sec; 0.223 sec/batch)
2016-12-08 03:18:15.214076: step 470, loss = 3.37 (605.8 examples/sec; 0.211 sec/batch)
2016-12-08 03:18:17.389811: step 480, loss = 3.16 (529.7 examples/sec; 0.242 sec/batch)
2016-12-08 03:18:19.563743: step 490, loss = 3.40 (587.6 examples/sec; 0.218 sec/batch)
2016-12-08 03:18:21.632919: step 500, loss = 3.26 (585.9 examples/sec; 0.218 sec/batch)
2016-12-08 03:18:23.976609: step 510, loss = 3.30 (582.3 examples/sec; 0.220 sec/batch)
2016-12-08 03:18:26.095478: step 520, loss = 3.04 (648.7 examples/sec; 0.197 sec/batch)
2016-12-08 03:18:28.206368: step 530, loss = 3.15 (572.3 examples/sec; 0.224 sec/batch)
2016-12-08 03:18:30.349477: step 540, loss = 3.28 (590.6 examples/sec; 0.217 sec/batch)
2016-12-08 03:18:32.518598: step 550, loss = 3.12 (559.8 examples/sec; 0.229 sec/batch)
2016-12-08 03:18:34.638678: step 560, loss = 3.09 (648.3 examples/sec; 0.197 sec/batch)
2016-12-08 03:18:36.707787: step 570, loss = 2.99 (613.9 examples/sec; 0.209 sec/batch)
2016-12-08 03:18:38.842634: step 580, loss = 3.03 (600.3 examples/sec; 0.213 sec/batch)
2016-12-08 03:18:41.006971: step 590, loss = 3.05 (586.7 examples/sec; 0.218 sec/batch)
2016-12-08 03:18:43.109338: step 600, loss = 3.03 (608.9 examples/sec; 0.210 sec/batch)
2016-12-08 03:18:45.531736: step 610, loss = 3.07 (553.7 examples/sec; 0.231 sec/batch)
2016-12-08 03:18:47.700491: step 620, loss = 2.82 (625.8 examples/sec; 0.205 sec/batch)
2016-12-08 03:18:49.812915: step 630, loss = 2.83 (605.6 examples/sec; 0.211 sec/batch)
2016-12-08 03:18:51.919031: step 640, loss = 2.96 (602.2 examples/sec; 0.213 sec/batch)
2016-12-08 03:18:54.009669: step 650, loss = 2.99 (628.4 examples/sec; 0.204 sec/batch)
2016-12-08 03:18:56.193283: step 660, loss = 2.77 (615.4 examples/sec; 0.208 sec/batch)
2016-12-08 03:18:58.294514: step 670, loss = 3.01 (634.3 examples/sec; 0.202 sec/batch)
2016-12-08 03:19:00.375457: step 680, loss = 2.91 (659.2 examples/sec; 0.194 sec/batch)
2016-12-08 03:19:02.470143: step 690, loss = 2.80 (589.9 examples/sec; 0.217 sec/batch)
2016-12-08 03:19:04.536056: step 700, loss = 2.72 (591.7 examples/sec; 0.216 sec/batch)
2016-12-08 03:19:06.922669: step 710, loss = 2.88 (582.4 examples/sec; 0.220 sec/batch)
2016-12-08 03:19:09.039376: step 720, loss = 2.92 (640.9 examples/sec; 0.200 sec/batch)
2016-12-08 03:19:11.110684: step 730, loss = 2.69 (600.6 examples/sec; 0.213 sec/batch)
2016-12-08 03:19:13.279587: step 740, loss = 2.64 (601.1 examples/sec; 0.213 sec/batch)
2016-12-08 03:19:15.409887: step 750, loss = 2.79 (596.8 examples/sec; 0.214 sec/batch)
2016-12-08 03:19:17.556844: step 760, loss = 2.71 (532.6 examples/sec; 0.240 sec/batch)
2016-12-08 03:19:19.636550: step 770, loss = 2.84 (649.3 examples/sec; 0.197 sec/batch)
2016-12-08 03:19:21.760634: step 780, loss = 2.64 (562.3 examples/sec; 0.228 sec/batch)
2016-12-08 03:19:23.993347: step 790, loss = 2.79 (570.9 examples/sec; 0.224 sec/batch)
2016-12-08 03:19:26.023500: step 800, loss = 2.69 (631.4 examples/sec; 0.203 sec/batch)
2016-12-08 03:19:28.487155: step 810, loss = 2.75 (593.1 examples/sec; 0.216 sec/batch)
2016-12-08 03:19:30.499786: step 820, loss = 2.63 (622.6 examples/sec; 0.206 sec/batch)
2016-12-08 03:19:32.601665: step 830, loss = 2.63 (604.8 examples/sec; 0.212 sec/batch)
2016-12-08 03:19:34.698853: step 840, loss = 2.58 (569.1 examples/sec; 0.225 sec/batch)
2016-12-08 03:19:36.800527: step 850, loss = 2.67 (592.1 examples/sec; 0.216 sec/batch)
2016-12-08 03:19:38.901906: step 860, loss = 2.52 (603.1 examples/sec; 0.212 sec/batch)
2016-12-08 03:19:41.042939: step 870, loss = 2.66 (688.4 examples/sec; 0.186 sec/batch)
2016-12-08 03:19:43.230069: step 880, loss = 2.59 (553.6 examples/sec; 0.231 sec/batch)
2016-12-08 03:19:45.360831: step 890, loss = 2.63 (611.3 examples/sec; 0.209 sec/batch)
2016-12-08 03:19:47.479035: step 900, loss = 2.68 (579.2 examples/sec; 0.221 sec/batch)
2016-12-08 03:19:49.807999: step 910, loss = 2.79 (606.8 examples/sec; 0.211 sec/batch)
2016-12-08 03:19:51.833237: step 920, loss = 2.58 (653.7 examples/sec; 0.196 sec/batch)
2016-12-08 03:19:53.858078: step 930, loss = 2.53 (610.9 examples/sec; 0.210 sec/batch)
2016-12-08 03:19:55.908316: step 940, loss = 2.59 (629.7 examples/sec; 0.203 sec/batch)
2016-12-08 03:19:58.100039: step 950, loss = 2.87 (575.7 examples/sec; 0.222 sec/batch)
2016-12-08 03:20:00.219940: step 960, loss = 2.73 (565.0 examples/sec; 0.227 sec/batch)
2016-12-08 03:20:02.300528: step 970, loss = 2.56 (643.6 examples/sec; 0.199 sec/batch)
2016-12-08 03:20:04.403404: step 980, loss = 2.61 (585.1 examples/sec; 0.219 sec/batch)
2016-12-08 03:20:06.479357: step 990, loss = 2.41 (601.1 examples/sec; 0.213 sec/batch)
2016-12-08 03:20:08.577376: step 1000, loss = 2.38 (670.7 examples/sec; 0.191 sec/batch)
2016-12-08 03:20:11.614534: step 1010, loss = 2.38 (580.3 examples/sec; 0.221 sec/batch)
2016-12-08 03:20:13.661469: step 1020, loss = 2.48 (557.3 examples/sec; 0.230 sec/batch)
2016-12-08 03:20:15.685763: step 1030, loss = 2.23 (618.3 examples/sec; 0.207 sec/batch)
2016-12-08 03:20:17.810897: step 1040, loss = 2.33 (608.4 examples/sec; 0.210 sec/batch)
2016-12-08 03:20:19.841315: step 1050, loss = 2.38 (641.3 examples/sec; 0.200 sec/batch)
2016-12-08 03:20:21.989148: step 1060, loss = 2.18 (660.5 examples/sec; 0.194 sec/batch)
2016-12-08 03:20:24.056588: step 1070, loss = 2.44 (617.0 examples/sec; 0.207 sec/batch)
2016-12-08 03:20:26.188469: step 1080, loss = 2.19 (616.7 examples/sec; 0.208 sec/batch)
2016-12-08 03:20:28.297609: step 1090, loss = 2.51 (580.3 examples/sec; 0.221 sec/batch)
2016-12-08 03:20:30.410990: step 1100, loss = 2.23 (596.1 examples/sec; 0.215 sec/batch)
2016-12-08 03:20:32.778462: step 1110, loss = 2.36 (583.0 examples/sec; 0.220 sec/batch)
2016-12-08 03:20:34.837695: step 1120, loss = 2.33 (623.5 examples/sec; 0.205 sec/batch)
2016-12-08 03:20:36.885025: step 1130, loss = 2.11 (635.0 examples/sec; 0.202 sec/batch)
2016-12-08 03:20:38.976786: step 1140, loss = 2.26 (586.7 examples/sec; 0.218 sec/batch)
2016-12-08 03:20:41.145098: step 1150, loss = 2.30 (601.6 examples/sec; 0.213 sec/batch)
2016-12-08 03:20:43.223198: step 1160, loss = 2.17 (624.2 examples/sec; 0.205 sec/batch)
2016-12-08 03:20:45.300893: step 1170, loss = 2.10 (654.6 examples/sec; 0.196 sec/batch)
2016-12-08 03:20:47.482500: step 1180, loss = 2.12 (549.7 examples/sec; 0.233 sec/batch)
2016-12-08 03:20:49.552259: step 1190, loss = 2.11 (584.2 examples/sec; 0.219 sec/batch)
2016-12-08 03:20:51.660961: step 1200, loss = 2.25 (614.9 examples/sec; 0.208 sec/batch)
2016-12-08 03:20:54.027953: step 1210, loss = 2.21 (635.8 examples/sec; 0.201 sec/batch)
2016-12-08 03:20:56.129007: step 1220, loss = 2.33 (611.4 examples/sec; 0.209 sec/batch)
2016-12-08 03:20:58.217646: step 1230, loss = 2.41 (613.8 examples/sec; 0.209 sec/batch)
2016-12-08 03:21:00.323572: step 1240, loss = 2.23 (560.0 examples/sec; 0.229 sec/batch)
2016-12-08 03:21:02.361079: step 1250, loss = 2.11 (613.5 examples/sec; 0.209 sec/batch)
2016-12-08 03:21:04.457664: step 1260, loss = 2.44 (610.8 examples/sec; 0.210 sec/batch)
2016-12-08 03:21:06.510600: step 1270, loss = 2.28 (673.0 examples/sec; 0.190 sec/batch)
2016-12-08 03:21:08.601631: step 1280, loss = 2.11 (630.6 examples/sec; 0.203 sec/batch)
2016-12-08 03:21:10.718182: step 1290, loss = 2.41 (615.3 examples/sec; 0.208 sec/batch)
2016-12-08 03:21:12.770397: step 1300, loss = 1.98 (626.5 examples/sec; 0.204 sec/batch)
2016-12-08 03:21:15.201443: step 1310, loss = 2.04 (585.8 examples/sec; 0.219 sec/batch)
2016-12-08 03:21:17.421394: step 1320, loss = 2.24 (572.4 examples/sec; 0.224 sec/batch)
2016-12-08 03:21:19.552393: step 1330, loss = 2.16 (618.5 examples/sec; 0.207 sec/batch)
2016-12-08 03:21:21.655639: step 1340, loss = 2.22 (577.4 examples/sec; 0.222 sec/batch)
2016-12-08 03:21:23.820943: step 1350, loss = 2.08 (625.8 examples/sec; 0.205 sec/batch)
2016-12-08 03:21:25.920566: step 1360, loss = 1.96 (632.0 examples/sec; 0.203 sec/batch)
2016-12-08 03:21:28.057955: step 1370, loss = 2.16 (590.0 examples/sec; 0.217 sec/batch)
2016-12-08 03:21:30.202368: step 1380, loss = 1.97 (635.9 examples/sec; 0.201 sec/batch)
2016-12-08 03:21:32.309194: step 1390, loss = 2.07 (653.2 examples/sec; 0.196 sec/batch)
2016-12-08 03:21:34.375812: step 1400, loss = 2.01 (604.3 examples/sec; 0.212 sec/batch)
2016-12-08 03:21:36.701173: step 1410, loss = 2.17 (645.2 examples/sec; 0.198 sec/batch)
2016-12-08 03:21:38.828339: step 1420, loss = 2.01 (604.4 examples/sec; 0.212 sec/batch)
2016-12-08 03:21:40.940807: step 1430, loss = 1.90 (611.7 examples/sec; 0.209 sec/batch)
2016-12-08 03:21:42.973470: step 1440, loss = 1.90 (629.7 examples/sec; 0.203 sec/batch)
2016-12-08 03:21:45.055461: step 1450, loss = 2.05 (612.2 examples/sec; 0.209 sec/batch)
2016-12-08 03:21:47.156699: step 1460, loss = 2.05 (597.6 examples/sec; 0.214 sec/batch)
2016-12-08 03:21:49.299141: step 1470, loss = 1.83 (614.2 examples/sec; 0.208 sec/batch)
2016-12-08 03:21:51.463597: step 1480, loss = 1.99 (606.6 examples/sec; 0.211 sec/batch)
2016-12-08 03:21:53.605070: step 1490, loss = 1.96 (606.0 examples/sec; 0.211 sec/batch)
2016-12-08 03:21:55.773292: step 1500, loss = 1.92 (639.1 examples/sec; 0.200 sec/batch)
2016-12-08 03:21:58.018328: step 1510, loss = 1.81 (653.0 examples/sec; 0.196 sec/batch)
2016-12-08 03:22:00.154851: step 1520, loss = 1.85 (600.9 examples/sec; 0.213 sec/batch)
2016-12-08 03:22:02.315562: step 1530, loss = 1.87 (567.3 examples/sec; 0.226 sec/batch)
2016-12-08 03:22:04.426258: step 1540, loss = 1.77 (630.9 examples/sec; 0.203 sec/batch)
2016-12-08 03:22:06.463313: step 1550, loss = 1.76 (640.8 examples/sec; 0.200 sec/batch)
2016-12-08 03:22:08.724407: step 1560, loss = 2.06 (650.2 examples/sec; 0.197 sec/batch)
2016-12-08 03:22:10.881997: step 1570, loss = 1.96 (627.2 examples/sec; 0.204 sec/batch)
2016-12-08 03:22:13.031377: step 1580, loss = 2.06 (561.5 examples/sec; 0.228 sec/batch)
2016-12-08 03:22:15.094770: step 1590, loss = 1.88 (613.6 examples/sec; 0.209 sec/batch)
2016-12-08 03:22:17.206519: step 1600, loss = 1.88 (571.5 examples/sec; 0.224 sec/batch)
2016-12-08 03:22:19.666734: step 1610, loss = 1.81 (640.3 examples/sec; 0.200 sec/batch)
2016-12-08 03:22:21.727490: step 1620, loss = 1.78 (610.1 examples/sec; 0.210 sec/batch)
2016-12-08 03:22:23.812022: step 1630, loss = 1.75 (633.6 examples/sec; 0.202 sec/batch)
2016-12-08 03:22:25.946080: step 1640, loss = 1.97 (621.3 examples/sec; 0.206 sec/batch)
2016-12-08 03:22:28.035348: step 1650, loss = 1.83 (638.0 examples/sec; 0.201 sec/batch)
2016-12-08 03:22:30.098549: step 1660, loss = 1.83 (636.3 examples/sec; 0.201 sec/batch)
2016-12-08 03:22:32.223095: step 1670, loss = 1.94 (602.9 examples/sec; 0.212 sec/batch)
2016-12-08 03:22:34.327725: step 1680, loss = 1.84 (628.0 examples/sec; 0.204 sec/batch)
2016-12-08 03:22:36.373961: step 1690, loss = 1.94 (611.4 examples/sec; 0.209 sec/batch)
2016-12-08 03:22:38.397476: step 1700, loss = 2.00 (649.4 examples/sec; 0.197 sec/batch)
2016-12-08 03:22:40.809909: step 1710, loss = 1.65 (554.3 examples/sec; 0.231 sec/batch)
2016-12-08 03:22:42.965585: step 1720, loss = 1.76 (652.7 examples/sec; 0.196 sec/batch)
2016-12-08 03:22:45.138494: step 1730, loss = 1.75 (546.5 examples/sec; 0.234 sec/batch)
2016-12-08 03:22:47.315633: step 1740, loss = 1.73 (571.6 examples/sec; 0.224 sec/batch)
2016-12-08 03:22:49.408003: step 1750, loss = 1.74 (615.1 examples/sec; 0.208 sec/batch)
2016-12-08 03:22:51.501497: step 1760, loss = 1.54 (676.8 examples/sec; 0.189 sec/batch)
2016-12-08 03:22:53.558652: step 1770, loss = 1.76 (626.9 examples/sec; 0.204 sec/batch)
2016-12-08 03:22:55.687198: step 1780, loss = 1.73 (592.9 examples/sec; 0.216 sec/batch)
2016-12-08 03:22:57.869419: step 1790, loss = 1.71 (584.9 examples/sec; 0.219 sec/batch)
2016-12-08 03:23:00.051633: step 1800, loss = 1.75 (598.5 examples/sec; 0.214 sec/batch)
2016-12-08 03:23:02.476656: step 1810, loss = 1.74 (564.8 examples/sec; 0.227 sec/batch)
2016-12-08 03:23:04.623812: step 1820, loss = 1.78 (547.7 examples/sec; 0.234 sec/batch)
2016-12-08 03:23:06.773024: step 1830, loss = 1.75 (620.3 examples/sec; 0.206 sec/batch)
2016-12-08 03:23:08.906615: step 1840, loss = 1.67 (634.2 examples/sec; 0.202 sec/batch)
2016-12-08 03:23:10.972075: step 1850, loss = 1.77 (621.8 examples/sec; 0.206 sec/batch)
2016-12-08 03:23:13.105213: step 1860, loss = 1.73 (633.3 examples/sec; 0.202 sec/batch)
2016-12-08 03:23:15.273332: step 1870, loss = 1.79 (616.8 examples/sec; 0.208 sec/batch)
2016-12-08 03:23:17.382484: step 1880, loss = 1.72 (692.6 examples/sec; 0.185 sec/batch)
2016-12-08 03:23:19.442886: step 1890, loss = 1.88 (600.8 examples/sec; 0.213 sec/batch)
2016-12-08 03:23:21.596151: step 1900, loss = 1.63 (607.2 examples/sec; 0.211 sec/batch)
2016-12-08 03:23:23.941277: step 1910, loss = 1.46 (653.6 examples/sec; 0.196 sec/batch)
2016-12-08 03:23:26.012770: step 1920, loss = 1.76 (630.1 examples/sec; 0.203 sec/batch)
2016-12-08 03:23:28.063873: step 1930, loss = 1.92 (623.3 examples/sec; 0.205 sec/batch)
2016-12-08 03:23:30.186880: step 1940, loss = 1.60 (567.9 examples/sec; 0.225 sec/batch)
2016-12-08 03:23:32.241492: step 1950, loss = 1.69 (580.3 examples/sec; 0.221 sec/batch)
2016-12-08 03:23:34.306862: step 1960, loss = 1.63 (602.5 examples/sec; 0.212 sec/batch)
2016-12-08 03:23:36.474257: step 1970, loss = 1.82 (609.6 examples/sec; 0.210 sec/batch)
2016-12-08 03:23:38.555412: step 1980, loss = 1.82 (546.4 examples/sec; 0.234 sec/batch)
2016-12-08 03:23:40.707839: step 1990, loss = 1.69 (611.7 examples/sec; 0.209 sec/batch)
2016-12-08 03:23:42.889008: step 2000, loss = 1.68 (620.9 examples/sec; 0.206 sec/batch)
2016-12-08 03:23:45.827846: step 2010, loss = 1.64 (602.9 examples/sec; 0.212 sec/batch)
2016-12-08 03:23:47.955908: step 2020, loss = 1.75 (623.8 examples/sec; 0.205 sec/batch)
2016-12-08 03:23:50.033267: step 2030, loss = 1.64 (629.1 examples/sec; 0.203 sec/batch)
2016-12-08 03:23:52.115163: step 2040, loss = 1.61 (627.8 examples/sec; 0.204 sec/batch)
2016-12-08 03:23:54.160694: step 2050, loss = 1.61 (611.1 examples/sec; 0.209 sec/batch)
2016-12-08 03:23:56.255203: step 2060, loss = 1.92 (587.2 examples/sec; 0.218 sec/batch)
2016-12-08 03:23:58.307554: step 2070, loss = 1.74 (605.0 examples/sec; 0.212 sec/batch)
2016-12-08 03:24:00.533972: step 2080, loss = 1.61 (613.9 examples/sec; 0.209 sec/batch)
2016-12-08 03:24:02.609383: step 2090, loss = 1.57 (637.1 examples/sec; 0.201 sec/batch)
2016-12-08 03:24:04.656602: step 2100, loss = 1.64 (620.7 examples/sec; 0.206 sec/batch)
2016-12-08 03:24:07.064049: step 2110, loss = 1.45 (587.7 examples/sec; 0.218 sec/batch)
2016-12-08 03:24:09.208116: step 2120, loss = 1.67 (620.9 examples/sec; 0.206 sec/batch)
2016-12-08 03:24:11.357606: step 2130, loss = 1.58 (604.9 examples/sec; 0.212 sec/batch)
2016-12-08 03:24:13.514759: step 2140, loss = 1.40 (693.7 examples/sec; 0.185 sec/batch)
2016-12-08 03:24:15.644540: step 2150, loss = 1.72 (615.4 examples/sec; 0.208 sec/batch)
2016-12-08 03:24:17.703738: step 2160, loss = 1.56 (604.5 examples/sec; 0.212 sec/batch)
2016-12-08 03:24:19.771855: step 2170, loss = 1.48 (616.7 examples/sec; 0.208 sec/batch)
2016-12-08 03:24:21.921271: step 2180, loss = 1.49 (590.1 examples/sec; 0.217 sec/batch)
2016-12-08 03:24:24.039106: step 2190, loss = 1.53 (609.0 examples/sec; 0.210 sec/batch)
2016-12-08 03:24:26.162193: step 2200, loss = 1.60 (664.9 examples/sec; 0.193 sec/batch)
2016-12-08 03:24:28.527941: step 2210, loss = 1.57 (631.5 examples/sec; 0.203 sec/batch)
2016-12-08 03:24:30.666194: step 2220, loss = 1.55 (612.4 examples/sec; 0.209 sec/batch)
2016-12-08 03:24:32.854192: step 2230, loss = 1.72 (613.7 examples/sec; 0.209 sec/batch)
2016-12-08 03:24:35.028913: step 2240, loss = 1.44 (597.1 examples/sec; 0.214 sec/batch)
2016-12-08 03:24:37.158277: step 2250, loss = 1.38 (610.3 examples/sec; 0.210 sec/batch)
2016-12-08 03:24:39.312840: step 2260, loss = 1.41 (586.1 examples/sec; 0.218 sec/batch)
2016-12-08 03:24:41.397287: step 2270, loss = 1.49 (596.9 examples/sec; 0.214 sec/batch)
2016-12-08 03:24:43.548540: step 2280, loss = 1.39 (630.5 examples/sec; 0.203 sec/batch)
2016-12-08 03:24:45.611914: step 2290, loss = 1.50 (609.4 examples/sec; 0.210 sec/batch)
2016-12-08 03:24:47.730038: step 2300, loss = 1.62 (632.1 examples/sec; 0.203 sec/batch)
2016-12-08 03:24:50.018689: step 2310, loss = 1.51 (684.2 examples/sec; 0.187 sec/batch)
2016-12-08 03:24:52.073832: step 2320, loss = 1.56 (626.8 examples/sec; 0.204 sec/batch)
2016-12-08 03:24:54.137252: step 2330, loss = 1.22 (595.7 examples/sec; 0.215 sec/batch)
2016-12-08 03:24:56.260623: step 2340, loss = 1.53 (611.0 examples/sec; 0.209 sec/batch)
2016-12-08 03:24:58.347921: step 2350, loss = 1.33 (603.9 examples/sec; 0.212 sec/batch)
2016-12-08 03:25:00.416853: step 2360, loss = 1.46 (634.6 examples/sec; 0.202 sec/batch)
2016-12-08 03:25:02.484100: step 2370, loss = 1.54 (655.7 examples/sec; 0.195 sec/batch)
2016-12-08 03:25:04.573237: step 2380, loss = 1.36 (617.2 examples/sec; 0.207 sec/batch)
2016-12-08 03:25:06.605687: step 2390, loss = 1.50 (596.6 examples/sec; 0.215 sec/batch)
2016-12-08 03:25:08.674982: step 2400, loss = 1.43 (631.8 examples/sec; 0.203 sec/batch)
2016-12-08 03:25:11.047601: step 2410, loss = 1.52 (622.5 examples/sec; 0.206 sec/batch)
2016-12-08 03:25:13.173473: step 2420, loss = 1.35 (629.0 examples/sec; 0.204 sec/batch)
2016-12-08 03:25:15.211130: step 2430, loss = 1.30 (648.1 examples/sec; 0.197 sec/batch)
2016-12-08 03:25:17.307034: step 2440, loss = 1.45 (598.4 examples/sec; 0.214 sec/batch)
2016-12-08 03:25:19.389784: step 2450, loss = 1.32 (600.1 examples/sec; 0.213 sec/batch)
2016-12-08 03:25:21.411191: step 2460, loss = 1.38 (618.3 examples/sec; 0.207 sec/batch)
2016-12-08 03:25:23.481590: step 2470, loss = 1.43 (594.3 examples/sec; 0.215 sec/batch)
2016-12-08 03:25:25.614785: step 2480, loss = 1.22 (623.3 examples/sec; 0.205 sec/batch)
2016-12-08 03:25:27.717044: step 2490, loss = 1.45 (566.2 examples/sec; 0.226 sec/batch)
2016-12-08 03:25:29.809929: step 2500, loss = 1.44 (606.5 examples/sec; 0.211 sec/batch)
2016-12-08 03:25:32.228701: step 2510, loss = 1.43 (642.9 examples/sec; 0.199 sec/batch)
2016-12-08 03:25:34.285935: step 2520, loss = 1.48 (670.3 examples/sec; 0.191 sec/batch)
2016-12-08 03:25:36.427627: step 2530, loss = 1.49 (640.0 examples/sec; 0.200 sec/batch)
2016-12-08 03:25:38.534034: step 2540, loss = 1.51 (627.4 examples/sec; 0.204 sec/batch)
2016-12-08 03:25:40.628635: step 2550, loss = 1.26 (586.5 examples/sec; 0.218 sec/batch)
2016-12-08 03:25:42.666879: step 2560, loss = 1.38 (649.4 examples/sec; 0.197 sec/batch)
2016-12-08 03:25:44.753587: step 2570, loss = 1.39 (578.1 examples/sec; 0.221 sec/batch)
2016-12-08 03:25:46.874304: step 2580, loss = 1.27 (666.7 examples/sec; 0.192 sec/batch)
2016-12-08 03:25:48.955386: step 2590, loss = 1.21 (622.2 examples/sec; 0.206 sec/batch)
2016-12-08 03:25:50.969385: step 2600, loss = 1.28 (612.5 examples/sec; 0.209 sec/batch)
2016-12-08 03:25:53.355288: step 2610, loss = 1.30 (660.4 examples/sec; 0.194 sec/batch)
2016-12-08 03:25:55.423259: step 2620, loss = 1.66 (615.8 examples/sec; 0.208 sec/batch)
2016-12-08 03:25:57.517035: step 2630, loss = 1.33 (562.7 examples/sec; 0.227 sec/batch)
2016-12-08 03:25:59.647606: step 2640, loss = 1.37 (590.8 examples/sec; 0.217 sec/batch)
2016-12-08 03:26:01.724446: step 2650, loss = 1.46 (609.9 examples/sec; 0.210 sec/batch)
2016-12-08 03:26:03.810189: step 2660, loss = 1.16 (621.7 examples/sec; 0.206 sec/batch)
2016-12-08 03:26:05.964378: step 2670, loss = 1.48 (561.8 examples/sec; 0.228 sec/batch)
2016-12-08 03:26:08.143381: step 2680, loss = 1.38 (587.1 examples/sec; 0.218 sec/batch)
2016-12-08 03:26:10.296956: step 2690, loss = 1.28 (564.7 examples/sec; 0.227 sec/batch)
2016-12-08 03:26:12.416788: step 2700, loss = 1.27 (589.6 examples/sec; 0.217 sec/batch)
2016-12-08 03:26:14.856582: step 2710, loss = 1.49 (585.2 examples/sec; 0.219 sec/batch)
2016-12-08 03:26:17.045922: step 2720, loss = 1.52 (614.5 examples/sec; 0.208 sec/batch)
2016-12-08 03:26:19.124059: step 2730, loss = 1.38 (633.0 examples/sec; 0.202 sec/batch)
2016-12-08 03:26:21.218104: step 2740, loss = 1.39 (594.0 examples/sec; 0.215 sec/batch)
2016-12-08 03:26:23.397645: step 2750, loss = 1.18 (541.1 examples/sec; 0.237 sec/batch)
2016-12-08 03:26:25.513265: step 2760, loss = 1.37 (603.2 examples/sec; 0.212 sec/batch)
2016-12-08 03:26:27.635874: step 2770, loss = 1.37 (597.3 examples/sec; 0.214 sec/batch)
2016-12-08 03:26:29.723979: step 2780, loss = 1.18 (621.6 examples/sec; 0.206 sec/batch)
2016-12-08 03:26:31.804278: step 2790, loss = 1.40 (660.1 examples/sec; 0.194 sec/batch)
2016-12-08 03:26:33.908853: step 2800, loss = 1.42 (594.8 examples/sec; 0.215 sec/batch)
2016-12-08 03:26:36.210144: step 2810, loss = 1.42 (624.7 examples/sec; 0.205 sec/batch)
2016-12-08 03:26:38.281178: step 2820, loss = 1.32 (607.0 examples/sec; 0.211 sec/batch)
2016-12-08 03:26:40.385357: step 2830, loss = 1.22 (649.5 examples/sec; 0.197 sec/batch)
2016-12-08 03:26:42.440179: step 2840, loss = 1.30 (585.9 examples/sec; 0.218 sec/batch)
2016-12-08 03:26:44.595022: step 2850, loss = 1.27 (679.2 examples/sec; 0.188 sec/batch)
2016-12-08 03:26:46.692267: step 2860, loss = 1.35 (624.0 examples/sec; 0.205 sec/batch)
2016-12-08 03:26:48.801256: step 2870, loss = 1.16 (604.0 examples/sec; 0.212 sec/batch)
2016-12-08 03:26:50.886317: step 2880, loss = 1.14 (647.6 examples/sec; 0.198 sec/batch)
2016-12-08 03:26:52.973529: step 2890, loss = 1.23 (640.8 examples/sec; 0.200 sec/batch)
2016-12-08 03:26:55.129758: step 2900, loss = 1.32 (634.3 examples/sec; 0.202 sec/batch)
2016-12-08 03:26:57.440686: step 2910, loss = 1.19 (625.6 examples/sec; 0.205 sec/batch)
2016-12-08 03:26:59.521847: step 2920, loss = 1.11 (653.3 examples/sec; 0.196 sec/batch)
2016-12-08 03:27:01.671245: step 2930, loss = 1.22 (649.7 examples/sec; 0.197 sec/batch)
2016-12-08 03:27:03.762733: step 2940, loss = 1.23 (629.8 examples/sec; 0.203 sec/batch)
2016-12-08 03:27:05.869947: step 2950, loss = 1.26 (610.5 examples/sec; 0.210 sec/batch)
2016-12-08 03:27:07.910616: step 2960, loss = 1.29 (637.9 examples/sec; 0.201 sec/batch)
2016-12-08 03:27:10.022120: step 2970, loss = 1.20 (644.2 examples/sec; 0.199 sec/batch)
2016-12-08 03:27:12.205563: step 2980, loss = 1.21 (580.5 examples/sec; 0.221 sec/batch)
2016-12-08 03:27:14.262928: step 2990, loss = 1.32 (617.7 examples/sec; 0.207 sec/batch)
2016-12-08 03:27:16.440599: step 3000, loss = 1.23 (551.1 examples/sec; 0.232 sec/batch)
2016-12-08 03:27:19.439818: step 3010, loss = 1.21 (622.2 examples/sec; 0.206 sec/batch)
2016-12-08 03:27:21.556582: step 3020, loss = 1.36 (581.5 examples/sec; 0.220 sec/batch)
2016-12-08 03:27:23.675746: step 3030, loss = 1.30 (603.7 examples/sec; 0.212 sec/batch)
2016-12-08 03:27:25.697916: step 3040, loss = 1.35 (615.1 examples/sec; 0.208 sec/batch)
2016-12-08 03:27:27.773067: step 3050, loss = 1.18 (604.7 examples/sec; 0.212 sec/batch)
2016-12-08 03:27:29.875071: step 3060, loss = 1.30 (630.7 examples/sec; 0.203 sec/batch)
2016-12-08 03:27:32.038455: step 3070, loss = 1.42 (628.8 examples/sec; 0.204 sec/batch)
2016-12-08 03:27:34.161038: step 3080, loss = 1.37 (584.8 examples/sec; 0.219 sec/batch)
2016-12-08 03:27:36.258484: step 3090, loss = 1.32 (608.1 examples/sec; 0.210 sec/batch)
2016-12-08 03:27:38.278963: step 3100, loss = 1.23 (600.9 examples/sec; 0.213 sec/batch)
2016-12-08 03:27:40.641022: step 3110, loss = 1.20 (598.8 examples/sec; 0.214 sec/batch)
2016-12-08 03:27:42.787125: step 3120, loss = 1.34 (545.4 examples/sec; 0.235 sec/batch)
2016-12-08 03:27:44.890416: step 3130, loss = 1.21 (630.3 examples/sec; 0.203 sec/batch)
2016-12-08 03:27:47.039118: step 3140, loss = 1.33 (603.7 examples/sec; 0.212 sec/batch)
2016-12-08 03:27:49.206716: step 3150, loss = 1.37 (583.0 examples/sec; 0.220 sec/batch)
2016-12-08 03:27:51.357681: step 3160, loss = 1.19 (557.4 examples/sec; 0.230 sec/batch)
2016-12-08 03:27:53.483205: step 3170, loss = 1.24 (608.6 examples/sec; 0.210 sec/batch)
2016-12-08 03:27:55.586195: step 3180, loss = 1.29 (628.2 examples/sec; 0.204 sec/batch)
2016-12-08 03:27:57.673391: step 3190, loss = 1.31 (608.3 examples/sec; 0.210 sec/batch)
2016-12-08 03:27:59.722525: step 3200, loss = 1.26 (618.5 examples/sec; 0.207 sec/batch)
2016-12-08 03:28:02.106369: step 3210, loss = 1.21 (608.5 examples/sec; 0.210 sec/batch)
2016-12-08 03:28:04.215416: step 3220, loss = 1.00 (605.3 examples/sec; 0.211 sec/batch)
2016-12-08 03:28:06.397308: step 3230, loss = 1.37 (637.2 examples/sec; 0.201 sec/batch)
2016-12-08 03:28:08.565382: step 3240, loss = 1.23 (552.6 examples/sec; 0.232 sec/batch)
2016-12-08 03:28:10.673218: step 3250, loss = 1.20 (589.6 examples/sec; 0.217 sec/batch)
2016-12-08 03:28:12.739810: step 3260, loss = 1.14 (616.1 examples/sec; 0.208 sec/batch)
2016-12-08 03:28:14.770949: step 3270, loss = 1.22 (619.0 examples/sec; 0.207 sec/batch)
2016-12-08 03:28:16.908599: step 3280, loss = 1.06 (613.6 examples/sec; 0.209 sec/batch)
2016-12-08 03:28:19.097642: step 3290, loss = 1.25 (592.1 examples/sec; 0.216 sec/batch)
2016-12-08 03:28:21.241792: step 3300, loss = 1.24 (672.7 examples/sec; 0.190 sec/batch)
2016-12-08 03:28:23.556616: step 3310, loss = 1.32 (632.0 examples/sec; 0.203 sec/batch)
2016-12-08 03:28:25.620212: step 3320, loss = 1.14 (582.2 examples/sec; 0.220 sec/batch)
2016-12-08 03:28:27.729302: step 3330, loss = 1.01 (612.8 examples/sec; 0.209 sec/batch)
2016-12-08 03:28:29.897444: step 3340, loss = 1.20 (614.5 examples/sec; 0.208 sec/batch)
2016-12-08 03:28:31.976251: step 3350, loss = 1.19 (633.5 examples/sec; 0.202 sec/batch)
2016-12-08 03:28:34.003688: step 3360, loss = 1.06 (655.9 examples/sec; 0.195 sec/batch)
2016-12-08 03:28:36.066517: step 3370, loss = 1.19 (657.1 examples/sec; 0.195 sec/batch)
2016-12-08 03:28:38.117461: step 3380, loss = 1.23 (675.9 examples/sec; 0.189 sec/batch)
2016-12-08 03:28:40.139752: step 3390, loss = 1.17 (623.4 examples/sec; 0.205 sec/batch)
2016-12-08 03:28:42.266847: step 3400, loss = 1.36 (618.6 examples/sec; 0.207 sec/batch)
2016-12-08 03:28:44.588412: step 3410, loss = 1.03 (632.7 examples/sec; 0.202 sec/batch)
2016-12-08 03:28:46.756269: step 3420, loss = 1.20 (596.2 examples/sec; 0.215 sec/batch)
2016-12-08 03:28:48.930707: step 3430, loss = 1.13 (603.3 examples/sec; 0.212 sec/batch)
2016-12-08 03:28:51.017658: step 3440, loss = 1.08 (615.6 examples/sec; 0.208 sec/batch)
2016-12-08 03:28:53.206940: step 3450, loss = 1.19 (641.1 examples/sec; 0.200 sec/batch)
2016-12-08 03:28:55.300809: step 3460, loss = 1.26 (629.8 examples/sec; 0.203 sec/batch)
2016-12-08 03:28:57.374899: step 3470, loss = 1.06 (584.6 examples/sec; 0.219 sec/batch)
2016-12-08 03:28:59.452774: step 3480, loss = 1.20 (623.2 examples/sec; 0.205 sec/batch)
2016-12-08 03:29:01.619483: step 3490, loss = 1.08 (619.2 examples/sec; 0.207 sec/batch)
2016-12-08 03:29:03.653842: step 3500, loss = 1.05 (689.8 examples/sec; 0.186 sec/batch)
2016-12-08 03:29:05.957460: step 3510, loss = 1.19 (660.8 examples/sec; 0.194 sec/batch)
2016-12-08 03:29:08.089266: step 3520, loss = 1.16 (606.6 examples/sec; 0.211 sec/batch)
2016-12-08 03:29:10.237670: step 3530, loss = 1.26 (626.8 examples/sec; 0.204 sec/batch)
2016-12-08 03:29:12.292545: step 3540, loss = 1.06 (576.5 examples/sec; 0.222 sec/batch)
2016-12-08 03:29:14.423969: step 3550, loss = 1.09 (594.4 examples/sec; 0.215 sec/batch)
2016-12-08 03:29:16.559545: step 3560, loss = 1.19 (630.5 examples/sec; 0.203 sec/batch)
2016-12-08 03:29:18.714280: step 3570, loss = 1.37 (619.1 examples/sec; 0.207 sec/batch)
2016-12-08 03:29:20.868257: step 3580, loss = 1.33 (547.7 examples/sec; 0.234 sec/batch)
2016-12-08 03:29:22.932445: step 3590, loss = 1.33 (631.4 examples/sec; 0.203 sec/batch)
2016-12-08 03:29:25.033120: step 3600, loss = 1.18 (613.8 examples/sec; 0.209 sec/batch)
2016-12-08 03:29:27.430664: step 3610, loss = 1.30 (550.0 examples/sec; 0.233 sec/batch)
2016-12-08 03:29:29.568844: step 3620, loss = 1.28 (596.3 examples/sec; 0.215 sec/batch)
2016-12-08 03:29:31.697246: step 3630, loss = 1.06 (597.0 examples/sec; 0.214 sec/batch)
2016-12-08 03:29:33.789639: step 3640, loss = 1.22 (600.8 examples/sec; 0.213 sec/batch)
2016-12-08 03:29:35.873057: step 3650, loss = 1.23 (611.6 examples/sec; 0.209 sec/batch)
2016-12-08 03:29:37.918122: step 3660, loss = 1.14 (560.0 examples/sec; 0.229 sec/batch)
2016-12-08 03:29:40.026619: step 3670, loss = 1.25 (669.5 examples/sec; 0.191 sec/batch)
2016-12-08 03:29:42.105366: step 3680, loss = 1.18 (571.0 examples/sec; 0.224 sec/batch)
2016-12-08 03:29:44.213255: step 3690, loss = 1.13 (643.0 examples/sec; 0.199 sec/batch)
2016-12-08 03:29:46.263704: step 3700, loss = 1.11 (634.0 examples/sec; 0.202 sec/batch)
2016-12-08 03:29:48.639529: step 3710, loss = 1.11 (619.4 examples/sec; 0.207 sec/batch)
2016-12-08 03:29:50.808315: step 3720, loss = 1.11 (648.5 examples/sec; 0.197 sec/batch)
2016-12-08 03:29:52.895391: step 3730, loss = 1.15 (629.3 examples/sec; 0.203 sec/batch)
2016-12-08 03:29:55.012413: step 3740, loss = 1.21 (637.8 examples/sec; 0.201 sec/batch)
2016-12-08 03:29:57.102401: step 3750, loss = 0.93 (629.4 examples/sec; 0.203 sec/batch)
2016-12-08 03:29:59.288491: step 3760, loss = 1.12 (610.0 examples/sec; 0.210 sec/batch)
2016-12-08 03:30:01.439091: step 3770, loss = 1.06 (584.6 examples/sec; 0.219 sec/batch)
2016-12-08 03:30:03.584932: step 3780, loss = 1.26 (608.4 examples/sec; 0.210 sec/batch)
2016-12-08 03:30:05.619819: step 3790, loss = 1.15 (562.9 examples/sec; 0.227 sec/batch)
2016-12-08 03:30:07.695031: step 3800, loss = 1.06 (611.7 examples/sec; 0.209 sec/batch)
2016-12-08 03:30:10.100171: step 3810, loss = 1.08 (569.5 examples/sec; 0.225 sec/batch)
2016-12-08 03:30:12.195512: step 3820, loss = 1.11 (681.3 examples/sec; 0.188 sec/batch)
2016-12-08 03:30:14.369285: step 3830, loss = 1.34 (512.7 examples/sec; 0.250 sec/batch)
2016-12-08 03:30:16.457979: step 3840, loss = 1.20 (610.0 examples/sec; 0.210 sec/batch)
2016-12-08 03:30:18.541646: step 3850, loss = 1.02 (695.4 examples/sec; 0.184 sec/batch)
2016-12-08 03:30:20.639096: step 3860, loss = 1.02 (652.6 examples/sec; 0.196 sec/batch)
2016-12-08 03:30:22.697895: step 3870, loss = 1.08 (618.3 examples/sec; 0.207 sec/batch)
2016-12-08 03:30:24.772134: step 3880, loss = 1.01 (639.4 examples/sec; 0.200 sec/batch)
2016-12-08 03:30:26.934661: step 3890, loss = 1.16 (621.0 examples/sec; 0.206 sec/batch)
2016-12-08 03:30:29.080984: step 3900, loss = 1.17 (557.6 examples/sec; 0.230 sec/batch)
2016-12-08 03:30:31.403437: step 3910, loss = 1.02 (597.6 examples/sec; 0.214 sec/batch)
2016-12-08 03:30:33.514866: step 3920, loss = 1.31 (631.2 examples/sec; 0.203 sec/batch)
2016-12-08 03:30:35.598793: step 3930, loss = 1.21 (641.9 examples/sec; 0.199 sec/batch)
2016-12-08 03:30:37.698638: step 3940, loss = 1.16 (622.4 examples/sec; 0.206 sec/batch)
2016-12-08 03:30:39.820226: step 3950, loss = 1.21 (610.0 examples/sec; 0.210 sec/batch)
2016-12-08 03:30:41.923096: step 3960, loss = 1.08 (578.5 examples/sec; 0.221 sec/batch)
2016-12-08 03:30:44.015293: step 3970, loss = 1.02 (573.3 examples/sec; 0.223 sec/batch)
2016-12-08 03:30:46.211576: step 3980, loss = 1.36 (597.4 examples/sec; 0.214 sec/batch)
2016-12-08 03:30:48.313012: step 3990, loss = 0.92 (618.7 examples/sec; 0.207 sec/batch)
2016-12-08 03:30:50.430720: step 4000, loss = 1.11 (582.2 examples/sec; 0.220 sec/batch)
2016-12-08 03:30:53.390366: step 4010, loss = 1.01 (628.3 examples/sec; 0.204 sec/batch)
2016-12-08 03:30:55.429834: step 4020, loss = 1.06 (650.2 examples/sec; 0.197 sec/batch)
2016-12-08 03:30:57.600106: step 4030, loss = 0.99 (577.6 examples/sec; 0.222 sec/batch)
2016-12-08 03:30:59.724930: step 4040, loss = 1.24 (580.0 examples/sec; 0.221 sec/batch)
2016-12-08 03:31:01.811563: step 4050, loss = 1.17 (600.9 examples/sec; 0.213 sec/batch)
2016-12-08 03:31:03.886249: step 4060, loss = 1.14 (627.1 examples/sec; 0.204 sec/batch)
2016-12-08 03:31:05.965746: step 4070, loss = 1.10 (572.9 examples/sec; 0.223 sec/batch)
2016-12-08 03:31:08.047462: step 4080, loss = 1.08 (665.2 examples/sec; 0.192 sec/batch)
2016-12-08 03:31:10.121883: step 4090, loss = 1.10 (630.6 examples/sec; 0.203 sec/batch)
2016-12-08 03:31:12.196520: step 4100, loss = 1.19 (632.3 examples/sec; 0.202 sec/batch)
2016-12-08 03:31:14.467315: step 4110, loss = 0.97 (593.5 examples/sec; 0.216 sec/batch)
2016-12-08 03:31:16.570734: step 4120, loss = 0.93 (555.5 examples/sec; 0.230 sec/batch)
2016-12-08 03:31:18.663594: step 4130, loss = 1.03 (605.9 examples/sec; 0.211 sec/batch)
2016-12-08 03:31:20.825075: step 4140, loss = 0.95 (592.3 examples/sec; 0.216 sec/batch)
2016-12-08 03:31:22.956919: step 4150, loss = 0.99 (598.6 examples/sec; 0.214 sec/batch)
2016-12-08 03:31:25.089889: step 4160, loss = 1.29 (626.2 examples/sec; 0.204 sec/batch)
2016-12-08 03:31:27.242634: step 4170, loss = 1.06 (612.7 examples/sec; 0.209 sec/batch)
2016-12-08 03:31:29.369603: step 4180, loss = 1.08 (592.2 examples/sec; 0.216 sec/batch)
2016-12-08 03:31:31.472311: step 4190, loss = 1.10 (571.6 examples/sec; 0.224 sec/batch)
2016-12-08 03:31:33.571901: step 4200, loss = 0.99 (626.7 examples/sec; 0.204 sec/batch)
2016-12-08 03:31:35.900398: step 4210, loss = 1.00 (612.1 examples/sec; 0.209 sec/batch)
2016-12-08 03:31:37.986995: step 4220, loss = 0.99 (556.2 examples/sec; 0.230 sec/batch)
2016-12-08 03:31:40.154947: step 4230, loss = 1.07 (597.5 examples/sec; 0.214 sec/batch)
2016-12-08 03:31:42.204058: step 4240, loss = 1.08 (672.1 examples/sec; 0.190 sec/batch)
2016-12-08 03:31:44.333541: step 4250, loss = 1.00 (576.4 examples/sec; 0.222 sec/batch)
2016-12-08 03:31:46.393692: step 4260, loss = 1.05 (646.4 examples/sec; 0.198 sec/batch)
2016-12-08 03:31:48.449990: step 4270, loss = 0.82 (620.2 examples/sec; 0.206 sec/batch)
2016-12-08 03:31:50.510019: step 4280, loss = 1.23 (576.4 examples/sec; 0.222 sec/batch)
2016-12-08 03:31:52.526865: step 4290, loss = 1.04 (690.8 examples/sec; 0.185 sec/batch)
2016-12-08 03:31:54.577888: step 4300, loss = 1.17 (620.3 examples/sec; 0.206 sec/batch)
2016-12-08 03:31:56.838924: step 4310, loss = 0.99 (621.5 examples/sec; 0.206 sec/batch)
2016-12-08 03:31:58.891223: step 4320, loss = 1.10 (619.0 examples/sec; 0.207 sec/batch)
2016-12-08 03:32:00.975487: step 4330, loss = 0.95 (628.4 examples/sec; 0.204 sec/batch)
2016-12-08 03:32:03.028448: step 4340, loss = 1.11 (627.6 examples/sec; 0.204 sec/batch)
2016-12-08 03:32:05.149112: step 4350, loss = 1.06 (635.4 examples/sec; 0.201 sec/batch)
2016-12-08 03:32:07.176948: step 4360, loss = 1.00 (594.4 examples/sec; 0.215 sec/batch)
2016-12-08 03:32:09.280334: step 4370, loss = 1.00 (561.8 examples/sec; 0.228 sec/batch)
2016-12-08 03:32:11.383976: step 4380, loss = 1.06 (607.4 examples/sec; 0.211 sec/batch)
2016-12-08 03:32:13.382560: step 4390, loss = 1.06 (637.8 examples/sec; 0.201 sec/batch)
2016-12-08 03:32:15.466374: step 4400, loss = 1.09 (560.1 examples/sec; 0.229 sec/batch)
2016-12-08 03:32:17.838505: step 4410, loss = 1.11 (661.6 examples/sec; 0.193 sec/batch)
2016-12-08 03:32:19.985151: step 4420, loss = 1.00 (571.9 examples/sec; 0.224 sec/batch)
2016-12-08 03:32:22.033421: step 4430, loss = 1.24 (594.8 examples/sec; 0.215 sec/batch)
2016-12-08 03:32:24.165649: step 4440, loss = 1.43 (564.4 examples/sec; 0.227 sec/batch)
2016-12-08 03:32:26.348420: step 4450, loss = 1.08 (564.8 examples/sec; 0.227 sec/batch)
2016-12-08 03:32:28.441923: step 4460, loss = 1.06 (574.6 examples/sec; 0.223 sec/batch)
2016-12-08 03:32:30.504160: step 4470, loss = 1.20 (628.9 examples/sec; 0.204 sec/batch)
2016-12-08 03:32:32.635141: step 4480, loss = 1.12 (597.1 examples/sec; 0.214 sec/batch)
2016-12-08 03:32:34.720602: step 4490, loss = 0.91 (620.5 examples/sec; 0.206 sec/batch)
2016-12-08 03:32:36.879025: step 4500, loss = 1.19 (532.8 examples/sec; 0.240 sec/batch)
2016-12-08 03:32:39.291466: step 4510, loss = 1.21 (644.5 examples/sec; 0.199 sec/batch)
2016-12-08 03:32:41.432953: step 4520, loss = 1.09 (636.2 examples/sec; 0.201 sec/batch)
2016-12-08 03:32:43.502194: step 4530, loss = 1.05 (631.8 examples/sec; 0.203 sec/batch)
2016-12-08 03:32:45.579439: step 4540, loss = 1.20 (634.8 examples/sec; 0.202 sec/batch)
2016-12-08 03:32:47.609197: step 4550, loss = 1.03 (643.5 examples/sec; 0.199 sec/batch)
2016-12-08 03:32:49.680555: step 4560, loss = 0.84 (571.4 examples/sec; 0.224 sec/batch)
2016-12-08 03:32:51.845442: step 4570, loss = 1.22 (607.5 examples/sec; 0.211 sec/batch)
2016-12-08 03:32:53.935441: step 4580, loss = 0.95 (591.2 examples/sec; 0.217 sec/batch)
2016-12-08 03:32:55.987972: step 4590, loss = 0.97 (612.4 examples/sec; 0.209 sec/batch)
2016-12-08 03:32:58.046630: step 4600, loss = 1.09 (584.6 examples/sec; 0.219 sec/batch)
2016-12-08 03:33:00.389228: step 4610, loss = 1.10 (621.3 examples/sec; 0.206 sec/batch)
2016-12-08 03:33:02.453804: step 4620, loss = 0.93 (625.9 examples/sec; 0.205 sec/batch)
2016-12-08 03:33:04.575252: step 4630, loss = 1.08 (602.5 examples/sec; 0.212 sec/batch)
2016-12-08 03:33:06.681836: step 4640, loss = 0.89 (590.3 examples/sec; 0.217 sec/batch)
2016-12-08 03:33:08.785700: step 4650, loss = 1.14 (596.0 examples/sec; 0.215 sec/batch)
2016-12-08 03:33:10.858803: step 4660, loss = 1.02 (560.3 examples/sec; 0.228 sec/batch)
2016-12-08 03:33:13.057442: step 4670, loss = 1.00 (573.5 examples/sec; 0.223 sec/batch)
2016-12-08 03:33:15.151281: step 4680, loss = 1.00 (647.4 examples/sec; 0.198 sec/batch)
2016-12-08 03:33:17.264673: step 4690, loss = 1.02 (618.6 examples/sec; 0.207 sec/batch)
2016-12-08 03:33:19.296847: step 4700, loss = 0.94 (573.2 examples/sec; 0.223 sec/batch)
2016-12-08 03:33:21.688080: step 4710, loss = 1.08 (581.2 examples/sec; 0.220 sec/batch)
2016-12-08 03:33:23.779026: step 4720, loss = 1.01 (603.3 examples/sec; 0.212 sec/batch)
2016-12-08 03:33:25.934494: step 4730, loss = 1.22 (588.4 examples/sec; 0.218 sec/batch)
2016-12-08 03:33:28.079084: step 4740, loss = 0.87 (584.8 examples/sec; 0.219 sec/batch)
2016-12-08 03:33:30.180414: step 4750, loss = 1.13 (638.1 examples/sec; 0.201 sec/batch)
2016-12-08 03:33:32.324975: step 4760, loss = 0.96 (573.2 examples/sec; 0.223 sec/batch)
2016-12-08 03:33:34.449362: step 4770, loss = 0.96 (607.5 examples/sec; 0.211 sec/batch)
2016-12-08 03:33:36.568869: step 4780, loss = 0.78 (531.9 examples/sec; 0.241 sec/batch)
2016-12-08 03:33:38.659829: step 4790, loss = 0.90 (635.6 examples/sec; 0.201 sec/batch)
2016-12-08 03:33:40.761191: step 4800, loss = 1.19 (638.9 examples/sec; 0.200 sec/batch)
2016-12-08 03:33:43.124637: step 4810, loss = 1.19 (613.5 examples/sec; 0.209 sec/batch)
2016-12-08 03:33:45.167716: step 4820, loss = 1.10 (615.5 examples/sec; 0.208 sec/batch)
2016-12-08 03:33:47.337689: step 4830, loss = 1.06 (607.1 examples/sec; 0.211 sec/batch)
2016-12-08 03:33:49.470673: step 4840, loss = 0.99 (593.4 examples/sec; 0.216 sec/batch)
2016-12-08 03:33:51.613803: step 4850, loss = 1.00 (649.3 examples/sec; 0.197 sec/batch)
2016-12-08 03:33:53.695780: step 4860, loss = 1.14 (609.9 examples/sec; 0.210 sec/batch)
2016-12-08 03:33:55.784322: step 4870, loss = 0.89 (659.9 examples/sec; 0.194 sec/batch)
2016-12-08 03:33:57.842314: step 4880, loss = 1.12 (620.7 examples/sec; 0.206 sec/batch)
2016-12-08 03:33:59.912879: step 4890, loss = 0.97 (632.1 examples/sec; 0.203 sec/batch)
2016-12-08 03:34:01.968792: step 4900, loss = 1.14 (603.7 examples/sec; 0.212 sec/batch)
2016-12-08 03:34:04.267402: step 4910, loss = 1.01 (641.6 examples/sec; 0.199 sec/batch)
2016-12-08 03:34:06.391014: step 4920, loss = 1.25 (630.8 examples/sec; 0.203 sec/batch)
2016-12-08 03:34:08.457400: step 4930, loss = 1.07 (642.3 examples/sec; 0.199 sec/batch)
2016-12-08 03:34:10.523490: step 4940, loss = 0.93 (589.0 examples/sec; 0.217 sec/batch)
2016-12-08 03:34:12.610692: step 4950, loss = 0.84 (609.4 examples/sec; 0.210 sec/batch)
2016-12-08 03:34:14.695565: step 4960, loss = 1.08 (653.2 examples/sec; 0.196 sec/batch)
2016-12-08 03:34:16.765891: step 4970, loss = 0.99 (675.5 examples/sec; 0.189 sec/batch)
2016-12-08 03:34:18.861576: step 4980, loss = 1.05 (655.2 examples/sec; 0.195 sec/batch)
2016-12-08 03:34:20.885031: step 4990, loss = 1.10 (659.1 examples/sec; 0.194 sec/batch)
2016-12-08 03:34:22.998713: step 5000, loss = 1.25 (626.8 examples/sec; 0.204 sec/batch)
2016-12-08 03:34:26.017889: step 5010, loss = 0.94 (574.3 examples/sec; 0.223 sec/batch)
2016-12-08 03:34:28.058063: step 5020, loss = 1.01 (627.1 examples/sec; 0.204 sec/batch)
2016-12-08 03:34:30.218279: step 5030, loss = 0.89 (468.9 examples/sec; 0.273 sec/batch)
2016-12-08 03:34:32.295202: step 5040, loss = 1.26 (609.4 examples/sec; 0.210 sec/batch)
2016-12-08 03:34:34.367263: step 5050, loss = 0.96 (647.9 examples/sec; 0.198 sec/batch)
2016-12-08 03:34:36.394803: step 5060, loss = 1.09 (646.9 examples/sec; 0.198 sec/batch)
2016-12-08 03:34:38.474748: step 5070, loss = 0.95 (638.9 examples/sec; 0.200 sec/batch)
2016-12-08 03:34:40.540846: step 5080, loss = 0.86 (585.3 examples/sec; 0.219 sec/batch)
2016-12-08 03:34:42.598355: step 5090, loss = 0.98 (589.2 examples/sec; 0.217 sec/batch)
2016-12-08 03:34:44.666763: step 5100, loss = 0.94 (633.3 examples/sec; 0.202 sec/batch)
2016-12-08 03:34:47.013183: step 5110, loss = 1.18 (606.2 examples/sec; 0.211 sec/batch)
2016-12-08 03:34:49.038872: step 5120, loss = 0.99 (606.7 examples/sec; 0.211 sec/batch)
2016-12-08 03:34:51.101073: step 5130, loss = 0.92 (651.7 examples/sec; 0.196 sec/batch)
2016-12-08 03:34:53.146217: step 5140, loss = 1.01 (573.4 examples/sec; 0.223 sec/batch)
2016-12-08 03:34:55.234682: step 5150, loss = 1.02 (586.9 examples/sec; 0.218 sec/batch)
2016-12-08 03:34:57.359049: step 5160, loss = 0.98 (637.9 examples/sec; 0.201 sec/batch)
2016-12-08 03:34:59.389499: step 5170, loss = 1.10 (644.5 examples/sec; 0.199 sec/batch)
2016-12-08 03:35:01.494761: step 5180, loss = 1.07 (595.5 examples/sec; 0.215 sec/batch)
2016-12-08 03:35:03.583639: step 5190, loss = 1.16 (591.7 examples/sec; 0.216 sec/batch)
2016-12-08 03:35:05.671908: step 5200, loss = 0.96 (601.4 examples/sec; 0.213 sec/batch)
2016-12-08 03:35:08.007280: step 5210, loss = 0.97 (589.4 examples/sec; 0.217 sec/batch)
2016-12-08 03:35:10.152860: step 5220, loss = 0.89 (600.5 examples/sec; 0.213 sec/batch)
2016-12-08 03:35:12.242619: step 5230, loss = 1.09 (620.5 examples/sec; 0.206 sec/batch)
2016-12-08 03:35:14.315708: step 5240, loss = 0.99 (592.4 examples/sec; 0.216 sec/batch)
2016-12-08 03:35:16.418388: step 5250, loss = 1.17 (630.4 examples/sec; 0.203 sec/batch)
2016-12-08 03:35:18.496394: step 5260, loss = 1.15 (646.1 examples/sec; 0.198 sec/batch)
2016-12-08 03:35:20.627962: step 5270, loss = 0.98 (598.7 examples/sec; 0.214 sec/batch)
2016-12-08 03:35:22.748873: step 5280, loss = 0.77 (538.9 examples/sec; 0.238 sec/batch)
2016-12-08 03:35:24.884638: step 5290, loss = 1.04 (616.2 examples/sec; 0.208 sec/batch)
2016-12-08 03:35:26.982049: step 5300, loss = 1.27 (650.7 examples/sec; 0.197 sec/batch)
2016-12-08 03:35:29.368953: step 5310, loss = 1.05 (620.7 examples/sec; 0.206 sec/batch)
2016-12-08 03:35:31.493873: step 5320, loss = 1.01 (552.6 examples/sec; 0.232 sec/batch)
2016-12-08 03:35:33.574379: step 5330, loss = 1.07 (621.5 examples/sec; 0.206 sec/batch)
2016-12-08 03:35:35.729868: step 5340, loss = 1.00 (525.5 examples/sec; 0.244 sec/batch)
2016-12-08 03:35:37.819610: step 5350, loss = 1.10 (614.7 examples/sec; 0.208 sec/batch)
2016-12-08 03:35:39.864913: step 5360, loss = 1.05 (598.7 examples/sec; 0.214 sec/batch)
2016-12-08 03:35:41.961307: step 5370, loss = 1.16 (606.1 examples/sec; 0.211 sec/batch)
2016-12-08 03:35:44.029681: step 5380, loss = 1.34 (648.5 examples/sec; 0.197 sec/batch)
2016-12-08 03:35:46.117449: step 5390, loss = 0.96 (578.0 examples/sec; 0.221 sec/batch)
2016-12-08 03:35:48.303442: step 5400, loss = 0.94 (616.0 examples/sec; 0.208 sec/batch)
2016-12-08 03:35:50.611954: step 5410, loss = 1.09 (623.4 examples/sec; 0.205 sec/batch)
2016-12-08 03:35:52.797543: step 5420, loss = 0.91 (631.9 examples/sec; 0.203 sec/batch)
2016-12-08 03:35:54.798960: step 5430, loss = 1.05 (682.6 examples/sec; 0.188 sec/batch)
2016-12-08 03:35:56.844211: step 5440, loss = 0.94 (584.8 examples/sec; 0.219 sec/batch)
2016-12-08 03:35:58.937188: step 5450, loss = 0.99 (630.0 examples/sec; 0.203 sec/batch)
2016-12-08 03:36:01.052836: step 5460, loss = 0.99 (585.2 examples/sec; 0.219 sec/batch)
2016-12-08 03:36:03.099789: step 5470, loss = 0.86 (655.1 examples/sec; 0.195 sec/batch)
2016-12-08 03:36:05.228721: step 5480, loss = 0.87 (591.5 examples/sec; 0.216 sec/batch)
2016-12-08 03:36:07.320779: step 5490, loss = 1.10 (662.7 examples/sec; 0.193 sec/batch)
2016-12-08 03:36:09.367315: step 5500, loss = 1.09 (667.1 examples/sec; 0.192 sec/batch)
2016-12-08 03:36:11.691116: step 5510, loss = 0.93 (657.8 examples/sec; 0.195 sec/batch)
2016-12-08 03:36:13.757381: step 5520, loss = 0.97 (642.2 examples/sec; 0.199 sec/batch)
2016-12-08 03:36:15.919669: step 5530, loss = 0.98 (593.5 examples/sec; 0.216 sec/batch)
2016-12-08 03:36:17.950891: step 5540, loss = 1.14 (587.4 examples/sec; 0.218 sec/batch)
2016-12-08 03:36:20.106871: step 5550, loss = 0.94 (574.5 examples/sec; 0.223 sec/batch)
2016-12-08 03:36:22.221384: step 5560, loss = 1.04 (566.5 examples/sec; 0.226 sec/batch)
2016-12-08 03:36:24.339675: step 5570, loss = 0.95 (616.5 examples/sec; 0.208 sec/batch)
2016-12-08 03:36:26.399294: step 5580, loss = 1.07 (573.4 examples/sec; 0.223 sec/batch)
2016-12-08 03:36:28.415449: step 5590, loss = 0.88 (679.2 examples/sec; 0.188 sec/batch)
2016-12-08 03:36:30.471213: step 5600, loss = 0.70 (611.8 examples/sec; 0.209 sec/batch)
2016-12-08 03:36:32.832846: step 5610, loss = 0.98 (601.4 examples/sec; 0.213 sec/batch)
2016-12-08 03:36:34.923468: step 5620, loss = 0.91 (590.8 examples/sec; 0.217 sec/batch)
2016-12-08 03:36:37.040932: step 5630, loss = 0.97 (596.2 examples/sec; 0.215 sec/batch)
2016-12-08 03:36:39.159225: step 5640, loss = 1.06 (514.9 examples/sec; 0.249 sec/batch)
2016-12-08 03:36:41.295893: step 5650, loss = 1.02 (644.7 examples/sec; 0.199 sec/batch)
2016-12-08 03:36:43.447358: step 5660, loss = 1.03 (551.6 examples/sec; 0.232 sec/batch)
2016-12-08 03:36:45.487135: step 5670, loss = 1.18 (611.7 examples/sec; 0.209 sec/batch)
2016-12-08 03:36:47.507451: step 5680, loss = 0.98 (646.1 examples/sec; 0.198 sec/batch)
2016-12-08 03:36:49.596619: step 5690, loss = 1.00 (607.1 examples/sec; 0.211 sec/batch)
2016-12-08 03:36:51.773008: step 5700, loss = 1.20 (610.7 examples/sec; 0.210 sec/batch)
2016-12-08 03:36:54.116103: step 5710, loss = 0.94 (643.8 examples/sec; 0.199 sec/batch)
2016-12-08 03:36:56.175362: step 5720, loss = 1.24 (578.3 examples/sec; 0.221 sec/batch)
2016-12-08 03:36:58.244962: step 5730, loss = 0.97 (631.3 examples/sec; 0.203 sec/batch)
2016-12-08 03:37:00.430783: step 5740, loss = 0.96 (576.0 examples/sec; 0.222 sec/batch)
2016-12-08 03:37:02.523567: step 5750, loss = 0.92 (611.1 examples/sec; 0.209 sec/batch)
2016-12-08 03:37:04.654373: step 5760, loss = 0.88 (641.0 examples/sec; 0.200 sec/batch)
2016-12-08 03:37:06.733233: step 5770, loss = 1.05 (594.5 examples/sec; 0.215 sec/batch)
2016-12-08 03:37:08.844932: step 5780, loss = 0.98 (620.1 examples/sec; 0.206 sec/batch)
2016-12-08 03:37:10.901557: step 5790, loss = 1.24 (597.3 examples/sec; 0.214 sec/batch)
2016-12-08 03:37:12.940243: step 5800, loss = 0.82 (598.3 examples/sec; 0.214 sec/batch)
2016-12-08 03:37:15.241463: step 5810, loss = 1.09 (587.3 examples/sec; 0.218 sec/batch)
2016-12-08 03:37:17.370423: step 5820, loss = 1.00 (654.6 examples/sec; 0.196 sec/batch)
2016-12-08 03:37:19.468508: step 5830, loss = 0.89 (583.5 examples/sec; 0.219 sec/batch)
2016-12-08 03:37:21.504114: step 5840, loss = 0.94 (598.3 examples/sec; 0.214 sec/batch)
2016-12-08 03:37:23.678450: step 5850, loss = 0.98 (530.5 examples/sec; 0.241 sec/batch)
2016-12-08 03:37:25.778528: step 5860, loss = 0.92 (566.1 examples/sec; 0.226 sec/batch)
2016-12-08 03:37:27.916879: step 5870, loss = 1.14 (546.0 examples/sec; 0.234 sec/batch)
2016-12-08 03:37:30.012457: step 5880, loss = 1.06 (656.6 examples/sec; 0.195 sec/batch)
2016-12-08 03:37:32.161654: step 5890, loss = 0.89 (551.1 examples/sec; 0.232 sec/batch)
2016-12-08 03:37:34.296446: step 5900, loss = 0.91 (599.9 examples/sec; 0.213 sec/batch)
2016-12-08 03:37:36.649199: step 5910, loss = 0.98 (573.7 examples/sec; 0.223 sec/batch)
2016-12-08 03:37:38.726075: step 5920, loss = 0.90 (639.3 examples/sec; 0.200 sec/batch)
2016-12-08 03:37:40.876553: step 5930, loss = 0.95 (654.2 examples/sec; 0.196 sec/batch)
2016-12-08 03:37:43.008201: step 5940, loss = 0.81 (617.7 examples/sec; 0.207 sec/batch)
2016-12-08 03:37:45.055726: step 5950, loss = 1.09 (658.5 examples/sec; 0.194 sec/batch)
2016-12-08 03:37:47.109228: step 5960, loss = 0.90 (640.1 examples/sec; 0.200 sec/batch)
2016-12-08 03:37:49.225223: step 5970, loss = 0.89 (585.2 examples/sec; 0.219 sec/batch)
2016-12-08 03:37:51.315259: step 5980, loss = 0.98 (589.6 examples/sec; 0.217 sec/batch)
2016-12-08 03:37:53.432249: step 5990, loss = 0.85 (607.3 examples/sec; 0.211 sec/batch)
2016-12-08 03:37:55.522156: step 6000, loss = 0.97 (618.3 examples/sec; 0.207 sec/batch)
2016-12-08 03:37:58.445762: step 6010, loss = 0.89 (534.9 examples/sec; 0.239 sec/batch)
2016-12-08 03:38:00.612900: step 6020, loss = 1.04 (605.9 examples/sec; 0.211 sec/batch)
2016-12-08 03:38:02.699355: step 6030, loss = 0.94 (668.0 examples/sec; 0.192 sec/batch)
2016-12-08 03:38:04.781950: step 6040, loss = 1.12 (628.1 examples/sec; 0.204 sec/batch)
2016-12-08 03:38:06.951656: step 6050, loss = 0.88 (604.1 examples/sec; 0.212 sec/batch)
2016-12-08 03:38:09.082153: step 6060, loss = 0.93 (645.4 examples/sec; 0.198 sec/batch)
2016-12-08 03:38:11.195149: step 6070, loss = 0.93 (612.3 examples/sec; 0.209 sec/batch)
2016-12-08 03:38:13.227961: step 6080, loss = 1.00 (666.7 examples/sec; 0.192 sec/batch)
2016-12-08 03:38:15.318627: step 6090, loss = 1.09 (569.6 examples/sec; 0.225 sec/batch)
2016-12-08 03:38:17.413580: step 6100, loss = 1.00 (617.8 examples/sec; 0.207 sec/batch)
2016-12-08 03:38:19.689707: step 6110, loss = 0.81 (604.7 examples/sec; 0.212 sec/batch)
2016-12-08 03:38:21.774310: step 6120, loss = 0.79 (568.4 examples/sec; 0.225 sec/batch)
2016-12-08 03:38:23.834065: step 6130, loss = 0.96 (665.8 examples/sec; 0.192 sec/batch)
2016-12-08 03:38:25.932253: step 6140, loss = 0.80 (629.3 examples/sec; 0.203 sec/batch)
2016-12-08 03:38:28.070607: step 6150, loss = 1.11 (584.1 examples/sec; 0.219 sec/batch)
2016-12-08 03:38:30.096119: step 6160, loss = 0.90 (627.9 examples/sec; 0.204 sec/batch)
2016-12-08 03:38:32.147969: step 6170, loss = 1.03 (630.4 examples/sec; 0.203 sec/batch)
2016-12-08 03:38:34.299578: step 6180, loss = 1.12 (477.9 examples/sec; 0.268 sec/batch)
2016-12-08 03:38:36.405122: step 6190, loss = 0.97 (627.3 examples/sec; 0.204 sec/batch)
2016-12-08 03:38:38.499181: step 6200, loss = 1.07 (549.7 examples/sec; 0.233 sec/batch)
2016-12-08 03:38:40.916040: step 6210, loss = 0.86 (627.4 examples/sec; 0.204 sec/batch)
2016-12-08 03:38:43.038109: step 6220, loss = 1.00 (647.4 examples/sec; 0.198 sec/batch)
2016-12-08 03:38:45.163035: step 6230, loss = 1.13 (575.9 examples/sec; 0.222 sec/batch)
2016-12-08 03:38:47.249768: step 6240, loss = 1.09 (669.8 examples/sec; 0.191 sec/batch)
2016-12-08 03:38:49.378659: step 6250, loss = 0.84 (632.2 examples/sec; 0.202 sec/batch)
2016-12-08 03:38:51.468749: step 6260, loss = 0.97 (654.0 examples/sec; 0.196 sec/batch)
2016-12-08 03:38:53.583262: step 6270, loss = 0.82 (577.8 examples/sec; 0.222 sec/batch)
2016-12-08 03:38:55.605494: step 6280, loss = 1.05 (649.5 examples/sec; 0.197 sec/batch)
2016-12-08 03:38:57.671834: step 6290, loss = 0.96 (649.6 examples/sec; 0.197 sec/batch)
2016-12-08 03:38:59.833264: step 6300, loss = 0.86 (654.1 examples/sec; 0.196 sec/batch)
2016-12-08 03:39:02.130629: step 6310, loss = 0.88 (655.1 examples/sec; 0.195 sec/batch)
2016-12-08 03:39:04.188847: step 6320, loss = 0.94 (647.0 examples/sec; 0.198 sec/batch)
2016-12-08 03:39:06.226822: step 6330, loss = 0.86 (679.1 examples/sec; 0.188 sec/batch)
2016-12-08 03:39:08.318060: step 6340, loss = 0.94 (595.8 examples/sec; 0.215 sec/batch)
2016-12-08 03:39:10.425222: step 6350, loss = 0.96 (600.8 examples/sec; 0.213 sec/batch)
2016-12-08 03:39:12.540387: step 6360, loss = 0.88 (613.6 examples/sec; 0.209 sec/batch)
2016-12-08 03:39:14.563894: step 6370, loss = 0.85 (634.4 examples/sec; 0.202 sec/batch)
2016-12-08 03:39:16.641963: step 6380, loss = 1.01 (644.0 examples/sec; 0.199 sec/batch)
2016-12-08 03:39:18.685185: step 6390, loss = 0.98 (642.7 examples/sec; 0.199 sec/batch)
2016-12-08 03:39:20.749981: step 6400, loss = 0.91 (590.4 examples/sec; 0.217 sec/batch)
2016-12-08 03:39:23.237930: step 6410, loss = 1.05 (589.3 examples/sec; 0.217 sec/batch)
2016-12-08 03:39:25.302118: step 6420, loss = 1.01 (607.2 examples/sec; 0.211 sec/batch)
2016-12-08 03:39:27.391621: step 6430, loss = 1.09 (607.9 examples/sec; 0.211 sec/batch)
2016-12-08 03:39:29.421596: step 6440, loss = 1.10 (631.1 examples/sec; 0.203 sec/batch)
2016-12-08 03:39:31.521906: step 6450, loss = 1.09 (645.7 examples/sec; 0.198 sec/batch)
2016-12-08 03:39:33.545866: step 6460, loss = 0.93 (619.4 examples/sec; 0.207 sec/batch)
2016-12-08 03:39:35.604283: step 6470, loss = 1.05 (587.9 examples/sec; 0.218 sec/batch)
2016-12-08 03:39:37.697915: step 6480, loss = 0.86 (578.5 examples/sec; 0.221 sec/batch)
2016-12-08 03:39:39.810857: step 6490, loss = 0.93 (610.9 examples/sec; 0.210 sec/batch)
2016-12-08 03:39:41.905230: step 6500, loss = 0.89 (666.6 examples/sec; 0.192 sec/batch)
2016-12-08 03:39:44.264845: step 6510, loss = 0.84 (573.1 examples/sec; 0.223 sec/batch)
2016-12-08 03:39:46.392555: step 6520, loss = 0.84 (576.4 examples/sec; 0.222 sec/batch)
2016-12-08 03:39:48.514753: step 6530, loss = 0.84 (580.6 examples/sec; 0.220 sec/batch)
2016-12-08 03:39:50.562574: step 6540, loss = 0.79 (625.0 examples/sec; 0.205 sec/batch)
2016-12-08 03:39:52.592818: step 6550, loss = 1.01 (638.3 examples/sec; 0.201 sec/batch)
2016-12-08 03:39:54.673541: step 6560, loss = 0.92 (658.2 examples/sec; 0.194 sec/batch)
2016-12-08 03:39:56.714280: step 6570, loss = 1.14 (642.4 examples/sec; 0.199 sec/batch)
2016-12-08 03:39:58.764997: step 6580, loss = 1.04 (637.5 examples/sec; 0.201 sec/batch)
2016-12-08 03:40:00.929526: step 6590, loss = 1.00 (615.0 examples/sec; 0.208 sec/batch)
2016-12-08 03:40:03.042299: step 6600, loss = 0.88 (627.1 examples/sec; 0.204 sec/batch)
2016-12-08 03:40:05.404405: step 6610, loss = 0.93 (640.0 examples/sec; 0.200 sec/batch)
2016-12-08 03:40:07.501318: step 6620, loss = 0.90 (604.2 examples/sec; 0.212 sec/batch)
2016-12-08 03:40:09.497754: step 6630, loss = 0.89 (639.7 examples/sec; 0.200 sec/batch)
2016-12-08 03:40:11.559134: step 6640, loss = 0.78 (574.6 examples/sec; 0.223 sec/batch)
2016-12-08 03:40:13.618030: step 6650, loss = 1.34 (604.3 examples/sec; 0.212 sec/batch)
2016-12-08 03:40:15.752899: step 6660, loss = 0.97 (600.5 examples/sec; 0.213 sec/batch)
2016-12-08 03:40:17.823199: step 6670, loss = 0.97 (622.4 examples/sec; 0.206 sec/batch)
2016-12-08 03:40:19.979491: step 6680, loss = 0.95 (574.2 examples/sec; 0.223 sec/batch)
2016-12-08 03:40:22.009888: step 6690, loss = 1.03 (652.1 examples/sec; 0.196 sec/batch)
2016-12-08 03:40:24.056295: step 6700, loss = 0.78 (642.4 examples/sec; 0.199 sec/batch)
2016-12-08 03:40:26.420290: step 6710, loss = 0.94 (575.5 examples/sec; 0.222 sec/batch)
2016-12-08 03:40:28.473699: step 6720, loss = 0.89 (593.1 examples/sec; 0.216 sec/batch)
2016-12-08 03:40:30.607893: step 6730, loss = 0.82 (597.8 examples/sec; 0.214 sec/batch)
2016-12-08 03:40:32.773221: step 6740, loss = 1.09 (634.7 examples/sec; 0.202 sec/batch)
2016-12-08 03:40:34.848780: step 6750, loss = 0.90 (612.1 examples/sec; 0.209 sec/batch)
2016-12-08 03:40:36.980924: step 6760, loss = 0.95 (605.0 examples/sec; 0.212 sec/batch)
2016-12-08 03:40:39.113463: step 6770, loss = 0.84 (579.8 examples/sec; 0.221 sec/batch)
2016-12-08 03:40:41.174930: step 6780, loss = 1.05 (635.0 examples/sec; 0.202 sec/batch)
2016-12-08 03:40:43.251991: step 6790, loss = 0.97 (648.8 examples/sec; 0.197 sec/batch)
2016-12-08 03:40:45.350021: step 6800, loss = 0.85 (571.9 examples/sec; 0.224 sec/batch)
2016-12-08 03:40:47.687140: step 6810, loss = 0.89 (629.9 examples/sec; 0.203 sec/batch)
2016-12-08 03:40:49.855372: step 6820, loss = 1.05 (556.3 examples/sec; 0.230 sec/batch)
2016-12-08 03:40:51.918636: step 6830, loss = 1.03 (635.6 examples/sec; 0.201 sec/batch)
2016-12-08 03:40:53.960293: step 6840, loss = 0.93 (630.3 examples/sec; 0.203 sec/batch)
2016-12-08 03:40:56.014592: step 6850, loss = 0.76 (652.5 examples/sec; 0.196 sec/batch)
2016-12-08 03:40:58.121258: step 6860, loss = 0.98 (597.8 examples/sec; 0.214 sec/batch)
2016-12-08 03:41:00.250748: step 6870, loss = 0.89 (574.4 examples/sec; 0.223 sec/batch)
2016-12-08 03:41:02.367262: step 6880, loss = 0.86 (625.7 examples/sec; 0.205 sec/batch)
2016-12-08 03:41:04.363268: step 6890, loss = 1.03 (619.6 examples/sec; 0.207 sec/batch)
2016-12-08 03:41:06.370164: step 6900, loss = 0.94 (695.9 examples/sec; 0.184 sec/batch)
2016-12-08 03:41:08.689290: step 6910, loss = 1.03 (563.0 examples/sec; 0.227 sec/batch)
2016-12-08 03:41:10.790654: step 6920, loss = 0.82 (588.2 examples/sec; 0.218 sec/batch)
2016-12-08 03:41:12.982189: step 6930, loss = 0.87 (574.6 examples/sec; 0.223 sec/batch)
2016-12-08 03:41:15.139531: step 6940, loss = 0.85 (555.7 examples/sec; 0.230 sec/batch)
2016-12-08 03:41:17.300949: step 6950, loss = 1.00 (598.4 examples/sec; 0.214 sec/batch)
2016-12-08 03:41:19.417926: step 6960, loss = 0.85 (583.0 examples/sec; 0.220 sec/batch)
2016-12-08 03:41:21.569513: step 6970, loss = 0.95 (535.3 examples/sec; 0.239 sec/batch)
2016-12-08 03:41:23.727225: step 6980, loss = 0.95 (628.3 examples/sec; 0.204 sec/batch)
2016-12-08 03:41:25.852495: step 6990, loss = 1.00 (594.7 examples/sec; 0.215 sec/batch)
2016-12-08 03:41:27.949074: step 7000, loss = 0.96 (624.7 examples/sec; 0.205 sec/batch)
2016-12-08 03:41:30.939366: step 7010, loss = 0.83 (578.7 examples/sec; 0.221 sec/batch)
2016-12-08 03:41:33.059745: step 7020, loss = 1.02 (590.4 examples/sec; 0.217 sec/batch)
2016-12-08 03:41:35.184219: step 7030, loss = 0.87 (659.6 examples/sec; 0.194 sec/batch)
2016-12-08 03:41:37.297305: step 7040, loss = 1.21 (640.4 examples/sec; 0.200 sec/batch)
2016-12-08 03:41:39.405706: step 7050, loss = 0.88 (594.7 examples/sec; 0.215 sec/batch)
2016-12-08 03:41:41.514443: step 7060, loss = 1.06 (606.4 examples/sec; 0.211 sec/batch)
2016-12-08 03:41:43.575894: step 7070, loss = 0.76 (628.2 examples/sec; 0.204 sec/batch)
2016-12-08 03:41:45.570947: step 7080, loss = 1.02 (624.7 examples/sec; 0.205 sec/batch)
2016-12-08 03:41:47.622104: step 7090, loss = 0.99 (652.4 examples/sec; 0.196 sec/batch)
2016-12-08 03:41:49.720753: step 7100, loss = 1.00 (634.0 examples/sec; 0.202 sec/batch)
2016-12-08 03:41:52.092844: step 7110, loss = 0.89 (596.4 examples/sec; 0.215 sec/batch)
2016-12-08 03:41:54.222400: step 7120, loss = 0.88 (627.0 examples/sec; 0.204 sec/batch)
2016-12-08 03:41:56.381747: step 7130, loss = 0.97 (640.5 examples/sec; 0.200 sec/batch)
2016-12-08 03:41:58.384370: step 7140, loss = 0.97 (652.7 examples/sec; 0.196 sec/batch)
2016-12-08 03:42:00.478414: step 7150, loss = 1.09 (591.5 examples/sec; 0.216 sec/batch)
2016-12-08 03:42:02.535295: step 7160, loss = 0.81 (640.4 examples/sec; 0.200 sec/batch)
2016-12-08 03:42:04.683238: step 7170, loss = 0.74 (641.2 examples/sec; 0.200 sec/batch)
2016-12-08 03:42:06.768842: step 7180, loss = 0.88 (569.4 examples/sec; 0.225 sec/batch)
2016-12-08 03:42:08.851087: step 7190, loss = 0.96 (653.7 examples/sec; 0.196 sec/batch)
2016-12-08 03:42:10.922549: step 7200, loss = 1.08 (629.2 examples/sec; 0.203 sec/batch)
2016-12-08 03:42:13.166202: step 7210, loss = 0.92 (578.4 examples/sec; 0.221 sec/batch)
2016-12-08 03:42:15.341383: step 7220, loss = 0.98 (615.7 examples/sec; 0.208 sec/batch)
2016-12-08 03:42:17.425086: step 7230, loss = 0.98 (638.1 examples/sec; 0.201 sec/batch)
2016-12-08 03:42:19.515955: step 7240, loss = 1.24 (616.8 examples/sec; 0.208 sec/batch)
2016-12-08 03:42:21.557901: step 7250, loss = 1.05 (592.7 examples/sec; 0.216 sec/batch)
2016-12-08 03:42:23.599336: step 7260, loss = 0.80 (668.3 examples/sec; 0.192 sec/batch)
2016-12-08 03:42:25.662388: step 7270, loss = 0.93 (627.0 examples/sec; 0.204 sec/batch)
2016-12-08 03:42:27.680858: step 7280, loss = 0.95 (676.7 examples/sec; 0.189 sec/batch)
2016-12-08 03:42:29.749235: step 7290, loss = 1.02 (612.3 examples/sec; 0.209 sec/batch)
2016-12-08 03:42:31.807536: step 7300, loss = 0.87 (662.3 examples/sec; 0.193 sec/batch)
2016-12-08 03:42:34.247021: step 7310, loss = 0.88 (600.3 examples/sec; 0.213 sec/batch)
2016-12-08 03:42:36.366497: step 7320, loss = 1.01 (624.8 examples/sec; 0.205 sec/batch)
2016-12-08 03:42:38.498223: step 7330, loss = 0.98 (585.9 examples/sec; 0.218 sec/batch)
2016-12-08 03:42:40.604543: step 7340, loss = 0.98 (625.6 examples/sec; 0.205 sec/batch)
2016-12-08 03:42:42.767237: step 7350, loss = 1.04 (607.9 examples/sec; 0.211 sec/batch)
2016-12-08 03:42:44.850218: step 7360, loss = 0.88 (649.4 examples/sec; 0.197 sec/batch)
2016-12-08 03:42:46.987958: step 7370, loss = 0.94 (587.2 examples/sec; 0.218 sec/batch)
2016-12-08 03:42:49.080156: step 7380, loss = 0.87 (629.9 examples/sec; 0.203 sec/batch)
2016-12-08 03:42:51.164057: step 7390, loss = 0.97 (600.8 examples/sec; 0.213 sec/batch)
2016-12-08 03:42:53.247064: step 7400, loss = 1.00 (634.9 examples/sec; 0.202 sec/batch)
2016-12-08 03:42:55.550854: step 7410, loss = 0.96 (601.6 examples/sec; 0.213 sec/batch)
2016-12-08 03:42:57.615818: step 7420, loss = 0.97 (601.4 examples/sec; 0.213 sec/batch)
2016-12-08 03:42:59.726408: step 7430, loss = 1.00 (610.2 examples/sec; 0.210 sec/batch)
2016-12-08 03:43:01.754995: step 7440, loss = 0.83 (606.7 examples/sec; 0.211 sec/batch)
2016-12-08 03:43:03.847283: step 7450, loss = 0.98 (651.4 examples/sec; 0.196 sec/batch)
2016-12-08 03:43:05.957292: step 7460, loss = 0.77 (646.6 examples/sec; 0.198 sec/batch)
2016-12-08 03:43:07.967051: step 7470, loss = 0.85 (611.0 examples/sec; 0.209 sec/batch)
2016-12-08 03:43:09.998010: step 7480, loss = 1.02 (633.1 examples/sec; 0.202 sec/batch)
2016-12-08 03:43:12.118506: step 7490, loss = 0.93 (626.0 examples/sec; 0.204 sec/batch)
2016-12-08 03:43:14.158517: step 7500, loss = 0.94 (651.2 examples/sec; 0.197 sec/batch)
2016-12-08 03:43:16.523275: step 7510, loss = 0.87 (664.9 examples/sec; 0.192 sec/batch)
2016-12-08 03:43:18.613087: step 7520, loss = 0.94 (626.3 examples/sec; 0.204 sec/batch)
2016-12-08 03:43:20.681329: step 7530, loss = 0.89 (594.3 examples/sec; 0.215 sec/batch)
2016-12-08 03:43:22.797753: step 7540, loss = 0.88 (607.6 examples/sec; 0.211 sec/batch)
2016-12-08 03:43:24.923976: step 7550, loss = 0.89 (612.1 examples/sec; 0.209 sec/batch)
2016-12-08 03:43:26.933513: step 7560, loss = 0.87 (640.0 examples/sec; 0.200 sec/batch)
2016-12-08 03:43:29.071162: step 7570, loss = 0.82 (609.6 examples/sec; 0.210 sec/batch)
2016-12-08 03:43:31.143190: step 7580, loss = 0.87 (658.7 examples/sec; 0.194 sec/batch)
2016-12-08 03:43:33.254829: step 7590, loss = 1.04 (619.5 examples/sec; 0.207 sec/batch)
2016-12-08 03:43:35.347933: step 7600, loss = 0.93 (609.3 examples/sec; 0.210 sec/batch)
2016-12-08 03:43:37.674423: step 7610, loss = 0.74 (598.4 examples/sec; 0.214 sec/batch)
2016-12-08 03:43:39.840407: step 7620, loss = 1.06 (582.5 examples/sec; 0.220 sec/batch)
2016-12-08 03:43:42.012765: step 7630, loss = 1.08 (592.8 examples/sec; 0.216 sec/batch)
2016-12-08 03:43:44.160603: step 7640, loss = 0.82 (603.8 examples/sec; 0.212 sec/batch)
2016-12-08 03:43:46.153491: step 7650, loss = 0.95 (639.8 examples/sec; 0.200 sec/batch)
2016-12-08 03:43:48.248971: step 7660, loss = 0.70 (556.4 examples/sec; 0.230 sec/batch)
2016-12-08 03:43:50.330687: step 7670, loss = 1.02 (655.2 examples/sec; 0.195 sec/batch)
2016-12-08 03:43:52.422871: step 7680, loss = 1.10 (579.6 examples/sec; 0.221 sec/batch)
2016-12-08 03:43:54.539670: step 7690, loss = 0.83 (586.0 examples/sec; 0.218 sec/batch)
2016-12-08 03:43:56.591004: step 7700, loss = 1.03 (656.4 examples/sec; 0.195 sec/batch)
2016-12-08 03:43:58.955250: step 7710, loss = 0.90 (602.8 examples/sec; 0.212 sec/batch)
2016-12-08 03:44:01.041707: step 7720, loss = 1.01 (622.6 examples/sec; 0.206 sec/batch)
2016-12-08 03:44:03.220199: step 7730, loss = 0.96 (635.9 examples/sec; 0.201 sec/batch)
2016-12-08 03:44:05.292735: step 7740, loss = 0.76 (629.3 examples/sec; 0.203 sec/batch)
2016-12-08 03:44:07.300300: step 7750, loss = 0.98 (688.0 examples/sec; 0.186 sec/batch)
2016-12-08 03:44:09.367127: step 7760, loss = 0.83 (658.6 examples/sec; 0.194 sec/batch)
2016-12-08 03:44:11.440965: step 7770, loss = 0.81 (666.5 examples/sec; 0.192 sec/batch)
2016-12-08 03:44:13.487007: step 7780, loss = 0.84 (600.5 examples/sec; 0.213 sec/batch)
2016-12-08 03:44:15.644152: step 7790, loss = 0.74 (565.6 examples/sec; 0.226 sec/batch)
2016-12-08 03:44:17.759570: step 7800, loss = 0.78 (616.0 examples/sec; 0.208 sec/batch)
2016-12-08 03:44:20.070430: step 7810, loss = 0.87 (665.4 examples/sec; 0.192 sec/batch)
2016-12-08 03:44:22.122980: step 7820, loss = 0.80 (600.8 examples/sec; 0.213 sec/batch)
2016-12-08 03:44:24.179311: step 7830, loss = 0.98 (680.5 examples/sec; 0.188 sec/batch)
2016-12-08 03:44:26.232521: step 7840, loss = 0.92 (634.8 examples/sec; 0.202 sec/batch)
2016-12-08 03:44:28.340745: step 7850, loss = 0.78 (617.0 examples/sec; 0.207 sec/batch)
2016-12-08 03:44:30.378439: step 7860, loss = 1.03 (598.0 examples/sec; 0.214 sec/batch)
2016-12-08 03:44:32.488417: step 7870, loss = 0.99 (649.6 examples/sec; 0.197 sec/batch)
2016-12-08 03:44:34.642397: step 7880, loss = 0.80 (611.2 examples/sec; 0.209 sec/batch)
2016-12-08 03:44:36.757889: step 7890, loss = 0.94 (606.8 examples/sec; 0.211 sec/batch)
2016-12-08 03:44:38.837908: step 7900, loss = 1.00 (570.3 examples/sec; 0.224 sec/batch)
2016-12-08 03:44:41.187836: step 7910, loss = 0.83 (658.9 examples/sec; 0.194 sec/batch)
2016-12-08 03:44:43.286303: step 7920, loss = 1.03 (609.6 examples/sec; 0.210 sec/batch)
2016-12-08 03:44:45.427893: step 7930, loss = 0.90 (651.7 examples/sec; 0.196 sec/batch)
2016-12-08 03:44:47.576516: step 7940, loss = 1.04 (625.6 examples/sec; 0.205 sec/batch)
2016-12-08 03:44:49.636128: step 7950, loss = 0.91 (607.4 examples/sec; 0.211 sec/batch)
2016-12-08 03:44:51.662179: step 7960, loss = 0.89 (657.8 examples/sec; 0.195 sec/batch)
2016-12-08 03:44:53.728496: step 7970, loss = 1.02 (589.5 examples/sec; 0.217 sec/batch)
2016-12-08 03:44:55.796021: step 7980, loss = 1.06 (641.5 examples/sec; 0.200 sec/batch)
2016-12-08 03:44:57.810687: step 7990, loss = 1.05 (593.8 examples/sec; 0.216 sec/batch)
2016-12-08 03:44:59.957709: step 8000, loss = 0.87 (618.6 examples/sec; 0.207 sec/batch)
2016-12-08 03:45:03.051043: step 8010, loss = 0.91 (621.9 examples/sec; 0.206 sec/batch)
2016-12-08 03:45:05.202065: step 8020, loss = 0.88 (580.6 examples/sec; 0.220 sec/batch)
2016-12-08 03:45:07.256573: step 8030, loss = 0.94 (566.3 examples/sec; 0.226 sec/batch)
2016-12-08 03:45:09.329336: step 8040, loss = 0.86 (624.1 examples/sec; 0.205 sec/batch)
2016-12-08 03:45:11.529005: step 8050, loss = 0.94 (575.7 examples/sec; 0.222 sec/batch)
2016-12-08 03:45:13.622738: step 8060, loss = 0.91 (589.9 examples/sec; 0.217 sec/batch)
2016-12-08 03:45:15.781548: step 8070, loss = 1.03 (611.6 examples/sec; 0.209 sec/batch)
2016-12-08 03:45:17.828344: step 8080, loss = 1.04 (639.0 examples/sec; 0.200 sec/batch)
2016-12-08 03:45:20.062689: step 8090, loss = 0.81 (514.6 examples/sec; 0.249 sec/batch)
2016-12-08 03:45:22.216991: step 8100, loss = 0.90 (627.7 examples/sec; 0.204 sec/batch)
2016-12-08 03:45:24.568298: step 8110, loss = 0.78 (581.2 examples/sec; 0.220 sec/batch)
2016-12-08 03:45:26.695184: step 8120, loss = 0.95 (589.5 examples/sec; 0.217 sec/batch)
2016-12-08 03:45:28.847037: step 8130, loss = 0.94 (618.0 examples/sec; 0.207 sec/batch)
2016-12-08 03:45:30.924062: step 8140, loss = 0.93 (584.3 examples/sec; 0.219 sec/batch)
2016-12-08 03:45:33.055440: step 8150, loss = 1.03 (668.9 examples/sec; 0.191 sec/batch)
2016-12-08 03:45:35.120764: step 8160, loss = 0.95 (604.8 examples/sec; 0.212 sec/batch)
2016-12-08 03:45:37.228050: step 8170, loss = 0.92 (673.1 examples/sec; 0.190 sec/batch)
2016-12-08 03:45:39.373941: step 8180, loss = 0.82 (564.4 examples/sec; 0.227 sec/batch)
2016-12-08 03:45:41.453498: step 8190, loss = 0.84 (623.9 examples/sec; 0.205 sec/batch)
2016-12-08 03:45:43.560601: step 8200, loss = 0.87 (580.5 examples/sec; 0.220 sec/batch)
2016-12-08 03:45:45.953981: step 8210, loss = 0.94 (619.9 examples/sec; 0.206 sec/batch)
2016-12-08 03:45:48.084549: step 8220, loss = 1.09 (613.7 examples/sec; 0.209 sec/batch)
2016-12-08 03:45:50.188129: step 8230, loss = 0.93 (609.0 examples/sec; 0.210 sec/batch)
2016-12-08 03:45:52.346950: step 8240, loss = 0.93 (595.9 examples/sec; 0.215 sec/batch)
2016-12-08 03:45:54.485597: step 8250, loss = 0.94 (642.0 examples/sec; 0.199 sec/batch)
2016-12-08 03:45:56.583945: step 8260, loss = 0.87 (605.9 examples/sec; 0.211 sec/batch)
2016-12-08 03:45:58.628751: step 8270, loss = 0.98 (612.6 examples/sec; 0.209 sec/batch)
2016-12-08 03:46:00.694101: step 8280, loss = 1.08 (651.5 examples/sec; 0.196 sec/batch)
2016-12-08 03:46:02.764189: step 8290, loss = 0.72 (608.1 examples/sec; 0.210 sec/batch)
2016-12-08 03:46:04.852408: step 8300, loss = 0.78 (569.5 examples/sec; 0.225 sec/batch)
2016-12-08 03:46:07.233517: step 8310, loss = 0.88 (618.2 examples/sec; 0.207 sec/batch)
2016-12-08 03:46:09.325156: step 8320, loss = 1.09 (649.4 examples/sec; 0.197 sec/batch)
2016-12-08 03:46:11.513789: step 8330, loss = 0.82 (612.2 examples/sec; 0.209 sec/batch)
2016-12-08 03:46:13.634475: step 8340, loss = 0.92 (634.1 examples/sec; 0.202 sec/batch)
2016-12-08 03:46:15.709953: step 8350, loss = 0.94 (636.4 examples/sec; 0.201 sec/batch)
2016-12-08 03:46:17.819171: step 8360, loss = 1.00 (654.7 examples/sec; 0.195 sec/batch)
2016-12-08 03:46:19.928759: step 8370, loss = 0.80 (610.0 examples/sec; 0.210 sec/batch)
2016-12-08 03:46:21.936157: step 8380, loss = 0.88 (619.0 examples/sec; 0.207 sec/batch)
2016-12-08 03:46:23.987418: step 8390, loss = 0.89 (601.6 examples/sec; 0.213 sec/batch)
2016-12-08 03:46:26.070911: step 8400, loss = 1.15 (653.1 examples/sec; 0.196 sec/batch)
2016-12-08 03:46:28.431458: step 8410, loss = 0.93 (656.1 examples/sec; 0.195 sec/batch)
2016-12-08 03:46:30.541933: step 8420, loss = 0.81 (574.4 examples/sec; 0.223 sec/batch)
2016-12-08 03:46:32.669024: step 8430, loss = 1.03 (638.8 examples/sec; 0.200 sec/batch)
2016-12-08 03:46:34.779372: step 8440, loss = 1.03 (586.7 examples/sec; 0.218 sec/batch)
2016-12-08 03:46:36.807939: step 8450, loss = 1.10 (639.4 examples/sec; 0.200 sec/batch)
2016-12-08 03:46:38.777885: step 8460, loss = 0.89 (660.2 examples/sec; 0.194 sec/batch)
2016-12-08 03:46:40.843502: step 8470, loss = 0.83 (631.5 examples/sec; 0.203 sec/batch)
2016-12-08 03:46:42.935554: step 8480, loss = 0.86 (608.4 examples/sec; 0.210 sec/batch)
2016-12-08 03:46:45.064306: step 8490, loss = 0.98 (563.2 examples/sec; 0.227 sec/batch)
2016-12-08 03:46:47.226069: step 8500, loss = 0.84 (623.4 examples/sec; 0.205 sec/batch)
2016-12-08 03:46:49.570186: step 8510, loss = 0.98 (575.9 examples/sec; 0.222 sec/batch)
2016-12-08 03:46:51.620941: step 8520, loss = 0.66 (636.4 examples/sec; 0.201 sec/batch)
2016-12-08 03:46:53.672449: step 8530, loss = 1.00 (632.4 examples/sec; 0.202 sec/batch)
2016-12-08 03:46:55.799631: step 8540, loss = 0.99 (622.3 examples/sec; 0.206 sec/batch)
2016-12-08 03:46:57.915612: step 8550, loss = 0.84 (616.0 examples/sec; 0.208 sec/batch)
2016-12-08 03:46:59.945501: step 8560, loss = 0.97 (594.4 examples/sec; 0.215 sec/batch)
2016-12-08 03:47:01.934222: step 8570, loss = 1.01 (639.1 examples/sec; 0.200 sec/batch)
2016-12-08 03:47:04.092268: step 8580, loss = 0.82 (615.3 examples/sec; 0.208 sec/batch)
2016-12-08 03:47:06.146442: step 8590, loss = 0.83 (587.2 examples/sec; 0.218 sec/batch)
2016-12-08 03:47:08.278227: step 8600, loss = 0.99 (576.5 examples/sec; 0.222 sec/batch)
2016-12-08 03:47:10.653040: step 8610, loss = 0.81 (567.6 examples/sec; 0.226 sec/batch)
2016-12-08 03:47:12.722575: step 8620, loss = 0.87 (607.5 examples/sec; 0.211 sec/batch)
2016-12-08 03:47:14.825133: step 8630, loss = 0.93 (558.7 examples/sec; 0.229 sec/batch)
2016-12-08 03:47:16.916476: step 8640, loss = 0.95 (559.3 examples/sec; 0.229 sec/batch)
2016-12-08 03:47:19.008413: step 8650, loss = 0.74 (615.2 examples/sec; 0.208 sec/batch)
2016-12-08 03:47:21.112948: step 8660, loss = 1.11 (607.8 examples/sec; 0.211 sec/batch)
2016-12-08 03:47:23.215652: step 8670, loss = 0.97 (562.4 examples/sec; 0.228 sec/batch)
2016-12-08 03:47:25.279835: step 8680, loss = 0.99 (656.5 examples/sec; 0.195 sec/batch)
2016-12-08 03:47:27.333981: step 8690, loss = 0.90 (624.6 examples/sec; 0.205 sec/batch)
2016-12-08 03:47:29.431533: step 8700, loss = 0.73 (604.0 examples/sec; 0.212 sec/batch)
2016-12-08 03:47:31.718744: step 8710, loss = 1.06 (634.9 examples/sec; 0.202 sec/batch)
2016-12-08 03:47:33.788345: step 8720, loss = 0.83 (626.6 examples/sec; 0.204 sec/batch)
2016-12-08 03:47:35.913948: step 8730, loss = 1.10 (619.0 examples/sec; 0.207 sec/batch)
2016-12-08 03:47:37.960471: step 8740, loss = 1.03 (694.1 examples/sec; 0.184 sec/batch)
2016-12-08 03:47:39.964960: step 8750, loss = 1.07 (644.1 examples/sec; 0.199 sec/batch)
2016-12-08 03:47:42.052345: step 8760, loss = 0.95 (643.1 examples/sec; 0.199 sec/batch)
2016-12-08 03:47:44.138619: step 8770, loss = 0.91 (577.9 examples/sec; 0.221 sec/batch)
2016-12-08 03:47:46.210821: step 8780, loss = 1.01 (626.4 examples/sec; 0.204 sec/batch)
2016-12-08 03:47:48.372210: step 8790, loss = 1.00 (546.6 examples/sec; 0.234 sec/batch)
2016-12-08 03:47:50.453558: step 8800, loss = 0.78 (619.2 examples/sec; 0.207 sec/batch)
2016-12-08 03:47:52.696000: step 8810, loss = 0.88 (657.9 examples/sec; 0.195 sec/batch)
2016-12-08 03:47:54.772774: step 8820, loss = 0.79 (584.4 examples/sec; 0.219 sec/batch)
2016-12-08 03:47:56.951503: step 8830, loss = 0.87 (595.9 examples/sec; 0.215 sec/batch)
2016-12-08 03:47:59.014982: step 8840, loss = 1.04 (634.0 examples/sec; 0.202 sec/batch)
2016-12-08 03:48:01.139104: step 8850, loss = 0.83 (628.3 examples/sec; 0.204 sec/batch)
2016-12-08 03:48:03.241073: step 8860, loss = 1.00 (616.6 examples/sec; 0.208 sec/batch)
2016-12-08 03:48:05.353167: step 8870, loss = 0.79 (630.1 examples/sec; 0.203 sec/batch)
2016-12-08 03:48:07.432942: step 8880, loss = 0.96 (640.1 examples/sec; 0.200 sec/batch)
2016-12-08 03:48:09.574067: step 8890, loss = 0.83 (585.5 examples/sec; 0.219 sec/batch)
2016-12-08 03:48:11.705807: step 8900, loss = 1.05 (589.2 examples/sec; 0.217 sec/batch)
2016-12-08 03:48:14.029337: step 8910, loss = 0.95 (582.9 examples/sec; 0.220 sec/batch)
2016-12-08 03:48:16.168371: step 8920, loss = 0.77 (679.5 examples/sec; 0.188 sec/batch)
2016-12-08 03:48:18.238639: step 8930, loss = 0.87 (644.7 examples/sec; 0.199 sec/batch)
2016-12-08 03:48:20.327122: step 8940, loss = 0.91 (573.2 examples/sec; 0.223 sec/batch)
2016-12-08 03:48:22.528902: step 8950, loss = 0.75 (624.9 examples/sec; 0.205 sec/batch)
2016-12-08 03:48:24.676485: step 8960, loss = 0.83 (556.0 examples/sec; 0.230 sec/batch)
2016-12-08 03:48:26.743535: step 8970, loss = 0.74 (601.7 examples/sec; 0.213 sec/batch)
2016-12-08 03:48:28.794138: step 8980, loss = 0.91 (628.7 examples/sec; 0.204 sec/batch)
2016-12-08 03:48:30.885495: step 8990, loss = 0.93 (609.2 examples/sec; 0.210 sec/batch)
2016-12-08 03:48:33.032974: step 9000, loss = 0.96 (597.5 examples/sec; 0.214 sec/batch)
2016-12-08 03:48:36.091353: step 9010, loss = 0.85 (496.9 examples/sec; 0.258 sec/batch)
2016-12-08 03:48:38.230960: step 9020, loss = 0.83 (614.3 examples/sec; 0.208 sec/batch)
2016-12-08 03:48:40.294795: step 9030, loss = 0.96 (636.8 examples/sec; 0.201 sec/batch)
2016-12-08 03:48:42.394834: step 9040, loss = 0.81 (625.3 examples/sec; 0.205 sec/batch)
2016-12-08 03:48:44.503138: step 9050, loss = 0.93 (595.1 examples/sec; 0.215 sec/batch)
2016-12-08 03:48:46.619742: step 9060, loss = 1.02 (576.1 examples/sec; 0.222 sec/batch)
2016-12-08 03:48:48.735621: step 9070, loss = 0.97 (585.0 examples/sec; 0.219 sec/batch)
2016-12-08 03:48:50.816422: step 9080, loss = 0.79 (652.1 examples/sec; 0.196 sec/batch)
2016-12-08 03:48:52.874868: step 9090, loss = 0.85 (587.8 examples/sec; 0.218 sec/batch)
2016-12-08 03:48:55.013143: step 9100, loss = 0.97 (593.7 examples/sec; 0.216 sec/batch)
2016-12-08 03:48:57.386135: step 9110, loss = 0.93 (594.2 examples/sec; 0.215 sec/batch)
2016-12-08 03:48:59.468881: step 9120, loss = 0.88 (598.9 examples/sec; 0.214 sec/batch)
2016-12-08 03:49:01.538189: step 9130, loss = 0.82 (637.6 examples/sec; 0.201 sec/batch)
2016-12-08 03:49:03.664974: step 9140, loss = 0.61 (656.3 examples/sec; 0.195 sec/batch)
2016-12-08 03:49:05.756947: step 9150, loss = 0.84 (624.6 examples/sec; 0.205 sec/batch)
2016-12-08 03:49:07.823856: step 9160, loss = 0.87 (584.5 examples/sec; 0.219 sec/batch)
2016-12-08 03:49:09.935947: step 9170, loss = 0.75 (584.4 examples/sec; 0.219 sec/batch)
2016-12-08 03:49:12.075389: step 9180, loss = 0.76 (588.9 examples/sec; 0.217 sec/batch)
2016-12-08 03:49:14.182568: step 9190, loss = 0.94 (566.5 examples/sec; 0.226 sec/batch)
2016-12-08 03:49:16.421732: step 9200, loss = 0.91 (528.6 examples/sec; 0.242 sec/batch)
2016-12-08 03:49:18.753779: step 9210, loss = 0.83 (572.6 examples/sec; 0.224 sec/batch)
2016-12-08 03:49:20.883303: step 9220, loss = 0.90 (595.7 examples/sec; 0.215 sec/batch)
2016-12-08 03:49:23.009361: step 9230, loss = 0.85 (586.4 examples/sec; 0.218 sec/batch)
2016-12-08 03:49:25.148248: step 9240, loss = 0.78 (643.4 examples/sec; 0.199 sec/batch)
2016-12-08 03:49:27.202441: step 9250, loss = 0.95 (600.6 examples/sec; 0.213 sec/batch)
2016-12-08 03:49:29.215725: step 9260, loss = 0.91 (660.3 examples/sec; 0.194 sec/batch)
2016-12-08 03:49:31.289971: step 9270, loss = 0.95 (629.2 examples/sec; 0.203 sec/batch)
2016-12-08 03:49:33.385050: step 9280, loss = 0.83 (519.5 examples/sec; 0.246 sec/batch)
2016-12-08 03:49:35.457245: step 9290, loss = 1.03 (604.4 examples/sec; 0.212 sec/batch)
2016-12-08 03:49:37.570052: step 9300, loss = 0.80 (653.5 examples/sec; 0.196 sec/batch)
2016-12-08 03:49:39.868882: step 9310, loss = 0.98 (643.6 examples/sec; 0.199 sec/batch)
2016-12-08 03:49:41.881350: step 9320, loss = 0.96 (628.5 examples/sec; 0.204 sec/batch)
2016-12-08 03:49:44.000373: step 9330, loss = 0.81 (566.3 examples/sec; 0.226 sec/batch)
2016-12-08 03:49:46.023815: step 9340, loss = 0.89 (599.7 examples/sec; 0.213 sec/batch)
2016-12-08 03:49:48.125976: step 9350, loss = 0.97 (587.0 examples/sec; 0.218 sec/batch)
2016-12-08 03:49:50.259264: step 9360, loss = 0.91 (646.2 examples/sec; 0.198 sec/batch)
2016-12-08 03:49:52.403319: step 9370, loss = 0.97 (550.7 examples/sec; 0.232 sec/batch)
2016-12-08 03:49:54.639020: step 9380, loss = 0.86 (545.6 examples/sec; 0.235 sec/batch)
2016-12-08 03:49:56.744120: step 9390, loss = 0.83 (511.5 examples/sec; 0.250 sec/batch)
2016-12-08 03:49:58.850163: step 9400, loss = 1.00 (596.9 examples/sec; 0.214 sec/batch)
2016-12-08 03:50:01.131671: step 9410, loss = 0.76 (656.5 examples/sec; 0.195 sec/batch)
2016-12-08 03:50:03.265848: step 9420, loss = 0.79 (550.9 examples/sec; 0.232 sec/batch)
2016-12-08 03:50:05.298680: step 9430, loss = 0.85 (647.6 examples/sec; 0.198 sec/batch)
2016-12-08 03:50:07.368692: step 9440, loss = 0.76 (596.6 examples/sec; 0.215 sec/batch)
2016-12-08 03:50:09.387230: step 9450, loss = 0.86 (650.7 examples/sec; 0.197 sec/batch)
2016-12-08 03:50:11.454685: step 9460, loss = 0.99 (668.4 examples/sec; 0.191 sec/batch)
2016-12-08 03:50:13.581009: step 9470, loss = 0.85 (595.2 examples/sec; 0.215 sec/batch)
2016-12-08 03:50:15.770171: step 9480, loss = 0.80 (547.5 examples/sec; 0.234 sec/batch)
2016-12-08 03:50:17.829765: step 9490, loss = 0.92 (617.5 examples/sec; 0.207 sec/batch)
2016-12-08 03:50:19.883130: step 9500, loss = 0.76 (619.1 examples/sec; 0.207 sec/batch)
2016-12-08 03:50:22.194037: step 9510, loss = 0.76 (609.6 examples/sec; 0.210 sec/batch)
2016-12-08 03:50:24.311945: step 9520, loss = 0.82 (579.5 examples/sec; 0.221 sec/batch)
2016-12-08 03:50:26.394634: step 9530, loss = 0.81 (626.7 examples/sec; 0.204 sec/batch)
2016-12-08 03:50:28.411399: step 9540, loss = 0.98 (653.9 examples/sec; 0.196 sec/batch)
2016-12-08 03:50:30.522248: step 9550, loss = 0.95 (621.5 examples/sec; 0.206 sec/batch)
2016-12-08 03:50:32.676362: step 9560, loss = 0.84 (641.5 examples/sec; 0.200 sec/batch)
2016-12-08 03:50:34.864646: step 9570, loss = 0.95 (593.3 examples/sec; 0.216 sec/batch)
2016-12-08 03:50:36.991121: step 9580, loss = 0.89 (662.7 examples/sec; 0.193 sec/batch)
2016-12-08 03:50:39.047672: step 9590, loss = 0.88 (659.9 examples/sec; 0.194 sec/batch)
2016-12-08 03:50:41.090777: step 9600, loss = 0.83 (668.2 examples/sec; 0.192 sec/batch)
2016-12-08 03:50:43.346023: step 9610, loss = 0.90 (599.0 examples/sec; 0.214 sec/batch)
2016-12-08 03:50:45.329282: step 9620, loss = 1.05 (653.4 examples/sec; 0.196 sec/batch)
2016-12-08 03:50:47.363901: step 9630, loss = 0.93 (654.7 examples/sec; 0.196 sec/batch)
2016-12-08 03:50:49.444569: step 9640, loss = 0.82 (625.5 examples/sec; 0.205 sec/batch)
2016-12-08 03:50:51.583166: step 9650, loss = 0.81 (594.4 examples/sec; 0.215 sec/batch)
2016-12-08 03:50:53.671175: step 9660, loss = 0.74 (617.4 examples/sec; 0.207 sec/batch)
2016-12-08 03:50:55.746496: step 9670, loss = 0.82 (665.4 examples/sec; 0.192 sec/batch)
2016-12-08 03:50:57.796409: step 9680, loss = 0.79 (597.4 examples/sec; 0.214 sec/batch)
2016-12-08 03:50:59.803701: step 9690, loss = 1.01 (633.2 examples/sec; 0.202 sec/batch)
2016-12-08 03:51:01.861297: step 9700, loss = 0.74 (602.0 examples/sec; 0.213 sec/batch)
2016-12-08 03:51:04.248382: step 9710, loss = 0.89 (565.3 examples/sec; 0.226 sec/batch)
2016-12-08 03:51:06.349909: step 9720, loss = 0.83 (609.1 examples/sec; 0.210 sec/batch)
2016-12-08 03:51:08.391680: step 9730, loss = 0.89 (671.9 examples/sec; 0.191 sec/batch)
2016-12-08 03:51:10.413052: step 9740, loss = 0.88 (574.4 examples/sec; 0.223 sec/batch)
2016-12-08 03:51:12.502443: step 9750, loss = 1.10 (632.9 examples/sec; 0.202 sec/batch)
2016-12-08 03:51:14.696325: step 9760, loss = 0.76 (528.9 examples/sec; 0.242 sec/batch)
2016-12-08 03:51:16.825694: step 9770, loss = 1.02 (673.4 examples/sec; 0.190 sec/batch)
2016-12-08 03:51:18.907616: step 9780, loss = 0.90 (597.3 examples/sec; 0.214 sec/batch)
2016-12-08 03:51:21.002317: step 9790, loss = 0.98 (575.9 examples/sec; 0.222 sec/batch)
2016-12-08 03:51:23.054638: step 9800, loss = 0.90 (613.5 examples/sec; 0.209 sec/batch)
2016-12-08 03:51:25.346661: step 9810, loss = 0.72 (569.1 examples/sec; 0.225 sec/batch)
2016-12-08 03:51:27.440095: step 9820, loss = 0.81 (619.9 examples/sec; 0.206 sec/batch)
2016-12-08 03:51:29.495925: step 9830, loss = 0.89 (647.7 examples/sec; 0.198 sec/batch)
2016-12-08 03:51:31.612056: step 9840, loss = 0.89 (619.2 examples/sec; 0.207 sec/batch)
2016-12-08 03:51:33.711118: step 9850, loss = 0.88 (604.0 examples/sec; 0.212 sec/batch)
2016-12-08 03:51:35.816569: step 9860, loss = 0.77 (587.3 examples/sec; 0.218 sec/batch)
2016-12-08 03:51:37.921754: step 9870, loss = 0.95 (600.4 examples/sec; 0.213 sec/batch)
2016-12-08 03:51:39.986768: step 9880, loss = 0.96 (635.5 examples/sec; 0.201 sec/batch)
2016-12-08 03:51:42.095785: step 9890, loss = 0.82 (616.2 examples/sec; 0.208 sec/batch)
2016-12-08 03:51:44.229751: step 9900, loss = 0.83 (585.5 examples/sec; 0.219 sec/batch)
2016-12-08 03:51:46.578113: step 9910, loss = 0.77 (615.5 examples/sec; 0.208 sec/batch)
2016-12-08 03:51:48.660595: step 9920, loss = 0.76 (538.0 examples/sec; 0.238 sec/batch)
2016-12-08 03:51:50.770714: step 9930, loss = 0.82 (632.8 examples/sec; 0.202 sec/batch)
2016-12-08 03:51:52.853267: step 9940, loss = 0.78 (632.6 examples/sec; 0.202 sec/batch)
2016-12-08 03:51:54.911009: step 9950, loss = 1.10 (627.0 examples/sec; 0.204 sec/batch)
2016-12-08 03:51:56.935071: step 9960, loss = 0.95 (610.3 examples/sec; 0.210 sec/batch)
2016-12-08 03:51:59.066524: step 9970, loss = 0.74 (621.7 examples/sec; 0.206 sec/batch)
2016-12-08 03:52:01.135371: step 9980, loss = 0.90 (661.0 examples/sec; 0.194 sec/batch)
2016-12-08 03:52:03.200655: step 9990, loss = 0.75 (645.9 examples/sec; 0.198 sec/batch)
2016-12-08 03:52:05.254772: step 10000, loss = 1.03 (611.6 examples/sec; 0.209 sec/batch)
2016-12-08 03:52:08.273072: step 10010, loss = 0.71 (626.5 examples/sec; 0.204 sec/batch)
2016-12-08 03:52:10.387160: step 10020, loss = 0.88 (588.0 examples/sec; 0.218 sec/batch)
2016-12-08 03:52:12.468937: step 10030, loss = 0.78 (614.7 examples/sec; 0.208 sec/batch)
2016-12-08 03:52:14.590565: step 10040, loss = 0.78 (656.5 examples/sec; 0.195 sec/batch)
2016-12-08 03:52:16.662935: step 10050, loss = 0.82 (647.7 examples/sec; 0.198 sec/batch)
2016-12-08 03:52:18.719339: step 10060, loss = 0.93 (612.9 examples/sec; 0.209 sec/batch)
2016-12-08 03:52:20.847893: step 10070, loss = 0.87 (634.3 examples/sec; 0.202 sec/batch)
2016-12-08 03:52:22.930796: step 10080, loss = 0.86 (615.0 examples/sec; 0.208 sec/batch)
2016-12-08 03:52:24.950352: step 10090, loss = 0.88 (624.6 examples/sec; 0.205 sec/batch)
2016-12-08 03:52:27.008980: step 10100, loss = 0.89 (656.5 examples/sec; 0.195 sec/batch)
2016-12-08 03:52:29.276385: step 10110, loss = 0.91 (621.2 examples/sec; 0.206 sec/batch)
2016-12-08 03:52:31.382475: step 10120, loss = 1.06 (609.8 examples/sec; 0.210 sec/batch)
2016-12-08 03:52:33.501723: step 10130, loss = 0.78 (561.1 examples/sec; 0.228 sec/batch)
2016-12-08 03:52:35.598985: step 10140, loss = 0.89 (605.2 examples/sec; 0.212 sec/batch)
2016-12-08 03:52:37.651185: step 10150, loss = 0.78 (576.3 examples/sec; 0.222 sec/batch)
2016-12-08 03:52:39.740125: step 10160, loss = 0.96 (598.1 examples/sec; 0.214 sec/batch)
2016-12-08 03:52:41.763536: step 10170, loss = 0.76 (587.0 examples/sec; 0.218 sec/batch)
2016-12-08 03:52:43.871512: step 10180, loss = 0.95 (630.2 examples/sec; 0.203 sec/batch)
2016-12-08 03:52:46.029045: step 10190, loss = 1.01 (621.6 examples/sec; 0.206 sec/batch)
2016-12-08 03:52:48.182827: step 10200, loss = 0.84 (637.7 examples/sec; 0.201 sec/batch)
2016-12-08 03:52:50.450352: step 10210, loss = 0.89 (642.8 examples/sec; 0.199 sec/batch)
2016-12-08 03:52:52.614164: step 10220, loss = 1.04 (582.1 examples/sec; 0.220 sec/batch)
2016-12-08 03:52:54.706764: step 10230, loss = 0.87 (594.7 examples/sec; 0.215 sec/batch)
2016-12-08 03:52:56.845938: step 10240, loss = 0.71 (596.6 examples/sec; 0.215 sec/batch)
2016-12-08 03:52:59.065583: step 10250, loss = 0.95 (574.5 examples/sec; 0.223 sec/batch)
2016-12-08 03:53:01.140431: step 10260, loss = 0.92 (602.9 examples/sec; 0.212 sec/batch)
2016-12-08 03:53:03.235744: step 10270, loss = 0.86 (570.4 examples/sec; 0.224 sec/batch)
2016-12-08 03:53:05.371612: step 10280, loss = 0.91 (583.3 examples/sec; 0.219 sec/batch)
2016-12-08 03:53:07.549092: step 10290, loss = 0.94 (565.6 examples/sec; 0.226 sec/batch)
2016-12-08 03:53:09.628459: step 10300, loss = 0.90 (607.1 examples/sec; 0.211 sec/batch)
2016-12-08 03:53:11.959151: step 10310, loss = 0.97 (599.7 examples/sec; 0.213 sec/batch)
2016-12-08 03:53:14.002685: step 10320, loss = 0.89 (702.9 examples/sec; 0.182 sec/batch)
2016-12-08 03:53:16.113361: step 10330, loss = 0.89 (606.4 examples/sec; 0.211 sec/batch)
2016-12-08 03:53:18.169826: step 10340, loss = 0.96 (674.1 examples/sec; 0.190 sec/batch)
2016-12-08 03:53:20.221180: step 10350, loss = 1.08 (622.2 examples/sec; 0.206 sec/batch)
2016-12-08 03:53:22.254776: step 10360, loss = 0.88 (651.8 examples/sec; 0.196 sec/batch)
2016-12-08 03:53:24.319395: step 10370, loss = 0.81 (674.0 examples/sec; 0.190 sec/batch)
2016-12-08 03:53:26.345239: step 10380, loss = 0.91 (653.9 examples/sec; 0.196 sec/batch)
2016-12-08 03:53:28.422843: step 10390, loss = 0.84 (622.8 examples/sec; 0.206 sec/batch)
2016-12-08 03:53:30.522850: step 10400, loss = 0.92 (570.5 examples/sec; 0.224 sec/batch)
2016-12-08 03:53:32.845339: step 10410, loss = 1.03 (652.1 examples/sec; 0.196 sec/batch)
2016-12-08 03:53:34.909829: step 10420, loss = 0.81 (578.1 examples/sec; 0.221 sec/batch)
2016-12-08 03:53:37.063505: step 10430, loss = 0.88 (505.8 examples/sec; 0.253 sec/batch)
2016-12-08 03:53:39.182131: step 10440, loss = 0.85 (633.1 examples/sec; 0.202 sec/batch)
2016-12-08 03:53:41.255519: step 10450, loss = 0.83 (667.1 examples/sec; 0.192 sec/batch)
2016-12-08 03:53:43.390038: step 10460, loss = 0.87 (552.7 examples/sec; 0.232 sec/batch)
2016-12-08 03:53:45.447473: step 10470, loss = 0.85 (585.4 examples/sec; 0.219 sec/batch)
2016-12-08 03:53:47.495183: step 10480, loss = 0.93 (614.3 examples/sec; 0.208 sec/batch)
2016-12-08 03:53:49.570470: step 10490, loss = 0.98 (635.6 examples/sec; 0.201 sec/batch)
2016-12-08 03:53:51.677615: step 10500, loss = 0.81 (587.5 examples/sec; 0.218 sec/batch)
2016-12-08 03:53:54.056620: step 10510, loss = 0.94 (647.4 examples/sec; 0.198 sec/batch)
2016-12-08 03:53:56.098254: step 10520, loss = 0.82 (668.4 examples/sec; 0.192 sec/batch)
2016-12-08 03:53:58.160786: step 10530, loss = 1.01 (673.5 examples/sec; 0.190 sec/batch)
2016-12-08 03:54:00.217095: step 10540, loss = 0.85 (610.6 examples/sec; 0.210 sec/batch)
2016-12-08 03:54:02.306464: step 10550, loss = 0.87 (612.9 examples/sec; 0.209 sec/batch)
2016-12-08 03:54:04.432024: step 10560, loss = 0.77 (630.0 examples/sec; 0.203 sec/batch)
2016-12-08 03:54:06.522435: step 10570, loss = 0.97 (589.1 examples/sec; 0.217 sec/batch)
2016-12-08 03:54:08.625735: step 10580, loss = 0.93 (576.3 examples/sec; 0.222 sec/batch)
2016-12-08 03:54:10.723329: step 10590, loss = 0.95 (639.0 examples/sec; 0.200 sec/batch)
2016-12-08 03:54:12.777401: step 10600, loss = 0.99 (627.0 examples/sec; 0.204 sec/batch)
2016-12-08 03:54:15.098160: step 10610, loss = 0.87 (645.6 examples/sec; 0.198 sec/batch)
2016-12-08 03:54:17.254196: step 10620, loss = 0.75 (646.4 examples/sec; 0.198 sec/batch)
2016-12-08 03:54:19.356057: step 10630, loss = 0.92 (566.6 examples/sec; 0.226 sec/batch)
2016-12-08 03:54:21.502464: step 10640, loss = 0.83 (576.2 examples/sec; 0.222 sec/batch)
2016-12-08 03:54:23.591990: step 10650, loss = 0.84 (605.6 examples/sec; 0.211 sec/batch)
2016-12-08 03:54:25.778111: step 10660, loss = 0.95 (581.8 examples/sec; 0.220 sec/batch)
2016-12-08 03:54:27.928884: step 10670, loss = 0.92 (654.1 examples/sec; 0.196 sec/batch)
2016-12-08 03:54:30.029541: step 10680, loss = 1.02 (590.3 examples/sec; 0.217 sec/batch)
2016-12-08 03:54:32.092857: step 10690, loss = 0.85 (600.1 examples/sec; 0.213 sec/batch)
2016-12-08 03:54:34.218832: step 10700, loss = 0.88 (585.4 examples/sec; 0.219 sec/batch)
2016-12-08 03:54:36.578089: step 10710, loss = 0.82 (641.1 examples/sec; 0.200 sec/batch)
2016-12-08 03:54:38.641731: step 10720, loss = 0.94 (610.0 examples/sec; 0.210 sec/batch)
2016-12-08 03:54:40.728506: step 10730, loss = 0.72 (632.0 examples/sec; 0.203 sec/batch)
2016-12-08 03:54:42.895549: step 10740, loss = 0.85 (590.5 examples/sec; 0.217 sec/batch)
2016-12-08 03:54:44.981352: step 10750, loss = 1.01 (635.9 examples/sec; 0.201 sec/batch)
2016-12-08 03:54:47.199208: step 10760, loss = 0.91 (617.3 examples/sec; 0.207 sec/batch)
2016-12-08 03:54:49.254668: step 10770, loss = 1.00 (649.9 examples/sec; 0.197 sec/batch)
2016-12-08 03:54:51.344780: step 10780, loss = 0.88 (575.4 examples/sec; 0.222 sec/batch)
2016-12-08 03:54:53.524556: step 10790, loss = 0.94 (594.7 examples/sec; 0.215 sec/batch)
2016-12-08 03:54:55.626750: step 10800, loss = 0.80 (613.2 examples/sec; 0.209 sec/batch)
2016-12-08 03:54:57.968752: step 10810, loss = 0.88 (606.0 examples/sec; 0.211 sec/batch)
2016-12-08 03:55:00.094607: step 10820, loss = 0.83 (584.2 examples/sec; 0.219 sec/batch)
2016-12-08 03:55:02.259513: step 10830, loss = 0.99 (607.5 examples/sec; 0.211 sec/batch)
2016-12-08 03:55:04.330032: step 10840, loss = 0.91 (626.9 examples/sec; 0.204 sec/batch)
2016-12-08 03:55:06.410690: step 10850, loss = 0.85 (609.8 examples/sec; 0.210 sec/batch)
2016-12-08 03:55:08.449084: step 10860, loss = 0.73 (650.1 examples/sec; 0.197 sec/batch)
2016-12-08 03:55:10.515035: step 10870, loss = 0.79 (640.6 examples/sec; 0.200 sec/batch)
2016-12-08 03:55:12.601537: step 10880, loss = 1.08 (610.8 examples/sec; 0.210 sec/batch)
2016-12-08 03:55:14.710895: step 10890, loss = 0.80 (613.5 examples/sec; 0.209 sec/batch)
2016-12-08 03:55:16.729520: step 10900, loss = 0.82 (632.6 examples/sec; 0.202 sec/batch)
2016-12-08 03:55:19.034199: step 10910, loss = 0.86 (637.5 examples/sec; 0.201 sec/batch)
2016-12-08 03:55:21.138053: step 10920, loss = 0.78 (639.6 examples/sec; 0.200 sec/batch)
2016-12-08 03:55:23.213695: step 10930, loss = 0.92 (600.3 examples/sec; 0.213 sec/batch)
2016-12-08 03:55:25.302952: step 10940, loss = 0.77 (603.8 examples/sec; 0.212 sec/batch)
2016-12-08 03:55:27.354723: step 10950, loss = 0.82 (653.4 examples/sec; 0.196 sec/batch)
2016-12-08 03:55:29.368769: step 10960, loss = 0.94 (616.8 examples/sec; 0.208 sec/batch)
2016-12-08 03:55:31.493748: step 10970, loss = 0.92 (660.9 examples/sec; 0.194 sec/batch)
2016-12-08 03:55:33.581987: step 10980, loss = 0.97 (644.3 examples/sec; 0.199 sec/batch)
2016-12-08 03:55:35.699714: step 10990, loss = 0.96 (638.1 examples/sec; 0.201 sec/batch)
2016-12-08 03:55:37.764648: step 11000, loss = 0.89 (672.3 examples/sec; 0.190 sec/batch)
2016-12-08 03:55:40.695151: step 11010, loss = 0.73 (582.8 examples/sec; 0.220 sec/batch)
2016-12-08 03:55:42.844623: step 11020, loss = 0.79 (619.6 examples/sec; 0.207 sec/batch)
2016-12-08 03:55:44.894946: step 11030, loss = 1.02 (645.3 examples/sec; 0.198 sec/batch)
2016-12-08 03:55:46.981775: step 11040, loss = 0.83 (613.2 examples/sec; 0.209 sec/batch)
2016-12-08 03:55:49.024329: step 11050, loss = 1.17 (603.6 examples/sec; 0.212 sec/batch)
2016-12-08 03:55:51.167130: step 11060, loss = 0.94 (611.0 examples/sec; 0.209 sec/batch)
2016-12-08 03:55:53.154799: step 11070, loss = 0.78 (615.5 examples/sec; 0.208 sec/batch)
2016-12-08 03:55:55.264835: step 11080, loss = 0.89 (628.3 examples/sec; 0.204 sec/batch)
2016-12-08 03:55:57.313167: step 11090, loss = 0.84 (630.2 examples/sec; 0.203 sec/batch)
2016-12-08 03:55:59.467940: step 11100, loss = 0.86 (588.1 examples/sec; 0.218 sec/batch)
2016-12-08 03:56:01.794185: step 11110, loss = 0.84 (604.1 examples/sec; 0.212 sec/batch)
2016-12-08 03:56:03.867504: step 11120, loss = 0.98 (553.0 examples/sec; 0.231 sec/batch)
2016-12-08 03:56:06.037945: step 11130, loss = 0.92 (562.7 examples/sec; 0.227 sec/batch)
2016-12-08 03:56:08.174170: step 11140, loss = 0.93 (595.2 examples/sec; 0.215 sec/batch)
2016-12-08 03:56:10.247015: step 11150, loss = 1.00 (658.1 examples/sec; 0.194 sec/batch)
2016-12-08 03:56:12.351812: step 11160, loss = 1.08 (610.2 examples/sec; 0.210 sec/batch)
2016-12-08 03:56:14.378162: step 11170, loss = 0.82 (667.8 examples/sec; 0.192 sec/batch)
2016-12-08 03:56:16.359257: step 11180, loss = 0.93 (631.9 examples/sec; 0.203 sec/batch)
2016-12-08 03:56:18.458713: step 11190, loss = 0.93 (498.3 examples/sec; 0.257 sec/batch)
2016-12-08 03:56:20.571554: step 11200, loss = 0.86 (575.4 examples/sec; 0.222 sec/batch)
2016-12-08 03:56:22.928747: step 11210, loss = 1.15 (671.5 examples/sec; 0.191 sec/batch)
2016-12-08 03:56:24.999968: step 11220, loss = 1.02 (585.2 examples/sec; 0.219 sec/batch)
2016-12-08 03:56:27.119045: step 11230, loss = 0.76 (620.4 examples/sec; 0.206 sec/batch)
2016-12-08 03:56:29.222350: step 11240, loss = 0.80 (638.6 examples/sec; 0.200 sec/batch)
2016-12-08 03:56:31.391717: step 11250, loss = 1.15 (588.7 examples/sec; 0.217 sec/batch)
2016-12-08 03:56:33.485173: step 11260, loss = 0.93 (621.4 examples/sec; 0.206 sec/batch)
2016-12-08 03:56:35.610678: step 11270, loss = 0.80 (636.1 examples/sec; 0.201 sec/batch)
2016-12-08 03:56:37.700170: step 11280, loss = 0.85 (618.6 examples/sec; 0.207 sec/batch)
2016-12-08 03:56:39.850346: step 11290, loss = 0.90 (642.8 examples/sec; 0.199 sec/batch)
2016-12-08 03:56:42.030164: step 11300, loss = 0.84 (603.7 examples/sec; 0.212 sec/batch)
2016-12-08 03:56:44.336351: step 11310, loss = 0.86 (608.1 examples/sec; 0.211 sec/batch)
2016-12-08 03:56:46.430256: step 11320, loss = 0.74 (616.9 examples/sec; 0.207 sec/batch)
2016-12-08 03:56:48.539586: step 11330, loss = 0.88 (614.2 examples/sec; 0.208 sec/batch)
2016-12-08 03:56:50.635542: step 11340, loss = 0.80 (615.5 examples/sec; 0.208 sec/batch)
2016-12-08 03:56:52.738680: step 11350, loss = 0.88 (655.5 examples/sec; 0.195 sec/batch)
2016-12-08 03:56:54.784350: step 11360, loss = 0.94 (688.0 examples/sec; 0.186 sec/batch)
2016-12-08 03:56:56.963967: step 11370, loss = 0.78 (600.7 examples/sec; 0.213 sec/batch)
2016-12-08 03:56:59.146231: step 11380, loss = 0.87 (555.2 examples/sec; 0.231 sec/batch)
2016-12-08 03:57:01.252410: step 11390, loss = 0.87 (576.6 examples/sec; 0.222 sec/batch)
2016-12-08 03:57:03.337074: step 11400, loss = 0.83 (604.5 examples/sec; 0.212 sec/batch)
2016-12-08 03:57:05.622169: step 11410, loss = 0.82 (595.1 examples/sec; 0.215 sec/batch)
2016-12-08 03:57:07.728995: step 11420, loss = 0.88 (573.1 examples/sec; 0.223 sec/batch)
2016-12-08 03:57:09.774571: step 11430, loss = 0.86 (585.9 examples/sec; 0.218 sec/batch)
2016-12-08 03:57:11.845117: step 11440, loss = 0.89 (572.3 examples/sec; 0.224 sec/batch)
2016-12-08 03:57:13.865509: step 11450, loss = 1.19 (620.3 examples/sec; 0.206 sec/batch)
2016-12-08 03:57:15.954570: step 11460, loss = 0.88 (636.7 examples/sec; 0.201 sec/batch)
2016-12-08 03:57:17.973848: step 11470, loss = 0.81 (647.2 examples/sec; 0.198 sec/batch)
2016-12-08 03:57:20.072683: step 11480, loss = 0.94 (641.5 examples/sec; 0.200 sec/batch)
2016-12-08 03:57:22.136610: step 11490, loss = 0.91 (623.8 examples/sec; 0.205 sec/batch)
2016-12-08 03:57:24.207211: step 11500, loss = 0.91 (610.5 examples/sec; 0.210 sec/batch)
2016-12-08 03:57:26.545030: step 11510, loss = 0.90 (584.8 examples/sec; 0.219 sec/batch)
2016-12-08 03:57:28.626534: step 11520, loss = 0.97 (640.8 examples/sec; 0.200 sec/batch)
2016-12-08 03:57:30.813442: step 11530, loss = 0.87 (583.2 examples/sec; 0.219 sec/batch)
2016-12-08 03:57:32.920256: step 11540, loss = 0.78 (626.5 examples/sec; 0.204 sec/batch)
2016-12-08 03:57:35.062216: step 11550, loss = 0.71 (618.7 examples/sec; 0.207 sec/batch)
2016-12-08 03:57:37.188078: step 11560, loss = 1.05 (636.8 examples/sec; 0.201 sec/batch)
2016-12-08 03:57:39.292838: step 11570, loss = 0.85 (513.3 examples/sec; 0.249 sec/batch)
2016-12-08 03:57:41.305733: step 11580, loss = 0.77 (665.1 examples/sec; 0.192 sec/batch)
2016-12-08 03:57:43.402207: step 11590, loss = 0.91 (526.1 examples/sec; 0.243 sec/batch)
2016-12-08 03:57:45.467206: step 11600, loss = 0.84 (587.0 examples/sec; 0.218 sec/batch)
2016-12-08 03:57:47.805599: step 11610, loss = 0.99 (651.3 examples/sec; 0.197 sec/batch)
2016-12-08 03:57:49.873794: step 11620, loss = 0.93 (666.6 examples/sec; 0.192 sec/batch)
2016-12-08 03:57:51.934183: step 11630, loss = 0.94 (597.4 examples/sec; 0.214 sec/batch)
2016-12-08 03:57:54.017174: step 11640, loss = 0.81 (617.5 examples/sec; 0.207 sec/batch)
2016-12-08 03:57:56.089098: step 11650, loss = 1.02 (621.5 examples/sec; 0.206 sec/batch)
2016-12-08 03:57:58.122047: step 11660, loss = 0.94 (606.7 examples/sec; 0.211 sec/batch)
2016-12-08 03:58:00.184228: step 11670, loss = 0.96 (624.4 examples/sec; 0.205 sec/batch)
2016-12-08 03:58:02.207970: step 11680, loss = 0.87 (595.5 examples/sec; 0.215 sec/batch)
2016-12-08 03:58:04.313686: step 11690, loss = 0.96 (630.9 examples/sec; 0.203 sec/batch)
2016-12-08 03:58:06.319508: step 11700, loss = 0.97 (622.9 examples/sec; 0.206 sec/batch)
2016-12-08 03:58:08.624136: step 11710, loss = 0.78 (684.9 examples/sec; 0.187 sec/batch)
2016-12-08 03:58:10.714869: step 11720, loss = 0.91 (596.8 examples/sec; 0.214 sec/batch)
2016-12-08 03:58:12.800491: step 11730, loss = 0.85 (609.1 examples/sec; 0.210 sec/batch)
2016-12-08 03:58:14.938065: step 11740, loss = 0.81 (616.6 examples/sec; 0.208 sec/batch)
2016-12-08 03:58:17.110782: step 11750, loss = 0.90 (614.4 examples/sec; 0.208 sec/batch)
2016-12-08 03:58:19.244007: step 11760, loss = 0.84 (551.9 examples/sec; 0.232 sec/batch)
2016-12-08 03:58:21.267000: step 11770, loss = 0.82 (637.0 examples/sec; 0.201 sec/batch)
2016-12-08 03:58:23.343410: step 11780, loss = 0.81 (618.1 examples/sec; 0.207 sec/batch)
2016-12-08 03:58:25.388141: step 11790, loss = 0.89 (599.0 examples/sec; 0.214 sec/batch)
2016-12-08 03:58:27.417643: step 11800, loss = 0.70 (655.8 examples/sec; 0.195 sec/batch)
2016-12-08 03:58:29.788275: step 11810, loss = 1.00 (597.2 examples/sec; 0.214 sec/batch)
2016-12-08 03:58:31.884142: step 11820, loss = 0.86 (661.6 examples/sec; 0.193 sec/batch)
2016-12-08 03:58:33.991131: step 11830, loss = 0.76 (634.4 examples/sec; 0.202 sec/batch)
2016-12-08 03:58:36.026565: step 11840, loss = 0.91 (645.3 examples/sec; 0.198 sec/batch)
2016-12-08 03:58:38.146567: step 11850, loss = 0.86 (664.0 examples/sec; 0.193 sec/batch)
2016-12-08 03:58:40.251231: step 11860, loss = 0.84 (603.0 examples/sec; 0.212 sec/batch)
2016-12-08 03:58:42.312704: step 11870, loss = 0.90 (642.4 examples/sec; 0.199 sec/batch)
2016-12-08 03:58:44.369828: step 11880, loss = 0.79 (652.8 examples/sec; 0.196 sec/batch)
2016-12-08 03:58:46.395927: step 11890, loss = 1.02 (598.0 examples/sec; 0.214 sec/batch)
2016-12-08 03:58:48.485129: step 11900, loss = 0.76 (641.4 examples/sec; 0.200 sec/batch)
2016-12-08 03:58:50.780764: step 11910, loss = 0.99 (656.3 examples/sec; 0.195 sec/batch)
2016-12-08 03:58:52.874871: step 11920, loss = 0.89 (612.0 examples/sec; 0.209 sec/batch)
2016-12-08 03:58:55.019191: step 11930, loss = 1.01 (615.3 examples/sec; 0.208 sec/batch)
2016-12-08 03:58:57.035853: step 11940, loss = 0.86 (621.7 examples/sec; 0.206 sec/batch)
2016-12-08 03:58:59.097018: step 11950, loss = 0.80 (594.1 examples/sec; 0.215 sec/batch)
2016-12-08 03:59:01.212529: step 11960, loss = 0.89 (596.3 examples/sec; 0.215 sec/batch)
2016-12-08 03:59:03.326517: step 11970, loss = 0.90 (631.6 examples/sec; 0.203 sec/batch)
2016-12-08 03:59:05.425478: step 11980, loss = 0.97 (611.0 examples/sec; 0.209 sec/batch)
2016-12-08 03:59:07.523625: step 11990, loss = 0.76 (660.0 examples/sec; 0.194 sec/batch)
2016-12-08 03:59:09.578389: step 12000, loss = 0.72 (577.9 examples/sec; 0.221 sec/batch)
2016-12-08 03:59:12.584409: step 12010, loss = 0.86 (644.3 examples/sec; 0.199 sec/batch)
2016-12-08 03:59:14.726127: step 12020, loss = 0.79 (604.9 examples/sec; 0.212 sec/batch)
2016-12-08 03:59:16.771587: step 12030, loss = 0.84 (650.2 examples/sec; 0.197 sec/batch)
2016-12-08 03:59:18.895510: step 12040, loss = 0.95 (618.1 examples/sec; 0.207 sec/batch)
2016-12-08 03:59:20.958819: step 12050, loss = 0.71 (630.4 examples/sec; 0.203 sec/batch)
2016-12-08 03:59:23.162946: step 12060, loss = 0.95 (587.1 examples/sec; 0.218 sec/batch)
2016-12-08 03:59:25.212876: step 12070, loss = 0.87 (681.3 examples/sec; 0.188 sec/batch)
2016-12-08 03:59:27.288240: step 12080, loss = 0.86 (689.1 examples/sec; 0.186 sec/batch)
2016-12-08 03:59:29.385290: step 12090, loss = 1.03 (611.3 examples/sec; 0.209 sec/batch)
2016-12-08 03:59:31.523949: step 12100, loss = 1.00 (613.8 examples/sec; 0.209 sec/batch)
2016-12-08 03:59:33.995400: step 12110, loss = 0.82 (536.1 examples/sec; 0.239 sec/batch)
2016-12-08 03:59:36.174167: step 12120, loss = 1.00 (568.4 examples/sec; 0.225 sec/batch)
2016-12-08 03:59:38.328241: step 12130, loss = 0.85 (596.6 examples/sec; 0.215 sec/batch)
2016-12-08 03:59:40.370940: step 12140, loss = 0.76 (640.8 examples/sec; 0.200 sec/batch)
2016-12-08 03:59:42.401723: step 12150, loss = 0.90 (626.2 examples/sec; 0.204 sec/batch)
2016-12-08 03:59:44.461300: step 12160, loss = 0.82 (580.6 examples/sec; 0.220 sec/batch)
2016-12-08 03:59:46.555532: step 12170, loss = 0.76 (622.0 examples/sec; 0.206 sec/batch)
2016-12-08 03:59:48.692050: step 12180, loss = 0.86 (583.7 examples/sec; 0.219 sec/batch)
2016-12-08 03:59:50.845732: step 12190, loss = 0.93 (575.1 examples/sec; 0.223 sec/batch)
2016-12-08 03:59:53.033303: step 12200, loss = 0.78 (634.5 examples/sec; 0.202 sec/batch)
2016-12-08 03:59:55.365251: step 12210, loss = 0.91 (604.0 examples/sec; 0.212 sec/batch)
2016-12-08 03:59:57.445453: step 12220, loss = 0.84 (664.9 examples/sec; 0.193 sec/batch)
2016-12-08 03:59:59.536062: step 12230, loss = 0.71 (640.9 examples/sec; 0.200 sec/batch)
2016-12-08 04:00:01.538381: step 12240, loss = 0.93 (616.4 examples/sec; 0.208 sec/batch)
2016-12-08 04:00:03.559288: step 12250, loss = 1.03 (596.9 examples/sec; 0.214 sec/batch)
2016-12-08 04:00:05.609055: step 12260, loss = 1.03 (656.5 examples/sec; 0.195 sec/batch)
2016-12-08 04:00:07.749029: step 12270, loss = 0.81 (528.0 examples/sec; 0.242 sec/batch)
2016-12-08 04:00:09.789084: step 12280, loss = 1.10 (667.0 examples/sec; 0.192 sec/batch)
2016-12-08 04:00:11.832762: step 12290, loss = 0.81 (640.3 examples/sec; 0.200 sec/batch)
2016-12-08 04:00:14.039876: step 12300, loss = 0.87 (507.6 examples/sec; 0.252 sec/batch)
2016-12-08 04:00:16.425379: step 12310, loss = 0.84 (590.9 examples/sec; 0.217 sec/batch)
2016-12-08 04:00:18.462544: step 12320, loss = 0.80 (681.3 examples/sec; 0.188 sec/batch)
2016-12-08 04:00:20.559118: step 12330, loss = 0.88 (604.2 examples/sec; 0.212 sec/batch)
2016-12-08 04:00:22.616625: step 12340, loss = 0.84 (630.6 examples/sec; 0.203 sec/batch)
2016-12-08 04:00:24.738784: step 12350, loss = 1.11 (655.0 examples/sec; 0.195 sec/batch)
2016-12-08 04:00:26.814121: step 12360, loss = 0.80 (560.9 examples/sec; 0.228 sec/batch)
2016-12-08 04:00:28.944082: step 12370, loss = 0.97 (634.4 examples/sec; 0.202 sec/batch)
2016-12-08 04:00:31.006048: step 12380, loss = 0.94 (630.0 examples/sec; 0.203 sec/batch)
2016-12-08 04:00:33.128691: step 12390, loss = 0.81 (581.1 examples/sec; 0.220 sec/batch)
2016-12-08 04:00:35.230584: step 12400, loss = 0.81 (590.8 examples/sec; 0.217 sec/batch)
2016-12-08 04:00:37.461849: step 12410, loss = 0.84 (564.1 examples/sec; 0.227 sec/batch)
2016-12-08 04:00:39.533630: step 12420, loss = 0.85 (594.0 examples/sec; 0.215 sec/batch)
2016-12-08 04:00:41.623959: step 12430, loss = 0.93 (618.8 examples/sec; 0.207 sec/batch)
2016-12-08 04:00:43.704672: step 12440, loss = 0.62 (553.3 examples/sec; 0.231 sec/batch)
2016-12-08 04:00:45.729343: step 12450, loss = 0.89 (636.6 examples/sec; 0.201 sec/batch)
2016-12-08 04:00:47.793252: step 12460, loss = 0.88 (586.1 examples/sec; 0.218 sec/batch)
2016-12-08 04:00:49.862550: step 12470, loss = 0.96 (629.9 examples/sec; 0.203 sec/batch)
2016-12-08 04:00:51.904187: step 12480, loss = 0.84 (611.3 examples/sec; 0.209 sec/batch)
2016-12-08 04:00:53.927462: step 12490, loss = 0.72 (673.5 examples/sec; 0.190 sec/batch)
2016-12-08 04:00:55.992671: step 12500, loss = 0.84 (649.5 examples/sec; 0.197 sec/batch)
2016-12-08 04:00:58.287874: step 12510, loss = 0.83 (575.9 examples/sec; 0.222 sec/batch)
2016-12-08 04:01:00.397365: step 12520, loss = 0.90 (560.2 examples/sec; 0.228 sec/batch)
2016-12-08 04:01:02.439635: step 12530, loss = 0.92 (625.8 examples/sec; 0.205 sec/batch)
2016-12-08 04:01:04.510476: step 12540, loss = 0.97 (618.0 examples/sec; 0.207 sec/batch)
2016-12-08 04:01:06.611444: step 12550, loss = 0.94 (586.4 examples/sec; 0.218 sec/batch)
2016-12-08 04:01:08.719049: step 12560, loss = 0.85 (597.4 examples/sec; 0.214 sec/batch)
2016-12-08 04:01:10.790540: step 12570, loss = 0.91 (641.6 examples/sec; 0.200 sec/batch)
2016-12-08 04:01:12.778628: step 12580, loss = 0.98 (630.8 examples/sec; 0.203 sec/batch)
2016-12-08 04:01:14.872908: step 12590, loss = 0.91 (589.1 examples/sec; 0.217 sec/batch)
2016-12-08 04:01:17.005541: step 12600, loss = 1.03 (629.3 examples/sec; 0.203 sec/batch)
2016-12-08 04:01:19.247591: step 12610, loss = 0.77 (633.1 examples/sec; 0.202 sec/batch)
2016-12-08 04:01:21.294794: step 12620, loss = 0.84 (587.7 examples/sec; 0.218 sec/batch)
2016-12-08 04:01:23.337067: step 12630, loss = 0.78 (586.4 examples/sec; 0.218 sec/batch)
2016-12-08 04:01:25.437538: step 12640, loss = 0.85 (596.7 examples/sec; 0.215 sec/batch)
2016-12-08 04:01:27.457192: step 12650, loss = 0.92 (611.3 examples/sec; 0.209 sec/batch)
2016-12-08 04:01:29.519134: step 12660, loss = 0.91 (649.6 examples/sec; 0.197 sec/batch)
2016-12-08 04:01:31.608703: step 12670, loss = 0.93 (575.3 examples/sec; 0.222 sec/batch)
2016-12-08 04:01:33.681521: step 12680, loss = 0.83 (584.8 examples/sec; 0.219 sec/batch)
2016-12-08 04:01:35.836729: step 12690, loss = 0.85 (618.1 examples/sec; 0.207 sec/batch)
2016-12-08 04:01:37.933837: step 12700, loss = 0.83 (576.3 examples/sec; 0.222 sec/batch)
2016-12-08 04:01:40.203325: step 12710, loss = 0.79 (636.2 examples/sec; 0.201 sec/batch)
2016-12-08 04:01:42.248726: step 12720, loss = 0.71 (647.9 examples/sec; 0.198 sec/batch)
2016-12-08 04:01:44.373710: step 12730, loss = 0.75 (593.6 examples/sec; 0.216 sec/batch)
2016-12-08 04:01:46.436044: step 12740, loss = 0.95 (618.1 examples/sec; 0.207 sec/batch)
2016-12-08 04:01:48.515117: step 12750, loss = 0.65 (666.3 examples/sec; 0.192 sec/batch)
2016-12-08 04:01:50.592108: step 12760, loss = 0.77 (600.7 examples/sec; 0.213 sec/batch)
2016-12-08 04:01:52.657188: step 12770, loss = 0.99 (593.6 examples/sec; 0.216 sec/batch)
2016-12-08 04:01:54.700117: step 12780, loss = 0.89 (597.7 examples/sec; 0.214 sec/batch)
2016-12-08 04:01:56.804675: step 12790, loss = 0.83 (652.4 examples/sec; 0.196 sec/batch)
2016-12-08 04:01:58.852779: step 12800, loss = 0.80 (597.4 examples/sec; 0.214 sec/batch)
2016-12-08 04:02:01.204707: step 12810, loss = 0.88 (604.0 examples/sec; 0.212 sec/batch)
2016-12-08 04:02:03.373753: step 12820, loss = 0.89 (598.6 examples/sec; 0.214 sec/batch)
2016-12-08 04:02:05.464972: step 12830, loss = 0.94 (622.9 examples/sec; 0.205 sec/batch)
2016-12-08 04:02:07.564437: step 12840, loss = 0.76 (634.4 examples/sec; 0.202 sec/batch)
2016-12-08 04:02:09.627332: step 12850, loss = 0.93 (647.6 examples/sec; 0.198 sec/batch)
2016-12-08 04:02:11.709781: step 12860, loss = 0.95 (669.8 examples/sec; 0.191 sec/batch)
2016-12-08 04:02:13.772935: step 12870, loss = 0.85 (591.1 examples/sec; 0.217 sec/batch)
2016-12-08 04:02:15.885630: step 12880, loss = 0.76 (635.7 examples/sec; 0.201 sec/batch)
2016-12-08 04:02:18.008476: step 12890, loss = 0.85 (561.2 examples/sec; 0.228 sec/batch)
2016-12-08 04:02:20.161406: step 12900, loss = 0.71 (590.9 examples/sec; 0.217 sec/batch)
2016-12-08 04:02:22.512464: step 12910, loss = 0.92 (671.8 examples/sec; 0.191 sec/batch)
2016-12-08 04:02:24.556352: step 12920, loss = 0.87 (607.0 examples/sec; 0.211 sec/batch)
2016-12-08 04:02:26.552607: step 12930, loss = 0.79 (649.5 examples/sec; 0.197 sec/batch)
2016-12-08 04:02:28.624371: step 12940, loss = 0.67 (576.5 examples/sec; 0.222 sec/batch)
2016-12-08 04:02:30.810459: step 12950, loss = 1.02 (571.7 examples/sec; 0.224 sec/batch)
2016-12-08 04:02:32.903056: step 12960, loss = 0.97 (564.0 examples/sec; 0.227 sec/batch)
2016-12-08 04:02:34.944638: step 12970, loss = 0.97 (627.2 examples/sec; 0.204 sec/batch)
2016-12-08 04:02:36.952790: step 12980, loss = 0.81 (666.7 examples/sec; 0.192 sec/batch)
2016-12-08 04:02:39.024448: step 12990, loss = 0.96 (618.4 examples/sec; 0.207 sec/batch)
2016-12-08 04:02:41.137793: step 13000, loss = 0.76 (590.0 examples/sec; 0.217 sec/batch)
2016-12-08 04:02:44.020482: step 13010, loss = 0.89 (599.9 examples/sec; 0.213 sec/batch)
2016-12-08 04:02:46.114343: step 13020, loss = 0.96 (608.7 examples/sec; 0.210 sec/batch)
2016-12-08 04:02:48.178281: step 13030, loss = 0.82 (594.7 examples/sec; 0.215 sec/batch)
2016-12-08 04:02:50.320811: step 13040, loss = 0.85 (617.2 examples/sec; 0.207 sec/batch)
2016-12-08 04:02:52.391773: step 13050, loss = 0.92 (611.1 examples/sec; 0.209 sec/batch)
2016-12-08 04:02:54.388575: step 13060, loss = 0.93 (629.4 examples/sec; 0.203 sec/batch)
2016-12-08 04:02:56.486663: step 13070, loss = 0.67 (609.1 examples/sec; 0.210 sec/batch)
2016-12-08 04:02:58.581202: step 13080, loss = 0.77 (613.3 examples/sec; 0.209 sec/batch)
2016-12-08 04:03:00.609518: step 13090, loss = 0.88 (650.8 examples/sec; 0.197 sec/batch)
2016-12-08 04:03:02.629199: step 13100, loss = 0.92 (592.6 examples/sec; 0.216 sec/batch)
2016-12-08 04:03:04.866362: step 13110, loss = 0.90 (662.6 examples/sec; 0.193 sec/batch)
2016-12-08 04:03:06.964224: step 13120, loss = 0.70 (538.5 examples/sec; 0.238 sec/batch)
2016-12-08 04:03:09.118715: step 13130, loss = 0.88 (601.6 examples/sec; 0.213 sec/batch)
2016-12-08 04:03:11.178833: step 13140, loss = 0.78 (629.2 examples/sec; 0.203 sec/batch)
2016-12-08 04:03:13.251514: step 13150, loss = 0.87 (612.6 examples/sec; 0.209 sec/batch)
2016-12-08 04:03:15.545574: step 13160, loss = 0.84 (550.7 examples/sec; 0.232 sec/batch)
2016-12-08 04:03:18.071770: step 13170, loss = 0.97 (445.5 examples/sec; 0.287 sec/batch)
2016-12-08 04:03:20.490127: step 13180, loss = 0.81 (643.4 examples/sec; 0.199 sec/batch)
2016-12-08 04:03:22.956236: step 13190, loss = 0.95 (468.2 examples/sec; 0.273 sec/batch)
2016-12-08 04:03:25.247465: step 13200, loss = 0.84 (458.0 examples/sec; 0.279 sec/batch)
2016-12-08 04:03:28.015694: step 13210, loss = 0.86 (597.8 examples/sec; 0.214 sec/batch)
2016-12-08 04:03:30.478623: step 13220, loss = 0.77 (512.3 examples/sec; 0.250 sec/batch)
2016-12-08 04:03:33.013252: step 13230, loss = 0.81 (585.7 examples/sec; 0.219 sec/batch)
2016-12-08 04:03:35.483213: step 13240, loss = 0.97 (476.5 examples/sec; 0.269 sec/batch)
2016-12-08 04:03:38.068920: step 13250, loss = 0.98 (504.3 examples/sec; 0.254 sec/batch)
2016-12-08 04:03:40.533226: step 13260, loss = 0.86 (565.0 examples/sec; 0.227 sec/batch)
2016-12-08 04:03:42.961676: step 13270, loss = 0.90 (507.7 examples/sec; 0.252 sec/batch)
2016-12-08 04:03:45.546063: step 13280, loss = 0.97 (508.8 examples/sec; 0.252 sec/batch)
2016-12-08 04:03:47.855017: step 13290, loss = 0.94 (528.2 examples/sec; 0.242 sec/batch)
2016-12-08 04:03:50.327230: step 13300, loss = 0.80 (580.0 examples/sec; 0.221 sec/batch)
2016-12-08 04:03:53.066574: step 13310, loss = 0.89 (570.9 examples/sec; 0.224 sec/batch)
2016-12-08 04:03:55.690045: step 13320, loss = 0.91 (479.0 examples/sec; 0.267 sec/batch)
2016-12-08 04:03:58.264237: step 13330, loss = 0.80 (516.7 examples/sec; 0.248 sec/batch)
2016-12-08 04:04:00.871982: step 13340, loss = 0.99 (449.2 examples/sec; 0.285 sec/batch)
2016-12-08 04:04:03.337788: step 13350, loss = 0.93 (610.4 examples/sec; 0.210 sec/batch)
2016-12-08 04:04:05.769642: step 13360, loss = 0.72 (458.5 examples/sec; 0.279 sec/batch)
2016-12-08 04:04:08.241853: step 13370, loss = 0.71 (508.8 examples/sec; 0.252 sec/batch)
2016-12-08 04:04:10.736328: step 13380, loss = 0.85 (504.5 examples/sec; 0.254 sec/batch)
2016-12-08 04:04:13.200838: step 13390, loss = 1.06 (572.0 examples/sec; 0.224 sec/batch)
2016-12-08 04:04:15.818889: step 13400, loss = 0.91 (453.6 examples/sec; 0.282 sec/batch)
2016-12-08 04:04:18.548702: step 13410, loss = 0.81 (529.4 examples/sec; 0.242 sec/batch)
2016-12-08 04:04:20.890825: step 13420, loss = 1.02 (554.6 examples/sec; 0.231 sec/batch)
2016-12-08 04:04:23.344948: step 13430, loss = 0.82 (575.2 examples/sec; 0.223 sec/batch)
2016-12-08 04:04:25.822002: step 13440, loss = 0.79 (547.5 examples/sec; 0.234 sec/batch)
2016-12-08 04:04:28.275248: step 13450, loss = 0.90 (575.8 examples/sec; 0.222 sec/batch)
2016-12-08 04:04:30.780593: step 13460, loss = 1.03 (480.8 examples/sec; 0.266 sec/batch)
2016-12-08 04:04:33.331307: step 13470, loss = 0.79 (518.9 examples/sec; 0.247 sec/batch)
2016-12-08 04:04:35.879933: step 13480, loss = 0.64 (504.8 examples/sec; 0.254 sec/batch)
2016-12-08 04:04:38.385911: step 13490, loss = 0.77 (486.9 examples/sec; 0.263 sec/batch)
2016-12-08 04:04:40.828934: step 13500, loss = 0.81 (509.1 examples/sec; 0.251 sec/batch)
2016-12-08 04:04:43.593032: step 13510, loss = 0.81 (467.7 examples/sec; 0.274 sec/batch)
2016-12-08 04:04:46.073902: step 13520, loss = 0.89 (481.2 examples/sec; 0.266 sec/batch)
2016-12-08 04:04:48.548259: step 13530, loss = 0.80 (513.4 examples/sec; 0.249 sec/batch)
2016-12-08 04:04:50.961998: step 13540, loss = 1.11 (546.7 examples/sec; 0.234 sec/batch)
2016-12-08 04:04:53.603042: step 13550, loss = 0.92 (479.9 examples/sec; 0.267 sec/batch)
2016-12-08 04:04:56.120006: step 13560, loss = 0.86 (569.6 examples/sec; 0.225 sec/batch)
2016-12-08 04:04:58.701409: step 13570, loss = 0.89 (523.6 examples/sec; 0.244 sec/batch)
2016-12-08 04:05:01.203545: step 13580, loss = 0.83 (546.4 examples/sec; 0.234 sec/batch)
2016-12-08 04:05:03.707478: step 13590, loss = 1.03 (515.8 examples/sec; 0.248 sec/batch)
2016-12-08 04:05:06.125587: step 13600, loss = 0.90 (572.0 examples/sec; 0.224 sec/batch)
2016-12-08 04:05:08.839098: step 13610, loss = 0.85 (545.0 examples/sec; 0.235 sec/batch)
2016-12-08 04:05:11.304708: step 13620, loss = 0.82 (507.2 examples/sec; 0.252 sec/batch)
2016-12-08 04:05:13.844527: step 13630, loss = 0.89 (501.7 examples/sec; 0.255 sec/batch)
2016-12-08 04:05:16.347187: step 13640, loss = 1.03 (555.0 examples/sec; 0.231 sec/batch)
2016-12-08 04:05:18.889819: step 13650, loss = 0.89 (499.5 examples/sec; 0.256 sec/batch)
2016-12-08 04:05:21.305195: step 13660, loss = 1.01 (530.7 examples/sec; 0.241 sec/batch)
2016-12-08 04:05:23.787961: step 13670, loss = 0.64 (487.5 examples/sec; 0.263 sec/batch)
2016-12-08 04:05:26.161893: step 13680, loss = 0.83 (612.3 examples/sec; 0.209 sec/batch)
2016-12-08 04:05:28.752107: step 13690, loss = 0.91 (471.3 examples/sec; 0.272 sec/batch)
2016-12-08 04:05:31.137286: step 13700, loss = 0.86 (529.5 examples/sec; 0.242 sec/batch)
2016-12-08 04:05:33.998523: step 13710, loss = 0.76 (552.1 examples/sec; 0.232 sec/batch)
2016-12-08 04:05:36.479873: step 13720, loss = 0.88 (541.2 examples/sec; 0.236 sec/batch)
2016-12-08 04:05:38.960478: step 13730, loss = 0.88 (568.0 examples/sec; 0.225 sec/batch)
2016-12-08 04:05:41.542000: step 13740, loss = 0.81 (456.6 examples/sec; 0.280 sec/batch)
2016-12-08 04:05:44.084683: step 13750, loss = 0.81 (572.7 examples/sec; 0.223 sec/batch)
2016-12-08 04:05:46.550102: step 13760, loss = 0.92 (463.6 examples/sec; 0.276 sec/batch)
2016-12-08 04:05:48.991574: step 13770, loss = 0.71 (571.0 examples/sec; 0.224 sec/batch)
2016-12-08 04:05:51.553502: step 13780, loss = 0.73 (484.9 examples/sec; 0.264 sec/batch)
2016-12-08 04:05:54.085205: step 13790, loss = 0.87 (516.8 examples/sec; 0.248 sec/batch)
2016-12-08 04:05:56.611022: step 13800, loss = 0.94 (507.7 examples/sec; 0.252 sec/batch)
2016-12-08 04:05:59.366915: step 13810, loss = 0.88 (544.6 examples/sec; 0.235 sec/batch)
2016-12-08 04:06:01.780687: step 13820, loss = 0.72 (554.3 examples/sec; 0.231 sec/batch)
2016-12-08 04:06:04.358619: step 13830, loss = 0.92 (489.8 examples/sec; 0.261 sec/batch)
2016-12-08 04:06:06.739410: step 13840, loss = 0.65 (589.2 examples/sec; 0.217 sec/batch)
2016-12-08 04:06:09.181361: step 13850, loss = 0.78 (540.0 examples/sec; 0.237 sec/batch)
2016-12-08 04:06:11.669103: step 13860, loss = 0.81 (482.9 examples/sec; 0.265 sec/batch)
2016-12-08 04:06:13.974552: step 13870, loss = 0.72 (477.4 examples/sec; 0.268 sec/batch)
2016-12-08 04:06:16.455168: step 13880, loss = 0.87 (518.5 examples/sec; 0.247 sec/batch)
2016-12-08 04:06:18.969599: step 13890, loss = 0.69 (568.6 examples/sec; 0.225 sec/batch)
2016-12-08 04:06:21.461114: step 13900, loss = 0.82 (505.4 examples/sec; 0.253 sec/batch)
2016-12-08 04:06:24.330136: step 13910, loss = 1.01 (521.5 examples/sec; 0.245 sec/batch)
2016-12-08 04:06:26.770721: step 13920, loss = 0.89 (517.2 examples/sec; 0.248 sec/batch)
2016-12-08 04:06:29.227555: step 13930, loss = 0.84 (549.4 examples/sec; 0.233 sec/batch)
2016-12-08 04:06:31.733429: step 13940, loss = 0.78 (550.8 examples/sec; 0.232 sec/batch)
2016-12-08 04:06:34.255505: step 13950, loss = 0.81 (518.7 examples/sec; 0.247 sec/batch)
2016-12-08 04:06:36.709418: step 13960, loss = 0.98 (489.5 examples/sec; 0.261 sec/batch)
2016-12-08 04:06:39.236186: step 13970, loss = 0.93 (484.9 examples/sec; 0.264 sec/batch)
2016-12-08 04:06:41.736865: step 13980, loss = 0.82 (466.4 examples/sec; 0.274 sec/batch)
2016-12-08 04:06:44.267539: step 13990, loss = 0.96 (469.6 examples/sec; 0.273 sec/batch)
2016-12-08 04:06:46.739904: step 14000, loss = 0.83 (479.0 examples/sec; 0.267 sec/batch)
2016-12-08 04:06:50.113353: step 14010, loss = 0.88 (538.6 examples/sec; 0.238 sec/batch)
2016-12-08 04:06:52.526994: step 14020, loss = 0.96 (591.4 examples/sec; 0.216 sec/batch)
2016-12-08 04:06:55.068036: step 14030, loss = 0.70 (479.1 examples/sec; 0.267 sec/batch)
2016-12-08 04:06:57.536523: step 14040, loss = 0.89 (475.9 examples/sec; 0.269 sec/batch)
2016-12-08 04:07:00.036063: step 14050, loss = 0.96 (505.3 examples/sec; 0.253 sec/batch)
2016-12-08 04:07:02.343871: step 14060, loss = 0.98 (558.6 examples/sec; 0.229 sec/batch)
2016-12-08 04:07:04.677668: step 14070, loss = 1.11 (566.8 examples/sec; 0.226 sec/batch)
2016-12-08 04:07:07.207776: step 14080, loss = 0.75 (442.1 examples/sec; 0.290 sec/batch)
2016-12-08 04:07:09.685071: step 14090, loss = 0.82 (462.6 examples/sec; 0.277 sec/batch)
2016-12-08 04:07:12.136161: step 14100, loss = 0.84 (604.2 examples/sec; 0.212 sec/batch)
2016-12-08 04:07:14.811570: step 14110, loss = 0.91 (476.8 examples/sec; 0.268 sec/batch)
2016-12-08 04:07:17.184671: step 14120, loss = 0.70 (624.4 examples/sec; 0.205 sec/batch)
2016-12-08 04:07:19.676152: step 14130, loss = 1.01 (490.8 examples/sec; 0.261 sec/batch)
2016-12-08 04:07:22.155733: step 14140, loss = 0.83 (536.2 examples/sec; 0.239 sec/batch)
2016-12-08 04:07:24.620219: step 14150, loss = 0.76 (511.3 examples/sec; 0.250 sec/batch)
2016-12-08 04:07:27.186884: step 14160, loss = 0.87 (495.5 examples/sec; 0.258 sec/batch)
2016-12-08 04:07:29.761285: step 14170, loss = 0.84 (520.3 examples/sec; 0.246 sec/batch)
2016-12-08 04:07:32.314354: step 14180, loss = 0.95 (492.4 examples/sec; 0.260 sec/batch)
2016-12-08 04:07:34.839121: step 14190, loss = 0.85 (507.1 examples/sec; 0.252 sec/batch)
2016-12-08 04:07:37.431270: step 14200, loss = 0.95 (502.5 examples/sec; 0.255 sec/batch)
2016-12-08 04:07:40.160214: step 14210, loss = 0.83 (438.1 examples/sec; 0.292 sec/batch)
2016-12-08 04:07:42.745436: step 14220, loss = 0.76 (510.8 examples/sec; 0.251 sec/batch)
2016-12-08 04:07:45.183950: step 14230, loss = 0.93 (573.1 examples/sec; 0.223 sec/batch)
2016-12-08 04:07:47.624376: step 14240, loss = 0.90 (476.8 examples/sec; 0.268 sec/batch)
2016-12-08 04:07:50.126064: step 14250, loss = 0.76 (557.5 examples/sec; 0.230 sec/batch)
2016-12-08 04:07:52.639148: step 14260, loss = 0.77 (460.7 examples/sec; 0.278 sec/batch)
2016-12-08 04:07:55.126664: step 14270, loss = 0.75 (485.7 examples/sec; 0.264 sec/batch)
2016-12-08 04:07:57.542931: step 14280, loss = 0.77 (542.2 examples/sec; 0.236 sec/batch)
2016-12-08 04:08:00.101326: step 14290, loss = 0.80 (538.5 examples/sec; 0.238 sec/batch)
2016-12-08 04:08:02.644603: step 14300, loss = 0.86 (522.5 examples/sec; 0.245 sec/batch)
2016-12-08 04:08:05.313632: step 14310, loss = 0.93 (529.8 examples/sec; 0.242 sec/batch)
2016-12-08 04:08:07.727857: step 14320, loss = 0.69 (549.8 examples/sec; 0.233 sec/batch)
2016-12-08 04:08:10.194902: step 14330, loss = 1.01 (523.8 examples/sec; 0.244 sec/batch)
2016-12-08 04:08:12.738851: step 14340, loss = 0.87 (475.4 examples/sec; 0.269 sec/batch)
2016-12-08 04:08:15.193466: step 14350, loss = 0.80 (530.5 examples/sec; 0.241 sec/batch)
2016-12-08 04:08:17.664792: step 14360, loss = 1.03 (455.0 examples/sec; 0.281 sec/batch)
2016-12-08 04:08:20.196328: step 14370, loss = 0.89 (464.0 examples/sec; 0.276 sec/batch)
2016-12-08 04:08:22.734248: step 14380, loss = 0.83 (509.8 examples/sec; 0.251 sec/batch)
2016-12-08 04:08:25.168816: step 14390, loss = 0.72 (543.0 examples/sec; 0.236 sec/batch)
2016-12-08 04:08:27.652027: step 14400, loss = 0.75 (491.1 examples/sec; 0.261 sec/batch)
2016-12-08 04:08:30.526569: step 14410, loss = 0.82 (464.5 examples/sec; 0.276 sec/batch)
2016-12-08 04:08:33.082540: step 14420, loss = 0.94 (422.2 examples/sec; 0.303 sec/batch)
2016-12-08 04:08:35.463824: step 14430, loss = 0.82 (544.2 examples/sec; 0.235 sec/batch)
2016-12-08 04:08:37.855100: step 14440, loss = 0.66 (537.6 examples/sec; 0.238 sec/batch)
2016-12-08 04:08:40.389480: step 14450, loss = 0.87 (591.1 examples/sec; 0.217 sec/batch)
2016-12-08 04:08:42.956410: step 14460, loss = 0.70 (502.5 examples/sec; 0.255 sec/batch)
2016-12-08 04:08:45.396295: step 14470, loss = 0.92 (528.0 examples/sec; 0.242 sec/batch)
2016-12-08 04:08:47.861784: step 14480, loss = 0.93 (569.4 examples/sec; 0.225 sec/batch)
2016-12-08 04:08:50.333836: step 14490, loss = 0.81 (492.6 examples/sec; 0.260 sec/batch)
2016-12-08 04:08:52.870350: step 14500, loss = 0.81 (458.2 examples/sec; 0.279 sec/batch)
2016-12-08 04:08:55.600279: step 14510, loss = 0.66 (484.1 examples/sec; 0.264 sec/batch)
2016-12-08 04:08:58.002937: step 14520, loss = 0.73 (538.7 examples/sec; 0.238 sec/batch)
2016-12-08 04:09:00.547670: step 14530, loss = 0.77 (633.7 examples/sec; 0.202 sec/batch)
2016-12-08 04:09:03.093835: step 14540, loss = 0.78 (551.8 examples/sec; 0.232 sec/batch)
2016-12-08 04:09:05.646065: step 14550, loss = 0.71 (483.1 examples/sec; 0.265 sec/batch)
2016-12-08 04:09:08.179201: step 14560, loss = 0.88 (499.4 examples/sec; 0.256 sec/batch)
2016-12-08 04:09:10.583036: step 14570, loss = 0.85 (514.3 examples/sec; 0.249 sec/batch)
2016-12-08 04:09:13.014937: step 14580, loss = 0.71 (503.1 examples/sec; 0.254 sec/batch)
2016-12-08 04:09:15.454167: step 14590, loss = 1.03 (613.6 examples/sec; 0.209 sec/batch)
2016-12-08 04:09:17.896930: step 14600, loss = 0.88 (557.3 examples/sec; 0.230 sec/batch)
2016-12-08 04:09:20.660785: step 14610, loss = 0.90 (453.9 examples/sec; 0.282 sec/batch)
2016-12-08 04:09:23.188529: step 14620, loss = 0.88 (541.3 examples/sec; 0.236 sec/batch)
2016-12-08 04:09:25.615470: step 14630, loss = 0.87 (549.5 examples/sec; 0.233 sec/batch)
2016-12-08 04:09:27.983164: step 14640, loss = 0.71 (527.2 examples/sec; 0.243 sec/batch)
2016-12-08 04:09:30.432498: step 14650, loss = 0.86 (482.1 examples/sec; 0.266 sec/batch)
2016-12-08 04:09:33.013812: step 14660, loss = 0.83 (542.0 examples/sec; 0.236 sec/batch)
2016-12-08 04:09:35.606002: step 14670, loss = 0.63 (524.1 examples/sec; 0.244 sec/batch)
2016-12-08 04:09:38.176017: step 14680, loss = 1.09 (508.8 examples/sec; 0.252 sec/batch)
2016-12-08 04:09:40.753385: step 14690, loss = 0.69 (454.7 examples/sec; 0.282 sec/batch)
2016-12-08 04:09:43.224645: step 14700, loss = 1.01 (498.1 examples/sec; 0.257 sec/batch)
2016-12-08 04:09:46.057336: step 14710, loss = 0.78 (469.9 examples/sec; 0.272 sec/batch)
2016-12-08 04:09:48.694670: step 14720, loss = 0.83 (461.6 examples/sec; 0.277 sec/batch)
2016-12-08 04:09:51.209522: step 14730, loss = 0.80 (535.3 examples/sec; 0.239 sec/batch)
2016-12-08 04:09:53.745612: step 14740, loss = 1.01 (449.9 examples/sec; 0.284 sec/batch)
2016-12-08 04:09:56.197939: step 14750, loss = 0.99 (563.8 examples/sec; 0.227 sec/batch)
2016-12-08 04:09:58.712014: step 14760, loss = 0.75 (528.7 examples/sec; 0.242 sec/batch)
2016-12-08 04:10:01.255051: step 14770, loss = 0.87 (520.1 examples/sec; 0.246 sec/batch)
2016-12-08 04:10:03.666129: step 14780, loss = 0.67 (548.8 examples/sec; 0.233 sec/batch)
2016-12-08 04:10:06.120042: step 14790, loss = 0.91 (557.9 examples/sec; 0.229 sec/batch)
2016-12-08 04:10:08.627481: step 14800, loss = 0.99 (495.4 examples/sec; 0.258 sec/batch)
2016-12-08 04:10:11.229303: step 14810, loss = 0.87 (556.0 examples/sec; 0.230 sec/batch)
2016-12-08 04:10:13.683036: step 14820, loss = 0.88 (522.3 examples/sec; 0.245 sec/batch)
2016-12-08 04:10:16.144451: step 14830, loss = 1.04 (574.0 examples/sec; 0.223 sec/batch)
2016-12-08 04:10:18.610341: step 14840, loss = 1.09 (546.0 examples/sec; 0.234 sec/batch)
2016-12-08 04:10:21.149759: step 14850, loss = 0.82 (536.1 examples/sec; 0.239 sec/batch)
2016-12-08 04:10:23.526482: step 14860, loss = 0.77 (552.1 examples/sec; 0.232 sec/batch)
2016-12-08 04:10:26.112033: step 14870, loss = 0.88 (453.7 examples/sec; 0.282 sec/batch)
2016-12-08 04:10:28.704301: step 14880, loss = 0.72 (507.3 examples/sec; 0.252 sec/batch)
2016-12-08 04:10:31.231434: step 14890, loss = 0.89 (495.1 examples/sec; 0.259 sec/batch)
2016-12-08 04:10:33.710793: step 14900, loss = 0.89 (489.7 examples/sec; 0.261 sec/batch)
2016-12-08 04:10:36.350507: step 14910, loss = 0.80 (463.3 examples/sec; 0.276 sec/batch)
2016-12-08 04:10:38.858417: step 14920, loss = 0.79 (551.1 examples/sec; 0.232 sec/batch)
2016-12-08 04:10:41.295026: step 14930, loss = 0.73 (515.7 examples/sec; 0.248 sec/batch)
2016-12-08 04:10:43.867303: step 14940, loss = 0.86 (518.2 examples/sec; 0.247 sec/batch)
2016-12-08 04:10:46.345384: step 14950, loss = 0.89 (597.8 examples/sec; 0.214 sec/batch)
2016-12-08 04:10:48.794919: step 14960, loss = 0.90 (505.7 examples/sec; 0.253 sec/batch)
2016-12-08 04:10:51.242604: step 14970, loss = 0.77 (610.1 examples/sec; 0.210 sec/batch)
2016-12-08 04:10:53.565712: step 14980, loss = 0.64 (491.5 examples/sec; 0.260 sec/batch)
2016-12-08 04:10:55.999933: step 14990, loss = 0.98 (563.6 examples/sec; 0.227 sec/batch)
2016-12-08 04:10:58.473483: step 15000, loss = 0.93 (553.1 examples/sec; 0.231 sec/batch)
2016-12-08 04:11:01.999560: step 15010, loss = 0.74 (523.5 examples/sec; 0.244 sec/batch)
2016-12-08 04:11:04.444939: step 15020, loss = 0.85 (633.6 examples/sec; 0.202 sec/batch)
2016-12-08 04:11:06.980694: step 15030, loss = 0.90 (587.2 examples/sec; 0.218 sec/batch)
2016-12-08 04:11:09.560720: step 15040, loss = 0.70 (489.5 examples/sec; 0.261 sec/batch)
2016-12-08 04:11:12.145242: step 15050, loss = 0.59 (474.4 examples/sec; 0.270 sec/batch)
2016-12-08 04:11:14.639177: step 15060, loss = 0.83 (479.2 examples/sec; 0.267 sec/batch)
2016-12-08 04:11:17.076890: step 15070, loss = 0.73 (510.5 examples/sec; 0.251 sec/batch)
2016-12-08 04:11:19.517531: step 15080, loss = 0.92 (533.1 examples/sec; 0.240 sec/batch)
2016-12-08 04:11:21.962904: step 15090, loss = 0.73 (539.8 examples/sec; 0.237 sec/batch)
2016-12-08 04:11:24.349426: step 15100, loss = 0.85 (544.7 examples/sec; 0.235 sec/batch)
2016-12-08 04:11:27.201585: step 15110, loss = 0.92 (558.6 examples/sec; 0.229 sec/batch)
2016-12-08 04:11:29.612085: step 15120, loss = 0.68 (593.6 examples/sec; 0.216 sec/batch)
2016-12-08 04:11:32.071295: step 15130, loss = 0.88 (517.6 examples/sec; 0.247 sec/batch)
2016-12-08 04:11:34.467142: step 15140, loss = 0.88 (529.3 examples/sec; 0.242 sec/batch)
2016-12-08 04:11:37.052853: step 15150, loss = 0.94 (545.3 examples/sec; 0.235 sec/batch)
2016-12-08 04:11:39.528199: step 15160, loss = 0.76 (497.8 examples/sec; 0.257 sec/batch)
2016-12-08 04:11:42.010323: step 15170, loss = 1.08 (514.3 examples/sec; 0.249 sec/batch)
2016-12-08 04:11:44.454911: step 15180, loss = 0.84 (547.9 examples/sec; 0.234 sec/batch)
2016-12-08 04:11:46.928283: step 15190, loss = 0.84 (511.3 examples/sec; 0.250 sec/batch)
2016-12-08 04:11:49.533560: step 15200, loss = 0.72 (446.4 examples/sec; 0.287 sec/batch)
2016-12-08 04:11:52.325507: step 15210, loss = 0.90 (574.3 examples/sec; 0.223 sec/batch)
2016-12-08 04:11:54.903138: step 15220, loss = 0.85 (462.8 examples/sec; 0.277 sec/batch)
2016-12-08 04:11:57.367384: step 15230, loss = 0.82 (590.5 examples/sec; 0.217 sec/batch)
2016-12-08 04:11:59.885029: step 15240, loss = 0.75 (513.7 examples/sec; 0.249 sec/batch)
2016-12-08 04:12:02.327980: step 15250, loss = 0.90 (541.9 examples/sec; 0.236 sec/batch)
2016-12-08 04:12:04.861650: step 15260, loss = 0.71 (529.1 examples/sec; 0.242 sec/batch)
2016-12-08 04:12:07.302284: step 15270, loss = 0.81 (464.0 examples/sec; 0.276 sec/batch)
2016-12-08 04:12:09.757227: step 15280, loss = 0.91 (590.8 examples/sec; 0.217 sec/batch)
2016-12-08 04:12:12.350318: step 15290, loss = 1.10 (459.5 examples/sec; 0.279 sec/batch)
2016-12-08 04:12:14.895152: step 15300, loss = 0.83 (465.5 examples/sec; 0.275 sec/batch)
2016-12-08 04:12:17.575444: step 15310, loss = 0.91 (486.2 examples/sec; 0.263 sec/batch)
2016-12-08 04:12:20.026167: step 15320, loss = 0.87 (486.3 examples/sec; 0.263 sec/batch)
2016-12-08 04:12:22.577205: step 15330, loss = 0.86 (511.0 examples/sec; 0.250 sec/batch)
2016-12-08 04:12:25.105273: step 15340, loss = 0.75 (589.7 examples/sec; 0.217 sec/batch)
2016-12-08 04:12:27.577528: step 15350, loss = 0.91 (554.8 examples/sec; 0.231 sec/batch)
2016-12-08 04:12:30.103989: step 15360, loss = 0.85 (492.2 examples/sec; 0.260 sec/batch)
2016-12-08 04:12:32.670995: step 15370, loss = 0.73 (466.5 examples/sec; 0.274 sec/batch)
2016-12-08 04:12:35.100811: step 15380, loss = 0.89 (590.2 examples/sec; 0.217 sec/batch)
2016-12-08 04:12:37.607966: step 15390, loss = 0.74 (526.2 examples/sec; 0.243 sec/batch)
2016-12-08 04:12:39.976851: step 15400, loss = 0.82 (510.1 examples/sec; 0.251 sec/batch)
2016-12-08 04:12:42.779981: step 15410, loss = 0.88 (452.0 examples/sec; 0.283 sec/batch)
2016-12-08 04:12:45.264926: step 15420, loss = 0.81 (521.9 examples/sec; 0.245 sec/batch)
2016-12-08 04:12:47.683931: step 15430, loss = 0.86 (447.9 examples/sec; 0.286 sec/batch)
2016-12-08 04:12:50.072726: step 15440, loss = 0.74 (538.2 examples/sec; 0.238 sec/batch)
2016-12-08 04:12:52.633968: step 15450, loss = 0.90 (495.8 examples/sec; 0.258 sec/batch)
2016-12-08 04:12:55.133339: step 15460, loss = 0.91 (528.5 examples/sec; 0.242 sec/batch)
2016-12-08 04:12:57.711797: step 15470, loss = 1.07 (426.1 examples/sec; 0.300 sec/batch)
2016-12-08 04:13:00.228708: step 15480, loss = 0.84 (504.3 examples/sec; 0.254 sec/batch)
2016-12-08 04:13:02.587912: step 15490, loss = 0.74 (488.7 examples/sec; 0.262 sec/batch)
2016-12-08 04:13:05.006399: step 15500, loss = 0.68 (508.9 examples/sec; 0.252 sec/batch)
2016-12-08 04:13:07.731874: step 15510, loss = 0.70 (584.4 examples/sec; 0.219 sec/batch)
2016-12-08 04:13:10.228762: step 15520, loss = 0.85 (502.5 examples/sec; 0.255 sec/batch)
2016-12-08 04:13:12.625341: step 15530, loss = 0.88 (564.2 examples/sec; 0.227 sec/batch)
2016-12-08 04:13:15.092907: step 15540, loss = 0.89 (517.7 examples/sec; 0.247 sec/batch)
2016-12-08 04:13:17.545250: step 15550, loss = 0.81 (510.2 examples/sec; 0.251 sec/batch)
2016-12-08 04:13:20.047149: step 15560, loss = 0.77 (566.3 examples/sec; 0.226 sec/batch)
2016-12-08 04:13:22.427695: step 15570, loss = 1.09 (529.7 examples/sec; 0.242 sec/batch)
2016-12-08 04:13:24.876467: step 15580, loss = 1.00 (447.0 examples/sec; 0.286 sec/batch)
2016-12-08 04:13:27.305426: step 15590, loss = 0.80 (451.3 examples/sec; 0.284 sec/batch)
2016-12-08 04:13:29.777207: step 15600, loss = 0.92 (518.6 examples/sec; 0.247 sec/batch)
2016-12-08 04:13:32.603674: step 15610, loss = 0.95 (536.2 examples/sec; 0.239 sec/batch)
2016-12-08 04:13:35.113428: step 15620, loss = 0.98 (466.5 examples/sec; 0.274 sec/batch)
2016-12-08 04:13:37.588031: step 15630, loss = 0.74 (480.9 examples/sec; 0.266 sec/batch)
2016-12-08 04:13:40.045330: step 15640, loss = 0.86 (457.9 examples/sec; 0.280 sec/batch)
2016-12-08 04:13:42.761097: step 15650, loss = 0.91 (512.3 examples/sec; 0.250 sec/batch)
2016-12-08 04:13:45.304133: step 15660, loss = 0.91 (474.2 examples/sec; 0.270 sec/batch)
2016-12-08 04:13:47.769943: step 15670, loss = 0.90 (443.5 examples/sec; 0.289 sec/batch)
2016-12-08 04:13:50.242318: step 15680, loss = 0.87 (567.6 examples/sec; 0.226 sec/batch)
2016-12-08 04:13:52.711094: step 15690, loss = 0.91 (478.1 examples/sec; 0.268 sec/batch)
2016-12-08 04:13:55.228519: step 15700, loss = 1.05 (535.6 examples/sec; 0.239 sec/batch)
2016-12-08 04:13:57.988523: step 15710, loss = 0.83 (569.9 examples/sec; 0.225 sec/batch)
2016-12-08 04:14:00.608334: step 15720, loss = 0.75 (531.6 examples/sec; 0.241 sec/batch)
2016-12-08 04:14:03.228077: step 15730, loss = 0.92 (475.5 examples/sec; 0.269 sec/batch)
2016-12-08 04:14:05.686971: step 15740, loss = 0.84 (509.6 examples/sec; 0.251 sec/batch)
2016-12-08 04:14:08.074325: step 15750, loss = 0.80 (567.9 examples/sec; 0.225 sec/batch)
2016-12-08 04:14:10.678589: step 15760, loss = 0.81 (473.2 examples/sec; 0.271 sec/batch)
2016-12-08 04:14:13.244106: step 15770, loss = 0.79 (490.4 examples/sec; 0.261 sec/batch)
2016-12-08 04:14:15.689882: step 15780, loss = 0.81 (623.6 examples/sec; 0.205 sec/batch)
2016-12-08 04:14:18.200154: step 15790, loss = 0.88 (481.5 examples/sec; 0.266 sec/batch)
2016-12-08 04:14:20.685147: step 15800, loss = 0.77 (517.9 examples/sec; 0.247 sec/batch)
2016-12-08 04:14:23.469725: step 15810, loss = 0.79 (472.9 examples/sec; 0.271 sec/batch)
2016-12-08 04:14:26.071112: step 15820, loss = 0.86 (500.4 examples/sec; 0.256 sec/batch)
2016-12-08 04:14:28.540233: step 15830, loss = 1.00 (459.3 examples/sec; 0.279 sec/batch)
2016-12-08 04:14:30.945918: step 15840, loss = 0.94 (553.1 examples/sec; 0.231 sec/batch)
2016-12-08 04:14:33.489923: step 15850, loss = 0.81 (459.5 examples/sec; 0.279 sec/batch)
2016-12-08 04:14:36.075608: step 15860, loss = 0.99 (476.2 examples/sec; 0.269 sec/batch)
2016-12-08 04:14:38.566553: step 15870, loss = 0.82 (497.7 examples/sec; 0.257 sec/batch)
2016-12-08 04:14:41.082280: step 15880, loss = 0.72 (481.7 examples/sec; 0.266 sec/batch)
2016-12-08 04:14:43.576610: step 15890, loss = 0.88 (506.6 examples/sec; 0.253 sec/batch)
2016-12-08 04:14:46.119630: step 15900, loss = 0.82 (579.1 examples/sec; 0.221 sec/batch)
2016-12-08 04:14:48.697120: step 15910, loss = 0.88 (569.8 examples/sec; 0.225 sec/batch)
2016-12-08 04:14:51.132564: step 15920, loss = 0.90 (557.6 examples/sec; 0.230 sec/batch)
2016-12-08 04:14:53.564257: step 15930, loss = 0.81 (599.5 examples/sec; 0.214 sec/batch)
2016-12-08 04:14:55.878384: step 15940, loss = 0.92 (532.0 examples/sec; 0.241 sec/batch)
2016-12-08 04:14:58.240359: step 15950, loss = 0.85 (577.1 examples/sec; 0.222 sec/batch)
2016-12-08 04:15:00.709432: step 15960, loss = 0.86 (561.5 examples/sec; 0.228 sec/batch)
2016-12-08 04:15:03.195154: step 15970, loss = 0.93 (499.5 examples/sec; 0.256 sec/batch)
2016-12-08 04:15:05.611742: step 15980, loss = 0.81 (497.2 examples/sec; 0.257 sec/batch)
2016-12-08 04:15:08.103922: step 15990, loss = 0.85 (509.6 examples/sec; 0.251 sec/batch)
2016-12-08 04:15:10.617374: step 16000, loss = 0.83 (548.2 examples/sec; 0.233 sec/batch)
2016-12-08 04:15:13.868961: step 16010, loss = 0.76 (534.7 examples/sec; 0.239 sec/batch)
2016-12-08 04:15:16.354581: step 16020, loss = 0.67 (520.8 examples/sec; 0.246 sec/batch)
2016-12-08 04:15:18.844788: step 16030, loss = 0.87 (480.5 examples/sec; 0.266 sec/batch)
2016-12-08 04:15:21.363153: step 16040, loss = 0.69 (535.6 examples/sec; 0.239 sec/batch)
2016-12-08 04:15:23.778023: step 16050, loss = 0.72 (486.7 examples/sec; 0.263 sec/batch)
2016-12-08 04:15:26.356067: step 16060, loss = 0.90 (428.1 examples/sec; 0.299 sec/batch)
2016-12-08 04:15:28.938395: step 16070, loss = 0.75 (553.8 examples/sec; 0.231 sec/batch)
2016-12-08 04:15:31.475925: step 16080, loss = 0.89 (527.5 examples/sec; 0.243 sec/batch)
2016-12-08 04:15:33.945601: step 16090, loss = 0.84 (538.5 examples/sec; 0.238 sec/batch)
2016-12-08 04:15:36.449037: step 16100, loss = 0.91 (497.2 examples/sec; 0.257 sec/batch)
2016-12-08 04:15:39.199165: step 16110, loss = 0.89 (551.1 examples/sec; 0.232 sec/batch)
2016-12-08 04:15:41.836599: step 16120, loss = 0.99 (450.5 examples/sec; 0.284 sec/batch)
2016-12-08 04:15:44.380957: step 16130, loss = 0.69 (502.0 examples/sec; 0.255 sec/batch)
2016-12-08 04:15:46.935593: step 16140, loss = 0.76 (530.4 examples/sec; 0.241 sec/batch)
2016-12-08 04:15:49.242849: step 16150, loss = 0.81 (587.3 examples/sec; 0.218 sec/batch)
2016-12-08 04:15:51.752768: step 16160, loss = 0.78 (464.1 examples/sec; 0.276 sec/batch)
2016-12-08 04:15:54.223366: step 16170, loss = 1.03 (622.7 examples/sec; 0.206 sec/batch)
2016-12-08 04:15:56.667199: step 16180, loss = 0.79 (532.3 examples/sec; 0.240 sec/batch)
2016-12-08 04:15:59.127504: step 16190, loss = 0.77 (498.6 examples/sec; 0.257 sec/batch)
2016-12-08 04:16:01.604743: step 16200, loss = 0.86 (523.9 examples/sec; 0.244 sec/batch)
2016-12-08 04:16:04.293722: step 16210, loss = 0.85 (492.1 examples/sec; 0.260 sec/batch)
2016-12-08 04:16:06.740876: step 16220, loss = 0.70 (459.8 examples/sec; 0.278 sec/batch)
2016-12-08 04:16:09.110293: step 16230, loss = 0.79 (531.1 examples/sec; 0.241 sec/batch)
2016-12-08 04:16:11.543647: step 16240, loss = 0.90 (544.3 examples/sec; 0.235 sec/batch)
2016-12-08 04:16:13.906984: step 16250, loss = 0.85 (519.4 examples/sec; 0.246 sec/batch)
2016-12-08 04:16:16.328850: step 16260, loss = 0.82 (520.0 examples/sec; 0.246 sec/batch)
2016-12-08 04:16:18.671733: step 16270, loss = 0.87 (538.8 examples/sec; 0.238 sec/batch)
