Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
WARNING:tensorflow:From cifar10_train.py:81 in train.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2016-03-02.
Instructions for updating:
Please use tf.global_variables instead.
2016-12-08 17:43:23.322993: step 0, loss = 4.68 (27.8 examples/sec; 4.610 sec/batch)
2016-12-08 17:43:25.346033: step 10, loss = 4.63 (1153.8 examples/sec; 0.111 sec/batch)
2016-12-08 17:43:26.451970: step 20, loss = 4.46 (1157.2 examples/sec; 0.111 sec/batch)
2016-12-08 17:43:27.760034: step 30, loss = 4.45 (840.3 examples/sec; 0.152 sec/batch)
2016-12-08 17:43:29.138820: step 40, loss = 4.76 (1021.6 examples/sec; 0.125 sec/batch)
2016-12-08 17:43:30.393133: step 50, loss = 4.20 (1095.9 examples/sec; 0.117 sec/batch)
2016-12-08 17:43:31.645848: step 60, loss = 4.20 (927.8 examples/sec; 0.138 sec/batch)
2016-12-08 17:43:32.765914: step 70, loss = 4.33 (1211.9 examples/sec; 0.106 sec/batch)
2016-12-08 17:43:33.892059: step 80, loss = 4.12 (1070.9 examples/sec; 0.120 sec/batch)
2016-12-08 17:43:35.051051: step 90, loss = 4.16 (1020.6 examples/sec; 0.125 sec/batch)
2016-12-08 17:43:36.173813: step 100, loss = 4.09 (1127.1 examples/sec; 0.114 sec/batch)
2016-12-08 17:43:37.501037: step 110, loss = 4.08 (1105.3 examples/sec; 0.116 sec/batch)
2016-12-08 17:43:38.626177: step 120, loss = 4.16 (1144.7 examples/sec; 0.112 sec/batch)
2016-12-08 17:43:39.774630: step 130, loss = 4.14 (1070.9 examples/sec; 0.120 sec/batch)
2016-12-08 17:43:40.884116: step 140, loss = 4.05 (1147.4 examples/sec; 0.112 sec/batch)
2016-12-08 17:43:42.013734: step 150, loss = 3.84 (1142.9 examples/sec; 0.112 sec/batch)
2016-12-08 17:43:43.164859: step 160, loss = 4.06 (1142.7 examples/sec; 0.112 sec/batch)
2016-12-08 17:43:44.350810: step 170, loss = 3.84 (924.8 examples/sec; 0.138 sec/batch)
2016-12-08 17:43:45.421589: step 180, loss = 3.71 (1121.8 examples/sec; 0.114 sec/batch)
2016-12-08 17:43:46.576640: step 190, loss = 3.84 (1117.2 examples/sec; 0.115 sec/batch)
2016-12-08 17:43:47.747579: step 200, loss = 3.76 (1066.6 examples/sec; 0.120 sec/batch)
2016-12-08 17:43:49.036845: step 210, loss = 3.72 (1251.6 examples/sec; 0.102 sec/batch)
2016-12-08 17:43:50.147918: step 220, loss = 3.89 (1167.3 examples/sec; 0.110 sec/batch)
2016-12-08 17:43:51.277615: step 230, loss = 3.76 (1180.1 examples/sec; 0.108 sec/batch)
2016-12-08 17:43:52.393483: step 240, loss = 3.54 (1196.6 examples/sec; 0.107 sec/batch)
2016-12-08 17:43:53.496444: step 250, loss = 4.10 (1145.4 examples/sec; 0.112 sec/batch)
2016-12-08 17:43:54.618232: step 260, loss = 3.79 (1190.3 examples/sec; 0.108 sec/batch)
2016-12-08 17:43:55.864099: step 270, loss = 3.67 (964.0 examples/sec; 0.133 sec/batch)
2016-12-08 17:43:57.132551: step 280, loss = 3.74 (894.2 examples/sec; 0.143 sec/batch)
2016-12-08 17:43:58.302942: step 290, loss = 3.63 (1034.6 examples/sec; 0.124 sec/batch)
2016-12-08 17:43:59.442606: step 300, loss = 3.70 (1191.7 examples/sec; 0.107 sec/batch)
2016-12-08 17:44:00.777045: step 310, loss = 3.36 (1214.1 examples/sec; 0.105 sec/batch)
2016-12-08 17:44:01.875650: step 320, loss = 3.53 (1195.7 examples/sec; 0.107 sec/batch)
2016-12-08 17:44:02.987278: step 330, loss = 3.44 (1160.5 examples/sec; 0.110 sec/batch)
2016-12-08 17:44:04.144152: step 340, loss = 3.42 (1139.9 examples/sec; 0.112 sec/batch)
2016-12-08 17:44:05.205712: step 350, loss = 3.34 (1218.8 examples/sec; 0.105 sec/batch)
2016-12-08 17:44:06.366973: step 360, loss = 3.51 (1119.1 examples/sec; 0.114 sec/batch)
2016-12-08 17:44:07.473500: step 370, loss = 3.24 (1173.2 examples/sec; 0.109 sec/batch)
2016-12-08 17:44:08.613609: step 380, loss = 3.34 (1122.4 examples/sec; 0.114 sec/batch)
2016-12-08 17:44:09.679829: step 390, loss = 3.30 (1162.3 examples/sec; 0.110 sec/batch)
2016-12-08 17:44:10.821107: step 400, loss = 3.54 (1125.0 examples/sec; 0.114 sec/batch)
2016-12-08 17:44:12.174582: step 410, loss = 3.26 (1112.6 examples/sec; 0.115 sec/batch)
2016-12-08 17:44:13.281270: step 420, loss = 3.19 (1150.2 examples/sec; 0.111 sec/batch)
2016-12-08 17:44:14.399987: step 430, loss = 3.14 (1122.4 examples/sec; 0.114 sec/batch)
2016-12-08 17:44:15.568376: step 440, loss = 3.57 (1037.2 examples/sec; 0.123 sec/batch)
2016-12-08 17:44:16.662698: step 450, loss = 3.17 (1187.0 examples/sec; 0.108 sec/batch)
2016-12-08 17:44:17.760764: step 460, loss = 3.45 (1156.5 examples/sec; 0.111 sec/batch)
2016-12-08 17:44:18.986591: step 470, loss = 3.02 (1112.5 examples/sec; 0.115 sec/batch)
2016-12-08 17:44:20.117438: step 480, loss = 3.28 (1059.9 examples/sec; 0.121 sec/batch)
2016-12-08 17:44:21.184384: step 490, loss = 3.42 (1194.5 examples/sec; 0.107 sec/batch)
2016-12-08 17:44:22.329848: step 500, loss = 3.22 (1104.9 examples/sec; 0.116 sec/batch)
2016-12-08 17:44:23.665297: step 510, loss = 3.12 (1122.5 examples/sec; 0.114 sec/batch)
2016-12-08 17:44:24.754687: step 520, loss = 3.12 (1236.8 examples/sec; 0.103 sec/batch)
2016-12-08 17:44:25.862221: step 530, loss = 2.86 (1122.8 examples/sec; 0.114 sec/batch)
2016-12-08 17:44:27.020326: step 540, loss = 3.13 (1168.0 examples/sec; 0.110 sec/batch)
2016-12-08 17:44:28.181720: step 550, loss = 2.93 (1140.6 examples/sec; 0.112 sec/batch)
2016-12-08 17:44:29.295447: step 560, loss = 3.11 (1176.4 examples/sec; 0.109 sec/batch)
2016-12-08 17:44:30.425031: step 570, loss = 3.02 (1108.6 examples/sec; 0.115 sec/batch)
2016-12-08 17:44:31.588288: step 580, loss = 3.02 (1036.4 examples/sec; 0.123 sec/batch)
2016-12-08 17:44:32.935263: step 590, loss = 2.90 (1027.3 examples/sec; 0.125 sec/batch)
2016-12-08 17:44:34.078215: step 600, loss = 3.14 (1027.3 examples/sec; 0.125 sec/batch)
2016-12-08 17:44:35.437256: step 610, loss = 2.87 (1148.9 examples/sec; 0.111 sec/batch)
2016-12-08 17:44:36.588558: step 620, loss = 3.08 (1203.7 examples/sec; 0.106 sec/batch)
2016-12-08 17:44:37.667659: step 630, loss = 3.12 (1073.8 examples/sec; 0.119 sec/batch)
2016-12-08 17:44:38.829391: step 640, loss = 2.85 (1017.5 examples/sec; 0.126 sec/batch)
2016-12-08 17:44:39.958442: step 650, loss = 2.93 (1146.2 examples/sec; 0.112 sec/batch)
2016-12-08 17:44:41.039223: step 660, loss = 2.84 (1225.7 examples/sec; 0.104 sec/batch)
2016-12-08 17:44:42.213285: step 670, loss = 2.78 (1150.3 examples/sec; 0.111 sec/batch)
2016-12-08 17:44:43.373496: step 680, loss = 2.62 (1160.4 examples/sec; 0.110 sec/batch)
2016-12-08 17:44:44.498507: step 690, loss = 2.84 (1150.9 examples/sec; 0.111 sec/batch)
2016-12-08 17:44:45.575894: step 700, loss = 2.66 (1022.6 examples/sec; 0.125 sec/batch)
2016-12-08 17:44:46.894445: step 710, loss = 2.90 (1124.1 examples/sec; 0.114 sec/batch)
2016-12-08 17:44:48.024980: step 720, loss = 2.73 (1104.8 examples/sec; 0.116 sec/batch)
2016-12-08 17:44:49.101100: step 730, loss = 2.78 (1233.2 examples/sec; 0.104 sec/batch)
2016-12-08 17:44:50.251660: step 740, loss = 2.72 (1176.2 examples/sec; 0.109 sec/batch)
2016-12-08 17:44:51.422512: step 750, loss = 2.71 (1037.3 examples/sec; 0.123 sec/batch)
2016-12-08 17:44:52.531981: step 760, loss = 2.72 (1207.4 examples/sec; 0.106 sec/batch)
2016-12-08 17:44:53.606306: step 770, loss = 2.68 (1215.9 examples/sec; 0.105 sec/batch)
2016-12-08 17:44:54.719746: step 780, loss = 2.70 (1180.2 examples/sec; 0.108 sec/batch)
2016-12-08 17:44:55.891135: step 790, loss = 2.68 (1103.2 examples/sec; 0.116 sec/batch)
2016-12-08 17:44:56.985724: step 800, loss = 2.52 (1290.8 examples/sec; 0.099 sec/batch)
2016-12-08 17:44:58.317195: step 810, loss = 2.66 (1061.5 examples/sec; 0.121 sec/batch)
2016-12-08 17:44:59.486110: step 820, loss = 2.76 (1154.1 examples/sec; 0.111 sec/batch)
2016-12-08 17:45:00.622228: step 830, loss = 2.71 (952.5 examples/sec; 0.134 sec/batch)
2016-12-08 17:45:01.722450: step 840, loss = 2.73 (1152.6 examples/sec; 0.111 sec/batch)
2016-12-08 17:45:02.870028: step 850, loss = 2.47 (1126.6 examples/sec; 0.114 sec/batch)
2016-12-08 17:45:03.986393: step 860, loss = 2.68 (1160.0 examples/sec; 0.110 sec/batch)
2016-12-08 17:45:05.069299: step 870, loss = 2.65 (1238.8 examples/sec; 0.103 sec/batch)
2016-12-08 17:45:06.199694: step 880, loss = 2.59 (1068.7 examples/sec; 0.120 sec/batch)
2016-12-08 17:45:07.361560: step 890, loss = 2.58 (1124.7 examples/sec; 0.114 sec/batch)
2016-12-08 17:45:08.485232: step 900, loss = 2.55 (1205.0 examples/sec; 0.106 sec/batch)
2016-12-08 17:45:09.773777: step 910, loss = 2.77 (1186.4 examples/sec; 0.108 sec/batch)
2016-12-08 17:45:10.884695: step 920, loss = 2.48 (1184.0 examples/sec; 0.108 sec/batch)
2016-12-08 17:45:12.005609: step 930, loss = 2.40 (1166.9 examples/sec; 0.110 sec/batch)
2016-12-08 17:45:13.135121: step 940, loss = 2.42 (1057.0 examples/sec; 0.121 sec/batch)
2016-12-08 17:45:14.414161: step 950, loss = 2.28 (933.0 examples/sec; 0.137 sec/batch)
2016-12-08 17:45:15.599256: step 960, loss = 2.56 (1164.6 examples/sec; 0.110 sec/batch)
2016-12-08 17:45:16.721822: step 970, loss = 2.68 (1260.9 examples/sec; 0.102 sec/batch)
2016-12-08 17:45:17.813991: step 980, loss = 2.38 (1181.7 examples/sec; 0.108 sec/batch)
2016-12-08 17:45:18.936463: step 990, loss = 2.27 (1105.1 examples/sec; 0.116 sec/batch)
2016-12-08 17:45:20.079285: step 1000, loss = 2.48 (1140.9 examples/sec; 0.112 sec/batch)
2016-12-08 17:45:22.027586: step 1010, loss = 2.48 (1145.5 examples/sec; 0.112 sec/batch)
2016-12-08 17:45:23.176479: step 1020, loss = 2.43 (1101.5 examples/sec; 0.116 sec/batch)
2016-12-08 17:45:24.335066: step 1030, loss = 2.47 (1131.2 examples/sec; 0.113 sec/batch)
2016-12-08 17:45:25.397471: step 1040, loss = 2.36 (1129.6 examples/sec; 0.113 sec/batch)
2016-12-08 17:45:26.517972: step 1050, loss = 2.32 (1106.3 examples/sec; 0.116 sec/batch)
2016-12-08 17:45:27.633078: step 1060, loss = 2.50 (1174.9 examples/sec; 0.109 sec/batch)
2016-12-08 17:45:28.748716: step 1070, loss = 2.36 (1242.9 examples/sec; 0.103 sec/batch)
2016-12-08 17:45:29.831295: step 1080, loss = 2.26 (1094.4 examples/sec; 0.117 sec/batch)
2016-12-08 17:45:30.959053: step 1090, loss = 2.35 (1078.8 examples/sec; 0.119 sec/batch)
2016-12-08 17:45:32.085025: step 1100, loss = 2.65 (1068.4 examples/sec; 0.120 sec/batch)
2016-12-08 17:45:33.553591: step 1110, loss = 2.37 (872.4 examples/sec; 0.147 sec/batch)
2016-12-08 17:45:34.794231: step 1120, loss = 2.23 (1143.3 examples/sec; 0.112 sec/batch)
2016-12-08 17:45:35.932806: step 1130, loss = 2.22 (941.8 examples/sec; 0.136 sec/batch)
2016-12-08 17:45:37.028457: step 1140, loss = 2.22 (1230.9 examples/sec; 0.104 sec/batch)
2016-12-08 17:45:38.194079: step 1150, loss = 2.11 (1007.7 examples/sec; 0.127 sec/batch)
2016-12-08 17:45:39.307361: step 1160, loss = 2.47 (1160.4 examples/sec; 0.110 sec/batch)
2016-12-08 17:45:40.486136: step 1170, loss = 2.17 (1150.6 examples/sec; 0.111 sec/batch)
2016-12-08 17:45:41.574083: step 1180, loss = 2.23 (1078.5 examples/sec; 0.119 sec/batch)
2016-12-08 17:45:42.669851: step 1190, loss = 2.17 (1171.4 examples/sec; 0.109 sec/batch)
2016-12-08 17:45:43.833296: step 1200, loss = 2.35 (1037.0 examples/sec; 0.123 sec/batch)
2016-12-08 17:45:45.078677: step 1210, loss = 2.18 (1228.2 examples/sec; 0.104 sec/batch)
2016-12-08 17:45:46.233420: step 1220, loss = 2.27 (1096.7 examples/sec; 0.117 sec/batch)
2016-12-08 17:45:47.404165: step 1230, loss = 2.08 (1107.5 examples/sec; 0.116 sec/batch)
2016-12-08 17:45:48.529355: step 1240, loss = 2.27 (1214.1 examples/sec; 0.105 sec/batch)
2016-12-08 17:45:49.683797: step 1250, loss = 2.05 (970.5 examples/sec; 0.132 sec/batch)
2016-12-08 17:45:50.918680: step 1260, loss = 2.25 (1019.6 examples/sec; 0.126 sec/batch)
2016-12-08 17:45:52.169270: step 1270, loss = 2.19 (664.6 examples/sec; 0.193 sec/batch)
2016-12-08 17:45:53.271570: step 1280, loss = 2.01 (903.9 examples/sec; 0.142 sec/batch)
2016-12-08 17:45:54.415879: step 1290, loss = 1.95 (1201.8 examples/sec; 0.107 sec/batch)
2016-12-08 17:45:55.545536: step 1300, loss = 2.12 (1181.7 examples/sec; 0.108 sec/batch)
2016-12-08 17:45:56.831695: step 1310, loss = 2.05 (1240.0 examples/sec; 0.103 sec/batch)
2016-12-08 17:45:57.927894: step 1320, loss = 2.13 (977.6 examples/sec; 0.131 sec/batch)
2016-12-08 17:45:59.054320: step 1330, loss = 2.17 (1158.0 examples/sec; 0.111 sec/batch)
2016-12-08 17:46:00.189864: step 1340, loss = 2.03 (1175.5 examples/sec; 0.109 sec/batch)
2016-12-08 17:46:01.244577: step 1350, loss = 2.07 (1217.0 examples/sec; 0.105 sec/batch)
2016-12-08 17:46:02.359753: step 1360, loss = 2.09 (1188.4 examples/sec; 0.108 sec/batch)
2016-12-08 17:46:03.480938: step 1370, loss = 2.19 (1152.6 examples/sec; 0.111 sec/batch)
2016-12-08 17:46:04.627300: step 1380, loss = 1.80 (1249.9 examples/sec; 0.102 sec/batch)
2016-12-08 17:46:05.722696: step 1390, loss = 2.12 (1174.6 examples/sec; 0.109 sec/batch)
2016-12-08 17:46:06.845113: step 1400, loss = 2.07 (1023.9 examples/sec; 0.125 sec/batch)
2016-12-08 17:46:08.145620: step 1410, loss = 2.16 (1110.0 examples/sec; 0.115 sec/batch)
2016-12-08 17:46:09.210372: step 1420, loss = 2.09 (1186.3 examples/sec; 0.108 sec/batch)
2016-12-08 17:46:10.328341: step 1430, loss = 1.97 (1169.8 examples/sec; 0.109 sec/batch)
2016-12-08 17:46:11.443162: step 1440, loss = 1.99 (1121.5 examples/sec; 0.114 sec/batch)
2016-12-08 17:46:12.648703: step 1450, loss = 2.06 (986.1 examples/sec; 0.130 sec/batch)
2016-12-08 17:46:13.842727: step 1460, loss = 2.03 (1064.2 examples/sec; 0.120 sec/batch)
2016-12-08 17:46:15.045188: step 1470, loss = 1.88 (1164.9 examples/sec; 0.110 sec/batch)
2016-12-08 17:46:16.183901: step 1480, loss = 1.76 (1115.3 examples/sec; 0.115 sec/batch)
2016-12-08 17:46:17.251143: step 1490, loss = 1.92 (1227.6 examples/sec; 0.104 sec/batch)
2016-12-08 17:46:18.380692: step 1500, loss = 2.00 (1089.2 examples/sec; 0.118 sec/batch)
2016-12-08 17:46:19.723102: step 1510, loss = 1.80 (1103.3 examples/sec; 0.116 sec/batch)
2016-12-08 17:46:20.822589: step 1520, loss = 1.83 (1205.9 examples/sec; 0.106 sec/batch)
2016-12-08 17:46:21.921334: step 1530, loss = 1.97 (1085.6 examples/sec; 0.118 sec/batch)
2016-12-08 17:46:23.031158: step 1540, loss = 1.89 (1102.3 examples/sec; 0.116 sec/batch)
2016-12-08 17:46:24.124318: step 1550, loss = 1.89 (1227.2 examples/sec; 0.104 sec/batch)
2016-12-08 17:46:25.185021: step 1560, loss = 1.87 (1231.9 examples/sec; 0.104 sec/batch)
2016-12-08 17:46:26.354623: step 1570, loss = 1.83 (1176.5 examples/sec; 0.109 sec/batch)
2016-12-08 17:46:27.510031: step 1580, loss = 1.90 (1090.1 examples/sec; 0.117 sec/batch)
2016-12-08 17:46:28.657304: step 1590, loss = 1.69 (1189.8 examples/sec; 0.108 sec/batch)
2016-12-08 17:46:29.712997: step 1600, loss = 1.70 (1170.0 examples/sec; 0.109 sec/batch)
2016-12-08 17:46:31.037015: step 1610, loss = 1.97 (1116.0 examples/sec; 0.115 sec/batch)
2016-12-08 17:46:32.148264: step 1620, loss = 1.81 (1179.1 examples/sec; 0.109 sec/batch)
2016-12-08 17:46:33.194326: step 1630, loss = 1.78 (1232.2 examples/sec; 0.104 sec/batch)
2016-12-08 17:46:34.312943: step 1640, loss = 1.90 (1112.8 examples/sec; 0.115 sec/batch)
2016-12-08 17:46:35.433389: step 1650, loss = 1.93 (1190.4 examples/sec; 0.108 sec/batch)
2016-12-08 17:46:36.548381: step 1660, loss = 1.86 (1134.5 examples/sec; 0.113 sec/batch)
2016-12-08 17:46:37.723880: step 1670, loss = 1.74 (1072.6 examples/sec; 0.119 sec/batch)
2016-12-08 17:46:38.850950: step 1680, loss = 1.89 (1183.5 examples/sec; 0.108 sec/batch)
2016-12-08 17:46:40.001561: step 1690, loss = 1.93 (1139.5 examples/sec; 0.112 sec/batch)
2016-12-08 17:46:41.070446: step 1700, loss = 1.67 (1222.1 examples/sec; 0.105 sec/batch)
2016-12-08 17:46:42.403669: step 1710, loss = 1.94 (1178.2 examples/sec; 0.109 sec/batch)
2016-12-08 17:46:43.549838: step 1720, loss = 1.78 (914.7 examples/sec; 0.140 sec/batch)
2016-12-08 17:46:44.644151: step 1730, loss = 1.68 (1239.2 examples/sec; 0.103 sec/batch)
2016-12-08 17:46:45.682115: step 1740, loss = 1.83 (1182.1 examples/sec; 0.108 sec/batch)
2016-12-08 17:46:46.778585: step 1750, loss = 1.78 (1184.2 examples/sec; 0.108 sec/batch)
2016-12-08 17:46:47.861523: step 1760, loss = 1.87 (1222.5 examples/sec; 0.105 sec/batch)
2016-12-08 17:46:48.935426: step 1770, loss = 1.84 (1176.7 examples/sec; 0.109 sec/batch)
2016-12-08 17:46:50.029403: step 1780, loss = 1.81 (1202.6 examples/sec; 0.106 sec/batch)
2016-12-08 17:46:51.109362: step 1790, loss = 1.80 (1131.4 examples/sec; 0.113 sec/batch)
2016-12-08 17:46:52.186282: step 1800, loss = 1.56 (1197.5 examples/sec; 0.107 sec/batch)
2016-12-08 17:46:53.441425: step 1810, loss = 1.67 (1190.5 examples/sec; 0.108 sec/batch)
2016-12-08 17:46:54.582299: step 1820, loss = 1.72 (1176.1 examples/sec; 0.109 sec/batch)
2016-12-08 17:46:55.765880: step 1830, loss = 1.92 (1104.4 examples/sec; 0.116 sec/batch)
2016-12-08 17:46:56.877181: step 1840, loss = 1.66 (1267.8 examples/sec; 0.101 sec/batch)
2016-12-08 17:46:57.989752: step 1850, loss = 1.65 (1106.4 examples/sec; 0.116 sec/batch)
2016-12-08 17:46:59.110439: step 1860, loss = 1.72 (1190.8 examples/sec; 0.107 sec/batch)
2016-12-08 17:47:00.212594: step 1870, loss = 1.77 (1182.7 examples/sec; 0.108 sec/batch)
2016-12-08 17:47:01.309858: step 1880, loss = 1.68 (1233.4 examples/sec; 0.104 sec/batch)
2016-12-08 17:47:02.429678: step 1890, loss = 1.65 (1220.9 examples/sec; 0.105 sec/batch)
2016-12-08 17:47:03.545722: step 1900, loss = 1.70 (1164.7 examples/sec; 0.110 sec/batch)
2016-12-08 17:47:04.823600: step 1910, loss = 1.81 (1239.5 examples/sec; 0.103 sec/batch)
2016-12-08 17:47:05.918744: step 1920, loss = 1.46 (1123.7 examples/sec; 0.114 sec/batch)
2016-12-08 17:47:07.032691: step 1930, loss = 1.64 (1163.6 examples/sec; 0.110 sec/batch)
2016-12-08 17:47:08.135612: step 1940, loss = 1.58 (1134.0 examples/sec; 0.113 sec/batch)
2016-12-08 17:47:09.218654: step 1950, loss = 1.49 (1194.7 examples/sec; 0.107 sec/batch)
2016-12-08 17:47:10.313277: step 1960, loss = 1.66 (1163.7 examples/sec; 0.110 sec/batch)
2016-12-08 17:47:11.515132: step 1970, loss = 1.65 (961.1 examples/sec; 0.133 sec/batch)
2016-12-08 17:47:12.842724: step 1980, loss = 1.53 (1077.5 examples/sec; 0.119 sec/batch)
2016-12-08 17:47:13.928911: step 1990, loss = 1.64 (1144.4 examples/sec; 0.112 sec/batch)
2016-12-08 17:47:15.090536: step 2000, loss = 1.54 (1176.1 examples/sec; 0.109 sec/batch)
2016-12-08 17:47:16.985897: step 2010, loss = 1.67 (1249.3 examples/sec; 0.102 sec/batch)
2016-12-08 17:47:18.086652: step 2020, loss = 1.52 (1208.4 examples/sec; 0.106 sec/batch)
2016-12-08 17:47:19.190446: step 2030, loss = 1.67 (1157.0 examples/sec; 0.111 sec/batch)
2016-12-08 17:47:20.323945: step 2040, loss = 1.61 (1178.6 examples/sec; 0.109 sec/batch)
2016-12-08 17:47:21.395946: step 2050, loss = 1.51 (1171.2 examples/sec; 0.109 sec/batch)
2016-12-08 17:47:22.501337: step 2060, loss = 1.66 (1168.8 examples/sec; 0.110 sec/batch)
2016-12-08 17:47:23.592795: step 2070, loss = 1.51 (1182.0 examples/sec; 0.108 sec/batch)
2016-12-08 17:47:24.768700: step 2080, loss = 1.51 (1247.4 examples/sec; 0.103 sec/batch)
2016-12-08 17:47:25.878080: step 2090, loss = 1.64 (1048.2 examples/sec; 0.122 sec/batch)
2016-12-08 17:47:26.995532: step 2100, loss = 1.81 (1102.7 examples/sec; 0.116 sec/batch)
2016-12-08 17:47:28.344008: step 2110, loss = 1.46 (1029.9 examples/sec; 0.124 sec/batch)
2016-12-08 17:47:29.397192: step 2120, loss = 1.62 (1158.5 examples/sec; 0.110 sec/batch)
2016-12-08 17:47:30.537236: step 2130, loss = 1.61 (1193.3 examples/sec; 0.107 sec/batch)
2016-12-08 17:47:31.703266: step 2140, loss = 1.47 (1123.7 examples/sec; 0.114 sec/batch)
2016-12-08 17:47:32.794625: step 2150, loss = 1.53 (1197.5 examples/sec; 0.107 sec/batch)
2016-12-08 17:47:33.876368: step 2160, loss = 1.59 (1183.9 examples/sec; 0.108 sec/batch)
2016-12-08 17:47:35.001147: step 2170, loss = 1.67 (1092.5 examples/sec; 0.117 sec/batch)
2016-12-08 17:47:36.141718: step 2180, loss = 1.66 (973.5 examples/sec; 0.131 sec/batch)
2016-12-08 17:47:37.202469: step 2190, loss = 1.64 (1220.0 examples/sec; 0.105 sec/batch)
2016-12-08 17:47:38.358780: step 2200, loss = 1.46 (1180.0 examples/sec; 0.108 sec/batch)
2016-12-08 17:47:39.683498: step 2210, loss = 1.57 (1123.4 examples/sec; 0.114 sec/batch)
2016-12-08 17:47:40.774418: step 2220, loss = 1.56 (1209.6 examples/sec; 0.106 sec/batch)
2016-12-08 17:47:41.861493: step 2230, loss = 1.76 (1152.8 examples/sec; 0.111 sec/batch)
2016-12-08 17:47:43.066698: step 2240, loss = 1.53 (954.2 examples/sec; 0.134 sec/batch)
2016-12-08 17:47:44.235981: step 2250, loss = 1.51 (1157.6 examples/sec; 0.111 sec/batch)
2016-12-08 17:47:45.346987: step 2260, loss = 1.59 (1090.5 examples/sec; 0.117 sec/batch)
2016-12-08 17:47:46.684836: step 2270, loss = 1.33 (1133.5 examples/sec; 0.113 sec/batch)
2016-12-08 17:47:47.784587: step 2280, loss = 1.56 (1169.3 examples/sec; 0.109 sec/batch)
2016-12-08 17:47:48.846181: step 2290, loss = 1.47 (1243.5 examples/sec; 0.103 sec/batch)
2016-12-08 17:47:49.941201: step 2300, loss = 1.44 (1025.6 examples/sec; 0.125 sec/batch)
2016-12-08 17:47:51.260913: step 2310, loss = 1.63 (1176.8 examples/sec; 0.109 sec/batch)
2016-12-08 17:47:52.375298: step 2320, loss = 1.42 (1220.8 examples/sec; 0.105 sec/batch)
2016-12-08 17:47:53.429880: step 2330, loss = 1.37 (1154.8 examples/sec; 0.111 sec/batch)
2016-12-08 17:47:54.520667: step 2340, loss = 1.35 (1188.1 examples/sec; 0.108 sec/batch)
2016-12-08 17:47:55.610368: step 2350, loss = 1.49 (1193.9 examples/sec; 0.107 sec/batch)
2016-12-08 17:47:56.674825: step 2360, loss = 1.51 (1254.8 examples/sec; 0.102 sec/batch)
2016-12-08 17:47:57.768180: step 2370, loss = 1.53 (953.9 examples/sec; 0.134 sec/batch)
2016-12-08 17:47:58.902801: step 2380, loss = 1.49 (1111.2 examples/sec; 0.115 sec/batch)
2016-12-08 17:48:00.012825: step 2390, loss = 1.51 (1189.9 examples/sec; 0.108 sec/batch)
2016-12-08 17:48:01.121107: step 2400, loss = 1.49 (1232.0 examples/sec; 0.104 sec/batch)
2016-12-08 17:48:02.646528: step 2410, loss = 1.42 (1184.7 examples/sec; 0.108 sec/batch)
2016-12-08 17:48:03.828441: step 2420, loss = 1.26 (1121.1 examples/sec; 0.114 sec/batch)
2016-12-08 17:48:04.885237: step 2430, loss = 1.45 (1213.7 examples/sec; 0.105 sec/batch)
2016-12-08 17:48:05.976641: step 2440, loss = 1.40 (1160.3 examples/sec; 0.110 sec/batch)
2016-12-08 17:48:07.070309: step 2450, loss = 1.46 (1217.1 examples/sec; 0.105 sec/batch)
2016-12-08 17:48:08.181207: step 2460, loss = 1.43 (1171.6 examples/sec; 0.109 sec/batch)
2016-12-08 17:48:09.221168: step 2470, loss = 1.41 (1217.2 examples/sec; 0.105 sec/batch)
2016-12-08 17:48:10.308047: step 2480, loss = 1.32 (1168.1 examples/sec; 0.110 sec/batch)
2016-12-08 17:48:11.394699: step 2490, loss = 1.29 (1221.7 examples/sec; 0.105 sec/batch)
2016-12-08 17:48:12.521002: step 2500, loss = 1.23 (1060.5 examples/sec; 0.121 sec/batch)
2016-12-08 17:48:13.788427: step 2510, loss = 1.60 (1126.4 examples/sec; 0.114 sec/batch)
2016-12-08 17:48:14.896163: step 2520, loss = 1.31 (1175.9 examples/sec; 0.109 sec/batch)
2016-12-08 17:48:16.047944: step 2530, loss = 1.37 (1194.7 examples/sec; 0.107 sec/batch)
2016-12-08 17:48:17.097902: step 2540, loss = 1.40 (1215.7 examples/sec; 0.105 sec/batch)
2016-12-08 17:48:18.248227: step 2550, loss = 1.36 (1172.6 examples/sec; 0.109 sec/batch)
2016-12-08 17:48:19.355288: step 2560, loss = 1.54 (1089.8 examples/sec; 0.117 sec/batch)
2016-12-08 17:48:20.467350: step 2570, loss = 1.38 (1138.4 examples/sec; 0.112 sec/batch)
2016-12-08 17:48:21.529890: step 2580, loss = 1.41 (1189.7 examples/sec; 0.108 sec/batch)
2016-12-08 17:48:22.638969: step 2590, loss = 1.66 (1053.6 examples/sec; 0.121 sec/batch)
2016-12-08 17:48:23.753107: step 2600, loss = 1.29 (1084.8 examples/sec; 0.118 sec/batch)
2016-12-08 17:48:25.061781: step 2610, loss = 1.41 (1196.4 examples/sec; 0.107 sec/batch)
2016-12-08 17:48:26.167701: step 2620, loss = 1.39 (1155.1 examples/sec; 0.111 sec/batch)
2016-12-08 17:48:27.321730: step 2630, loss = 1.30 (1099.0 examples/sec; 0.116 sec/batch)
2016-12-08 17:48:28.473654: step 2640, loss = 1.40 (1089.4 examples/sec; 0.117 sec/batch)
2016-12-08 17:48:29.541082: step 2650, loss = 1.52 (1186.4 examples/sec; 0.108 sec/batch)
2016-12-08 17:48:30.683889: step 2660, loss = 1.31 (1178.2 examples/sec; 0.109 sec/batch)
2016-12-08 17:48:31.806181: step 2670, loss = 1.36 (1201.5 examples/sec; 0.107 sec/batch)
2016-12-08 17:48:32.863782: step 2680, loss = 1.38 (1237.9 examples/sec; 0.103 sec/batch)
2016-12-08 17:48:33.941755: step 2690, loss = 1.43 (1183.6 examples/sec; 0.108 sec/batch)
2016-12-08 17:48:35.038016: step 2700, loss = 1.41 (1157.5 examples/sec; 0.111 sec/batch)
2016-12-08 17:48:36.323720: step 2710, loss = 1.64 (1167.2 examples/sec; 0.110 sec/batch)
2016-12-08 17:48:37.359889: step 2720, loss = 1.30 (1218.0 examples/sec; 0.105 sec/batch)
2016-12-08 17:48:38.441107: step 2730, loss = 1.38 (1202.2 examples/sec; 0.106 sec/batch)
2016-12-08 17:48:39.538030: step 2740, loss = 1.38 (1179.3 examples/sec; 0.109 sec/batch)
2016-12-08 17:48:40.616678: step 2750, loss = 1.27 (1267.6 examples/sec; 0.101 sec/batch)
2016-12-08 17:48:41.662721: step 2760, loss = 1.32 (1150.9 examples/sec; 0.111 sec/batch)
2016-12-08 17:48:42.788279: step 2770, loss = 1.32 (1173.6 examples/sec; 0.109 sec/batch)
2016-12-08 17:48:43.902362: step 2780, loss = 1.26 (1114.0 examples/sec; 0.115 sec/batch)
2016-12-08 17:48:44.978863: step 2790, loss = 1.29 (1238.4 examples/sec; 0.103 sec/batch)
2016-12-08 17:48:46.139823: step 2800, loss = 1.33 (1031.1 examples/sec; 0.124 sec/batch)
2016-12-08 17:48:47.712727: step 2810, loss = 1.32 (860.1 examples/sec; 0.149 sec/batch)
2016-12-08 17:48:48.794320: step 2820, loss = 1.39 (1202.8 examples/sec; 0.106 sec/batch)
2016-12-08 17:48:49.883870: step 2830, loss = 1.32 (1127.6 examples/sec; 0.114 sec/batch)
2016-12-08 17:48:50.994659: step 2840, loss = 1.23 (1195.7 examples/sec; 0.107 sec/batch)
2016-12-08 17:48:52.091968: step 2850, loss = 1.43 (1160.9 examples/sec; 0.110 sec/batch)
2016-12-08 17:48:53.122874: step 2860, loss = 1.30 (1242.2 examples/sec; 0.103 sec/batch)
2016-12-08 17:48:54.193027: step 2870, loss = 1.37 (1201.3 examples/sec; 0.107 sec/batch)
2016-12-08 17:48:55.302071: step 2880, loss = 1.40 (1019.4 examples/sec; 0.126 sec/batch)
2016-12-08 17:48:56.441832: step 2890, loss = 1.31 (1221.2 examples/sec; 0.105 sec/batch)
2016-12-08 17:48:57.492007: step 2900, loss = 1.11 (1189.9 examples/sec; 0.108 sec/batch)
2016-12-08 17:48:58.892985: step 2910, loss = 1.35 (966.6 examples/sec; 0.132 sec/batch)
2016-12-08 17:49:00.249750: step 2920, loss = 1.27 (913.9 examples/sec; 0.140 sec/batch)
2016-12-08 17:49:01.355506: step 2930, loss = 1.24 (1176.8 examples/sec; 0.109 sec/batch)
2016-12-08 17:49:02.466340: step 2940, loss = 1.38 (1166.0 examples/sec; 0.110 sec/batch)
2016-12-08 17:49:03.604413: step 2950, loss = 1.42 (1155.7 examples/sec; 0.111 sec/batch)
2016-12-08 17:49:04.726978: step 2960, loss = 1.33 (1016.1 examples/sec; 0.126 sec/batch)
2016-12-08 17:49:05.810341: step 2970, loss = 1.15 (1215.4 examples/sec; 0.105 sec/batch)
2016-12-08 17:49:06.907968: step 2980, loss = 1.25 (1131.6 examples/sec; 0.113 sec/batch)
2016-12-08 17:49:08.047459: step 2990, loss = 1.18 (1153.3 examples/sec; 0.111 sec/batch)
2016-12-08 17:49:09.122380: step 3000, loss = 1.39 (1230.7 examples/sec; 0.104 sec/batch)
2016-12-08 17:49:11.033458: step 3010, loss = 1.17 (1218.1 examples/sec; 0.105 sec/batch)
2016-12-08 17:49:12.161457: step 3020, loss = 1.24 (1138.0 examples/sec; 0.112 sec/batch)
2016-12-08 17:49:13.231321: step 3030, loss = 1.33 (1179.8 examples/sec; 0.108 sec/batch)
2016-12-08 17:49:14.320887: step 3040, loss = 1.28 (1161.9 examples/sec; 0.110 sec/batch)
2016-12-08 17:49:15.394531: step 3050, loss = 1.31 (1178.4 examples/sec; 0.109 sec/batch)
2016-12-08 17:49:16.482806: step 3060, loss = 1.16 (1199.6 examples/sec; 0.107 sec/batch)
2016-12-08 17:49:17.606146: step 3070, loss = 1.58 (1178.5 examples/sec; 0.109 sec/batch)
2016-12-08 17:49:18.706316: step 3080, loss = 1.29 (1173.8 examples/sec; 0.109 sec/batch)
2016-12-08 17:49:19.889635: step 3090, loss = 1.36 (1066.3 examples/sec; 0.120 sec/batch)
2016-12-08 17:49:21.187877: step 3100, loss = 1.18 (1157.3 examples/sec; 0.111 sec/batch)
2016-12-08 17:49:22.537999: step 3110, loss = 1.36 (1199.1 examples/sec; 0.107 sec/batch)
2016-12-08 17:49:23.655295: step 3120, loss = 1.34 (1190.3 examples/sec; 0.108 sec/batch)
2016-12-08 17:49:24.745349: step 3130, loss = 1.15 (1285.9 examples/sec; 0.100 sec/batch)
2016-12-08 17:49:25.830343: step 3140, loss = 1.45 (1156.9 examples/sec; 0.111 sec/batch)
2016-12-08 17:49:26.967522: step 3150, loss = 1.11 (1201.9 examples/sec; 0.106 sec/batch)
2016-12-08 17:49:28.104061: step 3160, loss = 1.24 (1151.8 examples/sec; 0.111 sec/batch)
2016-12-08 17:49:29.174100: step 3170, loss = 1.22 (1205.1 examples/sec; 0.106 sec/batch)
2016-12-08 17:49:30.280577: step 3180, loss = 1.23 (1159.1 examples/sec; 0.110 sec/batch)
2016-12-08 17:49:31.431777: step 3190, loss = 1.20 (1197.7 examples/sec; 0.107 sec/batch)
2016-12-08 17:49:32.528372: step 3200, loss = 1.33 (1207.4 examples/sec; 0.106 sec/batch)
2016-12-08 17:49:33.891338: step 3210, loss = 1.25 (1133.1 examples/sec; 0.113 sec/batch)
2016-12-08 17:49:35.031454: step 3220, loss = 1.25 (1096.2 examples/sec; 0.117 sec/batch)
2016-12-08 17:49:36.130520: step 3230, loss = 1.34 (1148.1 examples/sec; 0.111 sec/batch)
2016-12-08 17:49:37.254654: step 3240, loss = 1.18 (979.2 examples/sec; 0.131 sec/batch)
2016-12-08 17:49:38.566506: step 3250, loss = 1.33 (1016.3 examples/sec; 0.126 sec/batch)
2016-12-08 17:49:39.819797: step 3260, loss = 1.16 (1124.7 examples/sec; 0.114 sec/batch)
2016-12-08 17:49:40.921571: step 3270, loss = 1.33 (1237.0 examples/sec; 0.103 sec/batch)
2016-12-08 17:49:42.027683: step 3280, loss = 1.23 (1179.1 examples/sec; 0.109 sec/batch)
2016-12-08 17:49:43.135184: step 3290, loss = 1.24 (1109.4 examples/sec; 0.115 sec/batch)
2016-12-08 17:49:44.239958: step 3300, loss = 1.26 (1164.1 examples/sec; 0.110 sec/batch)
2016-12-08 17:49:45.528238: step 3310, loss = 1.51 (1127.5 examples/sec; 0.114 sec/batch)
2016-12-08 17:49:46.650614: step 3320, loss = 1.24 (965.2 examples/sec; 0.133 sec/batch)
2016-12-08 17:49:47.760376: step 3330, loss = 1.26 (1196.7 examples/sec; 0.107 sec/batch)
2016-12-08 17:49:48.824236: step 3340, loss = 1.19 (1250.5 examples/sec; 0.102 sec/batch)
2016-12-08 17:49:49.897184: step 3350, loss = 1.24 (1158.1 examples/sec; 0.111 sec/batch)
2016-12-08 17:49:51.029594: step 3360, loss = 1.25 (1167.6 examples/sec; 0.110 sec/batch)
2016-12-08 17:49:52.175991: step 3370, loss = 1.12 (1165.5 examples/sec; 0.110 sec/batch)
2016-12-08 17:49:53.301730: step 3380, loss = 1.30 (1156.4 examples/sec; 0.111 sec/batch)
2016-12-08 17:49:54.613318: step 3390, loss = 1.38 (771.7 examples/sec; 0.166 sec/batch)
2016-12-08 17:49:55.731881: step 3400, loss = 1.11 (1126.1 examples/sec; 0.114 sec/batch)
2016-12-08 17:49:57.046120: step 3410, loss = 1.06 (1270.0 examples/sec; 0.101 sec/batch)
2016-12-08 17:49:58.154791: step 3420, loss = 1.12 (1161.1 examples/sec; 0.110 sec/batch)
2016-12-08 17:49:59.266268: step 3430, loss = 1.18 (1177.1 examples/sec; 0.109 sec/batch)
2016-12-08 17:50:00.372880: step 3440, loss = 1.14 (1136.7 examples/sec; 0.113 sec/batch)
2016-12-08 17:50:01.471403: step 3450, loss = 1.15 (1159.2 examples/sec; 0.110 sec/batch)
2016-12-08 17:50:02.591198: step 3460, loss = 0.97 (1184.4 examples/sec; 0.108 sec/batch)
2016-12-08 17:50:03.738194: step 3470, loss = 1.31 (1167.1 examples/sec; 0.110 sec/batch)
2016-12-08 17:50:04.836950: step 3480, loss = 1.30 (1228.9 examples/sec; 0.104 sec/batch)
2016-12-08 17:50:05.958379: step 3490, loss = 1.17 (957.8 examples/sec; 0.134 sec/batch)
2016-12-08 17:50:07.105816: step 3500, loss = 1.04 (1064.5 examples/sec; 0.120 sec/batch)
2016-12-08 17:50:08.471691: step 3510, loss = 1.14 (1133.1 examples/sec; 0.113 sec/batch)
2016-12-08 17:50:09.534926: step 3520, loss = 1.09 (1126.7 examples/sec; 0.114 sec/batch)
2016-12-08 17:50:10.694805: step 3530, loss = 1.17 (1118.6 examples/sec; 0.114 sec/batch)
2016-12-08 17:50:11.822794: step 3540, loss = 1.12 (1137.5 examples/sec; 0.113 sec/batch)
2016-12-08 17:50:12.916659: step 3550, loss = 1.17 (1234.1 examples/sec; 0.104 sec/batch)
2016-12-08 17:50:14.026806: step 3560, loss = 1.17 (1079.1 examples/sec; 0.119 sec/batch)
2016-12-08 17:50:15.138382: step 3570, loss = 1.31 (1163.9 examples/sec; 0.110 sec/batch)
2016-12-08 17:50:16.302027: step 3580, loss = 1.34 (1099.3 examples/sec; 0.116 sec/batch)
2016-12-08 17:50:17.370329: step 3590, loss = 1.22 (1130.6 examples/sec; 0.113 sec/batch)
2016-12-08 17:50:18.519011: step 3600, loss = 1.12 (1132.2 examples/sec; 0.113 sec/batch)
2016-12-08 17:50:19.856849: step 3610, loss = 1.23 (1074.5 examples/sec; 0.119 sec/batch)
2016-12-08 17:50:20.956387: step 3620, loss = 1.11 (1224.7 examples/sec; 0.105 sec/batch)
2016-12-08 17:50:22.066265: step 3630, loss = 1.04 (1176.9 examples/sec; 0.109 sec/batch)
2016-12-08 17:50:23.166314: step 3640, loss = 1.23 (1163.2 examples/sec; 0.110 sec/batch)
2016-12-08 17:50:24.300422: step 3650, loss = 1.28 (1103.2 examples/sec; 0.116 sec/batch)
2016-12-08 17:50:25.336698: step 3660, loss = 1.30 (1147.0 examples/sec; 0.112 sec/batch)
2016-12-08 17:50:26.450168: step 3670, loss = 0.99 (1174.2 examples/sec; 0.109 sec/batch)
2016-12-08 17:50:27.576000: step 3680, loss = 1.15 (1170.6 examples/sec; 0.109 sec/batch)
2016-12-08 17:50:28.660451: step 3690, loss = 1.23 (1238.4 examples/sec; 0.103 sec/batch)
2016-12-08 17:50:29.773091: step 3700, loss = 1.24 (1034.8 examples/sec; 0.124 sec/batch)
2016-12-08 17:50:31.108076: step 3710, loss = 1.06 (1135.0 examples/sec; 0.113 sec/batch)
2016-12-08 17:50:32.236532: step 3720, loss = 1.46 (1143.8 examples/sec; 0.112 sec/batch)
2016-12-08 17:50:33.330913: step 3730, loss = 1.06 (1191.0 examples/sec; 0.107 sec/batch)
2016-12-08 17:50:34.448544: step 3740, loss = 0.99 (1151.4 examples/sec; 0.111 sec/batch)
2016-12-08 17:50:35.594032: step 3750, loss = 1.29 (1155.7 examples/sec; 0.111 sec/batch)
2016-12-08 17:50:36.705932: step 3760, loss = 1.16 (1221.6 examples/sec; 0.105 sec/batch)
2016-12-08 17:50:37.792128: step 3770, loss = 1.23 (1056.8 examples/sec; 0.121 sec/batch)
2016-12-08 17:50:38.886396: step 3780, loss = 0.95 (1143.5 examples/sec; 0.112 sec/batch)
2016-12-08 17:50:40.029774: step 3790, loss = 1.35 (1141.1 examples/sec; 0.112 sec/batch)
2016-12-08 17:50:41.084853: step 3800, loss = 1.17 (1246.7 examples/sec; 0.103 sec/batch)
2016-12-08 17:50:42.431005: step 3810, loss = 1.03 (1164.6 examples/sec; 0.110 sec/batch)
2016-12-08 17:50:43.525967: step 3820, loss = 1.21 (1151.2 examples/sec; 0.111 sec/batch)
2016-12-08 17:50:44.643377: step 3830, loss = 0.92 (1228.3 examples/sec; 0.104 sec/batch)
2016-12-08 17:50:45.753011: step 3840, loss = 1.00 (1135.1 examples/sec; 0.113 sec/batch)
2016-12-08 17:50:46.884454: step 3850, loss = 1.20 (1174.4 examples/sec; 0.109 sec/batch)
2016-12-08 17:50:48.036562: step 3860, loss = 1.19 (1163.7 examples/sec; 0.110 sec/batch)
2016-12-08 17:50:49.125543: step 3870, loss = 1.26 (1181.2 examples/sec; 0.108 sec/batch)
2016-12-08 17:50:50.266982: step 3880, loss = 1.13 (1115.7 examples/sec; 0.115 sec/batch)
2016-12-08 17:50:51.377153: step 3890, loss = 1.22 (1083.2 examples/sec; 0.118 sec/batch)
2016-12-08 17:50:52.494595: step 3900, loss = 1.12 (1208.8 examples/sec; 0.106 sec/batch)
2016-12-08 17:50:53.755794: step 3910, loss = 1.10 (1095.5 examples/sec; 0.117 sec/batch)
2016-12-08 17:50:54.882379: step 3920, loss = 1.09 (1168.9 examples/sec; 0.110 sec/batch)
2016-12-08 17:50:56.002473: step 3930, loss = 0.96 (1169.2 examples/sec; 0.109 sec/batch)
2016-12-08 17:50:57.049545: step 3940, loss = 0.95 (1251.3 examples/sec; 0.102 sec/batch)
2016-12-08 17:50:58.141678: step 3950, loss = 1.03 (1151.5 examples/sec; 0.111 sec/batch)
2016-12-08 17:50:59.275452: step 3960, loss = 1.12 (1178.3 examples/sec; 0.109 sec/batch)
2016-12-08 17:51:00.376256: step 3970, loss = 1.08 (1153.8 examples/sec; 0.111 sec/batch)
2016-12-08 17:51:01.474747: step 3980, loss = 1.06 (1179.0 examples/sec; 0.109 sec/batch)
2016-12-08 17:51:02.594473: step 3990, loss = 0.80 (1138.1 examples/sec; 0.112 sec/batch)
2016-12-08 17:51:03.774510: step 4000, loss = 1.23 (1164.1 examples/sec; 0.110 sec/batch)
2016-12-08 17:51:05.584240: step 4010, loss = 1.06 (1167.7 examples/sec; 0.110 sec/batch)
2016-12-08 17:51:06.755341: step 4020, loss = 1.00 (1132.8 examples/sec; 0.113 sec/batch)
2016-12-08 17:51:07.856637: step 4030, loss = 1.15 (1142.1 examples/sec; 0.112 sec/batch)
2016-12-08 17:51:08.964721: step 4040, loss = 1.00 (1206.5 examples/sec; 0.106 sec/batch)
2016-12-08 17:51:10.058674: step 4050, loss = 1.17 (1069.7 examples/sec; 0.120 sec/batch)
2016-12-08 17:51:11.232639: step 4060, loss = 1.17 (1185.9 examples/sec; 0.108 sec/batch)
2016-12-08 17:51:12.345849: step 4070, loss = 1.22 (1131.1 examples/sec; 0.113 sec/batch)
2016-12-08 17:51:13.427408: step 4080, loss = 1.02 (1196.0 examples/sec; 0.107 sec/batch)
2016-12-08 17:51:14.537787: step 4090, loss = 1.21 (1142.8 examples/sec; 0.112 sec/batch)
2016-12-08 17:51:15.674175: step 4100, loss = 1.00 (1124.6 examples/sec; 0.114 sec/batch)
2016-12-08 17:51:16.953082: step 4110, loss = 1.00 (1219.3 examples/sec; 0.105 sec/batch)
2016-12-08 17:51:18.113353: step 4120, loss = 1.05 (1139.5 examples/sec; 0.112 sec/batch)
2016-12-08 17:51:19.226219: step 4130, loss = 1.05 (1209.4 examples/sec; 0.106 sec/batch)
2016-12-08 17:51:20.351635: step 4140, loss = 1.16 (1030.3 examples/sec; 0.124 sec/batch)
2016-12-08 17:51:21.404649: step 4150, loss = 1.06 (1188.6 examples/sec; 0.108 sec/batch)
2016-12-08 17:51:22.516973: step 4160, loss = 1.17 (1127.8 examples/sec; 0.113 sec/batch)
2016-12-08 17:51:23.606645: step 4170, loss = 1.09 (1155.1 examples/sec; 0.111 sec/batch)
2016-12-08 17:51:24.726151: step 4180, loss = 1.04 (1198.9 examples/sec; 0.107 sec/batch)
2016-12-08 17:51:25.810098: step 4190, loss = 1.25 (1163.6 examples/sec; 0.110 sec/batch)
2016-12-08 17:51:26.963904: step 4200, loss = 1.01 (1179.4 examples/sec; 0.109 sec/batch)
2016-12-08 17:51:28.307170: step 4210, loss = 1.14 (1140.8 examples/sec; 0.112 sec/batch)
2016-12-08 17:51:29.390487: step 4220, loss = 1.01 (1142.8 examples/sec; 0.112 sec/batch)
2016-12-08 17:51:30.475222: step 4230, loss = 1.15 (1185.8 examples/sec; 0.108 sec/batch)
2016-12-08 17:51:31.618714: step 4240, loss = 1.11 (1177.4 examples/sec; 0.109 sec/batch)
2016-12-08 17:51:32.710199: step 4250, loss = 1.16 (1259.7 examples/sec; 0.102 sec/batch)
2016-12-08 17:51:33.756923: step 4260, loss = 1.22 (1173.9 examples/sec; 0.109 sec/batch)
2016-12-08 17:51:34.845881: step 4270, loss = 0.99 (1132.1 examples/sec; 0.113 sec/batch)
2016-12-08 17:51:35.971629: step 4280, loss = 1.09 (1203.1 examples/sec; 0.106 sec/batch)
2016-12-08 17:51:37.015739: step 4290, loss = 1.01 (1255.8 examples/sec; 0.102 sec/batch)
2016-12-08 17:51:38.182919: step 4300, loss = 1.05 (954.8 examples/sec; 0.134 sec/batch)
2016-12-08 17:51:39.686314: step 4310, loss = 1.18 (989.4 examples/sec; 0.129 sec/batch)
2016-12-08 17:51:40.960560: step 4320, loss = 0.97 (1282.3 examples/sec; 0.100 sec/batch)
2016-12-08 17:51:42.044812: step 4330, loss = 1.09 (1154.9 examples/sec; 0.111 sec/batch)
2016-12-08 17:51:43.170639: step 4340, loss = 0.99 (1081.9 examples/sec; 0.118 sec/batch)
2016-12-08 17:51:44.285915: step 4350, loss = 0.83 (1183.7 examples/sec; 0.108 sec/batch)
2016-12-08 17:51:45.313914: step 4360, loss = 1.11 (1237.1 examples/sec; 0.103 sec/batch)
2016-12-08 17:51:46.400493: step 4370, loss = 1.13 (1178.6 examples/sec; 0.109 sec/batch)
2016-12-08 17:51:47.489577: step 4380, loss = 1.19 (1129.5 examples/sec; 0.113 sec/batch)
2016-12-08 17:51:48.573053: step 4390, loss = 1.03 (1273.7 examples/sec; 0.100 sec/batch)
2016-12-08 17:51:49.635607: step 4400, loss = 1.16 (1170.9 examples/sec; 0.109 sec/batch)
2016-12-08 17:51:50.956763: step 4410, loss = 1.12 (1178.9 examples/sec; 0.109 sec/batch)
2016-12-08 17:51:52.039454: step 4420, loss = 1.16 (1164.7 examples/sec; 0.110 sec/batch)
2016-12-08 17:51:53.097593: step 4430, loss = 0.89 (1293.4 examples/sec; 0.099 sec/batch)
2016-12-08 17:51:54.239440: step 4440, loss = 1.18 (1126.3 examples/sec; 0.114 sec/batch)
2016-12-08 17:51:55.364349: step 4450, loss = 1.11 (1134.1 examples/sec; 0.113 sec/batch)
2016-12-08 17:51:56.469110: step 4460, loss = 1.00 (1138.3 examples/sec; 0.112 sec/batch)
2016-12-08 17:51:57.550006: step 4470, loss = 1.08 (1166.3 examples/sec; 0.110 sec/batch)
2016-12-08 17:51:58.651837: step 4480, loss = 0.94 (1111.2 examples/sec; 0.115 sec/batch)
2016-12-08 17:51:59.803300: step 4490, loss = 1.02 (1094.4 examples/sec; 0.117 sec/batch)
2016-12-08 17:52:00.873400: step 4500, loss = 1.05 (1239.4 examples/sec; 0.103 sec/batch)
2016-12-08 17:52:02.215619: step 4510, loss = 0.99 (1149.2 examples/sec; 0.111 sec/batch)
2016-12-08 17:52:03.312288: step 4520, loss = 1.41 (1166.6 examples/sec; 0.110 sec/batch)
2016-12-08 17:52:04.413901: step 4530, loss = 0.95 (1223.4 examples/sec; 0.105 sec/batch)
2016-12-08 17:52:05.474553: step 4540, loss = 1.20 (1127.5 examples/sec; 0.114 sec/batch)
2016-12-08 17:52:06.561352: step 4550, loss = 0.92 (1204.8 examples/sec; 0.106 sec/batch)
2016-12-08 17:52:07.638752: step 4560, loss = 1.12 (1206.6 examples/sec; 0.106 sec/batch)
2016-12-08 17:52:08.716777: step 4570, loss = 1.26 (1282.9 examples/sec; 0.100 sec/batch)
2016-12-08 17:52:09.770982: step 4580, loss = 1.18 (1166.1 examples/sec; 0.110 sec/batch)
2016-12-08 17:52:10.853434: step 4590, loss = 1.03 (1185.2 examples/sec; 0.108 sec/batch)
2016-12-08 17:52:11.953367: step 4600, loss = 0.94 (1145.7 examples/sec; 0.112 sec/batch)
2016-12-08 17:52:13.228381: step 4610, loss = 1.20 (1157.9 examples/sec; 0.111 sec/batch)
2016-12-08 17:52:14.312605: step 4620, loss = 1.14 (1186.3 examples/sec; 0.108 sec/batch)
2016-12-08 17:52:15.405396: step 4630, loss = 0.97 (1235.9 examples/sec; 0.104 sec/batch)
2016-12-08 17:52:16.545999: step 4640, loss = 0.99 (1181.9 examples/sec; 0.108 sec/batch)
2016-12-08 17:52:17.600411: step 4650, loss = 1.12 (1166.5 examples/sec; 0.110 sec/batch)
2016-12-08 17:52:18.708271: step 4660, loss = 0.93 (1172.5 examples/sec; 0.109 sec/batch)
2016-12-08 17:52:19.831464: step 4670, loss = 0.96 (1162.7 examples/sec; 0.110 sec/batch)
2016-12-08 17:52:20.900044: step 4680, loss = 0.78 (1214.5 examples/sec; 0.105 sec/batch)
2016-12-08 17:52:21.965080: step 4690, loss = 1.02 (1178.2 examples/sec; 0.109 sec/batch)
2016-12-08 17:52:23.126241: step 4700, loss = 1.07 (1150.3 examples/sec; 0.111 sec/batch)
2016-12-08 17:52:24.418749: step 4710, loss = 0.95 (1111.3 examples/sec; 0.115 sec/batch)
2016-12-08 17:52:25.532192: step 4720, loss = 1.01 (1071.4 examples/sec; 0.119 sec/batch)
2016-12-08 17:52:26.643515: step 4730, loss = 1.10 (1058.7 examples/sec; 0.121 sec/batch)
2016-12-08 17:52:27.802483: step 4740, loss = 0.94 (1131.5 examples/sec; 0.113 sec/batch)
2016-12-08 17:52:28.877399: step 4750, loss = 1.08 (1177.0 examples/sec; 0.109 sec/batch)
2016-12-08 17:52:29.965445: step 4760, loss = 0.98 (1150.0 examples/sec; 0.111 sec/batch)
2016-12-08 17:52:31.117715: step 4770, loss = 1.00 (1201.1 examples/sec; 0.107 sec/batch)
2016-12-08 17:52:32.250475: step 4780, loss = 1.22 (1143.5 examples/sec; 0.112 sec/batch)
2016-12-08 17:52:33.356750: step 4790, loss = 1.10 (960.8 examples/sec; 0.133 sec/batch)
2016-12-08 17:52:34.635956: step 4800, loss = 1.05 (901.9 examples/sec; 0.142 sec/batch)
2016-12-08 17:52:36.017498: step 4810, loss = 0.83 (1168.4 examples/sec; 0.110 sec/batch)
2016-12-08 17:52:37.085478: step 4820, loss = 0.98 (1238.5 examples/sec; 0.103 sec/batch)
2016-12-08 17:52:38.186001: step 4830, loss = 1.06 (1139.4 examples/sec; 0.112 sec/batch)
2016-12-08 17:52:39.318478: step 4840, loss = 1.11 (1149.2 examples/sec; 0.111 sec/batch)
2016-12-08 17:52:40.426572: step 4850, loss = 0.99 (1222.6 examples/sec; 0.105 sec/batch)
2016-12-08 17:52:41.451878: step 4860, loss = 0.89 (1199.1 examples/sec; 0.107 sec/batch)
2016-12-08 17:52:42.537381: step 4870, loss = 1.24 (1138.2 examples/sec; 0.112 sec/batch)
2016-12-08 17:52:43.616604: step 4880, loss = 1.00 (1182.7 examples/sec; 0.108 sec/batch)
2016-12-08 17:52:44.678917: step 4890, loss = 1.17 (1280.5 examples/sec; 0.100 sec/batch)
2016-12-08 17:52:45.742475: step 4900, loss = 1.04 (1207.0 examples/sec; 0.106 sec/batch)
2016-12-08 17:52:47.082127: step 4910, loss = 1.16 (1157.8 examples/sec; 0.111 sec/batch)
2016-12-08 17:52:48.184780: step 4920, loss = 1.11 (1198.3 examples/sec; 0.107 sec/batch)
2016-12-08 17:52:49.232031: step 4930, loss = 1.00 (1203.1 examples/sec; 0.106 sec/batch)
2016-12-08 17:52:50.332776: step 4940, loss = 1.08 (1208.9 examples/sec; 0.106 sec/batch)
2016-12-08 17:52:51.438341: step 4950, loss = 1.12 (1185.1 examples/sec; 0.108 sec/batch)
2016-12-08 17:52:52.539494: step 4960, loss = 0.94 (1086.5 examples/sec; 0.118 sec/batch)
2016-12-08 17:52:53.610508: step 4970, loss = 1.13 (1163.6 examples/sec; 0.110 sec/batch)
2016-12-08 17:52:54.756106: step 4980, loss = 0.95 (1136.0 examples/sec; 0.113 sec/batch)
2016-12-08 17:52:55.877152: step 4990, loss = 0.81 (1172.4 examples/sec; 0.109 sec/batch)
2016-12-08 17:52:56.978588: step 5000, loss = 1.04 (1253.1 examples/sec; 0.102 sec/batch)
2016-12-08 17:52:58.948911: step 5010, loss = 0.97 (1206.6 examples/sec; 0.106 sec/batch)
2016-12-08 17:53:00.037467: step 5020, loss = 1.04 (1229.8 examples/sec; 0.104 sec/batch)
2016-12-08 17:53:01.076537: step 5030, loss = 1.02 (1249.9 examples/sec; 0.102 sec/batch)
2016-12-08 17:53:02.155123: step 5040, loss = 0.93 (1143.9 examples/sec; 0.112 sec/batch)
2016-12-08 17:53:03.291939: step 5050, loss = 0.95 (1149.9 examples/sec; 0.111 sec/batch)
2016-12-08 17:53:04.390587: step 5060, loss = 0.90 (1109.2 examples/sec; 0.115 sec/batch)
2016-12-08 17:53:05.443093: step 5070, loss = 1.08 (1155.0 examples/sec; 0.111 sec/batch)
2016-12-08 17:53:06.550595: step 5080, loss = 0.98 (1116.3 examples/sec; 0.115 sec/batch)
2016-12-08 17:53:07.672234: step 5090, loss = 0.94 (1122.1 examples/sec; 0.114 sec/batch)
2016-12-08 17:53:08.802490: step 5100, loss = 1.09 (1174.6 examples/sec; 0.109 sec/batch)
2016-12-08 17:53:10.143499: step 5110, loss = 1.23 (1151.0 examples/sec; 0.111 sec/batch)
2016-12-08 17:53:11.351689: step 5120, loss = 1.00 (1053.9 examples/sec; 0.121 sec/batch)
2016-12-08 17:53:12.562318: step 5130, loss = 0.78 (1146.0 examples/sec; 0.112 sec/batch)
2016-12-08 17:53:13.710905: step 5140, loss = 1.10 (1165.5 examples/sec; 0.110 sec/batch)
2016-12-08 17:53:14.816538: step 5150, loss = 0.94 (1178.9 examples/sec; 0.109 sec/batch)
2016-12-08 17:53:15.954386: step 5160, loss = 1.01 (1203.6 examples/sec; 0.106 sec/batch)
2016-12-08 17:53:17.015835: step 5170, loss = 1.08 (1262.4 examples/sec; 0.101 sec/batch)
2016-12-08 17:53:18.146370: step 5180, loss = 0.94 (1146.5 examples/sec; 0.112 sec/batch)
2016-12-08 17:53:19.252694: step 5190, loss = 0.87 (1089.7 examples/sec; 0.117 sec/batch)
2016-12-08 17:53:20.394812: step 5200, loss = 1.08 (1121.5 examples/sec; 0.114 sec/batch)
2016-12-08 17:53:21.639660: step 5210, loss = 1.06 (1210.5 examples/sec; 0.106 sec/batch)
2016-12-08 17:53:22.753451: step 5220, loss = 0.89 (1113.2 examples/sec; 0.115 sec/batch)
2016-12-08 17:53:23.931373: step 5230, loss = 1.10 (1066.9 examples/sec; 0.120 sec/batch)
2016-12-08 17:53:24.992728: step 5240, loss = 0.95 (1238.0 examples/sec; 0.103 sec/batch)
2016-12-08 17:53:26.060808: step 5250, loss = 1.08 (1200.6 examples/sec; 0.107 sec/batch)
2016-12-08 17:53:27.142866: step 5260, loss = 1.13 (1155.6 examples/sec; 0.111 sec/batch)
2016-12-08 17:53:28.238526: step 5270, loss = 0.96 (1164.7 examples/sec; 0.110 sec/batch)
2016-12-08 17:53:29.325122: step 5280, loss = 1.07 (1009.6 examples/sec; 0.127 sec/batch)
2016-12-08 17:53:30.438404: step 5290, loss = 0.83 (1140.4 examples/sec; 0.112 sec/batch)
2016-12-08 17:53:31.599627: step 5300, loss = 1.10 (1118.4 examples/sec; 0.114 sec/batch)
2016-12-08 17:53:32.846003: step 5310, loss = 1.09 (1291.2 examples/sec; 0.099 sec/batch)
2016-12-08 17:53:33.945242: step 5320, loss = 1.02 (1051.9 examples/sec; 0.122 sec/batch)
2016-12-08 17:53:35.183444: step 5330, loss = 0.95 (828.0 examples/sec; 0.155 sec/batch)
2016-12-08 17:53:36.477152: step 5340, loss = 1.29 (1224.3 examples/sec; 0.105 sec/batch)
2016-12-08 17:53:37.510406: step 5350, loss = 1.03 (1192.8 examples/sec; 0.107 sec/batch)
2016-12-08 17:53:38.608126: step 5360, loss = 0.93 (1231.6 examples/sec; 0.104 sec/batch)
2016-12-08 17:53:39.741902: step 5370, loss = 0.97 (1198.9 examples/sec; 0.107 sec/batch)
2016-12-08 17:53:40.803926: step 5380, loss = 0.94 (1182.7 examples/sec; 0.108 sec/batch)
2016-12-08 17:53:41.899538: step 5390, loss = 1.03 (1162.4 examples/sec; 0.110 sec/batch)
2016-12-08 17:53:43.010593: step 5400, loss = 0.91 (1160.0 examples/sec; 0.110 sec/batch)
2016-12-08 17:53:44.314808: step 5410, loss = 1.02 (1146.7 examples/sec; 0.112 sec/batch)
2016-12-08 17:53:45.370390: step 5420, loss = 0.91 (1191.9 examples/sec; 0.107 sec/batch)
2016-12-08 17:53:46.460648: step 5430, loss = 1.04 (1173.6 examples/sec; 0.109 sec/batch)
2016-12-08 17:53:47.580759: step 5440, loss = 0.98 (1043.4 examples/sec; 0.123 sec/batch)
2016-12-08 17:53:48.669731: step 5450, loss = 1.09 (1247.0 examples/sec; 0.103 sec/batch)
2016-12-08 17:53:49.747842: step 5460, loss = 0.94 (1113.9 examples/sec; 0.115 sec/batch)
2016-12-08 17:53:50.859232: step 5470, loss = 0.85 (1197.5 examples/sec; 0.107 sec/batch)
2016-12-08 17:53:51.946717: step 5480, loss = 0.97 (1173.6 examples/sec; 0.109 sec/batch)
2016-12-08 17:53:53.080755: step 5490, loss = 0.96 (1241.3 examples/sec; 0.103 sec/batch)
2016-12-08 17:53:54.343638: step 5500, loss = 0.90 (897.6 examples/sec; 0.143 sec/batch)
2016-12-08 17:53:55.793050: step 5510, loss = 1.03 (1118.1 examples/sec; 0.114 sec/batch)
2016-12-08 17:53:56.862791: step 5520, loss = 1.16 (1248.3 examples/sec; 0.103 sec/batch)
2016-12-08 17:53:57.988452: step 5530, loss = 0.84 (1159.6 examples/sec; 0.110 sec/batch)
