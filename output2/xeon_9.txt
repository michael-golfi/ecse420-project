Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
WARNING:tensorflow:From cifar10_train.py:81 in train.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2016-03-02.
Instructions for updating:
Please use tf.global_variables instead.
2016-12-08 17:31:28.502092: step 0, loss = 4.68 (26.6 examples/sec; 4.805 sec/batch)
2016-12-08 17:31:31.734916: step 10, loss = 4.64 (545.7 examples/sec; 0.235 sec/batch)
2016-12-08 17:31:34.041066: step 20, loss = 4.55 (561.0 examples/sec; 0.228 sec/batch)
2016-12-08 17:31:36.372788: step 30, loss = 4.57 (538.0 examples/sec; 0.238 sec/batch)
2016-12-08 17:31:38.762248: step 40, loss = 4.38 (534.4 examples/sec; 0.240 sec/batch)
2016-12-08 17:31:41.097342: step 50, loss = 4.36 (532.5 examples/sec; 0.240 sec/batch)
2016-12-08 17:31:43.439290: step 60, loss = 4.27 (631.6 examples/sec; 0.203 sec/batch)
2016-12-08 17:31:45.706305: step 70, loss = 4.19 (588.2 examples/sec; 0.218 sec/batch)
2016-12-08 17:31:48.049646: step 80, loss = 4.30 (538.7 examples/sec; 0.238 sec/batch)
2016-12-08 17:31:50.349694: step 90, loss = 4.17 (609.4 examples/sec; 0.210 sec/batch)
2016-12-08 17:31:52.696295: step 100, loss = 4.07 (523.4 examples/sec; 0.245 sec/batch)
2016-12-08 17:31:55.215519: step 110, loss = 4.16 (480.5 examples/sec; 0.266 sec/batch)
2016-12-08 17:31:57.387825: step 120, loss = 4.27 (642.9 examples/sec; 0.199 sec/batch)
2016-12-08 17:31:59.752098: step 130, loss = 4.00 (575.3 examples/sec; 0.223 sec/batch)
2016-12-08 17:32:02.064064: step 140, loss = 4.07 (588.3 examples/sec; 0.218 sec/batch)
2016-12-08 17:32:04.335249: step 150, loss = 3.94 (496.4 examples/sec; 0.258 sec/batch)
2016-12-08 17:32:06.660223: step 160, loss = 3.94 (515.9 examples/sec; 0.248 sec/batch)
2016-12-08 17:32:09.016802: step 170, loss = 3.75 (645.9 examples/sec; 0.198 sec/batch)
2016-12-08 17:32:11.394796: step 180, loss = 3.81 (526.0 examples/sec; 0.243 sec/batch)
2016-12-08 17:32:13.713292: step 190, loss = 3.73 (513.9 examples/sec; 0.249 sec/batch)
2016-12-08 17:32:15.976158: step 200, loss = 3.71 (543.2 examples/sec; 0.236 sec/batch)
2016-12-08 17:32:18.430765: step 210, loss = 3.54 (531.7 examples/sec; 0.241 sec/batch)
2016-12-08 17:32:20.705853: step 220, loss = 3.71 (525.4 examples/sec; 0.244 sec/batch)
2016-12-08 17:32:23.091832: step 230, loss = 3.68 (472.5 examples/sec; 0.271 sec/batch)
2016-12-08 17:32:25.318102: step 240, loss = 3.65 (544.1 examples/sec; 0.235 sec/batch)
2016-12-08 17:32:27.639371: step 250, loss = 3.72 (510.8 examples/sec; 0.251 sec/batch)
2016-12-08 17:32:29.895572: step 260, loss = 3.73 (515.6 examples/sec; 0.248 sec/batch)
2016-12-08 17:32:32.231228: step 270, loss = 3.57 (558.5 examples/sec; 0.229 sec/batch)
2016-12-08 17:32:34.601240: step 280, loss = 3.56 (465.7 examples/sec; 0.275 sec/batch)
2016-12-08 17:32:36.875054: step 290, loss = 3.67 (547.5 examples/sec; 0.234 sec/batch)
2016-12-08 17:32:39.216839: step 300, loss = 3.85 (503.8 examples/sec; 0.254 sec/batch)
2016-12-08 17:32:41.723626: step 310, loss = 3.52 (517.0 examples/sec; 0.248 sec/batch)
2016-12-08 17:32:43.962515: step 320, loss = 3.71 (593.1 examples/sec; 0.216 sec/batch)
2016-12-08 17:32:46.278351: step 330, loss = 3.40 (555.4 examples/sec; 0.230 sec/batch)
2016-12-08 17:32:48.734800: step 340, loss = 3.46 (494.7 examples/sec; 0.259 sec/batch)
2016-12-08 17:32:51.093652: step 350, loss = 3.46 (626.2 examples/sec; 0.204 sec/batch)
2016-12-08 17:32:53.285081: step 360, loss = 3.40 (506.0 examples/sec; 0.253 sec/batch)
2016-12-08 17:32:55.577754: step 370, loss = 3.51 (481.0 examples/sec; 0.266 sec/batch)
2016-12-08 17:32:57.877246: step 380, loss = 3.41 (552.0 examples/sec; 0.232 sec/batch)
2016-12-08 17:33:00.179917: step 390, loss = 3.50 (603.1 examples/sec; 0.212 sec/batch)
2016-12-08 17:33:02.548425: step 400, loss = 3.54 (537.9 examples/sec; 0.238 sec/batch)
2016-12-08 17:33:05.130383: step 410, loss = 3.29 (596.6 examples/sec; 0.215 sec/batch)
2016-12-08 17:33:07.473615: step 420, loss = 3.27 (567.5 examples/sec; 0.226 sec/batch)
2016-12-08 17:33:09.731253: step 430, loss = 3.48 (531.5 examples/sec; 0.241 sec/batch)
2016-12-08 17:33:12.025143: step 440, loss = 3.40 (633.0 examples/sec; 0.202 sec/batch)
2016-12-08 17:33:14.366931: step 450, loss = 3.14 (588.8 examples/sec; 0.217 sec/batch)
2016-12-08 17:33:16.743471: step 460, loss = 3.32 (477.2 examples/sec; 0.268 sec/batch)
2016-12-08 17:33:19.114903: step 470, loss = 3.28 (547.1 examples/sec; 0.234 sec/batch)
2016-12-08 17:33:21.295330: step 480, loss = 3.10 (616.2 examples/sec; 0.208 sec/batch)
2016-12-08 17:33:23.411824: step 490, loss = 3.24 (576.8 examples/sec; 0.222 sec/batch)
2016-12-08 17:33:25.677929: step 500, loss = 3.28 (531.0 examples/sec; 0.241 sec/batch)
2016-12-08 17:33:28.296873: step 510, loss = 2.88 (483.4 examples/sec; 0.265 sec/batch)
2016-12-08 17:33:30.622925: step 520, loss = 3.05 (570.5 examples/sec; 0.224 sec/batch)
2016-12-08 17:33:32.985908: step 530, loss = 3.06 (540.9 examples/sec; 0.237 sec/batch)
2016-12-08 17:33:35.289015: step 540, loss = 3.50 (549.6 examples/sec; 0.233 sec/batch)
2016-12-08 17:33:37.533857: step 550, loss = 3.14 (534.8 examples/sec; 0.239 sec/batch)
2016-12-08 17:33:39.799112: step 560, loss = 3.02 (499.2 examples/sec; 0.256 sec/batch)
2016-12-08 17:33:42.003612: step 570, loss = 3.08 (579.9 examples/sec; 0.221 sec/batch)
2016-12-08 17:33:44.342658: step 580, loss = 3.03 (572.1 examples/sec; 0.224 sec/batch)
2016-12-08 17:33:46.782173: step 590, loss = 3.13 (523.8 examples/sec; 0.244 sec/batch)
2016-12-08 17:33:49.161516: step 600, loss = 3.01 (517.4 examples/sec; 0.247 sec/batch)
2016-12-08 17:33:51.685693: step 610, loss = 2.91 (653.6 examples/sec; 0.196 sec/batch)
2016-12-08 17:33:53.997263: step 620, loss = 2.89 (553.5 examples/sec; 0.231 sec/batch)
2016-12-08 17:33:56.221715: step 630, loss = 2.82 (629.3 examples/sec; 0.203 sec/batch)
2016-12-08 17:33:58.620399: step 640, loss = 3.08 (507.0 examples/sec; 0.252 sec/batch)
2016-12-08 17:34:00.850536: step 650, loss = 2.71 (618.6 examples/sec; 0.207 sec/batch)
2016-12-08 17:34:03.129311: step 660, loss = 2.78 (575.5 examples/sec; 0.222 sec/batch)
2016-12-08 17:34:05.514541: step 670, loss = 3.10 (498.6 examples/sec; 0.257 sec/batch)
2016-12-08 17:34:07.749105: step 680, loss = 2.82 (576.1 examples/sec; 0.222 sec/batch)
2016-12-08 17:34:09.946175: step 690, loss = 2.98 (575.2 examples/sec; 0.223 sec/batch)
2016-12-08 17:34:12.341673: step 700, loss = 2.85 (613.7 examples/sec; 0.209 sec/batch)
2016-12-08 17:34:14.904987: step 710, loss = 2.72 (535.3 examples/sec; 0.239 sec/batch)
2016-12-08 17:34:17.253812: step 720, loss = 2.71 (560.0 examples/sec; 0.229 sec/batch)
2016-12-08 17:34:19.496645: step 730, loss = 2.95 (582.7 examples/sec; 0.220 sec/batch)
2016-12-08 17:34:21.819125: step 740, loss = 2.82 (508.0 examples/sec; 0.252 sec/batch)
2016-12-08 17:34:24.132878: step 750, loss = 2.82 (534.3 examples/sec; 0.240 sec/batch)
2016-12-08 17:34:26.435043: step 760, loss = 2.81 (545.5 examples/sec; 0.235 sec/batch)
2016-12-08 17:34:28.701105: step 770, loss = 2.69 (535.1 examples/sec; 0.239 sec/batch)
2016-12-08 17:34:30.988353: step 780, loss = 2.71 (646.8 examples/sec; 0.198 sec/batch)
2016-12-08 17:34:33.247370: step 790, loss = 2.71 (572.6 examples/sec; 0.224 sec/batch)
2016-12-08 17:34:35.608086: step 800, loss = 2.82 (509.7 examples/sec; 0.251 sec/batch)
2016-12-08 17:34:38.152095: step 810, loss = 2.52 (543.8 examples/sec; 0.235 sec/batch)
2016-12-08 17:34:40.414340: step 820, loss = 2.55 (540.6 examples/sec; 0.237 sec/batch)
2016-12-08 17:34:42.689528: step 830, loss = 2.57 (543.8 examples/sec; 0.235 sec/batch)
2016-12-08 17:34:45.056392: step 840, loss = 2.56 (571.1 examples/sec; 0.224 sec/batch)
2016-12-08 17:34:47.412517: step 850, loss = 2.47 (505.2 examples/sec; 0.253 sec/batch)
2016-12-08 17:34:49.679656: step 860, loss = 2.53 (570.4 examples/sec; 0.224 sec/batch)
2016-12-08 17:34:51.972566: step 870, loss = 2.76 (533.9 examples/sec; 0.240 sec/batch)
2016-12-08 17:34:54.165388: step 880, loss = 2.58 (562.3 examples/sec; 0.228 sec/batch)
2016-12-08 17:34:56.547558: step 890, loss = 2.60 (498.3 examples/sec; 0.257 sec/batch)
2016-12-08 17:34:58.854374: step 900, loss = 2.65 (543.5 examples/sec; 0.235 sec/batch)
2016-12-08 17:35:01.343445: step 910, loss = 2.59 (546.4 examples/sec; 0.234 sec/batch)
2016-12-08 17:35:03.664087: step 920, loss = 2.43 (592.0 examples/sec; 0.216 sec/batch)
2016-12-08 17:35:06.033566: step 930, loss = 2.47 (546.6 examples/sec; 0.234 sec/batch)
2016-12-08 17:35:08.356553: step 940, loss = 2.40 (580.9 examples/sec; 0.220 sec/batch)
2016-12-08 17:35:10.571266: step 950, loss = 2.78 (578.2 examples/sec; 0.221 sec/batch)
2016-12-08 17:35:12.895576: step 960, loss = 2.39 (558.6 examples/sec; 0.229 sec/batch)
2016-12-08 17:35:15.143503: step 970, loss = 2.34 (635.9 examples/sec; 0.201 sec/batch)
2016-12-08 17:35:17.440820: step 980, loss = 2.55 (595.4 examples/sec; 0.215 sec/batch)
2016-12-08 17:35:19.823604: step 990, loss = 2.54 (521.7 examples/sec; 0.245 sec/batch)
2016-12-08 17:35:22.115502: step 1000, loss = 2.28 (519.6 examples/sec; 0.246 sec/batch)
2016-12-08 17:35:25.119043: step 1010, loss = 2.28 (534.0 examples/sec; 0.240 sec/batch)
2016-12-08 17:35:27.366459: step 1020, loss = 2.34 (605.1 examples/sec; 0.212 sec/batch)
2016-12-08 17:35:29.659613: step 1030, loss = 2.30 (607.7 examples/sec; 0.211 sec/batch)
2016-12-08 17:35:31.915195: step 1040, loss = 2.35 (651.5 examples/sec; 0.196 sec/batch)
2016-12-08 17:35:34.193574: step 1050, loss = 2.13 (566.3 examples/sec; 0.226 sec/batch)
2016-12-08 17:35:36.425267: step 1060, loss = 2.17 (710.7 examples/sec; 0.180 sec/batch)
2016-12-08 17:35:38.697483: step 1070, loss = 2.65 (527.8 examples/sec; 0.243 sec/batch)
2016-12-08 17:35:41.009567: step 1080, loss = 2.52 (560.8 examples/sec; 0.228 sec/batch)
2016-12-08 17:35:43.266111: step 1090, loss = 2.31 (573.1 examples/sec; 0.223 sec/batch)
2016-12-08 17:35:45.556670: step 1100, loss = 2.34 (493.0 examples/sec; 0.260 sec/batch)
2016-12-08 17:35:48.013202: step 1110, loss = 2.10 (520.8 examples/sec; 0.246 sec/batch)
2016-12-08 17:35:50.228983: step 1120, loss = 2.24 (547.8 examples/sec; 0.234 sec/batch)
2016-12-08 17:35:52.499078: step 1130, loss = 2.47 (523.9 examples/sec; 0.244 sec/batch)
2016-12-08 17:35:54.796474: step 1140, loss = 2.27 (540.4 examples/sec; 0.237 sec/batch)
2016-12-08 17:35:57.077473: step 1150, loss = 2.38 (575.2 examples/sec; 0.223 sec/batch)
2016-12-08 17:35:59.372692: step 1160, loss = 2.33 (510.1 examples/sec; 0.251 sec/batch)
2016-12-08 17:36:01.617878: step 1170, loss = 2.14 (538.5 examples/sec; 0.238 sec/batch)
2016-12-08 17:36:03.911135: step 1180, loss = 2.21 (487.1 examples/sec; 0.263 sec/batch)
2016-12-08 17:36:06.174050: step 1190, loss = 2.05 (550.5 examples/sec; 0.233 sec/batch)
2016-12-08 17:36:08.462709: step 1200, loss = 2.29 (610.6 examples/sec; 0.210 sec/batch)
2016-12-08 17:36:10.929588: step 1210, loss = 2.07 (574.0 examples/sec; 0.223 sec/batch)
2016-12-08 17:36:13.107565: step 1220, loss = 2.23 (564.5 examples/sec; 0.227 sec/batch)
2016-12-08 17:36:15.439043: step 1230, loss = 2.13 (664.5 examples/sec; 0.193 sec/batch)
2016-12-08 17:36:17.798551: step 1240, loss = 2.22 (585.4 examples/sec; 0.219 sec/batch)
2016-12-08 17:36:20.020512: step 1250, loss = 2.14 (612.7 examples/sec; 0.209 sec/batch)
2016-12-08 17:36:22.331595: step 1260, loss = 2.17 (605.4 examples/sec; 0.211 sec/batch)
2016-12-08 17:36:24.635109: step 1270, loss = 2.24 (540.4 examples/sec; 0.237 sec/batch)
2016-12-08 17:36:26.886237: step 1280, loss = 2.34 (539.6 examples/sec; 0.237 sec/batch)
