Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
WARNING:tensorflow:From cifar10_train.py:81 in train.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2016-03-02.
Instructions for updating:
Please use tf.global_variables instead.
2016-12-08 17:39:38.243636: step 0, loss = 4.67 (28.6 examples/sec; 4.482 sec/batch)
2016-12-08 17:39:40.271798: step 10, loss = 4.61 (1129.4 examples/sec; 0.113 sec/batch)
2016-12-08 17:39:41.366702: step 20, loss = 4.43 (1115.0 examples/sec; 0.115 sec/batch)
2016-12-08 17:39:42.513110: step 30, loss = 4.31 (1171.9 examples/sec; 0.109 sec/batch)
2016-12-08 17:39:43.667909: step 40, loss = 4.31 (1151.0 examples/sec; 0.111 sec/batch)
2016-12-08 17:39:44.972333: step 50, loss = 4.36 (894.9 examples/sec; 0.143 sec/batch)
2016-12-08 17:39:46.538277: step 60, loss = 4.21 (794.9 examples/sec; 0.161 sec/batch)
2016-12-08 17:39:47.922114: step 70, loss = 4.26 (830.7 examples/sec; 0.154 sec/batch)
2016-12-08 17:39:49.183149: step 80, loss = 4.08 (1111.1 examples/sec; 0.115 sec/batch)
2016-12-08 17:39:50.463330: step 90, loss = 4.50 (993.7 examples/sec; 0.129 sec/batch)
2016-12-08 17:39:51.695048: step 100, loss = 4.26 (1097.5 examples/sec; 0.117 sec/batch)
2016-12-08 17:39:53.040488: step 110, loss = 4.06 (1143.0 examples/sec; 0.112 sec/batch)
2016-12-08 17:39:54.214826: step 120, loss = 4.37 (1109.7 examples/sec; 0.115 sec/batch)
2016-12-08 17:39:55.386496: step 130, loss = 4.07 (1043.7 examples/sec; 0.123 sec/batch)
2016-12-08 17:39:56.521216: step 140, loss = 4.02 (1071.9 examples/sec; 0.119 sec/batch)
2016-12-08 17:39:57.658886: step 150, loss = 4.09 (1109.0 examples/sec; 0.115 sec/batch)
2016-12-08 17:39:58.856712: step 160, loss = 4.08 (1109.8 examples/sec; 0.115 sec/batch)
2016-12-08 17:40:00.082005: step 170, loss = 3.78 (1050.5 examples/sec; 0.122 sec/batch)
2016-12-08 17:40:01.290456: step 180, loss = 3.82 (1144.6 examples/sec; 0.112 sec/batch)
2016-12-08 17:40:02.468703: step 190, loss = 3.94 (1095.7 examples/sec; 0.117 sec/batch)
2016-12-08 17:40:03.636387: step 200, loss = 3.76 (1094.1 examples/sec; 0.117 sec/batch)
2016-12-08 17:40:05.042737: step 210, loss = 3.85 (1211.4 examples/sec; 0.106 sec/batch)
2016-12-08 17:40:06.192380: step 220, loss = 3.79 (1104.3 examples/sec; 0.116 sec/batch)
2016-12-08 17:40:07.345347: step 230, loss = 3.83 (1091.5 examples/sec; 0.117 sec/batch)
2016-12-08 17:40:08.502289: step 240, loss = 3.91 (1088.3 examples/sec; 0.118 sec/batch)
2016-12-08 17:40:09.627529: step 250, loss = 3.63 (1089.7 examples/sec; 0.117 sec/batch)
2016-12-08 17:40:10.810894: step 260, loss = 3.50 (1092.3 examples/sec; 0.117 sec/batch)
2016-12-08 17:40:12.027927: step 270, loss = 3.58 (1070.7 examples/sec; 0.120 sec/batch)
2016-12-08 17:40:13.190258: step 280, loss = 3.76 (1069.6 examples/sec; 0.120 sec/batch)
2016-12-08 17:40:14.434911: step 290, loss = 3.63 (1128.6 examples/sec; 0.113 sec/batch)
2016-12-08 17:40:15.595003: step 300, loss = 3.38 (1095.8 examples/sec; 0.117 sec/batch)
2016-12-08 17:40:16.957847: step 310, loss = 3.51 (1167.9 examples/sec; 0.110 sec/batch)
2016-12-08 17:40:18.100911: step 320, loss = 3.67 (1046.4 examples/sec; 0.122 sec/batch)
2016-12-08 17:40:19.298097: step 330, loss = 3.45 (1112.8 examples/sec; 0.115 sec/batch)
2016-12-08 17:40:20.461847: step 340, loss = 3.61 (1112.3 examples/sec; 0.115 sec/batch)
2016-12-08 17:40:21.637070: step 350, loss = 3.46 (1075.4 examples/sec; 0.119 sec/batch)
2016-12-08 17:40:22.813743: step 360, loss = 3.50 (1083.3 examples/sec; 0.118 sec/batch)
2016-12-08 17:40:23.977048: step 370, loss = 3.35 (1124.3 examples/sec; 0.114 sec/batch)
2016-12-08 17:40:25.118575: step 380, loss = 3.48 (1201.9 examples/sec; 0.106 sec/batch)
2016-12-08 17:40:26.370223: step 390, loss = 3.29 (1143.2 examples/sec; 0.112 sec/batch)
2016-12-08 17:40:27.626867: step 400, loss = 3.34 (1105.9 examples/sec; 0.116 sec/batch)
2016-12-08 17:40:29.003926: step 410, loss = 3.34 (1175.0 examples/sec; 0.109 sec/batch)
2016-12-08 17:40:30.137018: step 420, loss = 3.30 (1129.4 examples/sec; 0.113 sec/batch)
2016-12-08 17:40:31.291677: step 430, loss = 3.20 (1096.3 examples/sec; 0.117 sec/batch)
2016-12-08 17:40:32.454082: step 440, loss = 3.35 (1117.1 examples/sec; 0.115 sec/batch)
2016-12-08 17:40:33.594327: step 450, loss = 3.26 (1108.8 examples/sec; 0.115 sec/batch)
2016-12-08 17:40:34.765868: step 460, loss = 3.15 (1045.8 examples/sec; 0.122 sec/batch)
2016-12-08 17:40:35.949354: step 470, loss = 3.40 (1110.2 examples/sec; 0.115 sec/batch)
2016-12-08 17:40:37.083731: step 480, loss = 3.18 (1131.5 examples/sec; 0.113 sec/batch)
2016-12-08 17:40:38.241431: step 490, loss = 3.19 (1130.5 examples/sec; 0.113 sec/batch)
2016-12-08 17:40:39.385361: step 500, loss = 3.61 (1075.3 examples/sec; 0.119 sec/batch)
2016-12-08 17:40:40.724844: step 510, loss = 3.11 (1190.3 examples/sec; 0.108 sec/batch)
2016-12-08 17:40:41.911932: step 520, loss = 3.10 (1087.4 examples/sec; 0.118 sec/batch)
2016-12-08 17:40:43.077336: step 530, loss = 3.02 (1116.2 examples/sec; 0.115 sec/batch)
2016-12-08 17:40:44.282415: step 540, loss = 3.07 (1009.0 examples/sec; 0.127 sec/batch)
2016-12-08 17:40:45.496279: step 550, loss = 2.95 (1005.6 examples/sec; 0.127 sec/batch)
2016-12-08 17:40:46.675152: step 560, loss = 3.07 (1090.4 examples/sec; 0.117 sec/batch)
2016-12-08 17:40:47.862079: step 570, loss = 3.26 (1102.2 examples/sec; 0.116 sec/batch)
2016-12-08 17:40:48.998180: step 580, loss = 2.93 (1203.9 examples/sec; 0.106 sec/batch)
2016-12-08 17:40:50.140758: step 590, loss = 2.88 (1094.5 examples/sec; 0.117 sec/batch)
2016-12-08 17:40:51.334317: step 600, loss = 2.83 (1101.5 examples/sec; 0.116 sec/batch)
2016-12-08 17:40:52.694496: step 610, loss = 2.94 (1205.2 examples/sec; 0.106 sec/batch)
2016-12-08 17:40:53.790117: step 620, loss = 3.21 (1174.0 examples/sec; 0.109 sec/batch)
2016-12-08 17:40:54.937751: step 630, loss = 3.15 (1100.6 examples/sec; 0.116 sec/batch)
2016-12-08 17:40:56.096849: step 640, loss = 3.04 (1112.4 examples/sec; 0.115 sec/batch)
2016-12-08 17:40:57.213151: step 650, loss = 2.85 (1127.0 examples/sec; 0.114 sec/batch)
2016-12-08 17:40:58.359451: step 660, loss = 2.89 (1056.6 examples/sec; 0.121 sec/batch)
2016-12-08 17:40:59.594228: step 670, loss = 3.03 (1048.7 examples/sec; 0.122 sec/batch)
2016-12-08 17:41:00.817069: step 680, loss = 3.00 (1203.1 examples/sec; 0.106 sec/batch)
2016-12-08 17:41:01.952057: step 690, loss = 2.76 (1104.4 examples/sec; 0.116 sec/batch)
2016-12-08 17:41:03.125163: step 700, loss = 2.79 (1069.3 examples/sec; 0.120 sec/batch)
2016-12-08 17:41:04.495551: step 710, loss = 2.96 (1125.2 examples/sec; 0.114 sec/batch)
2016-12-08 17:41:05.612217: step 720, loss = 2.71 (1100.4 examples/sec; 0.116 sec/batch)
2016-12-08 17:41:06.780077: step 730, loss = 2.88 (1042.4 examples/sec; 0.123 sec/batch)
2016-12-08 17:41:07.958202: step 740, loss = 2.95 (1079.8 examples/sec; 0.119 sec/batch)
2016-12-08 17:41:09.077746: step 750, loss = 2.73 (1176.3 examples/sec; 0.109 sec/batch)
2016-12-08 17:41:10.251068: step 760, loss = 2.65 (1103.6 examples/sec; 0.116 sec/batch)
2016-12-08 17:41:11.432296: step 770, loss = 2.70 (1041.9 examples/sec; 0.123 sec/batch)
2016-12-08 17:41:12.687537: step 780, loss = 2.77 (1128.8 examples/sec; 0.113 sec/batch)
2016-12-08 17:41:13.844214: step 790, loss = 2.70 (1118.4 examples/sec; 0.114 sec/batch)
2016-12-08 17:41:14.986530: step 800, loss = 2.75 (1093.7 examples/sec; 0.117 sec/batch)
2016-12-08 17:41:16.367867: step 810, loss = 2.60 (1175.9 examples/sec; 0.109 sec/batch)
2016-12-08 17:41:17.475538: step 820, loss = 2.64 (1142.8 examples/sec; 0.112 sec/batch)
2016-12-08 17:41:18.612006: step 830, loss = 2.60 (1166.2 examples/sec; 0.110 sec/batch)
2016-12-08 17:41:19.789969: step 840, loss = 2.75 (1092.0 examples/sec; 0.117 sec/batch)
2016-12-08 17:41:20.939789: step 850, loss = 2.47 (1183.6 examples/sec; 0.108 sec/batch)
2016-12-08 17:41:22.109273: step 860, loss = 2.82 (1083.0 examples/sec; 0.118 sec/batch)
2016-12-08 17:41:23.271229: step 870, loss = 2.58 (1152.8 examples/sec; 0.111 sec/batch)
2016-12-08 17:41:24.486232: step 880, loss = 2.67 (1167.4 examples/sec; 0.110 sec/batch)
2016-12-08 17:41:25.621836: step 890, loss = 2.67 (1159.2 examples/sec; 0.110 sec/batch)
2016-12-08 17:41:26.780293: step 900, loss = 2.65 (1070.4 examples/sec; 0.120 sec/batch)
2016-12-08 17:41:28.167991: step 910, loss = 2.60 (1083.6 examples/sec; 0.118 sec/batch)
2016-12-08 17:41:29.327152: step 920, loss = 2.56 (1045.1 examples/sec; 0.122 sec/batch)
2016-12-08 17:41:30.603571: step 930, loss = 2.71 (1065.8 examples/sec; 0.120 sec/batch)
2016-12-08 17:41:31.737613: step 940, loss = 2.58 (1145.3 examples/sec; 0.112 sec/batch)
2016-12-08 17:41:32.862556: step 950, loss = 2.45 (1197.2 examples/sec; 0.107 sec/batch)
2016-12-08 17:41:34.000053: step 960, loss = 2.44 (1107.8 examples/sec; 0.116 sec/batch)
2016-12-08 17:41:35.180733: step 970, loss = 2.46 (1072.9 examples/sec; 0.119 sec/batch)
2016-12-08 17:41:36.338732: step 980, loss = 2.51 (1075.6 examples/sec; 0.119 sec/batch)
2016-12-08 17:41:37.497662: step 990, loss = 2.44 (1126.0 examples/sec; 0.114 sec/batch)
2016-12-08 17:41:38.700155: step 1000, loss = 2.68 (1150.8 examples/sec; 0.111 sec/batch)
2016-12-08 17:41:40.711072: step 1010, loss = 2.34 (1152.3 examples/sec; 0.111 sec/batch)
2016-12-08 17:41:41.842391: step 1020, loss = 2.56 (1076.4 examples/sec; 0.119 sec/batch)
2016-12-08 17:41:43.037281: step 1030, loss = 2.37 (1043.3 examples/sec; 0.123 sec/batch)
2016-12-08 17:41:44.225723: step 1040, loss = 2.33 (1058.1 examples/sec; 0.121 sec/batch)
2016-12-08 17:41:45.346247: step 1050, loss = 2.46 (1131.5 examples/sec; 0.113 sec/batch)
2016-12-08 17:41:46.517411: step 1060, loss = 2.36 (1096.1 examples/sec; 0.117 sec/batch)
2016-12-08 17:41:47.763828: step 1070, loss = 2.33 (918.9 examples/sec; 0.139 sec/batch)
2016-12-08 17:41:48.933752: step 1080, loss = 2.33 (1193.7 examples/sec; 0.107 sec/batch)
2016-12-08 17:41:50.062524: step 1090, loss = 2.34 (1114.9 examples/sec; 0.115 sec/batch)
2016-12-08 17:41:51.230664: step 1100, loss = 2.30 (1095.0 examples/sec; 0.117 sec/batch)
2016-12-08 17:41:52.564483: step 1110, loss = 2.33 (1227.3 examples/sec; 0.104 sec/batch)
2016-12-08 17:41:53.690509: step 1120, loss = 2.30 (1089.2 examples/sec; 0.118 sec/batch)
2016-12-08 17:41:54.850143: step 1130, loss = 2.22 (1113.0 examples/sec; 0.115 sec/batch)
2016-12-08 17:41:55.993657: step 1140, loss = 2.36 (1151.3 examples/sec; 0.111 sec/batch)
2016-12-08 17:41:57.142617: step 1150, loss = 2.15 (1042.5 examples/sec; 0.123 sec/batch)
2016-12-08 17:41:58.282832: step 1160, loss = 2.21 (1086.8 examples/sec; 0.118 sec/batch)
2016-12-08 17:41:59.440232: step 1170, loss = 2.11 (1107.9 examples/sec; 0.116 sec/batch)
2016-12-08 17:42:00.610430: step 1180, loss = 2.24 (1092.8 examples/sec; 0.117 sec/batch)
2016-12-08 17:42:01.731822: step 1190, loss = 2.37 (1118.3 examples/sec; 0.114 sec/batch)
2016-12-08 17:42:02.879219: step 1200, loss = 2.10 (1140.0 examples/sec; 0.112 sec/batch)
2016-12-08 17:42:04.252099: step 1210, loss = 2.24 (1035.3 examples/sec; 0.124 sec/batch)
2016-12-08 17:42:05.455464: step 1220, loss = 2.28 (914.3 examples/sec; 0.140 sec/batch)
2016-12-08 17:42:06.627289: step 1230, loss = 2.39 (1127.0 examples/sec; 0.114 sec/batch)
2016-12-08 17:42:07.795879: step 1240, loss = 2.31 (1144.8 examples/sec; 0.112 sec/batch)
2016-12-08 17:42:08.956637: step 1250, loss = 2.16 (1186.9 examples/sec; 0.108 sec/batch)
2016-12-08 17:42:10.129551: step 1260, loss = 2.08 (1073.9 examples/sec; 0.119 sec/batch)
2016-12-08 17:42:11.305383: step 1270, loss = 2.20 (1090.9 examples/sec; 0.117 sec/batch)
2016-12-08 17:42:12.475181: step 1280, loss = 2.00 (1110.2 examples/sec; 0.115 sec/batch)
2016-12-08 17:42:13.612840: step 1290, loss = 2.03 (1106.5 examples/sec; 0.116 sec/batch)
2016-12-08 17:42:14.780856: step 1300, loss = 2.22 (1057.0 examples/sec; 0.121 sec/batch)
2016-12-08 17:42:16.206141: step 1310, loss = 2.24 (1036.7 examples/sec; 0.123 sec/batch)
2016-12-08 17:42:17.393073: step 1320, loss = 2.09 (1093.6 examples/sec; 0.117 sec/batch)
2016-12-08 17:42:18.560615: step 1330, loss = 2.13 (1009.2 examples/sec; 0.127 sec/batch)
2016-12-08 17:42:19.725654: step 1340, loss = 2.04 (1089.0 examples/sec; 0.118 sec/batch)
2016-12-08 17:42:20.850889: step 1350, loss = 1.95 (1169.7 examples/sec; 0.109 sec/batch)
2016-12-08 17:42:21.979909: step 1360, loss = 2.18 (1103.1 examples/sec; 0.116 sec/batch)
2016-12-08 17:42:23.179147: step 1370, loss = 1.94 (1068.3 examples/sec; 0.120 sec/batch)
2016-12-08 17:42:24.325273: step 1380, loss = 2.19 (1096.5 examples/sec; 0.117 sec/batch)
2016-12-08 17:42:25.456092: step 1390, loss = 2.29 (1104.2 examples/sec; 0.116 sec/batch)
2016-12-08 17:42:26.657629: step 1400, loss = 2.13 (1092.9 examples/sec; 0.117 sec/batch)
2016-12-08 17:42:28.019376: step 1410, loss = 2.10 (1107.3 examples/sec; 0.116 sec/batch)
2016-12-08 17:42:29.164525: step 1420, loss = 1.96 (1111.0 examples/sec; 0.115 sec/batch)
2016-12-08 17:42:30.319025: step 1430, loss = 2.13 (1170.1 examples/sec; 0.109 sec/batch)
2016-12-08 17:42:31.468428: step 1440, loss = 1.92 (1135.0 examples/sec; 0.113 sec/batch)
2016-12-08 17:42:32.631246: step 1450, loss = 1.89 (1165.7 examples/sec; 0.110 sec/batch)
2016-12-08 17:42:33.761608: step 1460, loss = 1.98 (1067.6 examples/sec; 0.120 sec/batch)
2016-12-08 17:42:34.906233: step 1470, loss = 2.08 (1096.5 examples/sec; 0.117 sec/batch)
2016-12-08 17:42:36.075624: step 1480, loss = 1.95 (1061.0 examples/sec; 0.121 sec/batch)
2016-12-08 17:42:37.214765: step 1490, loss = 2.14 (1026.0 examples/sec; 0.125 sec/batch)
2016-12-08 17:42:38.497865: step 1500, loss = 1.80 (1112.2 examples/sec; 0.115 sec/batch)
2016-12-08 17:42:39.848977: step 1510, loss = 2.00 (1141.2 examples/sec; 0.112 sec/batch)
2016-12-08 17:42:40.983087: step 1520, loss = 1.91 (1172.3 examples/sec; 0.109 sec/batch)
2016-12-08 17:42:42.112693: step 1530, loss = 2.08 (1130.3 examples/sec; 0.113 sec/batch)
2016-12-08 17:42:43.271175: step 1540, loss = 1.86 (1130.4 examples/sec; 0.113 sec/batch)
2016-12-08 17:42:44.448043: step 1550, loss = 1.94 (1118.6 examples/sec; 0.114 sec/batch)
2016-12-08 17:42:45.580980: step 1560, loss = 2.08 (1069.4 examples/sec; 0.120 sec/batch)
2016-12-08 17:42:46.734940: step 1570, loss = 1.76 (1100.4 examples/sec; 0.116 sec/batch)
2016-12-08 17:42:47.875059: step 1580, loss = 2.00 (1160.4 examples/sec; 0.110 sec/batch)
2016-12-08 17:42:49.012795: step 1590, loss = 2.00 (1169.5 examples/sec; 0.109 sec/batch)
2016-12-08 17:42:50.162917: step 1600, loss = 2.14 (1065.2 examples/sec; 0.120 sec/batch)
2016-12-08 17:42:51.506135: step 1610, loss = 1.84 (1159.0 examples/sec; 0.110 sec/batch)
2016-12-08 17:42:52.642205: step 1620, loss = 1.87 (1123.3 examples/sec; 0.114 sec/batch)
2016-12-08 17:42:53.746929: step 1630, loss = 2.06 (1123.2 examples/sec; 0.114 sec/batch)
2016-12-08 17:42:54.927729: step 1640, loss = 2.08 (1115.5 examples/sec; 0.115 sec/batch)
2016-12-08 17:42:56.089507: step 1650, loss = 1.85 (1127.5 examples/sec; 0.114 sec/batch)
2016-12-08 17:42:57.192788: step 1660, loss = 1.89 (1183.8 examples/sec; 0.108 sec/batch)
2016-12-08 17:42:58.354226: step 1670, loss = 1.87 (1135.3 examples/sec; 0.113 sec/batch)
2016-12-08 17:42:59.531366: step 1680, loss = 1.78 (1063.0 examples/sec; 0.120 sec/batch)
2016-12-08 17:43:00.701804: step 1690, loss = 1.79 (1160.2 examples/sec; 0.110 sec/batch)
2016-12-08 17:43:01.840122: step 1700, loss = 1.95 (1060.7 examples/sec; 0.121 sec/batch)
2016-12-08 17:43:03.245317: step 1710, loss = 1.71 (1019.9 examples/sec; 0.125 sec/batch)
2016-12-08 17:43:04.418401: step 1720, loss = 1.88 (1126.1 examples/sec; 0.114 sec/batch)
2016-12-08 17:43:05.551864: step 1730, loss = 2.06 (1094.2 examples/sec; 0.117 sec/batch)
2016-12-08 17:43:06.720240: step 1740, loss = 2.01 (1061.1 examples/sec; 0.121 sec/batch)
2016-12-08 17:43:07.874394: step 1750, loss = 1.78 (1131.8 examples/sec; 0.113 sec/batch)
