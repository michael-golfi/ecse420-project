2016-12-08 02:25:14.075881: step 0, loss = 4.68 (25.4 examples/sec; 5.042 sec/batch)
2016-12-08 02:25:29.039883: step 10, loss = 4.64 (102.3 examples/sec; 1.252 sec/batch)
2016-12-08 02:25:41.936801: step 20, loss = 4.50 (100.2 examples/sec; 1.277 sec/batch)
2016-12-08 02:25:54.425371: step 30, loss = 4.35 (94.4 examples/sec; 1.356 sec/batch)
2016-12-08 02:26:06.577810: step 40, loss = 4.35 (107.4 examples/sec; 1.192 sec/batch)
2016-12-08 02:26:19.046330: step 50, loss = 4.34 (94.6 examples/sec; 1.353 sec/batch)
2016-12-08 02:26:31.205997: step 60, loss = 4.16 (100.3 examples/sec; 1.276 sec/batch)
2016-12-08 02:26:43.394145: step 70, loss = 4.21 (103.9 examples/sec; 1.232 sec/batch)
2016-12-08 02:26:55.526465: step 80, loss = 4.20 (103.5 examples/sec; 1.237 sec/batch)
2016-12-08 02:27:07.771545: step 90, loss = 4.11 (106.7 examples/sec; 1.199 sec/batch)
2016-12-08 02:27:20.032916: step 100, loss = 4.06 (104.9 examples/sec; 1.220 sec/batch)
2016-12-08 02:27:34.032443: step 110, loss = 4.05 (102.0 examples/sec; 1.255 sec/batch)
2016-12-08 02:27:46.377964: step 120, loss = 4.00 (100.7 examples/sec; 1.271 sec/batch)
2016-12-08 02:27:58.326399: step 130, loss = 4.08 (100.5 examples/sec; 1.274 sec/batch)
2016-12-08 02:28:10.388991: step 140, loss = 4.03 (104.3 examples/sec; 1.227 sec/batch)
2016-12-08 02:28:22.577848: step 150, loss = 3.87 (105.2 examples/sec; 1.217 sec/batch)
2016-12-08 02:28:35.063652: step 160, loss = 4.01 (103.1 examples/sec; 1.242 sec/batch)
2016-12-08 02:28:46.749542: step 170, loss = 4.05 (116.2 examples/sec; 1.101 sec/batch)
2016-12-08 02:28:58.894700: step 180, loss = 3.97 (103.4 examples/sec; 1.237 sec/batch)
2016-12-08 02:29:11.005730: step 190, loss = 3.86 (108.5 examples/sec; 1.180 sec/batch)
2016-12-08 02:29:23.110991: step 200, loss = 3.79 (95.3 examples/sec; 1.343 sec/batch)
2016-12-08 02:29:37.053195: step 210, loss = 3.93 (107.0 examples/sec; 1.196 sec/batch)
2016-12-08 02:29:49.368824: step 220, loss = 3.67 (97.4 examples/sec; 1.314 sec/batch)
2016-12-08 02:30:01.626614: step 230, loss = 3.80 (108.4 examples/sec; 1.181 sec/batch)
2016-12-08 02:30:13.957364: step 240, loss = 3.70 (106.2 examples/sec; 1.205 sec/batch)
2016-12-08 02:30:26.147062: step 250, loss = 3.60 (100.8 examples/sec; 1.269 sec/batch)
2016-12-08 02:30:38.186317: step 260, loss = 3.62 (118.0 examples/sec; 1.084 sec/batch)
2016-12-08 02:30:50.293505: step 270, loss = 3.87 (110.5 examples/sec; 1.158 sec/batch)
2016-12-08 02:31:02.298137: step 280, loss = 3.65 (100.7 examples/sec; 1.272 sec/batch)
2016-12-08 02:31:14.577988: step 290, loss = 3.51 (103.2 examples/sec; 1.241 sec/batch)
2016-12-08 02:31:26.873834: step 300, loss = 3.62 (103.7 examples/sec; 1.235 sec/batch)
2016-12-08 02:31:40.400246: step 310, loss = 3.56 (107.3 examples/sec; 1.192 sec/batch)
2016-12-08 02:31:52.356335: step 320, loss = 3.59 (106.1 examples/sec; 1.207 sec/batch)
2016-12-08 02:32:04.161045: step 330, loss = 3.47 (111.6 examples/sec; 1.147 sec/batch)
2016-12-08 02:32:16.257795: step 340, loss = 3.69 (103.0 examples/sec; 1.243 sec/batch)
2016-12-08 02:32:29.078893: step 350, loss = 3.68 (100.5 examples/sec; 1.274 sec/batch)
2016-12-08 02:32:41.548010: step 360, loss = 3.44 (106.7 examples/sec; 1.199 sec/batch)
2016-12-08 02:32:53.910485: step 370, loss = 3.38 (103.9 examples/sec; 1.232 sec/batch)
2016-12-08 02:33:06.321072: step 380, loss = 3.34 (113.3 examples/sec; 1.130 sec/batch)
2016-12-08 02:33:18.626537: step 390, loss = 3.41 (108.0 examples/sec; 1.185 sec/batch)
2016-12-08 02:33:30.662577: step 400, loss = 3.52 (99.3 examples/sec; 1.289 sec/batch)
2016-12-08 02:33:43.901847: step 410, loss = 3.73 (101.6 examples/sec; 1.259 sec/batch)
2016-12-08 02:33:55.655152: step 420, loss = 3.28 (103.5 examples/sec; 1.237 sec/batch)
2016-12-08 02:34:07.786950: step 430, loss = 3.19 (106.3 examples/sec; 1.205 sec/batch)
2016-12-08 02:34:19.675981: step 440, loss = 3.40 (108.3 examples/sec; 1.182 sec/batch)
2016-12-08 02:34:31.436780: step 450, loss = 3.20 (125.2 examples/sec; 1.022 sec/batch)
2016-12-08 02:34:43.154415: step 460, loss = 3.24 (118.2 examples/sec; 1.083 sec/batch)
2016-12-08 02:34:54.961039: step 470, loss = 3.22 (104.6 examples/sec; 1.224 sec/batch)
2016-12-08 02:35:06.799226: step 480, loss = 3.19 (119.5 examples/sec; 1.071 sec/batch)
2016-12-08 02:35:18.687054: step 490, loss = 3.29 (109.4 examples/sec; 1.170 sec/batch)
2016-12-08 02:35:30.752966: step 500, loss = 3.15 (101.6 examples/sec; 1.260 sec/batch)
2016-12-08 02:35:44.488674: step 510, loss = 3.28 (103.7 examples/sec; 1.235 sec/batch)
2016-12-08 02:35:56.781690: step 520, loss = 3.03 (98.8 examples/sec; 1.296 sec/batch)
2016-12-08 02:36:09.004008: step 530, loss = 3.16 (108.9 examples/sec; 1.175 sec/batch)
2016-12-08 02:36:21.469246: step 540, loss = 3.04 (97.6 examples/sec; 1.311 sec/batch)
2016-12-08 02:36:33.406464: step 550, loss = 2.98 (102.4 examples/sec; 1.250 sec/batch)
2016-12-08 02:36:45.411104: step 560, loss = 3.05 (109.1 examples/sec; 1.174 sec/batch)
2016-12-08 02:36:57.438672: step 570, loss = 3.11 (103.2 examples/sec; 1.240 sec/batch)
2016-12-08 02:37:09.743291: step 580, loss = 3.10 (107.6 examples/sec; 1.190 sec/batch)
2016-12-08 02:37:21.644981: step 590, loss = 2.95 (108.7 examples/sec; 1.177 sec/batch)
2016-12-08 02:37:34.019433: step 600, loss = 2.90 (107.7 examples/sec; 1.188 sec/batch)
2016-12-08 02:37:47.571703: step 610, loss = 2.96 (105.8 examples/sec; 1.210 sec/batch)
2016-12-08 02:37:59.747198: step 620, loss = 3.23 (111.4 examples/sec; 1.149 sec/batch)
2016-12-08 02:38:12.491449: step 630, loss = 2.92 (102.0 examples/sec; 1.255 sec/batch)
2016-12-08 02:38:24.229461: step 640, loss = 2.94 (99.0 examples/sec; 1.292 sec/batch)
2016-12-08 02:38:36.309541: step 650, loss = 2.98 (106.3 examples/sec; 1.205 sec/batch)
2016-12-08 02:38:48.101153: step 660, loss = 3.04 (113.6 examples/sec; 1.127 sec/batch)
2016-12-08 02:39:00.070409: step 670, loss = 2.84 (111.0 examples/sec; 1.153 sec/batch)
2016-12-08 02:39:12.153050: step 680, loss = 2.89 (106.4 examples/sec; 1.203 sec/batch)
2016-12-08 02:39:24.388082: step 690, loss = 2.85 (109.3 examples/sec; 1.171 sec/batch)
2016-12-08 02:39:36.861317: step 700, loss = 2.96 (107.5 examples/sec; 1.191 sec/batch)
2016-12-08 02:39:50.007715: step 710, loss = 2.98 (108.6 examples/sec; 1.179 sec/batch)
2016-12-08 02:40:01.702978: step 720, loss = 2.79 (117.5 examples/sec; 1.089 sec/batch)
2016-12-08 02:40:13.663106: step 730, loss = 2.82 (107.7 examples/sec; 1.189 sec/batch)
2016-12-08 02:40:25.350562: step 740, loss = 2.69 (96.6 examples/sec; 1.325 sec/batch)
2016-12-08 02:40:37.499640: step 750, loss = 2.73 (105.1 examples/sec; 1.218 sec/batch)
2016-12-08 02:40:49.461607: step 760, loss = 2.79 (95.0 examples/sec; 1.348 sec/batch)
2016-12-08 02:41:01.617972: step 770, loss = 2.71 (96.3 examples/sec; 1.329 sec/batch)
2016-12-08 02:41:13.739640: step 780, loss = 2.73 (109.2 examples/sec; 1.172 sec/batch)
2016-12-08 02:41:25.774647: step 790, loss = 2.67 (113.5 examples/sec; 1.127 sec/batch)
2016-12-08 02:41:37.940885: step 800, loss = 2.48 (105.8 examples/sec; 1.210 sec/batch)
2016-12-08 02:41:51.476752: step 810, loss = 2.83 (104.6 examples/sec; 1.223 sec/batch)
2016-12-08 02:42:03.285864: step 820, loss = 2.67 (105.8 examples/sec; 1.209 sec/batch)
2016-12-08 02:42:15.162057: step 830, loss = 2.54 (98.9 examples/sec; 1.294 sec/batch)
2016-12-08 02:42:27.228670: step 840, loss = 2.61 (101.3 examples/sec; 1.264 sec/batch)
2016-12-08 02:42:39.311563: step 850, loss = 2.79 (106.8 examples/sec; 1.198 sec/batch)
2016-12-08 02:42:51.236595: step 860, loss = 2.61 (102.3 examples/sec; 1.251 sec/batch)
2016-12-08 02:43:03.337588: step 870, loss = 2.59 (110.2 examples/sec; 1.162 sec/batch)
2016-12-08 02:43:14.946801: step 880, loss = 2.59 (120.7 examples/sec; 1.060 sec/batch)
2016-12-08 02:43:26.989641: step 890, loss = 2.69 (118.9 examples/sec; 1.077 sec/batch)
2016-12-08 02:43:38.876969: step 900, loss = 2.66 (100.3 examples/sec; 1.276 sec/batch)
2016-12-08 02:43:52.169076: step 910, loss = 2.66 (105.0 examples/sec; 1.219 sec/batch)
2016-12-08 02:44:04.026030: step 920, loss = 2.76 (104.3 examples/sec; 1.227 sec/batch)
2016-12-08 02:44:15.807589: step 930, loss = 2.68 (106.2 examples/sec; 1.205 sec/batch)
2016-12-08 02:44:28.277145: step 940, loss = 2.51 (98.2 examples/sec; 1.304 sec/batch)
2016-12-08 02:44:39.883623: step 950, loss = 2.36 (104.7 examples/sec; 1.222 sec/batch)
2016-12-08 02:44:51.455498: step 960, loss = 2.52 (113.6 examples/sec; 1.126 sec/batch)
2016-12-08 02:45:03.230399: step 970, loss = 2.50 (108.6 examples/sec; 1.179 sec/batch)
2016-12-08 02:45:15.315726: step 980, loss = 2.38 (110.5 examples/sec; 1.159 sec/batch)
2016-12-08 02:45:27.779898: step 990, loss = 2.52 (102.3 examples/sec; 1.251 sec/batch)
2016-12-08 02:45:39.722256: step 1000, loss = 2.39 (102.9 examples/sec; 1.244 sec/batch)
2016-12-08 02:45:53.916499: step 1010, loss = 2.45 (95.5 examples/sec; 1.341 sec/batch)
2016-12-08 02:46:05.788955: step 1020, loss = 2.54 (110.8 examples/sec; 1.155 sec/batch)
2016-12-08 02:46:17.461089: step 1030, loss = 2.50 (98.0 examples/sec; 1.306 sec/batch)
2016-12-08 02:46:29.610221: step 1040, loss = 2.56 (107.1 examples/sec; 1.195 sec/batch)
2016-12-08 02:46:41.846084: step 1050, loss = 2.68 (103.3 examples/sec; 1.239 sec/batch)
2016-12-08 02:46:54.274214: step 1060, loss = 2.42 (101.8 examples/sec; 1.257 sec/batch)
2016-12-08 02:47:06.348677: step 1070, loss = 2.44 (102.8 examples/sec; 1.245 sec/batch)
2016-12-08 02:47:18.042415: step 1080, loss = 2.35 (103.2 examples/sec; 1.240 sec/batch)
2016-12-08 02:47:30.371384: step 1090, loss = 2.21 (102.1 examples/sec; 1.254 sec/batch)
2016-12-08 02:47:42.440044: step 1100, loss = 2.44 (110.9 examples/sec; 1.154 sec/batch)
2016-12-08 02:47:55.606387: step 1110, loss = 2.18 (110.6 examples/sec; 1.158 sec/batch)
2016-12-08 02:48:07.386005: step 1120, loss = 2.37 (117.5 examples/sec; 1.089 sec/batch)
2016-12-08 02:48:19.346934: step 1130, loss = 2.39 (96.1 examples/sec; 1.332 sec/batch)
2016-12-08 02:48:31.349072: step 1140, loss = 2.20 (92.5 examples/sec; 1.383 sec/batch)
2016-12-08 02:48:43.390574: step 1150, loss = 2.14 (113.3 examples/sec; 1.130 sec/batch)
2016-12-08 02:48:55.287692: step 1160, loss = 2.48 (109.0 examples/sec; 1.174 sec/batch)
2016-12-08 02:49:07.249831: step 1170, loss = 2.36 (98.8 examples/sec; 1.296 sec/batch)
2016-12-08 02:49:19.568948: step 1180, loss = 2.16 (100.8 examples/sec; 1.270 sec/batch)
2016-12-08 02:49:31.648801: step 1190, loss = 2.21 (102.6 examples/sec; 1.247 sec/batch)
2016-12-08 02:49:43.561837: step 1200, loss = 2.55 (103.1 examples/sec; 1.241 sec/batch)
2016-12-08 02:49:57.433660: step 1210, loss = 2.19 (97.2 examples/sec; 1.317 sec/batch)
2016-12-08 02:50:09.849732: step 1220, loss = 2.05 (109.3 examples/sec; 1.171 sec/batch)
2016-12-08 02:50:22.120686: step 1230, loss = 2.18 (104.3 examples/sec; 1.227 sec/batch)
2016-12-08 02:50:34.044347: step 1240, loss = 2.27 (108.7 examples/sec; 1.178 sec/batch)
2016-12-08 02:50:46.310059: step 1250, loss = 2.14 (115.1 examples/sec; 1.112 sec/batch)
2016-12-08 02:50:58.361381: step 1260, loss = 2.16 (110.5 examples/sec; 1.159 sec/batch)
2016-12-08 02:51:10.745039: step 1270, loss = 2.14 (104.4 examples/sec; 1.226 sec/batch)
2016-12-08 02:51:23.000863: step 1280, loss = 2.10 (110.2 examples/sec; 1.162 sec/batch)
2016-12-08 02:51:34.935807: step 1290, loss = 2.24 (110.9 examples/sec; 1.154 sec/batch)
2016-12-08 02:51:46.640755: step 1300, loss = 2.21 (115.2 examples/sec; 1.111 sec/batch)
2016-12-08 02:51:59.851494: step 1310, loss = 2.06 (109.5 examples/sec; 1.169 sec/batch)
2016-12-08 02:52:11.751130: step 1320, loss = 2.14 (104.3 examples/sec; 1.227 sec/batch)
2016-12-08 02:52:24.107735: step 1330, loss = 2.13 (101.4 examples/sec; 1.262 sec/batch)
2016-12-08 02:52:36.095609: step 1340, loss = 2.28 (97.5 examples/sec; 1.313 sec/batch)
2016-12-08 02:52:47.974191: step 1350, loss = 2.05 (108.4 examples/sec; 1.181 sec/batch)
2016-12-08 02:52:59.940518: step 1360, loss = 1.94 (109.3 examples/sec; 1.171 sec/batch)
2016-12-08 02:53:12.092678: step 1370, loss = 2.22 (107.0 examples/sec; 1.197 sec/batch)
2016-12-08 02:53:24.350846: step 1380, loss = 2.04 (118.8 examples/sec; 1.077 sec/batch)
2016-12-08 02:53:35.863115: step 1390, loss = 2.05 (116.2 examples/sec; 1.102 sec/batch)
2016-12-08 02:53:48.068244: step 1400, loss = 2.06 (110.1 examples/sec; 1.163 sec/batch)
2016-12-08 02:54:01.686135: step 1410, loss = 1.95 (103.6 examples/sec; 1.236 sec/batch)
2016-12-08 02:54:13.520575: step 1420, loss = 1.98 (115.2 examples/sec; 1.111 sec/batch)
2016-12-08 02:54:25.364530: step 1430, loss = 2.08 (110.7 examples/sec; 1.157 sec/batch)
2016-12-08 02:54:37.740010: step 1440, loss = 1.94 (93.9 examples/sec; 1.363 sec/batch)
2016-12-08 02:54:49.544390: step 1450, loss = 1.89 (108.5 examples/sec; 1.179 sec/batch)
2016-12-08 02:55:01.616166: step 1460, loss = 1.86 (101.7 examples/sec; 1.259 sec/batch)
2016-12-08 02:55:13.539852: step 1470, loss = 1.97 (105.8 examples/sec; 1.210 sec/batch)
2016-12-08 02:55:25.241703: step 1480, loss = 2.04 (106.2 examples/sec; 1.205 sec/batch)
2016-12-08 02:55:37.122328: step 1490, loss = 1.99 (110.1 examples/sec; 1.163 sec/batch)
2016-12-08 02:55:49.284816: step 1500, loss = 2.14 (110.1 examples/sec; 1.162 sec/batch)
2016-12-08 02:56:02.557330: step 1510, loss = 2.00 (101.1 examples/sec; 1.267 sec/batch)
2016-12-08 02:56:14.528891: step 1520, loss = 1.99 (108.8 examples/sec; 1.176 sec/batch)
2016-12-08 02:56:26.317302: step 1530, loss = 1.90 (103.6 examples/sec; 1.235 sec/batch)
2016-12-08 02:56:37.902860: step 1540, loss = 2.07 (119.1 examples/sec; 1.074 sec/batch)
2016-12-08 02:56:50.227440: step 1550, loss = 1.94 (96.6 examples/sec; 1.325 sec/batch)
2016-12-08 02:57:02.089090: step 1560, loss = 1.90 (109.0 examples/sec; 1.174 sec/batch)
2016-12-08 02:57:14.546006: step 1570, loss = 1.79 (108.5 examples/sec; 1.180 sec/batch)
2016-12-08 02:57:26.758322: step 1580, loss = 2.00 (101.9 examples/sec; 1.257 sec/batch)
2016-12-08 02:57:38.632316: step 1590, loss = 1.85 (117.5 examples/sec; 1.089 sec/batch)
2016-12-08 02:57:50.475688: step 1600, loss = 2.03 (103.1 examples/sec; 1.242 sec/batch)
2016-12-08 02:58:04.067005: step 1610, loss = 1.93 (98.4 examples/sec; 1.301 sec/batch)
2016-12-08 02:58:16.018507: step 1620, loss = 1.89 (117.8 examples/sec; 1.087 sec/batch)
2016-12-08 02:58:28.003094: step 1630, loss = 1.69 (116.2 examples/sec; 1.101 sec/batch)
2016-12-08 02:58:40.110447: step 1640, loss = 1.93 (104.9 examples/sec; 1.221 sec/batch)
2016-12-08 02:58:51.929790: step 1650, loss = 1.86 (107.3 examples/sec; 1.193 sec/batch)
2016-12-08 02:59:03.802222: step 1660, loss = 2.04 (107.6 examples/sec; 1.190 sec/batch)
2016-12-08 02:59:15.636685: step 1670, loss = 1.78 (114.4 examples/sec; 1.119 sec/batch)
2016-12-08 02:59:27.551721: step 1680, loss = 1.81 (103.3 examples/sec; 1.239 sec/batch)
2016-12-08 02:59:39.374600: step 1690, loss = 1.79 (118.9 examples/sec; 1.077 sec/batch)
2016-12-08 02:59:51.407453: step 1700, loss = 1.74 (99.8 examples/sec; 1.283 sec/batch)
2016-12-08 03:00:04.045689: step 1710, loss = 1.67 (114.0 examples/sec; 1.123 sec/batch)
2016-12-08 03:00:15.813494: step 1720, loss = 1.89 (118.2 examples/sec; 1.083 sec/batch)
2016-12-08 03:00:27.405835: step 1730, loss = 1.71 (100.6 examples/sec; 1.272 sec/batch)
2016-12-08 03:00:39.292027: step 1740, loss = 1.98 (103.5 examples/sec; 1.236 sec/batch)
2016-12-08 03:00:51.265088: step 1750, loss = 1.81 (120.1 examples/sec; 1.066 sec/batch)
2016-12-08 03:01:03.627295: step 1760, loss = 1.67 (99.7 examples/sec; 1.284 sec/batch)
2016-12-08 03:01:15.709789: step 1770, loss = 1.69 (110.2 examples/sec; 1.162 sec/batch)
2016-12-08 03:01:27.621611: step 1780, loss = 1.90 (118.5 examples/sec; 1.080 sec/batch)
2016-12-08 03:01:39.788239: step 1790, loss = 1.74 (103.6 examples/sec; 1.236 sec/batch)
2016-12-08 03:01:51.683463: step 1800, loss = 1.77 (105.8 examples/sec; 1.210 sec/batch)
2016-12-08 03:02:05.096594: step 1810, loss = 1.49 (96.9 examples/sec; 1.320 sec/batch)
2016-12-08 03:02:17.010992: step 1820, loss = 2.02 (108.2 examples/sec; 1.183 sec/batch)
2016-12-08 03:02:28.813627: step 1830, loss = 2.02 (110.2 examples/sec; 1.161 sec/batch)
2016-12-08 03:02:40.789343: step 1840, loss = 1.70 (106.3 examples/sec; 1.205 sec/batch)
2016-12-08 03:02:52.590132: step 1850, loss = 1.59 (94.7 examples/sec; 1.352 sec/batch)
2016-12-08 03:03:04.140622: step 1860, loss = 1.89 (117.9 examples/sec; 1.085 sec/batch)
2016-12-08 03:03:16.174858: step 1870, loss = 1.72 (112.4 examples/sec; 1.139 sec/batch)
2016-12-08 03:03:28.057717: step 1880, loss = 1.93 (107.7 examples/sec; 1.188 sec/batch)
2016-12-08 03:03:40.159238: step 1890, loss = 1.85 (96.6 examples/sec; 1.325 sec/batch)
2016-12-08 03:03:51.886399: step 1900, loss = 1.45 (99.5 examples/sec; 1.286 sec/batch)
2016-12-08 03:04:04.978649: step 1910, loss = 1.86 (110.9 examples/sec; 1.154 sec/batch)
2016-12-08 03:04:16.913203: step 1920, loss = 1.58 (106.7 examples/sec; 1.199 sec/batch)
2016-12-08 03:04:28.797665: step 1930, loss = 1.96 (114.7 examples/sec; 1.116 sec/batch)
2016-12-08 03:04:40.896687: step 1940, loss = 1.82 (102.4 examples/sec; 1.250 sec/batch)
2016-12-08 03:04:52.708599: step 1950, loss = 1.60 (108.3 examples/sec; 1.182 sec/batch)
2016-12-08 03:05:04.948851: step 1960, loss = 1.65 (100.0 examples/sec; 1.280 sec/batch)
2016-12-08 03:05:16.902132: step 1970, loss = 1.77 (102.1 examples/sec; 1.253 sec/batch)
2016-12-08 03:05:29.223023: step 1980, loss = 1.81 (109.1 examples/sec; 1.174 sec/batch)
2016-12-08 03:05:41.377722: step 1990, loss = 1.52 (102.5 examples/sec; 1.249 sec/batch)
2016-12-08 03:05:53.348434: step 2000, loss = 1.55 (115.5 examples/sec; 1.108 sec/batch)
2016-12-08 03:06:07.189592: step 2010, loss = 1.56 (105.9 examples/sec; 1.209 sec/batch)
2016-12-08 03:06:19.302839: step 2020, loss = 1.61 (103.2 examples/sec; 1.241 sec/batch)
2016-12-08 03:06:31.018250: step 2030, loss = 1.71 (118.3 examples/sec; 1.082 sec/batch)
2016-12-08 03:06:42.847904: step 2040, loss = 1.72 (103.4 examples/sec; 1.238 sec/batch)
2016-12-08 03:06:54.454271: step 2050, loss = 1.52 (108.6 examples/sec; 1.179 sec/batch)
2016-12-08 03:07:06.605685: step 2060, loss = 1.71 (107.6 examples/sec; 1.189 sec/batch)
2016-12-08 03:07:18.682868: step 2070, loss = 1.70 (102.6 examples/sec; 1.247 sec/batch)
2016-12-08 03:07:30.632338: step 2080, loss = 1.91 (103.5 examples/sec; 1.236 sec/batch)
2016-12-08 03:07:42.301364: step 2090, loss = 1.75 (113.3 examples/sec; 1.130 sec/batch)
2016-12-08 03:07:54.145199: step 2100, loss = 1.55 (105.4 examples/sec; 1.214 sec/batch)
2016-12-08 03:08:07.316918: step 2110, loss = 1.47 (103.8 examples/sec; 1.233 sec/batch)
2016-12-08 03:08:19.382861: step 2120, loss = 1.78 (108.7 examples/sec; 1.177 sec/batch)
2016-12-08 03:08:31.008147: step 2130, loss = 1.78 (111.4 examples/sec; 1.150 sec/batch)
2016-12-08 03:08:42.827702: step 2140, loss = 1.30 (107.0 examples/sec; 1.197 sec/batch)
2016-12-08 03:08:54.657573: step 2150, loss = 1.54 (117.8 examples/sec; 1.087 sec/batch)
2016-12-08 03:09:06.436536: step 2160, loss = 1.63 (111.3 examples/sec; 1.150 sec/batch)
2016-12-08 03:09:18.922737: step 2170, loss = 1.56 (111.5 examples/sec; 1.148 sec/batch)
2016-12-08 03:09:30.793896: step 2180, loss = 1.58 (104.9 examples/sec; 1.220 sec/batch)
2016-12-08 03:09:42.619101: step 2190, loss = 1.47 (112.1 examples/sec; 1.142 sec/batch)
2016-12-08 03:09:54.379867: step 2200, loss = 1.75 (111.5 examples/sec; 1.148 sec/batch)
2016-12-08 03:10:07.485657: step 2210, loss = 1.51 (118.1 examples/sec; 1.084 sec/batch)
2016-12-08 03:10:19.321892: step 2220, loss = 1.42 (109.9 examples/sec; 1.165 sec/batch)
2016-12-08 03:10:31.127094: step 2230, loss = 1.56 (112.2 examples/sec; 1.140 sec/batch)
2016-12-08 03:10:43.248003: step 2240, loss = 1.63 (103.0 examples/sec; 1.243 sec/batch)
2016-12-08 03:10:54.974572: step 2250, loss = 1.50 (120.7 examples/sec; 1.061 sec/batch)
2016-12-08 03:11:06.781273: step 2260, loss = 1.51 (107.4 examples/sec; 1.192 sec/batch)
2016-12-08 03:11:18.765892: step 2270, loss = 1.60 (105.8 examples/sec; 1.210 sec/batch)
2016-12-08 03:11:30.683410: step 2280, loss = 1.65 (106.4 examples/sec; 1.203 sec/batch)
2016-12-08 03:11:42.435520: step 2290, loss = 1.58 (105.4 examples/sec; 1.214 sec/batch)
2016-12-08 03:11:54.745408: step 2300, loss = 1.35 (104.0 examples/sec; 1.231 sec/batch)
2016-12-08 03:12:07.770268: step 2310, loss = 1.51 (107.8 examples/sec; 1.188 sec/batch)
2016-12-08 03:12:20.020842: step 2320, loss = 1.53 (106.7 examples/sec; 1.199 sec/batch)
2016-12-08 03:12:31.413880: step 2330, loss = 1.38 (116.7 examples/sec; 1.097 sec/batch)
2016-12-08 03:12:43.561002: step 2340, loss = 1.48 (111.2 examples/sec; 1.152 sec/batch)
2016-12-08 03:12:55.290496: step 2350, loss = 1.53 (112.1 examples/sec; 1.142 sec/batch)
2016-12-08 03:13:07.227633: step 2360, loss = 1.60 (112.0 examples/sec; 1.142 sec/batch)
2016-12-08 03:13:18.963685: step 2370, loss = 1.48 (123.4 examples/sec; 1.037 sec/batch)
2016-12-08 03:13:30.922732: step 2380, loss = 1.46 (112.7 examples/sec; 1.136 sec/batch)
2016-12-08 03:13:42.053208: step 2390, loss = 1.32 (125.3 examples/sec; 1.021 sec/batch)
2016-12-08 03:13:53.845185: step 2400, loss = 1.62 (105.1 examples/sec; 1.217 sec/batch)
2016-12-08 03:14:06.606290: step 2410, loss = 1.34 (118.4 examples/sec; 1.081 sec/batch)
2016-12-08 03:14:17.827381: step 2420, loss = 1.45 (115.5 examples/sec; 1.108 sec/batch)
2016-12-08 03:14:29.808621: step 2430, loss = 1.64 (106.4 examples/sec; 1.202 sec/batch)
2016-12-08 03:14:41.183519: step 2440, loss = 1.39 (114.4 examples/sec; 1.119 sec/batch)
2016-12-08 03:14:52.544113: step 2450, loss = 1.53 (112.3 examples/sec; 1.140 sec/batch)
2016-12-08 03:15:03.674928: step 2460, loss = 1.73 (121.3 examples/sec; 1.055 sec/batch)
2016-12-08 03:15:14.947595: step 2470, loss = 1.48 (116.8 examples/sec; 1.096 sec/batch)
2016-12-08 03:15:26.570385: step 2480, loss = 1.40 (105.9 examples/sec; 1.209 sec/batch)
2016-12-08 03:15:38.019972: step 2490, loss = 1.47 (112.6 examples/sec; 1.137 sec/batch)
2016-12-08 03:15:49.574570: step 2500, loss = 1.81 (110.2 examples/sec; 1.162 sec/batch)
2016-12-08 03:16:02.296091: step 2510, loss = 1.34 (118.4 examples/sec; 1.081 sec/batch)
2016-12-08 03:16:14.018276: step 2520, loss = 1.48 (112.4 examples/sec; 1.139 sec/batch)
2016-12-08 03:12:51.474989: step 2440, loss = 1.39 (108.0 examples/sec; 1.185 sec/batch)
2016-12-08 03:13:03.409493: step 2450, loss = 1.32 (97.2 examples/sec; 1.317 sec/batch)
