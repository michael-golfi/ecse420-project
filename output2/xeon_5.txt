Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
WARNING:tensorflow:From cifar10_train.py:81 in train.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2016-03-02.
Instructions for updating:
Please use tf.global_variables instead.
2016-12-08 08:21:24.632577: step 0, loss = 4.68 (26.9 examples/sec; 4.754 sec/batch)
2016-12-08 08:21:30.378106: step 10, loss = 4.62 (260.0 examples/sec; 0.492 sec/batch)
2016-12-08 08:21:34.886240: step 20, loss = 4.63 (259.8 examples/sec; 0.493 sec/batch)
2016-12-08 08:21:39.377335: step 30, loss = 4.40 (358.5 examples/sec; 0.357 sec/batch)
2016-12-08 08:21:44.030818: step 40, loss = 4.37 (301.7 examples/sec; 0.424 sec/batch)
2016-12-08 08:21:48.480023: step 50, loss = 4.23 (259.8 examples/sec; 0.493 sec/batch)
2016-12-08 08:21:53.221364: step 60, loss = 4.30 (272.0 examples/sec; 0.471 sec/batch)
2016-12-08 08:21:57.980335: step 70, loss = 4.09 (263.0 examples/sec; 0.487 sec/batch)
2016-12-08 08:22:02.370249: step 80, loss = 4.03 (276.1 examples/sec; 0.464 sec/batch)
2016-12-08 08:22:06.704309: step 90, loss = 4.09 (312.1 examples/sec; 0.410 sec/batch)
2016-12-08 08:22:11.094162: step 100, loss = 4.08 (317.7 examples/sec; 0.403 sec/batch)
2016-12-08 08:22:16.061556: step 110, loss = 4.12 (275.2 examples/sec; 0.465 sec/batch)
2016-12-08 08:22:20.517265: step 120, loss = 3.98 (290.4 examples/sec; 0.441 sec/batch)
2016-12-08 08:22:25.047923: step 130, loss = 4.15 (262.3 examples/sec; 0.488 sec/batch)
2016-12-08 08:22:29.274494: step 140, loss = 4.04 (307.8 examples/sec; 0.416 sec/batch)
2016-12-08 08:22:33.712176: step 150, loss = 4.16 (296.7 examples/sec; 0.431 sec/batch)
2016-12-08 08:22:37.974989: step 160, loss = 3.97 (300.6 examples/sec; 0.426 sec/batch)
2016-12-08 08:22:42.440797: step 170, loss = 3.86 (289.6 examples/sec; 0.442 sec/batch)
2016-12-08 08:22:46.894059: step 180, loss = 4.07 (291.2 examples/sec; 0.440 sec/batch)
2016-12-08 08:22:51.563454: step 190, loss = 4.06 (291.4 examples/sec; 0.439 sec/batch)
2016-12-08 08:22:55.854083: step 200, loss = 4.01 (292.9 examples/sec; 0.437 sec/batch)
2016-12-08 08:23:00.616620: step 210, loss = 3.81 (283.2 examples/sec; 0.452 sec/batch)
2016-12-08 08:23:05.073533: step 220, loss = 3.86 (319.1 examples/sec; 0.401 sec/batch)
2016-12-08 08:23:09.540221: step 230, loss = 3.78 (278.8 examples/sec; 0.459 sec/batch)
2016-12-08 08:23:13.907806: step 240, loss = 3.67 (278.8 examples/sec; 0.459 sec/batch)
2016-12-08 08:23:18.406590: step 250, loss = 3.60 (256.4 examples/sec; 0.499 sec/batch)
2016-12-08 08:23:22.747973: step 260, loss = 3.55 (316.6 examples/sec; 0.404 sec/batch)
2016-12-08 08:23:26.824638: step 270, loss = 4.00 (265.0 examples/sec; 0.483 sec/batch)
2016-12-08 08:23:30.989528: step 280, loss = 3.54 (324.6 examples/sec; 0.394 sec/batch)
2016-12-08 08:23:35.352354: step 290, loss = 3.39 (340.4 examples/sec; 0.376 sec/batch)
2016-12-08 08:23:39.842529: step 300, loss = 3.47 (293.8 examples/sec; 0.436 sec/batch)
2016-12-08 08:23:45.056191: step 310, loss = 3.48 (280.9 examples/sec; 0.456 sec/batch)
2016-12-08 08:23:49.304568: step 320, loss = 3.52 (294.2 examples/sec; 0.435 sec/batch)
2016-12-08 08:23:53.747400: step 330, loss = 3.42 (282.3 examples/sec; 0.453 sec/batch)
2016-12-08 08:23:58.034109: step 340, loss = 3.42 (378.7 examples/sec; 0.338 sec/batch)
2016-12-08 08:24:02.519561: step 350, loss = 3.57 (279.7 examples/sec; 0.458 sec/batch)
2016-12-08 08:24:06.922647: step 360, loss = 3.41 (345.2 examples/sec; 0.371 sec/batch)
2016-12-08 08:24:11.636383: step 370, loss = 3.45 (268.6 examples/sec; 0.477 sec/batch)
2016-12-08 08:24:16.114922: step 380, loss = 3.61 (307.1 examples/sec; 0.417 sec/batch)
2016-12-08 08:24:20.509161: step 390, loss = 3.38 (264.4 examples/sec; 0.484 sec/batch)
2016-12-08 08:24:24.820709: step 400, loss = 3.49 (298.8 examples/sec; 0.428 sec/batch)
2016-12-08 08:24:29.652167: step 410, loss = 3.47 (288.0 examples/sec; 0.444 sec/batch)
2016-12-08 08:24:33.958126: step 420, loss = 3.31 (282.0 examples/sec; 0.454 sec/batch)
2016-12-08 08:24:38.249441: step 430, loss = 3.55 (314.1 examples/sec; 0.408 sec/batch)
2016-12-08 08:24:42.393020: step 440, loss = 3.34 (297.7 examples/sec; 0.430 sec/batch)
2016-12-08 08:24:46.644309: step 450, loss = 3.33 (345.6 examples/sec; 0.370 sec/batch)
2016-12-08 08:24:50.673147: step 460, loss = 3.26 (296.6 examples/sec; 0.432 sec/batch)
2016-12-08 08:24:54.910730: step 470, loss = 3.22 (289.1 examples/sec; 0.443 sec/batch)
2016-12-08 08:24:59.210155: step 480, loss = 3.18 (283.0 examples/sec; 0.452 sec/batch)
2016-12-08 08:25:03.585856: step 490, loss = 3.09 (290.7 examples/sec; 0.440 sec/batch)
2016-12-08 08:25:07.812021: step 500, loss = 3.21 (262.0 examples/sec; 0.489 sec/batch)
2016-12-08 08:25:13.088723: step 510, loss = 3.19 (262.2 examples/sec; 0.488 sec/batch)
2016-12-08 08:25:17.512233: step 520, loss = 3.33 (300.5 examples/sec; 0.426 sec/batch)
2016-12-08 08:25:21.878342: step 530, loss = 3.36 (265.1 examples/sec; 0.483 sec/batch)
2016-12-08 08:25:26.122714: step 540, loss = 3.27 (335.7 examples/sec; 0.381 sec/batch)
2016-12-08 08:25:30.267915: step 550, loss = 3.09 (353.9 examples/sec; 0.362 sec/batch)
2016-12-08 08:25:34.516515: step 560, loss = 3.04 (248.7 examples/sec; 0.515 sec/batch)
2016-12-08 08:25:38.979102: step 570, loss = 2.99 (328.0 examples/sec; 0.390 sec/batch)
2016-12-08 08:25:43.180581: step 580, loss = 3.04 (299.0 examples/sec; 0.428 sec/batch)
2016-12-08 08:25:47.568490: step 590, loss = 2.92 (299.5 examples/sec; 0.427 sec/batch)
2016-12-08 08:25:51.878581: step 600, loss = 2.95 (304.8 examples/sec; 0.420 sec/batch)
2016-12-08 08:25:56.703871: step 610, loss = 2.93 (303.0 examples/sec; 0.422 sec/batch)
2016-12-08 08:26:01.216883: step 620, loss = 2.85 (349.5 examples/sec; 0.366 sec/batch)
2016-12-08 08:26:05.733284: step 630, loss = 3.18 (248.9 examples/sec; 0.514 sec/batch)
2016-12-08 08:26:10.073324: step 640, loss = 2.80 (332.2 examples/sec; 0.385 sec/batch)
2016-12-08 08:26:14.488889: step 650, loss = 2.87 (314.6 examples/sec; 0.407 sec/batch)
2016-12-08 08:26:18.590747: step 660, loss = 2.83 (315.6 examples/sec; 0.406 sec/batch)
2016-12-08 08:26:23.291014: step 670, loss = 2.82 (258.6 examples/sec; 0.495 sec/batch)
2016-12-08 08:26:27.946654: step 680, loss = 2.77 (260.3 examples/sec; 0.492 sec/batch)
2016-12-08 08:26:32.083985: step 690, loss = 2.82 (257.3 examples/sec; 0.497 sec/batch)
2016-12-08 08:26:36.558152: step 700, loss = 3.35 (245.5 examples/sec; 0.521 sec/batch)
2016-12-08 08:26:41.507638: step 710, loss = 2.76 (275.1 examples/sec; 0.465 sec/batch)
2016-12-08 08:26:45.544512: step 720, loss = 2.89 (327.2 examples/sec; 0.391 sec/batch)
2016-12-08 08:26:49.887680: step 730, loss = 2.75 (289.4 examples/sec; 0.442 sec/batch)
2016-12-08 08:26:54.177154: step 740, loss = 2.73 (330.9 examples/sec; 0.387 sec/batch)
2016-12-08 08:26:58.862117: step 750, loss = 2.74 (255.6 examples/sec; 0.501 sec/batch)
2016-12-08 08:27:03.115737: step 760, loss = 2.85 (263.0 examples/sec; 0.487 sec/batch)
2016-12-08 08:27:07.527863: step 770, loss = 2.67 (289.8 examples/sec; 0.442 sec/batch)
2016-12-08 08:27:11.865693: step 780, loss = 2.76 (331.1 examples/sec; 0.387 sec/batch)
2016-12-08 08:27:16.039588: step 790, loss = 2.80 (327.5 examples/sec; 0.391 sec/batch)
2016-12-08 08:27:20.275423: step 800, loss = 2.48 (298.1 examples/sec; 0.429 sec/batch)
2016-12-08 08:27:25.509201: step 810, loss = 2.72 (248.8 examples/sec; 0.514 sec/batch)
2016-12-08 08:27:30.160263: step 820, loss = 2.79 (249.1 examples/sec; 0.514 sec/batch)
2016-12-08 08:27:34.742763: step 830, loss = 2.63 (276.5 examples/sec; 0.463 sec/batch)
2016-12-08 08:27:38.704475: step 840, loss = 2.45 (384.9 examples/sec; 0.333 sec/batch)
2016-12-08 08:27:42.884636: step 850, loss = 2.71 (325.2 examples/sec; 0.394 sec/batch)
2016-12-08 08:27:47.111916: step 860, loss = 2.69 (273.8 examples/sec; 0.467 sec/batch)
2016-12-08 08:27:51.578650: step 870, loss = 2.45 (272.1 examples/sec; 0.470 sec/batch)
2016-12-08 08:27:56.173704: step 880, loss = 2.62 (247.4 examples/sec; 0.517 sec/batch)
2016-12-08 08:28:00.379039: step 890, loss = 2.51 (316.7 examples/sec; 0.404 sec/batch)
2016-12-08 08:28:04.622035: step 900, loss = 2.42 (274.2 examples/sec; 0.467 sec/batch)
2016-12-08 08:28:09.231374: step 910, loss = 2.46 (250.1 examples/sec; 0.512 sec/batch)
2016-12-08 08:28:13.811070: step 920, loss = 2.57 (313.1 examples/sec; 0.409 sec/batch)
2016-12-08 08:28:18.012472: step 930, loss = 2.60 (268.5 examples/sec; 0.477 sec/batch)
2016-12-08 08:28:22.467644: step 940, loss = 2.46 (295.4 examples/sec; 0.433 sec/batch)
2016-12-08 08:28:26.368862: step 950, loss = 2.49 (356.8 examples/sec; 0.359 sec/batch)
2016-12-08 08:28:30.698192: step 960, loss = 2.61 (286.9 examples/sec; 0.446 sec/batch)
2016-12-08 08:28:34.611856: step 970, loss = 2.31 (314.7 examples/sec; 0.407 sec/batch)
2016-12-08 08:28:38.734666: step 980, loss = 2.33 (287.0 examples/sec; 0.446 sec/batch)
2016-12-08 08:28:42.863102: step 990, loss = 2.60 (321.1 examples/sec; 0.399 sec/batch)
2016-12-08 08:28:47.550138: step 1000, loss = 2.53 (257.7 examples/sec; 0.497 sec/batch)
2016-12-08 08:28:52.933387: step 1010, loss = 2.35 (286.0 examples/sec; 0.448 sec/batch)
2016-12-08 08:28:57.458147: step 1020, loss = 2.32 (265.2 examples/sec; 0.483 sec/batch)
2016-12-08 08:29:02.012846: step 1030, loss = 2.53 (265.4 examples/sec; 0.482 sec/batch)
2016-12-08 08:29:06.323687: step 1040, loss = 2.43 (341.4 examples/sec; 0.375 sec/batch)
2016-12-08 08:29:10.613804: step 1050, loss = 2.32 (336.7 examples/sec; 0.380 sec/batch)
2016-12-08 08:29:14.942979: step 1060, loss = 2.49 (255.7 examples/sec; 0.501 sec/batch)
2016-12-08 08:29:19.196673: step 1070, loss = 2.26 (303.3 examples/sec; 0.422 sec/batch)
2016-12-08 08:29:23.478217: step 1080, loss = 2.33 (310.9 examples/sec; 0.412 sec/batch)
2016-12-08 08:29:27.674469: step 1090, loss = 2.35 (261.4 examples/sec; 0.490 sec/batch)
2016-12-08 08:29:32.053076: step 1100, loss = 2.34 (268.9 examples/sec; 0.476 sec/batch)
2016-12-08 08:29:36.938193: step 1110, loss = 2.60 (279.9 examples/sec; 0.457 sec/batch)
2016-12-08 08:29:41.512779: step 1120, loss = 2.19 (256.5 examples/sec; 0.499 sec/batch)
2016-12-08 08:29:45.975740: step 1130, loss = 2.31 (283.0 examples/sec; 0.452 sec/batch)
2016-12-08 08:29:50.406266: step 1140, loss = 2.26 (273.2 examples/sec; 0.469 sec/batch)
2016-12-08 08:29:54.968706: step 1150, loss = 2.35 (261.1 examples/sec; 0.490 sec/batch)
2016-12-08 08:29:59.591226: step 1160, loss = 2.27 (246.3 examples/sec; 0.520 sec/batch)
2016-12-08 08:30:04.095058: step 1170, loss = 2.22 (312.5 examples/sec; 0.410 sec/batch)
2016-12-08 08:30:08.535283: step 1180, loss = 2.28 (278.5 examples/sec; 0.460 sec/batch)
2016-12-08 08:30:12.737954: step 1190, loss = 2.17 (271.7 examples/sec; 0.471 sec/batch)
2016-12-08 08:30:17.008204: step 1200, loss = 2.08 (259.3 examples/sec; 0.494 sec/batch)
2016-12-08 08:30:21.806264: step 1210, loss = 2.17 (263.3 examples/sec; 0.486 sec/batch)
2016-12-08 08:30:25.980777: step 1220, loss = 2.21 (308.0 examples/sec; 0.416 sec/batch)
2016-12-08 08:30:30.165259: step 1230, loss = 2.44 (388.7 examples/sec; 0.329 sec/batch)
2016-12-08 08:30:34.624523: step 1240, loss = 2.25 (260.2 examples/sec; 0.492 sec/batch)
2016-12-08 08:30:38.877346: step 1250, loss = 2.11 (343.8 examples/sec; 0.372 sec/batch)
2016-12-08 08:30:43.237333: step 1260, loss = 2.34 (294.0 examples/sec; 0.435 sec/batch)
2016-12-08 08:30:47.505850: step 1270, loss = 2.22 (298.3 examples/sec; 0.429 sec/batch)
2016-12-08 08:30:51.972976: step 1280, loss = 2.06 (264.4 examples/sec; 0.484 sec/batch)
2016-12-08 08:30:55.982525: step 1290, loss = 2.08 (308.4 examples/sec; 0.415 sec/batch)
2016-12-08 08:31:00.340895: step 1300, loss = 1.97 (383.3 examples/sec; 0.334 sec/batch)
2016-12-08 08:31:05.123369: step 1310, loss = 2.06 (306.0 examples/sec; 0.418 sec/batch)
2016-12-08 08:31:09.577085: step 1320, loss = 2.09 (270.8 examples/sec; 0.473 sec/batch)
2016-12-08 08:31:14.035226: step 1330, loss = 2.04 (281.6 examples/sec; 0.454 sec/batch)
2016-12-08 08:31:18.432149: step 1340, loss = 2.04 (283.9 examples/sec; 0.451 sec/batch)
2016-12-08 08:31:22.861357: step 1350, loss = 2.01 (283.4 examples/sec; 0.452 sec/batch)
2016-12-08 08:31:27.260736: step 1360, loss = 2.19 (265.1 examples/sec; 0.483 sec/batch)
2016-12-08 08:31:31.365754: step 1370, loss = 1.92 (309.7 examples/sec; 0.413 sec/batch)
2016-12-08 08:31:35.678282: step 1380, loss = 2.18 (288.6 examples/sec; 0.443 sec/batch)
2016-12-08 08:31:39.796564: step 1390, loss = 2.02 (283.5 examples/sec; 0.452 sec/batch)
2016-12-08 08:31:44.176441: step 1400, loss = 1.98 (315.4 examples/sec; 0.406 sec/batch)
2016-12-08 08:31:49.099099: step 1410, loss = 2.22 (252.5 examples/sec; 0.507 sec/batch)
2016-12-08 08:31:53.559271: step 1420, loss = 1.88 (262.1 examples/sec; 0.488 sec/batch)
2016-12-08 08:31:58.043244: step 1430, loss = 2.00 (301.0 examples/sec; 0.425 sec/batch)
2016-12-08 08:32:02.311399: step 1440, loss = 1.92 (297.1 examples/sec; 0.431 sec/batch)
2016-12-08 08:32:06.521458: step 1450, loss = 1.89 (349.2 examples/sec; 0.367 sec/batch)
2016-12-08 08:32:10.918786: step 1460, loss = 2.08 (286.8 examples/sec; 0.446 sec/batch)
2016-12-08 08:32:15.268246: step 1470, loss = 1.94 (313.3 examples/sec; 0.409 sec/batch)
2016-12-08 08:32:19.524684: step 1480, loss = 1.92 (272.1 examples/sec; 0.470 sec/batch)
2016-12-08 08:32:23.801145: step 1490, loss = 2.13 (285.0 examples/sec; 0.449 sec/batch)
2016-12-08 08:32:28.152888: step 1500, loss = 1.79 (304.5 examples/sec; 0.420 sec/batch)
2016-12-08 08:32:33.048939: step 1510, loss = 2.30 (301.8 examples/sec; 0.424 sec/batch)
2016-12-08 08:32:37.225014: step 1520, loss = 2.02 (313.6 examples/sec; 0.408 sec/batch)
2016-12-08 08:32:41.527961: step 1530, loss = 2.23 (284.6 examples/sec; 0.450 sec/batch)
2016-12-08 08:32:45.794318: step 1540, loss = 1.83 (344.6 examples/sec; 0.371 sec/batch)
2016-12-08 08:32:50.104592: step 1550, loss = 1.78 (264.9 examples/sec; 0.483 sec/batch)
2016-12-08 08:32:54.209820: step 1560, loss = 1.87 (330.4 examples/sec; 0.387 sec/batch)
2016-12-08 08:32:58.254523: step 1570, loss = 2.02 (328.5 examples/sec; 0.390 sec/batch)
2016-12-08 08:33:02.529100: step 1580, loss = 2.07 (256.7 examples/sec; 0.499 sec/batch)
2016-12-08 08:33:06.860575: step 1590, loss = 1.79 (260.5 examples/sec; 0.491 sec/batch)
2016-12-08 08:33:11.110375: step 1600, loss = 1.81 (349.8 examples/sec; 0.366 sec/batch)
2016-12-08 08:33:15.791924: step 1610, loss = 1.98 (269.0 examples/sec; 0.476 sec/batch)
2016-12-08 08:33:19.933181: step 1620, loss = 2.05 (266.0 examples/sec; 0.481 sec/batch)
2016-12-08 08:33:24.309070: step 1630, loss = 1.88 (307.0 examples/sec; 0.417 sec/batch)
2016-12-08 08:33:28.183723: step 1640, loss = 1.89 (305.3 examples/sec; 0.419 sec/batch)
2016-12-08 08:33:32.724400: step 1650, loss = 1.90 (248.4 examples/sec; 0.515 sec/batch)
2016-12-08 08:33:37.386543: step 1660, loss = 1.70 (266.1 examples/sec; 0.481 sec/batch)
2016-12-08 08:33:41.707567: step 1670, loss = 1.72 (292.2 examples/sec; 0.438 sec/batch)
2016-12-08 08:33:45.894455: step 1680, loss = 1.66 (327.0 examples/sec; 0.391 sec/batch)
2016-12-08 08:33:49.976747: step 1690, loss = 1.78 (288.5 examples/sec; 0.444 sec/batch)
2016-12-08 08:33:54.226590: step 1700, loss = 1.76 (372.5 examples/sec; 0.344 sec/batch)
2016-12-08 08:33:59.009237: step 1710, loss = 1.82 (337.4 examples/sec; 0.379 sec/batch)
2016-12-08 08:34:02.914641: step 1720, loss = 1.72 (318.1 examples/sec; 0.402 sec/batch)
2016-12-08 08:34:07.263484: step 1730, loss = 1.84 (298.9 examples/sec; 0.428 sec/batch)
2016-12-08 08:34:11.989840: step 1740, loss = 2.11 (271.0 examples/sec; 0.472 sec/batch)
2016-12-08 08:34:16.242320: step 1750, loss = 1.68 (289.8 examples/sec; 0.442 sec/batch)
2016-12-08 08:34:20.674750: step 1760, loss = 2.05 (305.6 examples/sec; 0.419 sec/batch)
2016-12-08 08:34:24.513385: step 1770, loss = 1.64 (393.1 examples/sec; 0.326 sec/batch)
2016-12-08 08:34:28.778774: step 1780, loss = 1.89 (283.7 examples/sec; 0.451 sec/batch)
2016-12-08 08:34:33.293461: step 1790, loss = 1.90 (319.4 examples/sec; 0.401 sec/batch)
2016-12-08 08:34:37.518786: step 1800, loss = 1.71 (250.7 examples/sec; 0.511 sec/batch)
2016-12-08 08:34:42.548174: step 1810, loss = 1.68 (263.1 examples/sec; 0.487 sec/batch)
2016-12-08 08:34:46.709713: step 1820, loss = 1.92 (323.4 examples/sec; 0.396 sec/batch)
2016-12-08 08:34:51.040104: step 1830, loss = 1.63 (295.3 examples/sec; 0.433 sec/batch)
2016-12-08 08:34:55.565425: step 1840, loss = 1.83 (287.8 examples/sec; 0.445 sec/batch)
2016-12-08 08:34:59.588248: step 1850, loss = 1.76 (398.9 examples/sec; 0.321 sec/batch)
2016-12-08 08:35:03.839384: step 1860, loss = 1.75 (285.5 examples/sec; 0.448 sec/batch)
2016-12-08 08:35:08.125983: step 1870, loss = 1.80 (301.5 examples/sec; 0.425 sec/batch)
2016-12-08 08:35:12.797401: step 1880, loss = 1.75 (280.9 examples/sec; 0.456 sec/batch)
2016-12-08 08:35:17.558234: step 1890, loss = 1.66 (267.4 examples/sec; 0.479 sec/batch)
2016-12-08 08:35:21.913059: step 1900, loss = 1.70 (276.8 examples/sec; 0.462 sec/batch)
2016-12-08 08:35:26.905258: step 1910, loss = 1.56 (328.1 examples/sec; 0.390 sec/batch)
2016-12-08 08:35:31.144507: step 1920, loss = 1.65 (289.8 examples/sec; 0.442 sec/batch)
2016-12-08 08:35:35.561166: step 1930, loss = 1.68 (296.3 examples/sec; 0.432 sec/batch)
2016-12-08 08:35:39.778917: step 1940, loss = 1.70 (276.3 examples/sec; 0.463 sec/batch)
2016-12-08 08:35:44.271402: step 1950, loss = 1.68 (261.7 examples/sec; 0.489 sec/batch)
2016-12-08 08:35:48.576768: step 1960, loss = 1.70 (273.7 examples/sec; 0.468 sec/batch)
2016-12-08 08:35:52.975249: step 1970, loss = 1.53 (274.1 examples/sec; 0.467 sec/batch)
2016-12-08 08:35:57.222520: step 1980, loss = 1.79 (311.1 examples/sec; 0.411 sec/batch)
2016-12-08 08:36:01.498966: step 1990, loss = 1.78 (314.5 examples/sec; 0.407 sec/batch)
2016-12-08 08:36:05.961866: step 2000, loss = 1.49 (277.3 examples/sec; 0.462 sec/batch)
2016-12-08 08:36:11.749240: step 2010, loss = 1.75 (248.7 examples/sec; 0.515 sec/batch)
2016-12-08 08:36:16.023838: step 2020, loss = 1.53 (302.0 examples/sec; 0.424 sec/batch)
2016-12-08 08:36:20.411042: step 2030, loss = 1.58 (382.6 examples/sec; 0.335 sec/batch)
2016-12-08 08:36:24.763111: step 2040, loss = 1.75 (270.0 examples/sec; 0.474 sec/batch)
2016-12-08 08:36:29.020198: step 2050, loss = 1.59 (320.1 examples/sec; 0.400 sec/batch)
2016-12-08 08:36:33.028921: step 2060, loss = 1.55 (335.1 examples/sec; 0.382 sec/batch)
2016-12-08 08:36:37.579119: step 2070, loss = 1.59 (277.5 examples/sec; 0.461 sec/batch)
2016-12-08 08:36:41.958647: step 2080, loss = 1.77 (280.9 examples/sec; 0.456 sec/batch)
2016-12-08 08:36:46.110547: step 2090, loss = 1.68 (351.3 examples/sec; 0.364 sec/batch)
2016-12-08 08:36:50.397741: step 2100, loss = 1.52 (290.4 examples/sec; 0.441 sec/batch)
2016-12-08 08:36:55.026032: step 2110, loss = 1.60 (328.3 examples/sec; 0.390 sec/batch)
2016-12-08 08:36:59.352180: step 2120, loss = 1.53 (296.4 examples/sec; 0.432 sec/batch)
2016-12-08 08:37:03.708023: step 2130, loss = 1.37 (242.7 examples/sec; 0.527 sec/batch)
2016-12-08 08:37:07.812968: step 2140, loss = 1.66 (319.9 examples/sec; 0.400 sec/batch)
2016-12-08 08:37:11.843708: step 2150, loss = 1.47 (339.1 examples/sec; 0.377 sec/batch)
2016-12-08 08:37:15.995937: step 2160, loss = 1.58 (265.8 examples/sec; 0.482 sec/batch)
2016-12-08 08:37:20.196121: step 2170, loss = 1.46 (292.6 examples/sec; 0.438 sec/batch)
2016-12-08 08:37:24.593131: step 2180, loss = 1.65 (334.4 examples/sec; 0.383 sec/batch)
2016-12-08 08:37:28.896196: step 2190, loss = 1.83 (304.4 examples/sec; 0.421 sec/batch)
2016-12-08 08:37:33.348606: step 2200, loss = 1.65 (319.0 examples/sec; 0.401 sec/batch)
2016-12-08 08:37:38.228879: step 2210, loss = 1.35 (332.7 examples/sec; 0.385 sec/batch)
2016-12-08 08:37:42.766866: step 2220, loss = 1.51 (330.0 examples/sec; 0.388 sec/batch)
2016-12-08 08:37:46.893093: step 2230, loss = 1.53 (284.3 examples/sec; 0.450 sec/batch)
2016-12-08 08:37:51.016534: step 2240, loss = 1.62 (345.0 examples/sec; 0.371 sec/batch)
2016-12-08 08:37:55.072721: step 2250, loss = 1.44 (295.6 examples/sec; 0.433 sec/batch)
2016-12-08 08:37:59.438585: step 2260, loss = 1.65 (316.0 examples/sec; 0.405 sec/batch)
2016-12-08 08:38:03.902161: step 2270, loss = 1.62 (284.2 examples/sec; 0.450 sec/batch)
2016-12-08 08:38:08.029630: step 2280, loss = 1.77 (300.2 examples/sec; 0.426 sec/batch)
2016-12-08 08:38:12.427311: step 2290, loss = 1.69 (362.5 examples/sec; 0.353 sec/batch)
2016-12-08 08:38:16.695190: step 2300, loss = 1.49 (329.3 examples/sec; 0.389 sec/batch)
2016-12-08 08:38:21.546371: step 2310, loss = 1.49 (275.5 examples/sec; 0.465 sec/batch)
2016-12-08 08:38:25.937355: step 2320, loss = 1.46 (272.4 examples/sec; 0.470 sec/batch)
2016-12-08 08:38:30.415821: step 2330, loss = 1.37 (286.7 examples/sec; 0.447 sec/batch)
2016-12-08 08:38:34.549046: step 2340, loss = 1.45 (303.0 examples/sec; 0.422 sec/batch)
2016-12-08 08:38:39.057568: step 2350, loss = 1.34 (287.0 examples/sec; 0.446 sec/batch)
2016-12-08 08:38:43.291723: step 2360, loss = 1.47 (330.2 examples/sec; 0.388 sec/batch)
2016-12-08 08:38:47.608920: step 2370, loss = 1.52 (305.9 examples/sec; 0.418 sec/batch)
2016-12-08 08:38:51.793174: step 2380, loss = 1.62 (310.6 examples/sec; 0.412 sec/batch)
2016-12-08 08:38:55.802556: step 2390, loss = 1.54 (327.7 examples/sec; 0.391 sec/batch)
2016-12-08 08:39:00.058992: step 2400, loss = 1.59 (276.3 examples/sec; 0.463 sec/batch)
2016-12-08 08:39:04.700174: step 2410, loss = 1.32 (353.9 examples/sec; 0.362 sec/batch)
2016-12-08 08:39:09.116768: step 2420, loss = 1.60 (282.3 examples/sec; 0.453 sec/batch)
2016-12-08 08:39:13.575493: step 2430, loss = 1.80 (272.4 examples/sec; 0.470 sec/batch)
2016-12-08 08:39:18.086883: step 2440, loss = 1.43 (266.3 examples/sec; 0.481 sec/batch)
2016-12-08 08:39:22.314148: step 2450, loss = 1.36 (291.8 examples/sec; 0.439 sec/batch)
2016-12-08 08:39:26.603262: step 2460, loss = 1.50 (297.1 examples/sec; 0.431 sec/batch)
2016-12-08 08:39:31.032623: step 2470, loss = 1.50 (319.7 examples/sec; 0.400 sec/batch)
2016-12-08 08:39:35.473574: step 2480, loss = 1.39 (277.1 examples/sec; 0.462 sec/batch)
2016-12-08 08:39:39.545501: step 2490, loss = 1.65 (319.2 examples/sec; 0.401 sec/batch)
2016-12-08 08:39:43.958172: step 2500, loss = 1.26 (247.4 examples/sec; 0.517 sec/batch)
2016-12-08 08:39:48.594863: step 2510, loss = 1.40 (318.9 examples/sec; 0.401 sec/batch)
2016-12-08 08:39:52.818121: step 2520, loss = 1.37 (306.8 examples/sec; 0.417 sec/batch)
2016-12-08 08:39:57.031192: step 2530, loss = 1.40 (377.8 examples/sec; 0.339 sec/batch)
2016-12-08 08:40:01.241646: step 2540, loss = 1.37 (319.2 examples/sec; 0.401 sec/batch)
2016-12-08 08:40:05.621898: step 2550, loss = 1.47 (265.9 examples/sec; 0.481 sec/batch)
2016-12-08 08:40:09.806562: step 2560, loss = 1.63 (336.6 examples/sec; 0.380 sec/batch)
2016-12-08 08:40:13.909204: step 2570, loss = 1.47 (368.6 examples/sec; 0.347 sec/batch)
2016-12-08 08:40:18.132877: step 2580, loss = 1.31 (309.3 examples/sec; 0.414 sec/batch)
2016-12-08 08:40:22.551500: step 2590, loss = 1.50 (276.8 examples/sec; 0.462 sec/batch)
2016-12-08 08:40:26.557268: step 2600, loss = 1.47 (299.8 examples/sec; 0.427 sec/batch)
2016-12-08 08:40:31.182963: step 2610, loss = 1.39 (343.8 examples/sec; 0.372 sec/batch)
2016-12-08 08:40:35.291856: step 2620, loss = 1.27 (326.7 examples/sec; 0.392 sec/batch)
2016-12-08 08:40:39.446402: step 2630, loss = 1.32 (398.8 examples/sec; 0.321 sec/batch)
2016-12-08 08:40:43.987825: step 2640, loss = 1.39 (273.7 examples/sec; 0.468 sec/batch)
2016-12-08 08:40:48.394995: step 2650, loss = 1.41 (331.8 examples/sec; 0.386 sec/batch)
2016-12-08 08:40:52.542550: step 2660, loss = 1.31 (336.0 examples/sec; 0.381 sec/batch)
2016-12-08 08:40:56.743393: step 2670, loss = 1.34 (267.5 examples/sec; 0.479 sec/batch)
2016-12-08 08:41:01.060946: step 2680, loss = 1.42 (268.5 examples/sec; 0.477 sec/batch)
2016-12-08 08:41:05.054630: step 2690, loss = 1.38 (315.7 examples/sec; 0.405 sec/batch)
2016-12-08 08:41:09.454733: step 2700, loss = 1.30 (287.5 examples/sec; 0.445 sec/batch)
2016-12-08 08:41:14.127579: step 2710, loss = 1.25 (336.8 examples/sec; 0.380 sec/batch)
2016-12-08 08:41:18.680368: step 2720, loss = 1.25 (274.9 examples/sec; 0.466 sec/batch)
2016-12-08 08:41:22.920556: step 2730, loss = 1.56 (290.1 examples/sec; 0.441 sec/batch)
2016-12-08 08:41:27.423815: step 2740, loss = 1.43 (288.5 examples/sec; 0.444 sec/batch)
2016-12-08 08:41:31.678187: step 2750, loss = 1.39 (300.5 examples/sec; 0.426 sec/batch)
2016-12-08 08:41:36.304833: step 2760, loss = 1.32 (276.8 examples/sec; 0.462 sec/batch)
2016-12-08 08:41:40.846568: step 2770, loss = 1.14 (310.3 examples/sec; 0.413 sec/batch)
2016-12-08 08:41:45.470166: step 2780, loss = 1.28 (241.0 examples/sec; 0.531 sec/batch)
2016-12-08 08:41:49.634907: step 2790, loss = 1.09 (320.1 examples/sec; 0.400 sec/batch)
2016-12-08 08:41:54.063334: step 2800, loss = 1.42 (333.8 examples/sec; 0.383 sec/batch)
2016-12-08 08:41:59.023354: step 2810, loss = 1.25 (319.1 examples/sec; 0.401 sec/batch)
2016-12-08 08:42:03.126611: step 2820, loss = 1.38 (311.0 examples/sec; 0.412 sec/batch)
2016-12-08 08:42:07.504194: step 2830, loss = 1.37 (356.3 examples/sec; 0.359 sec/batch)
2016-12-08 08:42:11.748059: step 2840, loss = 1.48 (278.2 examples/sec; 0.460 sec/batch)
2016-12-08 08:42:16.120362: step 2850, loss = 1.34 (315.7 examples/sec; 0.405 sec/batch)
2016-12-08 08:42:20.185516: step 2860, loss = 1.36 (330.4 examples/sec; 0.387 sec/batch)
2016-12-08 08:42:24.514095: step 2870, loss = 1.46 (271.1 examples/sec; 0.472 sec/batch)
2016-12-08 08:42:28.729960: step 2880, loss = 1.39 (322.5 examples/sec; 0.397 sec/batch)
2016-12-08 08:42:33.065535: step 2890, loss = 1.40 (277.1 examples/sec; 0.462 sec/batch)
2016-12-08 08:42:37.263576: step 2900, loss = 1.37 (329.5 examples/sec; 0.388 sec/batch)
2016-12-08 08:42:42.062651: step 2910, loss = 1.43 (259.7 examples/sec; 0.493 sec/batch)
2016-12-08 08:42:46.499332: step 2920, loss = 1.28 (276.7 examples/sec; 0.463 sec/batch)
2016-12-08 08:42:50.978052: step 2930, loss = 1.31 (294.0 examples/sec; 0.435 sec/batch)
2016-12-08 08:42:55.224563: step 2940, loss = 1.28 (363.7 examples/sec; 0.352 sec/batch)
2016-12-08 08:42:59.700651: step 2950, loss = 1.28 (277.6 examples/sec; 0.461 sec/batch)
2016-12-08 08:43:04.005185: step 2960, loss = 1.42 (317.7 examples/sec; 0.403 sec/batch)
2016-12-08 08:43:08.486803: step 2970, loss = 1.42 (276.5 examples/sec; 0.463 sec/batch)
2016-12-08 08:43:12.745840: step 2980, loss = 1.08 (341.3 examples/sec; 0.375 sec/batch)
2016-12-08 08:43:17.005191: step 2990, loss = 1.34 (332.5 examples/sec; 0.385 sec/batch)
2016-12-08 08:43:21.274453: step 3000, loss = 1.27 (319.2 examples/sec; 0.401 sec/batch)
2016-12-08 08:43:26.862664: step 3010, loss = 1.38 (312.8 examples/sec; 0.409 sec/batch)
2016-12-08 08:43:31.464302: step 3020, loss = 1.13 (331.6 examples/sec; 0.386 sec/batch)
2016-12-08 08:43:35.945457: step 3030, loss = 1.32 (272.7 examples/sec; 0.469 sec/batch)
2016-12-08 08:43:40.324135: step 3040, loss = 1.31 (261.7 examples/sec; 0.489 sec/batch)
2016-12-08 08:43:44.418414: step 3050, loss = 1.19 (347.5 examples/sec; 0.368 sec/batch)
2016-12-08 08:43:49.075263: step 3060, loss = 1.20 (273.6 examples/sec; 0.468 sec/batch)
2016-12-08 08:43:53.490153: step 3070, loss = 1.19 (329.3 examples/sec; 0.389 sec/batch)
2016-12-08 08:43:57.727572: step 3080, loss = 1.27 (353.4 examples/sec; 0.362 sec/batch)
2016-12-08 08:44:01.966275: step 3090, loss = 1.33 (327.1 examples/sec; 0.391 sec/batch)
2016-12-08 08:44:06.314588: step 3100, loss = 1.21 (287.8 examples/sec; 0.445 sec/batch)
2016-12-08 08:44:11.020756: step 3110, loss = 1.30 (311.4 examples/sec; 0.411 sec/batch)
2016-12-08 08:44:15.452177: step 3120, loss = 1.16 (322.2 examples/sec; 0.397 sec/batch)
2016-12-08 08:44:19.501503: step 3130, loss = 1.16 (284.8 examples/sec; 0.449 sec/batch)
2016-12-08 08:44:23.655357: step 3140, loss = 1.28 (324.8 examples/sec; 0.394 sec/batch)
2016-12-08 08:44:28.318313: step 3150, loss = 1.37 (264.4 examples/sec; 0.484 sec/batch)
2016-12-08 08:44:32.858648: step 3160, loss = 1.06 (263.4 examples/sec; 0.486 sec/batch)
2016-12-08 08:44:36.771125: step 3170, loss = 1.23 (332.0 examples/sec; 0.386 sec/batch)
2016-12-08 08:44:41.277124: step 3180, loss = 1.28 (288.3 examples/sec; 0.444 sec/batch)
2016-12-08 08:44:45.411011: step 3190, loss = 1.30 (295.8 examples/sec; 0.433 sec/batch)
2016-12-08 08:44:49.760819: step 3200, loss = 1.07 (291.7 examples/sec; 0.439 sec/batch)
2016-12-08 08:44:54.990759: step 3210, loss = 1.19 (290.3 examples/sec; 0.441 sec/batch)
2016-12-08 08:44:59.513438: step 3220, loss = 1.20 (274.4 examples/sec; 0.466 sec/batch)
2016-12-08 08:45:04.034846: step 3230, loss = 1.29 (272.1 examples/sec; 0.470 sec/batch)
2016-12-08 08:45:08.325333: step 3240, loss = 1.20 (277.3 examples/sec; 0.462 sec/batch)
2016-12-08 08:45:12.608581: step 3250, loss = 1.34 (291.7 examples/sec; 0.439 sec/batch)
2016-12-08 08:45:16.977945: step 3260, loss = 1.12 (293.0 examples/sec; 0.437 sec/batch)
2016-12-08 08:45:21.383546: step 3270, loss = 1.25 (263.7 examples/sec; 0.485 sec/batch)
2016-12-08 08:45:25.897772: step 3280, loss = 1.26 (262.3 examples/sec; 0.488 sec/batch)
2016-12-08 08:45:30.363708: step 3290, loss = 1.22 (330.3 examples/sec; 0.387 sec/batch)
2016-12-08 08:45:34.529760: step 3300, loss = 1.29 (283.1 examples/sec; 0.452 sec/batch)
2016-12-08 08:45:39.513200: step 3310, loss = 1.21 (270.9 examples/sec; 0.473 sec/batch)
2016-12-08 08:45:43.871961: step 3320, loss = 0.99 (298.0 examples/sec; 0.430 sec/batch)
2016-12-08 08:45:48.420356: step 3330, loss = 1.28 (269.2 examples/sec; 0.476 sec/batch)
2016-12-08 08:45:52.673680: step 3340, loss = 1.20 (289.3 examples/sec; 0.442 sec/batch)
2016-12-08 08:45:57.093267: step 3350, loss = 1.19 (306.0 examples/sec; 0.418 sec/batch)
2016-12-08 08:46:01.652580: step 3360, loss = 1.38 (248.7 examples/sec; 0.515 sec/batch)
2016-12-08 08:46:05.762822: step 3370, loss = 1.17 (295.7 examples/sec; 0.433 sec/batch)
2016-12-08 08:46:10.009586: step 3380, loss = 1.08 (254.8 examples/sec; 0.502 sec/batch)
2016-12-08 08:46:14.446562: step 3390, loss = 1.27 (243.1 examples/sec; 0.527 sec/batch)
2016-12-08 08:46:18.775074: step 3400, loss = 1.31 (287.5 examples/sec; 0.445 sec/batch)
2016-12-08 08:46:23.693428: step 3410, loss = 1.10 (273.8 examples/sec; 0.468 sec/batch)
2016-12-08 08:46:27.880116: step 3420, loss = 1.24 (389.0 examples/sec; 0.329 sec/batch)
2016-12-08 08:46:32.038754: step 3430, loss = 1.12 (294.3 examples/sec; 0.435 sec/batch)
2016-12-08 08:46:36.489091: step 3440, loss = 1.18 (282.3 examples/sec; 0.453 sec/batch)
2016-12-08 08:46:40.822043: step 3450, loss = 1.32 (303.9 examples/sec; 0.421 sec/batch)
2016-12-08 08:46:45.048581: step 3460, loss = 1.18 (298.3 examples/sec; 0.429 sec/batch)
2016-12-08 08:46:49.248990: step 3470, loss = 1.20 (298.4 examples/sec; 0.429 sec/batch)
2016-12-08 08:46:53.392617: step 3480, loss = 1.06 (348.2 examples/sec; 0.368 sec/batch)
2016-12-08 08:46:57.700401: step 3490, loss = 1.08 (319.3 examples/sec; 0.401 sec/batch)
2016-12-08 08:47:01.997360: step 3500, loss = 1.10 (280.1 examples/sec; 0.457 sec/batch)
2016-12-08 08:47:07.021305: step 3510, loss = 1.07 (309.9 examples/sec; 0.413 sec/batch)
2016-12-08 08:47:11.513527: step 3520, loss = 1.12 (272.4 examples/sec; 0.470 sec/batch)
2016-12-08 08:47:15.719595: step 3530, loss = 1.15 (315.5 examples/sec; 0.406 sec/batch)
2016-12-08 08:47:20.072327: step 3540, loss = 1.24 (289.9 examples/sec; 0.442 sec/batch)
2016-12-08 08:47:24.323066: step 3550, loss = 1.25 (295.3 examples/sec; 0.433 sec/batch)
2016-12-08 08:47:28.884647: step 3560, loss = 1.21 (265.0 examples/sec; 0.483 sec/batch)
2016-12-08 08:47:33.131192: step 3570, loss = 1.20 (332.4 examples/sec; 0.385 sec/batch)
2016-12-08 08:47:37.421883: step 3580, loss = 1.29 (326.9 examples/sec; 0.392 sec/batch)
2016-12-08 08:47:41.713549: step 3590, loss = 1.12 (331.4 examples/sec; 0.386 sec/batch)
2016-12-08 08:47:46.109267: step 3600, loss = 1.08 (245.2 examples/sec; 0.522 sec/batch)
2016-12-08 08:47:50.633214: step 3610, loss = 1.60 (316.2 examples/sec; 0.405 sec/batch)
2016-12-08 08:47:54.759156: step 3620, loss = 1.16 (287.7 examples/sec; 0.445 sec/batch)
2016-12-08 08:47:59.373939: step 3630, loss = 1.30 (288.2 examples/sec; 0.444 sec/batch)
2016-12-08 08:48:03.365990: step 3640, loss = 1.15 (300.7 examples/sec; 0.426 sec/batch)
2016-12-08 08:48:07.833836: step 3650, loss = 1.29 (345.2 examples/sec; 0.371 sec/batch)
2016-12-08 08:48:12.027790: step 3660, loss = 1.09 (337.5 examples/sec; 0.379 sec/batch)
2016-12-08 08:48:16.329162: step 3670, loss = 1.20 (328.5 examples/sec; 0.390 sec/batch)
2016-12-08 08:48:20.575332: step 3680, loss = 1.14 (305.2 examples/sec; 0.419 sec/batch)
2016-12-08 08:48:24.580587: step 3690, loss = 1.24 (260.9 examples/sec; 0.491 sec/batch)
2016-12-08 08:48:28.889987: step 3700, loss = 0.96 (319.5 examples/sec; 0.401 sec/batch)
2016-12-08 08:48:33.571379: step 3710, loss = 1.17 (319.2 examples/sec; 0.401 sec/batch)
2016-12-08 08:48:37.526074: step 3720, loss = 1.19 (342.8 examples/sec; 0.373 sec/batch)
2016-12-08 08:48:41.890289: step 3730, loss = 1.29 (371.9 examples/sec; 0.344 sec/batch)
2016-12-08 08:48:46.385351: step 3740, loss = 0.98 (280.6 examples/sec; 0.456 sec/batch)
2016-12-08 08:48:50.444496: step 3750, loss = 1.16 (312.0 examples/sec; 0.410 sec/batch)
2016-12-08 08:48:54.826945: step 3760, loss = 1.15 (268.3 examples/sec; 0.477 sec/batch)
2016-12-08 08:48:58.813553: step 3770, loss = 1.11 (286.4 examples/sec; 0.447 sec/batch)
2016-12-08 08:49:03.136690: step 3780, loss = 1.22 (255.5 examples/sec; 0.501 sec/batch)
2016-12-08 08:49:07.589746: step 3790, loss = 1.23 (284.4 examples/sec; 0.450 sec/batch)
2016-12-08 08:49:12.154768: step 3800, loss = 1.32 (281.0 examples/sec; 0.456 sec/batch)
2016-12-08 08:49:16.892730: step 3810, loss = 1.38 (291.6 examples/sec; 0.439 sec/batch)
2016-12-08 08:49:21.393251: step 3820, loss = 1.21 (311.1 examples/sec; 0.411 sec/batch)
2016-12-08 08:49:25.819506: step 3830, loss = 0.92 (301.8 examples/sec; 0.424 sec/batch)
2016-12-08 08:49:29.958884: step 3840, loss = 1.00 (319.7 examples/sec; 0.400 sec/batch)
2016-12-08 08:49:34.287779: step 3850, loss = 1.21 (337.4 examples/sec; 0.379 sec/batch)
2016-12-08 08:49:38.429686: step 3860, loss = 1.01 (287.7 examples/sec; 0.445 sec/batch)
2016-12-08 08:49:42.591061: step 3870, loss = 1.19 (324.6 examples/sec; 0.394 sec/batch)
2016-12-08 08:49:46.904244: step 3880, loss = 1.26 (298.0 examples/sec; 0.430 sec/batch)
2016-12-08 08:49:51.422833: step 3890, loss = 1.14 (265.0 examples/sec; 0.483 sec/batch)
2016-12-08 08:49:55.611012: step 3900, loss = 1.10 (388.2 examples/sec; 0.330 sec/batch)
2016-12-08 08:50:00.434700: step 3910, loss = 1.29 (309.0 examples/sec; 0.414 sec/batch)
2016-12-08 08:50:04.896201: step 3920, loss = 0.95 (291.5 examples/sec; 0.439 sec/batch)
2016-12-08 08:50:09.008986: step 3930, loss = 1.17 (289.8 examples/sec; 0.442 sec/batch)
2016-12-08 08:50:13.236869: step 3940, loss = 1.05 (306.1 examples/sec; 0.418 sec/batch)
2016-12-08 08:50:17.852273: step 3950, loss = 1.32 (246.8 examples/sec; 0.519 sec/batch)
2016-12-08 08:50:22.302819: step 3960, loss = 1.14 (289.1 examples/sec; 0.443 sec/batch)
2016-12-08 08:50:26.484859: step 3970, loss = 1.08 (385.5 examples/sec; 0.332 sec/batch)
2016-12-08 08:50:30.510927: step 3980, loss = 1.09 (329.4 examples/sec; 0.389 sec/batch)
2016-12-08 08:50:34.511312: step 3990, loss = 1.13 (336.4 examples/sec; 0.380 sec/batch)
2016-12-08 08:50:38.980771: step 4000, loss = 1.19 (310.7 examples/sec; 0.412 sec/batch)
2016-12-08 08:50:44.114913: step 4010, loss = 1.02 (295.5 examples/sec; 0.433 sec/batch)
2016-12-08 08:50:48.496215: step 4020, loss = 1.08 (264.4 examples/sec; 0.484 sec/batch)
2016-12-08 08:50:52.951705: step 4030, loss = 1.09 (263.0 examples/sec; 0.487 sec/batch)
2016-12-08 08:50:57.472690: step 4040, loss = 1.18 (305.4 examples/sec; 0.419 sec/batch)
2016-12-08 08:51:01.574002: step 4050, loss = 1.08 (316.5 examples/sec; 0.404 sec/batch)
2016-12-08 08:51:05.575911: step 4060, loss = 1.22 (280.7 examples/sec; 0.456 sec/batch)
2016-12-08 08:51:09.878414: step 4070, loss = 1.05 (255.6 examples/sec; 0.501 sec/batch)
2016-12-08 08:51:14.199106: step 4080, loss = 1.17 (297.3 examples/sec; 0.431 sec/batch)
2016-12-08 08:51:18.570739: step 4090, loss = 1.06 (268.4 examples/sec; 0.477 sec/batch)
2016-12-08 08:51:22.853139: step 4100, loss = 1.07 (286.1 examples/sec; 0.447 sec/batch)
2016-12-08 08:51:27.661342: step 4110, loss = 1.04 (300.6 examples/sec; 0.426 sec/batch)
2016-12-08 08:51:32.268756: step 4120, loss = 1.25 (299.9 examples/sec; 0.427 sec/batch)
2016-12-08 08:51:36.506313: step 4130, loss = 1.02 (300.7 examples/sec; 0.426 sec/batch)
2016-12-08 08:51:40.744487: step 4140, loss = 1.08 (365.2 examples/sec; 0.351 sec/batch)
2016-12-08 08:51:44.750983: step 4150, loss = 1.14 (359.8 examples/sec; 0.356 sec/batch)
2016-12-08 08:51:48.801700: step 4160, loss = 1.21 (315.9 examples/sec; 0.405 sec/batch)
2016-12-08 08:51:53.159895: step 4170, loss = 1.05 (255.6 examples/sec; 0.501 sec/batch)
2016-12-08 08:51:57.512569: step 4180, loss = 1.15 (313.7 examples/sec; 0.408 sec/batch)
2016-12-08 08:52:01.587237: step 4190, loss = 1.01 (323.1 examples/sec; 0.396 sec/batch)
2016-12-08 08:52:06.068572: step 4200, loss = 0.94 (282.6 examples/sec; 0.453 sec/batch)
2016-12-08 08:52:10.774069: step 4210, loss = 1.17 (317.7 examples/sec; 0.403 sec/batch)
2016-12-08 08:52:14.829069: step 4220, loss = 1.16 (378.1 examples/sec; 0.339 sec/batch)
2016-12-08 08:52:19.001341: step 4230, loss = 1.19 (286.2 examples/sec; 0.447 sec/batch)
2016-12-08 08:52:23.039210: step 4240, loss = 1.30 (348.6 examples/sec; 0.367 sec/batch)
2016-12-08 08:52:27.073236: step 4250, loss = 1.01 (284.3 examples/sec; 0.450 sec/batch)
2016-12-08 08:52:31.099340: step 4260, loss = 0.96 (251.7 examples/sec; 0.509 sec/batch)
2016-12-08 08:52:35.453474: step 4270, loss = 1.11 (256.1 examples/sec; 0.500 sec/batch)
2016-12-08 08:52:40.024960: step 4280, loss = 1.05 (270.6 examples/sec; 0.473 sec/batch)
2016-12-08 08:52:44.419874: step 4290, loss = 1.09 (256.4 examples/sec; 0.499 sec/batch)
2016-12-08 08:52:48.726439: step 4300, loss = 1.23 (292.1 examples/sec; 0.438 sec/batch)
2016-12-08 08:52:53.866423: step 4310, loss = 1.18 (286.4 examples/sec; 0.447 sec/batch)
2016-12-08 08:52:58.028040: step 4320, loss = 1.14 (305.3 examples/sec; 0.419 sec/batch)
2016-12-08 08:53:02.452494: step 4330, loss = 1.06 (319.2 examples/sec; 0.401 sec/batch)
2016-12-08 08:53:06.497954: step 4340, loss = 0.95 (342.9 examples/sec; 0.373 sec/batch)
2016-12-08 08:53:10.759941: step 4350, loss = 1.03 (277.5 examples/sec; 0.461 sec/batch)
2016-12-08 08:53:15.113311: step 4360, loss = 1.25 (313.2 examples/sec; 0.409 sec/batch)
2016-12-08 08:53:19.502034: step 4370, loss = 1.16 (274.4 examples/sec; 0.466 sec/batch)
2016-12-08 08:53:23.531720: step 4380, loss = 0.99 (341.1 examples/sec; 0.375 sec/batch)
2016-12-08 08:53:27.659345: step 4390, loss = 1.03 (262.6 examples/sec; 0.487 sec/batch)
2016-12-08 08:53:32.046377: step 4400, loss = 1.13 (291.6 examples/sec; 0.439 sec/batch)
2016-12-08 08:53:36.770479: step 4410, loss = 0.92 (285.7 examples/sec; 0.448 sec/batch)
2016-12-08 08:53:40.963790: step 4420, loss = 1.07 (278.4 examples/sec; 0.460 sec/batch)
2016-12-08 08:53:45.550223: step 4430, loss = 1.08 (257.7 examples/sec; 0.497 sec/batch)
2016-12-08 08:53:50.036360: step 4440, loss = 1.05 (283.5 examples/sec; 0.451 sec/batch)
2016-12-08 08:53:54.044894: step 4450, loss = 1.20 (291.4 examples/sec; 0.439 sec/batch)
2016-12-08 08:53:58.579696: step 4460, loss = 1.09 (324.7 examples/sec; 0.394 sec/batch)
2016-12-08 08:54:02.840326: step 4470, loss = 1.31 (323.2 examples/sec; 0.396 sec/batch)
2016-12-08 08:54:07.384004: step 4480, loss = 0.99 (332.8 examples/sec; 0.385 sec/batch)
2016-12-08 08:54:11.592998: step 4490, loss = 0.94 (360.4 examples/sec; 0.355 sec/batch)
2016-12-08 08:54:15.629609: step 4500, loss = 0.92 (306.8 examples/sec; 0.417 sec/batch)
2016-12-08 08:54:20.535000: step 4510, loss = 1.06 (262.1 examples/sec; 0.488 sec/batch)
2016-12-08 08:54:25.409522: step 4520, loss = 0.99 (299.2 examples/sec; 0.428 sec/batch)
2016-12-08 08:54:29.810628: step 4530, loss = 1.01 (290.1 examples/sec; 0.441 sec/batch)
2016-12-08 08:54:34.058989: step 4540, loss = 0.97 (290.7 examples/sec; 0.440 sec/batch)
2016-12-08 08:54:38.037464: step 4550, loss = 1.08 (325.8 examples/sec; 0.393 sec/batch)
2016-12-08 08:54:42.405586: step 4560, loss = 1.12 (289.3 examples/sec; 0.442 sec/batch)
2016-12-08 08:54:46.724445: step 4570, loss = 1.08 (252.6 examples/sec; 0.507 sec/batch)
2016-12-08 08:54:51.133777: step 4580, loss = 0.99 (252.9 examples/sec; 0.506 sec/batch)
2016-12-08 08:54:55.579261: step 4590, loss = 0.95 (327.2 examples/sec; 0.391 sec/batch)
2016-12-08 08:54:59.751141: step 4600, loss = 1.21 (288.5 examples/sec; 0.444 sec/batch)
2016-12-08 08:55:04.720833: step 4610, loss = 1.28 (281.3 examples/sec; 0.455 sec/batch)
2016-12-08 08:55:08.917327: step 4620, loss = 1.19 (286.6 examples/sec; 0.447 sec/batch)
2016-12-08 08:55:13.397077: step 4630, loss = 1.04 (281.8 examples/sec; 0.454 sec/batch)
2016-12-08 08:55:17.522356: step 4640, loss = 1.10 (277.5 examples/sec; 0.461 sec/batch)
2016-12-08 08:55:22.093164: step 4650, loss = 1.07 (261.1 examples/sec; 0.490 sec/batch)
2016-12-08 08:55:26.690640: step 4660, loss = 1.10 (259.9 examples/sec; 0.493 sec/batch)
2016-12-08 08:55:30.816834: step 4670, loss = 0.90 (319.8 examples/sec; 0.400 sec/batch)
2016-12-08 08:55:35.055910: step 4680, loss = 0.90 (270.0 examples/sec; 0.474 sec/batch)
2016-12-08 08:55:39.300989: step 4690, loss = 1.04 (276.4 examples/sec; 0.463 sec/batch)
2016-12-08 08:55:43.503412: step 4700, loss = 1.13 (300.3 examples/sec; 0.426 sec/batch)
2016-12-08 08:55:48.102040: step 4710, loss = 0.99 (343.5 examples/sec; 0.373 sec/batch)
2016-12-08 08:55:52.382470: step 4720, loss = 1.04 (312.3 examples/sec; 0.410 sec/batch)
2016-12-08 08:55:56.805951: step 4730, loss = 0.96 (315.3 examples/sec; 0.406 sec/batch)
2016-12-08 08:56:01.431884: step 4740, loss = 1.02 (302.1 examples/sec; 0.424 sec/batch)
2016-12-08 08:56:05.677656: step 4750, loss = 1.06 (279.8 examples/sec; 0.457 sec/batch)
2016-12-08 08:56:09.878640: step 4760, loss = 1.21 (311.5 examples/sec; 0.411 sec/batch)
2016-12-08 08:56:14.254596: step 4770, loss = 0.98 (283.3 examples/sec; 0.452 sec/batch)
2016-12-08 08:56:18.330693: step 4780, loss = 0.97 (339.0 examples/sec; 0.378 sec/batch)
2016-12-08 08:56:22.464607: step 4790, loss = 1.07 (351.0 examples/sec; 0.365 sec/batch)
2016-12-08 08:56:27.165022: step 4800, loss = 1.13 (265.3 examples/sec; 0.483 sec/batch)
2016-12-08 08:56:31.939519: step 4810, loss = 0.96 (310.7 examples/sec; 0.412 sec/batch)
2016-12-08 08:56:36.008635: step 4820, loss = 1.27 (302.8 examples/sec; 0.423 sec/batch)
2016-12-08 08:56:40.099652: step 4830, loss = 1.15 (349.8 examples/sec; 0.366 sec/batch)
2016-12-08 08:56:44.678077: step 4840, loss = 1.06 (258.4 examples/sec; 0.495 sec/batch)
2016-12-08 08:56:49.189915: step 4850, loss = 0.98 (305.6 examples/sec; 0.419 sec/batch)
2016-12-08 08:56:53.400145: step 4860, loss = 1.05 (297.6 examples/sec; 0.430 sec/batch)
2016-12-08 08:56:57.886025: step 4870, loss = 1.15 (301.5 examples/sec; 0.425 sec/batch)
2016-12-08 08:57:02.268654: step 4880, loss = 1.15 (334.6 examples/sec; 0.383 sec/batch)
2016-12-08 08:57:06.483035: step 4890, loss = 0.95 (288.2 examples/sec; 0.444 sec/batch)
2016-12-08 08:57:10.819749: step 4900, loss = 1.01 (283.6 examples/sec; 0.451 sec/batch)
2016-12-08 08:57:15.352247: step 4910, loss = 0.95 (286.6 examples/sec; 0.447 sec/batch)
2016-12-08 08:57:19.456869: step 4920, loss = 0.95 (341.9 examples/sec; 0.374 sec/batch)
2016-12-08 08:57:23.821578: step 4930, loss = 1.09 (255.5 examples/sec; 0.501 sec/batch)
2016-12-08 08:57:28.054158: step 4940, loss = 1.07 (285.2 examples/sec; 0.449 sec/batch)
2016-12-08 08:57:32.292115: step 4950, loss = 1.12 (279.2 examples/sec; 0.458 sec/batch)
2016-12-08 08:57:36.915137: step 4960, loss = 0.96 (251.3 examples/sec; 0.509 sec/batch)
2016-12-08 08:57:41.429372: step 4970, loss = 1.07 (309.7 examples/sec; 0.413 sec/batch)
2016-12-08 08:57:45.454051: step 4980, loss = 0.95 (315.7 examples/sec; 0.405 sec/batch)
2016-12-08 08:57:49.963413: step 4990, loss = 0.92 (245.3 examples/sec; 0.522 sec/batch)
2016-12-08 08:57:54.295039: step 5000, loss = 1.09 (278.8 examples/sec; 0.459 sec/batch)
2016-12-08 08:57:59.567981: step 5010, loss = 0.97 (340.9 examples/sec; 0.376 sec/batch)
2016-12-08 08:58:03.709122: step 5020, loss = 1.19 (297.5 examples/sec; 0.430 sec/batch)
2016-12-08 08:58:07.990286: step 5030, loss = 1.05 (322.2 examples/sec; 0.397 sec/batch)
2016-12-08 08:58:12.334947: step 5040, loss = 1.13 (268.7 examples/sec; 0.476 sec/batch)
2016-12-08 08:58:16.604484: step 5050, loss = 0.86 (323.6 examples/sec; 0.396 sec/batch)
2016-12-08 08:58:20.726500: step 5060, loss = 1.05 (418.6 examples/sec; 0.306 sec/batch)
2016-12-08 08:58:25.016833: step 5070, loss = 1.12 (274.6 examples/sec; 0.466 sec/batch)
2016-12-08 08:58:29.111655: step 5080, loss = 1.14 (380.3 examples/sec; 0.337 sec/batch)
2016-12-08 08:58:33.410734: step 5090, loss = 1.04 (273.7 examples/sec; 0.468 sec/batch)
2016-12-08 08:58:37.830845: step 5100, loss = 1.05 (277.1 examples/sec; 0.462 sec/batch)
2016-12-08 08:58:42.765924: step 5110, loss = 1.06 (283.2 examples/sec; 0.452 sec/batch)
2016-12-08 08:58:47.022015: step 5120, loss = 1.06 (382.9 examples/sec; 0.334 sec/batch)
2016-12-08 08:58:51.350531: step 5130, loss = 0.99 (286.6 examples/sec; 0.447 sec/batch)
2016-12-08 08:58:55.662979: step 5140, loss = 1.19 (305.7 examples/sec; 0.419 sec/batch)
2016-12-08 08:58:59.923605: step 5150, loss = 1.02 (270.7 examples/sec; 0.473 sec/batch)
2016-12-08 08:59:03.995245: step 5160, loss = 0.98 (288.4 examples/sec; 0.444 sec/batch)
2016-12-08 08:59:08.185809: step 5170, loss = 1.07 (335.2 examples/sec; 0.382 sec/batch)
2016-12-08 08:59:12.696307: step 5180, loss = 1.09 (300.2 examples/sec; 0.426 sec/batch)
2016-12-08 08:59:16.928555: step 5190, loss = 1.12 (330.3 examples/sec; 0.388 sec/batch)
2016-12-08 08:59:21.200809: step 5200, loss = 0.92 (285.3 examples/sec; 0.449 sec/batch)
2016-12-08 08:59:25.998716: step 5210, loss = 1.08 (279.8 examples/sec; 0.458 sec/batch)
2016-12-08 08:59:30.383731: step 5220, loss = 1.10 (341.9 examples/sec; 0.374 sec/batch)
2016-12-08 08:59:34.504381: step 5230, loss = 1.06 (288.0 examples/sec; 0.444 sec/batch)
2016-12-08 08:59:38.784431: step 5240, loss = 1.15 (386.3 examples/sec; 0.331 sec/batch)
2016-12-08 08:59:43.223563: step 5250, loss = 0.94 (289.8 examples/sec; 0.442 sec/batch)
2016-12-08 08:59:47.719730: step 5260, loss = 0.96 (289.6 examples/sec; 0.442 sec/batch)
2016-12-08 08:59:52.185428: step 5270, loss = 0.93 (301.8 examples/sec; 0.424 sec/batch)
2016-12-08 08:59:56.597057: step 5280, loss = 0.97 (293.2 examples/sec; 0.437 sec/batch)
2016-12-08 09:00:00.792429: step 5290, loss = 0.95 (286.4 examples/sec; 0.447 sec/batch)
2016-12-08 09:00:05.013688: step 5300, loss = 0.85 (322.9 examples/sec; 0.396 sec/batch)
2016-12-08 09:00:09.872162: step 5310, loss = 1.00 (277.7 examples/sec; 0.461 sec/batch)
2016-12-08 09:00:14.357570: step 5320, loss = 0.94 (266.7 examples/sec; 0.480 sec/batch)
2016-12-08 09:00:18.635616: step 5330, loss = 0.85 (305.3 examples/sec; 0.419 sec/batch)
2016-12-08 09:00:23.115093: step 5340, loss = 0.98 (259.8 examples/sec; 0.493 sec/batch)
2016-12-08 09:00:27.434987: step 5350, loss = 1.11 (296.7 examples/sec; 0.431 sec/batch)
2016-12-08 09:00:31.696469: step 5360, loss = 1.10 (355.0 examples/sec; 0.361 sec/batch)
2016-12-08 09:00:36.063531: step 5370, loss = 0.98 (247.0 examples/sec; 0.518 sec/batch)
2016-12-08 09:00:40.158959: step 5380, loss = 1.08 (279.1 examples/sec; 0.459 sec/batch)
2016-12-08 09:00:44.538209: step 5390, loss = 1.05 (344.2 examples/sec; 0.372 sec/batch)
2016-12-08 09:00:48.678330: step 5400, loss = 0.78 (348.9 examples/sec; 0.367 sec/batch)
2016-12-08 09:00:53.613428: step 5410, loss = 0.97 (284.1 examples/sec; 0.451 sec/batch)
2016-12-08 09:00:57.906435: step 5420, loss = 0.94 (306.0 examples/sec; 0.418 sec/batch)
2016-12-08 09:01:02.246262: step 5430, loss = 1.15 (275.3 examples/sec; 0.465 sec/batch)
2016-12-08 09:01:06.854427: step 5440, loss = 1.13 (262.0 examples/sec; 0.489 sec/batch)
2016-12-08 09:01:11.117516: step 5450, loss = 1.09 (396.0 examples/sec; 0.323 sec/batch)
2016-12-08 09:01:15.070084: step 5460, loss = 1.02 (298.2 examples/sec; 0.429 sec/batch)
2016-12-08 09:01:19.263657: step 5470, loss = 1.16 (309.0 examples/sec; 0.414 sec/batch)
2016-12-08 09:01:23.775124: step 5480, loss = 1.01 (290.6 examples/sec; 0.440 sec/batch)
2016-12-08 09:01:28.476763: step 5490, loss = 1.03 (291.5 examples/sec; 0.439 sec/batch)
2016-12-08 09:01:32.846330: step 5500, loss = 1.04 (293.6 examples/sec; 0.436 sec/batch)
2016-12-08 09:01:37.722868: step 5510, loss = 0.94 (261.3 examples/sec; 0.490 sec/batch)
2016-12-08 09:01:42.426778: step 5520, loss = 1.07 (279.3 examples/sec; 0.458 sec/batch)
2016-12-08 09:01:46.632902: step 5530, loss = 0.98 (317.3 examples/sec; 0.403 sec/batch)
2016-12-08 09:01:51.062434: step 5540, loss = 0.93 (291.9 examples/sec; 0.439 sec/batch)
2016-12-08 09:01:55.384068: step 5550, loss = 0.89 (299.9 examples/sec; 0.427 sec/batch)
2016-12-08 09:01:59.721780: step 5560, loss = 1.11 (276.3 examples/sec; 0.463 sec/batch)
2016-12-08 09:02:03.829203: step 5570, loss = 1.10 (291.3 examples/sec; 0.439 sec/batch)
2016-12-08 09:02:08.181385: step 5580, loss = 1.10 (265.3 examples/sec; 0.482 sec/batch)
2016-12-08 09:02:12.387283: step 5590, loss = 1.00 (278.2 examples/sec; 0.460 sec/batch)
2016-12-08 09:02:16.949498: step 5600, loss = 0.91 (317.0 examples/sec; 0.404 sec/batch)
2016-12-08 09:02:21.708643: step 5610, loss = 0.91 (335.2 examples/sec; 0.382 sec/batch)
2016-12-08 09:02:25.900311: step 5620, loss = 0.91 (296.7 examples/sec; 0.431 sec/batch)
2016-12-08 09:02:30.114236: step 5630, loss = 1.07 (281.1 examples/sec; 0.455 sec/batch)
2016-12-08 09:02:34.276835: step 5640, loss = 1.08 (303.8 examples/sec; 0.421 sec/batch)
2016-12-08 09:02:38.512461: step 5650, loss = 0.97 (309.6 examples/sec; 0.413 sec/batch)
2016-12-08 09:02:42.743189: step 5660, loss = 1.01 (309.4 examples/sec; 0.414 sec/batch)
2016-12-08 09:02:47.369773: step 5670, loss = 1.04 (273.9 examples/sec; 0.467 sec/batch)
2016-12-08 09:02:51.874400: step 5680, loss = 1.04 (276.4 examples/sec; 0.463 sec/batch)
2016-12-08 09:02:56.245389: step 5690, loss = 0.97 (284.9 examples/sec; 0.449 sec/batch)
2016-12-08 09:03:00.569144: step 5700, loss = 1.08 (270.3 examples/sec; 0.474 sec/batch)
2016-12-08 09:03:05.535656: step 5710, loss = 1.13 (334.2 examples/sec; 0.383 sec/batch)
2016-12-08 09:03:09.692674: step 5720, loss = 1.01 (317.8 examples/sec; 0.403 sec/batch)
2016-12-08 09:03:14.078930: step 5730, loss = 1.05 (335.4 examples/sec; 0.382 sec/batch)
2016-12-08 09:03:18.496313: step 5740, loss = 0.99 (295.9 examples/sec; 0.433 sec/batch)
2016-12-08 09:03:23.059216: step 5750, loss = 1.00 (289.3 examples/sec; 0.442 sec/batch)
2016-12-08 09:03:27.449935: step 5760, loss = 0.95 (307.7 examples/sec; 0.416 sec/batch)
2016-12-08 09:03:31.721762: step 5770, loss = 1.06 (314.2 examples/sec; 0.407 sec/batch)
2016-12-08 09:03:36.204662: step 5780, loss = 1.07 (364.2 examples/sec; 0.351 sec/batch)
2016-12-08 09:03:40.613418: step 5790, loss = 1.09 (316.1 examples/sec; 0.405 sec/batch)
2016-12-08 09:03:44.278360: step 5800, loss = 0.90 (316.3 examples/sec; 0.405 sec/batch)
2016-12-08 09:03:49.060831: step 5810, loss = 1.07 (359.5 examples/sec; 0.356 sec/batch)
2016-12-08 09:03:53.173437: step 5820, loss = 1.34 (276.6 examples/sec; 0.463 sec/batch)
2016-12-08 09:03:57.191041: step 5830, loss = 1.01 (301.0 examples/sec; 0.425 sec/batch)
2016-12-08 09:04:01.431550: step 5840, loss = 0.99 (315.4 examples/sec; 0.406 sec/batch)
2016-12-08 09:04:05.736156: step 5850, loss = 0.96 (351.1 examples/sec; 0.365 sec/batch)
2016-12-08 09:04:10.025475: step 5860, loss = 0.80 (343.6 examples/sec; 0.373 sec/batch)
2016-12-08 09:04:14.334363: step 5870, loss = 1.11 (353.4 examples/sec; 0.362 sec/batch)
2016-12-08 09:04:18.456726: step 5880, loss = 0.93 (322.1 examples/sec; 0.397 sec/batch)
2016-12-08 09:04:22.961222: step 5890, loss = 0.93 (267.7 examples/sec; 0.478 sec/batch)
2016-12-08 09:04:27.387675: step 5900, loss = 0.91 (278.1 examples/sec; 0.460 sec/batch)
2016-12-08 09:04:32.546710: step 5910, loss = 0.98 (259.6 examples/sec; 0.493 sec/batch)
2016-12-08 09:04:37.037441: step 5920, loss = 0.99 (281.8 examples/sec; 0.454 sec/batch)
2016-12-08 09:04:41.049253: step 5930, loss = 0.87 (314.0 examples/sec; 0.408 sec/batch)
2016-12-08 09:04:45.470475: step 5940, loss = 0.97 (272.7 examples/sec; 0.469 sec/batch)
2016-12-08 09:04:50.064157: step 5950, loss = 0.98 (265.7 examples/sec; 0.482 sec/batch)
2016-12-08 09:04:54.280598: step 5960, loss = 0.86 (311.9 examples/sec; 0.410 sec/batch)
2016-12-08 09:04:58.759874: step 5970, loss = 0.96 (322.2 examples/sec; 0.397 sec/batch)
2016-12-08 09:05:03.082510: step 5980, loss = 1.05 (280.2 examples/sec; 0.457 sec/batch)
2016-12-08 09:05:07.321151: step 5990, loss = 1.23 (355.9 examples/sec; 0.360 sec/batch)
2016-12-08 09:05:11.651808: step 6000, loss = 0.92 (259.3 examples/sec; 0.494 sec/batch)
2016-12-08 09:05:17.247064: step 6010, loss = 1.15 (284.4 examples/sec; 0.450 sec/batch)
2016-12-08 09:05:21.714666: step 6020, loss = 1.03 (296.0 examples/sec; 0.432 sec/batch)
2016-12-08 09:05:26.299727: step 6030, loss = 1.00 (284.0 examples/sec; 0.451 sec/batch)
2016-12-08 09:05:30.846746: step 6040, loss = 0.90 (316.7 examples/sec; 0.404 sec/batch)
2016-12-08 09:05:35.463902: step 6050, loss = 1.09 (289.7 examples/sec; 0.442 sec/batch)
2016-12-08 09:05:39.688498: step 6060, loss = 1.02 (260.3 examples/sec; 0.492 sec/batch)
2016-12-08 09:05:43.859590: step 6070, loss = 1.01 (271.3 examples/sec; 0.472 sec/batch)
2016-12-08 09:05:47.965191: step 6080, loss = 1.03 (325.9 examples/sec; 0.393 sec/batch)
2016-12-08 09:05:52.093044: step 6090, loss = 0.92 (310.4 examples/sec; 0.412 sec/batch)
2016-12-08 09:05:56.408169: step 6100, loss = 0.91 (409.0 examples/sec; 0.313 sec/batch)
2016-12-08 09:06:01.371310: step 6110, loss = 0.92 (264.2 examples/sec; 0.485 sec/batch)
2016-12-08 09:06:05.836166: step 6120, loss = 1.01 (264.0 examples/sec; 0.485 sec/batch)
2016-12-08 09:06:09.800671: step 6130, loss = 0.89 (318.0 examples/sec; 0.403 sec/batch)
2016-12-08 09:06:13.838548: step 6140, loss = 0.77 (303.2 examples/sec; 0.422 sec/batch)
2016-12-08 09:06:18.076867: step 6150, loss = 0.96 (268.2 examples/sec; 0.477 sec/batch)
2016-12-08 09:06:22.660267: step 6160, loss = 0.94 (249.1 examples/sec; 0.514 sec/batch)
2016-12-08 09:06:27.025183: step 6170, loss = 0.94 (344.1 examples/sec; 0.372 sec/batch)
2016-12-08 09:06:31.445263: step 6180, loss = 0.90 (285.0 examples/sec; 0.449 sec/batch)
2016-12-08 09:06:35.975189: step 6190, loss = 1.12 (298.7 examples/sec; 0.429 sec/batch)
2016-12-08 09:06:40.444462: step 6200, loss = 0.72 (273.3 examples/sec; 0.468 sec/batch)
2016-12-08 09:06:45.065713: step 6210, loss = 1.05 (374.0 examples/sec; 0.342 sec/batch)
2016-12-08 09:06:49.570360: step 6220, loss = 0.80 (248.1 examples/sec; 0.516 sec/batch)
2016-12-08 09:06:54.060866: step 6230, loss = 0.96 (334.3 examples/sec; 0.383 sec/batch)
2016-12-08 09:06:58.571550: step 6240, loss = 0.92 (269.4 examples/sec; 0.475 sec/batch)
2016-12-08 09:07:02.941007: step 6250, loss = 0.91 (256.9 examples/sec; 0.498 sec/batch)
2016-12-08 09:07:07.533822: step 6260, loss = 1.07 (271.4 examples/sec; 0.472 sec/batch)
2016-12-08 09:07:11.740111: step 6270, loss = 0.70 (294.6 examples/sec; 0.434 sec/batch)
2016-12-08 09:07:16.104221: step 6280, loss = 0.94 (268.4 examples/sec; 0.477 sec/batch)
2016-12-08 09:07:20.199311: step 6290, loss = 0.88 (360.2 examples/sec; 0.355 sec/batch)
2016-12-08 09:07:24.444195: step 6300, loss = 0.82 (330.6 examples/sec; 0.387 sec/batch)
2016-12-08 09:07:29.078188: step 6310, loss = 0.85 (269.1 examples/sec; 0.476 sec/batch)
2016-12-08 09:07:33.469114: step 6320, loss = 0.83 (336.5 examples/sec; 0.380 sec/batch)
2016-12-08 09:07:38.014377: step 6330, loss = 1.09 (257.1 examples/sec; 0.498 sec/batch)
2016-12-08 09:07:42.496972: step 6340, loss = 0.99 (290.7 examples/sec; 0.440 sec/batch)
2016-12-08 09:07:46.844474: step 6350, loss = 0.99 (268.9 examples/sec; 0.476 sec/batch)
2016-12-08 09:07:51.063296: step 6360, loss = 1.06 (300.8 examples/sec; 0.426 sec/batch)
2016-12-08 09:07:55.752216: step 6370, loss = 0.89 (248.1 examples/sec; 0.516 sec/batch)
2016-12-08 09:07:59.794143: step 6380, loss = 0.95 (290.1 examples/sec; 0.441 sec/batch)
2016-12-08 09:08:04.315309: step 6390, loss = 0.95 (267.9 examples/sec; 0.478 sec/batch)
2016-12-08 09:08:08.603947: step 6400, loss = 0.98 (281.2 examples/sec; 0.455 sec/batch)
2016-12-08 09:08:13.186025: step 6410, loss = 0.91 (289.5 examples/sec; 0.442 sec/batch)
2016-12-08 09:08:17.208531: step 6420, loss = 0.93 (321.5 examples/sec; 0.398 sec/batch)
2016-12-08 09:08:21.375336: step 6430, loss = 0.90 (312.2 examples/sec; 0.410 sec/batch)
2016-12-08 09:08:25.523130: step 6440, loss = 0.90 (353.1 examples/sec; 0.363 sec/batch)
2016-12-08 09:08:29.789332: step 6450, loss = 0.84 (290.1 examples/sec; 0.441 sec/batch)
2016-12-08 09:08:34.136299: step 6460, loss = 0.85 (331.6 examples/sec; 0.386 sec/batch)
2016-12-08 09:08:38.646277: step 6470, loss = 1.08 (291.4 examples/sec; 0.439 sec/batch)
2016-12-08 09:08:43.161946: step 6480, loss = 0.87 (266.7 examples/sec; 0.480 sec/batch)
2016-12-08 09:08:47.340292: step 6490, loss = 0.96 (369.4 examples/sec; 0.347 sec/batch)
2016-12-08 09:08:51.667215: step 6500, loss = 0.86 (287.2 examples/sec; 0.446 sec/batch)
2016-12-08 09:08:56.512873: step 6510, loss = 0.94 (330.2 examples/sec; 0.388 sec/batch)
2016-12-08 09:09:00.842519: step 6520, loss = 0.96 (265.6 examples/sec; 0.482 sec/batch)
2016-12-08 09:09:05.022536: step 6530, loss = 0.94 (346.0 examples/sec; 0.370 sec/batch)
2016-12-08 09:09:09.255021: step 6540, loss = 0.90 (316.4 examples/sec; 0.404 sec/batch)
2016-12-08 09:09:13.661538: step 6550, loss = 0.99 (264.6 examples/sec; 0.484 sec/batch)
2016-12-08 09:09:18.034906: step 6560, loss = 1.25 (341.1 examples/sec; 0.375 sec/batch)
2016-12-08 09:09:22.380511: step 6570, loss = 0.92 (288.2 examples/sec; 0.444 sec/batch)
2016-12-08 09:09:26.822437: step 6580, loss = 0.96 (288.1 examples/sec; 0.444 sec/batch)
2016-12-08 09:09:30.834249: step 6590, loss = 0.99 (334.0 examples/sec; 0.383 sec/batch)
2016-12-08 09:09:35.169547: step 6600, loss = 0.89 (288.4 examples/sec; 0.444 sec/batch)
2016-12-08 09:09:40.135243: step 6610, loss = 1.03 (282.4 examples/sec; 0.453 sec/batch)
2016-12-08 09:09:44.384020: step 6620, loss = 1.04 (272.4 examples/sec; 0.470 sec/batch)
2016-12-08 09:09:48.581697: step 6630, loss = 1.12 (281.3 examples/sec; 0.455 sec/batch)
2016-12-08 09:09:53.117549: step 6640, loss = 0.86 (269.2 examples/sec; 0.475 sec/batch)
2016-12-08 09:09:57.530036: step 6650, loss = 0.97 (260.6 examples/sec; 0.491 sec/batch)
2016-12-08 09:10:01.771377: step 6660, loss = 0.99 (271.5 examples/sec; 0.471 sec/batch)
2016-12-08 09:10:06.351520: step 6670, loss = 0.95 (251.7 examples/sec; 0.509 sec/batch)
2016-12-08 09:10:10.591813: step 6680, loss = 0.80 (314.9 examples/sec; 0.406 sec/batch)
2016-12-08 09:10:14.940541: step 6690, loss = 0.99 (317.6 examples/sec; 0.403 sec/batch)
2016-12-08 09:10:19.227019: step 6700, loss = 0.88 (286.1 examples/sec; 0.447 sec/batch)
2016-12-08 09:10:24.210939: step 6710, loss = 1.07 (274.1 examples/sec; 0.467 sec/batch)
2016-12-08 09:10:28.475153: step 6720, loss = 1.15 (311.8 examples/sec; 0.411 sec/batch)
2016-12-08 09:10:32.902921: step 6730, loss = 0.82 (312.7 examples/sec; 0.409 sec/batch)
2016-12-08 09:10:37.311154: step 6740, loss = 1.04 (293.3 examples/sec; 0.436 sec/batch)
2016-12-08 09:10:41.963290: step 6750, loss = 0.90 (285.8 examples/sec; 0.448 sec/batch)
2016-12-08 09:10:46.338339: step 6760, loss = 0.99 (316.8 examples/sec; 0.404 sec/batch)
2016-12-08 09:10:50.581452: step 6770, loss = 1.02 (262.4 examples/sec; 0.488 sec/batch)
2016-12-08 09:10:55.184710: step 6780, loss = 0.90 (265.4 examples/sec; 0.482 sec/batch)
2016-12-08 09:10:59.766932: step 6790, loss = 0.81 (298.1 examples/sec; 0.429 sec/batch)
2016-12-08 09:11:04.071817: step 6800, loss = 1.03 (315.2 examples/sec; 0.406 sec/batch)
2016-12-08 09:11:08.826592: step 6810, loss = 0.94 (313.1 examples/sec; 0.409 sec/batch)
2016-12-08 09:11:13.032550: step 6820, loss = 1.02 (270.6 examples/sec; 0.473 sec/batch)
2016-12-08 09:11:17.323101: step 6830, loss = 1.13 (286.8 examples/sec; 0.446 sec/batch)
2016-12-08 09:11:21.573656: step 6840, loss = 1.01 (272.6 examples/sec; 0.470 sec/batch)
2016-12-08 09:11:25.767143: step 6850, loss = 0.73 (298.2 examples/sec; 0.429 sec/batch)
2016-12-08 09:11:30.475883: step 6860, loss = 0.80 (292.2 examples/sec; 0.438 sec/batch)
2016-12-08 09:11:34.630575: step 6870, loss = 0.92 (296.1 examples/sec; 0.432 sec/batch)
2016-12-08 09:11:38.882541: step 6880, loss = 1.09 (267.9 examples/sec; 0.478 sec/batch)
2016-12-08 09:11:43.213222: step 6890, loss = 0.79 (299.4 examples/sec; 0.427 sec/batch)
2016-12-08 09:11:47.568926: step 6900, loss = 1.03 (304.0 examples/sec; 0.421 sec/batch)
2016-12-08 09:11:52.725739: step 6910, loss = 0.83 (310.5 examples/sec; 0.412 sec/batch)
2016-12-08 09:11:57.075218: step 6920, loss = 0.85 (283.5 examples/sec; 0.451 sec/batch)
2016-12-08 09:12:01.405510: step 6930, loss = 0.81 (276.7 examples/sec; 0.463 sec/batch)
2016-12-08 09:12:05.594944: step 6940, loss = 0.88 (298.3 examples/sec; 0.429 sec/batch)
2016-12-08 09:12:09.908594: step 6950, loss = 0.93 (300.9 examples/sec; 0.425 sec/batch)
2016-12-08 09:12:13.972262: step 6960, loss = 0.88 (270.1 examples/sec; 0.474 sec/batch)
2016-12-08 09:12:18.229694: step 6970, loss = 0.94 (317.3 examples/sec; 0.403 sec/batch)
2016-12-08 09:12:22.495241: step 6980, loss = 1.03 (276.5 examples/sec; 0.463 sec/batch)
2016-12-08 09:12:26.824771: step 6990, loss = 0.90 (275.1 examples/sec; 0.465 sec/batch)
2016-12-08 09:12:31.347986: step 7000, loss = 1.09 (288.7 examples/sec; 0.443 sec/batch)
2016-12-08 09:12:36.746158: step 7010, loss = 1.14 (269.7 examples/sec; 0.475 sec/batch)
2016-12-08 09:12:41.431844: step 7020, loss = 0.85 (285.1 examples/sec; 0.449 sec/batch)
2016-12-08 09:12:45.583230: step 7030, loss = 0.88 (334.2 examples/sec; 0.383 sec/batch)
2016-12-08 09:12:50.162860: step 7040, loss = 0.90 (260.1 examples/sec; 0.492 sec/batch)
2016-12-08 09:12:54.551203: step 7050, loss = 1.04 (275.2 examples/sec; 0.465 sec/batch)
2016-12-08 09:12:59.401089: step 7060, loss = 0.90 (268.7 examples/sec; 0.476 sec/batch)
2016-12-08 09:13:03.695011: step 7070, loss = 1.01 (306.2 examples/sec; 0.418 sec/batch)
2016-12-08 09:13:07.963643: step 7080, loss = 1.02 (371.1 examples/sec; 0.345 sec/batch)
2016-12-08 09:13:12.304741: step 7090, loss = 1.10 (367.2 examples/sec; 0.349 sec/batch)
2016-12-08 09:13:16.692909: step 7100, loss = 0.99 (271.5 examples/sec; 0.472 sec/batch)
2016-12-08 09:13:21.291260: step 7110, loss = 0.99 (332.2 examples/sec; 0.385 sec/batch)
2016-12-08 09:13:25.552948: step 7120, loss = 0.90 (292.1 examples/sec; 0.438 sec/batch)
2016-12-08 09:13:30.014589: step 7130, loss = 0.97 (266.5 examples/sec; 0.480 sec/batch)
2016-12-08 09:13:34.476355: step 7140, loss = 1.13 (274.8 examples/sec; 0.466 sec/batch)
2016-12-08 09:13:38.398062: step 7150, loss = 1.19 (317.8 examples/sec; 0.403 sec/batch)
2016-12-08 09:13:42.842896: step 7160, loss = 1.04 (293.6 examples/sec; 0.436 sec/batch)
2016-12-08 09:13:47.235864: step 7170, loss = 1.08 (333.6 examples/sec; 0.384 sec/batch)
2016-12-08 09:13:51.492248: step 7180, loss = 0.88 (276.7 examples/sec; 0.463 sec/batch)
2016-12-08 09:13:55.837537: step 7190, loss = 1.02 (267.4 examples/sec; 0.479 sec/batch)
2016-12-08 09:14:00.116955: step 7200, loss = 0.87 (329.5 examples/sec; 0.388 sec/batch)
2016-12-08 09:14:04.758969: step 7210, loss = 0.89 (277.7 examples/sec; 0.461 sec/batch)
2016-12-08 09:14:09.122754: step 7220, loss = 0.89 (280.3 examples/sec; 0.457 sec/batch)
2016-12-08 09:14:13.343017: step 7230, loss = 1.10 (291.4 examples/sec; 0.439 sec/batch)
2016-12-08 09:14:17.169388: step 7240, loss = 0.88 (403.6 examples/sec; 0.317 sec/batch)
2016-12-08 09:14:21.478122: step 7250, loss = 0.99 (276.4 examples/sec; 0.463 sec/batch)
2016-12-08 09:14:26.152778: step 7260, loss = 0.84 (303.0 examples/sec; 0.422 sec/batch)
2016-12-08 09:14:30.519261: step 7270, loss = 0.68 (270.4 examples/sec; 0.473 sec/batch)
2016-12-08 09:14:34.861357: step 7280, loss = 0.95 (291.9 examples/sec; 0.439 sec/batch)
2016-12-08 09:14:39.677762: step 7290, loss = 0.78 (281.0 examples/sec; 0.455 sec/batch)
2016-12-08 09:14:43.881066: step 7300, loss = 0.85 (286.8 examples/sec; 0.446 sec/batch)
2016-12-08 09:14:48.747519: step 7310, loss = 0.90 (332.5 examples/sec; 0.385 sec/batch)
2016-12-08 09:14:53.317763: step 7320, loss = 0.77 (312.5 examples/sec; 0.410 sec/batch)
2016-12-08 09:14:57.824657: step 7330, loss = 0.93 (331.1 examples/sec; 0.387 sec/batch)
2016-12-08 09:15:02.141282: step 7340, loss = 1.04 (321.0 examples/sec; 0.399 sec/batch)
2016-12-08 09:15:06.518972: step 7350, loss = 0.93 (257.1 examples/sec; 0.498 sec/batch)
2016-12-08 09:15:11.011687: step 7360, loss = 0.87 (266.8 examples/sec; 0.480 sec/batch)
2016-12-08 09:15:15.515540: step 7370, loss = 1.17 (256.1 examples/sec; 0.500 sec/batch)
2016-12-08 09:15:20.031592: step 7380, loss = 0.92 (259.2 examples/sec; 0.494 sec/batch)
2016-12-08 09:15:24.327412: step 7390, loss = 1.11 (280.4 examples/sec; 0.456 sec/batch)
2016-12-08 09:15:28.690821: step 7400, loss = 0.88 (320.8 examples/sec; 0.399 sec/batch)
2016-12-08 09:15:33.589084: step 7410, loss = 0.87 (276.7 examples/sec; 0.463 sec/batch)
2016-12-08 09:15:37.601590: step 7420, loss = 1.09 (326.7 examples/sec; 0.392 sec/batch)
2016-12-08 09:15:41.980026: step 7430, loss = 1.08 (276.4 examples/sec; 0.463 sec/batch)
2016-12-08 09:15:46.260408: step 7440, loss = 0.85 (291.9 examples/sec; 0.438 sec/batch)
2016-12-08 09:15:50.715958: step 7450, loss = 0.93 (262.6 examples/sec; 0.487 sec/batch)
2016-12-08 09:15:55.094492: step 7460, loss = 0.94 (279.5 examples/sec; 0.458 sec/batch)
2016-12-08 09:15:59.380853: step 7470, loss = 0.92 (335.9 examples/sec; 0.381 sec/batch)
2016-12-08 09:16:03.594397: step 7480, loss = 0.84 (269.8 examples/sec; 0.474 sec/batch)
2016-12-08 09:16:07.775101: step 7490, loss = 1.04 (312.9 examples/sec; 0.409 sec/batch)
2016-12-08 09:16:12.159915: step 7500, loss = 0.87 (277.0 examples/sec; 0.462 sec/batch)
2016-12-08 09:16:16.838580: step 7510, loss = 0.81 (301.7 examples/sec; 0.424 sec/batch)
2016-12-08 09:16:20.862948: step 7520, loss = 0.81 (335.9 examples/sec; 0.381 sec/batch)
2016-12-08 09:16:25.229661: step 7530, loss = 0.86 (286.5 examples/sec; 0.447 sec/batch)
2016-12-08 09:16:29.611906: step 7540, loss = 0.86 (266.3 examples/sec; 0.481 sec/batch)
2016-12-08 09:16:34.007415: step 7550, loss = 1.02 (288.4 examples/sec; 0.444 sec/batch)
2016-12-08 09:16:38.483494: step 7560, loss = 0.86 (268.0 examples/sec; 0.478 sec/batch)
2016-12-08 09:16:42.883384: step 7570, loss = 0.89 (329.8 examples/sec; 0.388 sec/batch)
2016-12-08 09:16:47.383970: step 7580, loss = 0.83 (281.0 examples/sec; 0.456 sec/batch)
2016-12-08 09:16:51.753612: step 7590, loss = 0.94 (280.6 examples/sec; 0.456 sec/batch)
2016-12-08 09:16:56.085011: step 7600, loss = 0.92 (276.9 examples/sec; 0.462 sec/batch)
2016-12-08 09:17:01.011598: step 7610, loss = 1.09 (305.3 examples/sec; 0.419 sec/batch)
2016-12-08 09:17:05.114965: step 7620, loss = 0.92 (297.3 examples/sec; 0.431 sec/batch)
2016-12-08 09:17:09.562803: step 7630, loss = 0.74 (294.6 examples/sec; 0.435 sec/batch)
2016-12-08 09:17:14.110009: step 7640, loss = 0.95 (271.6 examples/sec; 0.471 sec/batch)
2016-12-08 09:17:18.356740: step 7650, loss = 0.90 (336.1 examples/sec; 0.381 sec/batch)
2016-12-08 09:17:22.448432: step 7660, loss = 0.91 (298.8 examples/sec; 0.428 sec/batch)
2016-12-08 09:17:26.697177: step 7670, loss = 1.04 (298.9 examples/sec; 0.428 sec/batch)
2016-12-08 09:17:31.063336: step 7680, loss = 0.86 (278.1 examples/sec; 0.460 sec/batch)
2016-12-08 09:17:35.477279: step 7690, loss = 0.95 (363.5 examples/sec; 0.352 sec/batch)
2016-12-08 09:17:39.709645: step 7700, loss = 0.88 (274.5 examples/sec; 0.466 sec/batch)
2016-12-08 09:17:44.276085: step 7710, loss = 0.82 (320.8 examples/sec; 0.399 sec/batch)
2016-12-08 09:17:48.298914: step 7720, loss = 0.88 (298.7 examples/sec; 0.429 sec/batch)
2016-12-08 09:17:52.525224: step 7730, loss = 1.01 (307.6 examples/sec; 0.416 sec/batch)
2016-12-08 09:17:56.711506: step 7740, loss = 0.97 (304.8 examples/sec; 0.420 sec/batch)
2016-12-08 09:18:00.977002: step 7750, loss = 0.84 (298.6 examples/sec; 0.429 sec/batch)
2016-12-08 09:18:05.066242: step 7760, loss = 0.78 (269.0 examples/sec; 0.476 sec/batch)
2016-12-08 09:18:09.296685: step 7770, loss = 0.84 (306.9 examples/sec; 0.417 sec/batch)
2016-12-08 09:18:13.318735: step 7780, loss = 1.16 (332.8 examples/sec; 0.385 sec/batch)
2016-12-08 09:18:17.536556: step 7790, loss = 0.99 (317.2 examples/sec; 0.403 sec/batch)
2016-12-08 09:18:21.569911: step 7800, loss = 0.92 (304.9 examples/sec; 0.420 sec/batch)
2016-12-08 09:18:25.792858: step 7810, loss = 0.99 (367.7 examples/sec; 0.348 sec/batch)
2016-12-08 09:18:30.288273: step 7820, loss = 1.00 (413.8 examples/sec; 0.309 sec/batch)
2016-12-08 09:18:34.184563: step 7830, loss = 1.04 (266.2 examples/sec; 0.481 sec/batch)
2016-12-08 09:18:38.557657: step 7840, loss = 0.85 (290.0 examples/sec; 0.441 sec/batch)
2016-12-08 09:18:43.031120: step 7850, loss = 0.89 (281.9 examples/sec; 0.454 sec/batch)
2016-12-08 09:18:47.448757: step 7860, loss = 0.96 (257.7 examples/sec; 0.497 sec/batch)
2016-12-08 09:18:51.777449: step 7870, loss = 0.76 (292.9 examples/sec; 0.437 sec/batch)
2016-12-08 09:18:56.014759: step 7880, loss = 1.05 (372.2 examples/sec; 0.344 sec/batch)
2016-12-08 09:19:00.295648: step 7890, loss = 0.92 (297.8 examples/sec; 0.430 sec/batch)
2016-12-08 09:19:04.605193: step 7900, loss = 0.90 (326.6 examples/sec; 0.392 sec/batch)
2016-12-08 09:19:09.371926: step 7910, loss = 0.87 (262.3 examples/sec; 0.488 sec/batch)
2016-12-08 09:19:13.907830: step 7920, loss = 0.91 (315.6 examples/sec; 0.406 sec/batch)
2016-12-08 09:19:18.036041: step 7930, loss = 0.90 (315.4 examples/sec; 0.406 sec/batch)
2016-12-08 09:19:21.777733: step 7940, loss = 0.89 (342.4 examples/sec; 0.374 sec/batch)
2016-12-08 09:19:25.864004: step 7950, loss = 0.96 (283.4 examples/sec; 0.452 sec/batch)
2016-12-08 09:19:30.157621: step 7960, loss = 0.89 (337.2 examples/sec; 0.380 sec/batch)
2016-12-08 09:19:34.308416: step 7970, loss = 1.03 (284.9 examples/sec; 0.449 sec/batch)
2016-12-08 09:19:38.629547: step 7980, loss = 0.90 (275.9 examples/sec; 0.464 sec/batch)
2016-12-08 09:19:42.988286: step 7990, loss = 0.92 (306.2 examples/sec; 0.418 sec/batch)
2016-12-08 09:19:47.457262: step 8000, loss = 1.09 (261.8 examples/sec; 0.489 sec/batch)
2016-12-08 09:19:52.819796: step 8010, loss = 0.99 (339.3 examples/sec; 0.377 sec/batch)
2016-12-08 09:19:57.450168: step 8020, loss = 0.97 (297.9 examples/sec; 0.430 sec/batch)
2016-12-08 09:20:02.082081: step 8030, loss = 0.90 (242.9 examples/sec; 0.527 sec/batch)
2016-12-08 09:20:06.597341: step 8040, loss = 0.91 (284.9 examples/sec; 0.449 sec/batch)
2016-12-08 09:20:10.541939: step 8050, loss = 0.93 (305.1 examples/sec; 0.419 sec/batch)
2016-12-08 09:20:14.635182: step 8060, loss = 0.98 (270.6 examples/sec; 0.473 sec/batch)
2016-12-08 09:20:19.005163: step 8070, loss = 0.97 (323.9 examples/sec; 0.395 sec/batch)
2016-12-08 09:20:23.459596: step 8080, loss = 0.83 (298.9 examples/sec; 0.428 sec/batch)
2016-12-08 09:20:27.325273: step 8090, loss = 1.01 (362.2 examples/sec; 0.353 sec/batch)
2016-12-08 09:20:31.538348: step 8100, loss = 1.00 (258.6 examples/sec; 0.495 sec/batch)
2016-12-08 09:20:36.353311: step 8110, loss = 0.89 (289.3 examples/sec; 0.442 sec/batch)
2016-12-08 09:20:40.777195: step 8120, loss = 0.93 (308.4 examples/sec; 0.415 sec/batch)
2016-12-08 09:20:45.055882: step 8130, loss = 0.80 (289.0 examples/sec; 0.443 sec/batch)
2016-12-08 09:20:49.633457: step 8140, loss = 0.85 (260.0 examples/sec; 0.492 sec/batch)
2016-12-08 09:20:53.973482: step 8150, loss = 0.94 (316.5 examples/sec; 0.404 sec/batch)
2016-12-08 09:20:58.148991: step 8160, loss = 0.74 (361.8 examples/sec; 0.354 sec/batch)
2016-12-08 09:21:02.472218: step 8170, loss = 0.81 (336.0 examples/sec; 0.381 sec/batch)
2016-12-08 09:21:06.764683: step 8180, loss = 1.03 (255.1 examples/sec; 0.502 sec/batch)
2016-12-08 09:21:10.864354: step 8190, loss = 0.99 (385.8 examples/sec; 0.332 sec/batch)
