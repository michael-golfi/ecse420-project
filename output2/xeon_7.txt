Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
WARNING:tensorflow:From cifar10_train.py:81 in train.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2016-03-02.
Instructions for updating:
Please use tf.global_variables instead.
2016-12-08 10:21:24.376300: step 0, loss = 4.67 (27.6 examples/sec; 4.631 sec/batch)
2016-12-08 10:21:28.448508: step 10, loss = 4.62 (487.2 examples/sec; 0.263 sec/batch)
2016-12-08 10:21:31.617446: step 20, loss = 4.52 (400.8 examples/sec; 0.319 sec/batch)
2016-12-08 10:21:34.518553: step 30, loss = 4.54 (460.0 examples/sec; 0.278 sec/batch)
2016-12-08 10:21:37.483595: step 40, loss = 4.38 (464.8 examples/sec; 0.275 sec/batch)
2016-12-08 10:21:40.444426: step 50, loss = 4.29 (450.8 examples/sec; 0.284 sec/batch)
2016-12-08 10:21:43.328501: step 60, loss = 4.66 (445.7 examples/sec; 0.287 sec/batch)
2016-12-08 10:21:46.136264: step 70, loss = 4.20 (460.8 examples/sec; 0.278 sec/batch)
2016-12-08 10:21:49.092860: step 80, loss = 4.18 (446.3 examples/sec; 0.287 sec/batch)
2016-12-08 10:21:51.919462: step 90, loss = 4.11 (380.6 examples/sec; 0.336 sec/batch)
2016-12-08 10:21:54.762051: step 100, loss = 4.26 (452.4 examples/sec; 0.283 sec/batch)
2016-12-08 10:21:57.939068: step 110, loss = 4.09 (416.5 examples/sec; 0.307 sec/batch)
2016-12-08 10:22:00.771370: step 120, loss = 4.12 (418.2 examples/sec; 0.306 sec/batch)
2016-12-08 10:22:03.695076: step 130, loss = 4.02 (402.9 examples/sec; 0.318 sec/batch)
2016-12-08 10:22:06.435863: step 140, loss = 3.99 (534.0 examples/sec; 0.240 sec/batch)
2016-12-08 10:22:09.284162: step 150, loss = 4.16 (424.2 examples/sec; 0.302 sec/batch)
2016-12-08 10:22:12.380503: step 160, loss = 4.01 (376.0 examples/sec; 0.340 sec/batch)
2016-12-08 10:22:15.408684: step 170, loss = 3.87 (414.6 examples/sec; 0.309 sec/batch)
2016-12-08 10:22:18.337754: step 180, loss = 3.90 (509.6 examples/sec; 0.251 sec/batch)
2016-12-08 10:22:21.199019: step 190, loss = 3.79 (482.7 examples/sec; 0.265 sec/batch)
2016-12-08 10:22:24.201426: step 200, loss = 3.99 (457.1 examples/sec; 0.280 sec/batch)
2016-12-08 10:22:27.437953: step 210, loss = 3.81 (463.0 examples/sec; 0.276 sec/batch)
2016-12-08 10:22:30.351167: step 220, loss = 3.78 (506.6 examples/sec; 0.253 sec/batch)
2016-12-08 10:22:33.274700: step 230, loss = 3.80 (448.2 examples/sec; 0.286 sec/batch)
2016-12-08 10:22:36.172088: step 240, loss = 3.58 (478.9 examples/sec; 0.267 sec/batch)
2016-12-08 10:22:39.032190: step 250, loss = 3.79 (422.3 examples/sec; 0.303 sec/batch)
2016-12-08 10:22:41.693759: step 260, loss = 3.80 (447.1 examples/sec; 0.286 sec/batch)
2016-12-08 10:22:44.520056: step 270, loss = 3.69 (478.1 examples/sec; 0.268 sec/batch)
2016-12-08 10:22:47.326240: step 280, loss = 3.57 (540.0 examples/sec; 0.237 sec/batch)
2016-12-08 10:22:50.026507: step 290, loss = 3.64 (440.2 examples/sec; 0.291 sec/batch)
2016-12-08 10:22:52.874667: step 300, loss = 3.54 (491.7 examples/sec; 0.260 sec/batch)
2016-12-08 10:22:55.981756: step 310, loss = 3.52 (438.9 examples/sec; 0.292 sec/batch)
2016-12-08 10:22:58.861404: step 320, loss = 3.55 (405.5 examples/sec; 0.316 sec/batch)
2016-12-08 10:23:01.727298: step 330, loss = 3.56 (461.5 examples/sec; 0.277 sec/batch)
2016-12-08 10:23:04.498231: step 340, loss = 3.49 (493.4 examples/sec; 0.259 sec/batch)
2016-12-08 10:23:07.305712: step 350, loss = 3.47 (428.5 examples/sec; 0.299 sec/batch)
2016-12-08 10:23:10.169010: step 360, loss = 3.39 (525.7 examples/sec; 0.243 sec/batch)
2016-12-08 10:23:13.000991: step 370, loss = 3.40 (425.9 examples/sec; 0.301 sec/batch)
2016-12-08 10:23:15.880694: step 380, loss = 3.17 (443.6 examples/sec; 0.289 sec/batch)
2016-12-08 10:23:18.679183: step 390, loss = 3.44 (418.4 examples/sec; 0.306 sec/batch)
2016-12-08 10:23:21.734350: step 400, loss = 3.36 (466.7 examples/sec; 0.274 sec/batch)
2016-12-08 10:23:24.910810: step 410, loss = 3.20 (485.4 examples/sec; 0.264 sec/batch)
2016-12-08 10:23:27.826016: step 420, loss = 3.28 (424.9 examples/sec; 0.301 sec/batch)
2016-12-08 10:23:30.611721: step 430, loss = 3.38 (472.7 examples/sec; 0.271 sec/batch)
2016-12-08 10:23:33.389254: step 440, loss = 3.21 (450.4 examples/sec; 0.284 sec/batch)
2016-12-08 10:23:36.217767: step 450, loss = 3.47 (400.8 examples/sec; 0.319 sec/batch)
2016-12-08 10:23:39.040212: step 460, loss = 3.22 (454.4 examples/sec; 0.282 sec/batch)
2016-12-08 10:23:41.957391: step 470, loss = 3.23 (401.3 examples/sec; 0.319 sec/batch)
2016-12-08 10:23:44.862660: step 480, loss = 3.27 (381.7 examples/sec; 0.335 sec/batch)
2016-12-08 10:23:47.650208: step 490, loss = 3.30 (445.0 examples/sec; 0.288 sec/batch)
2016-12-08 10:23:50.575045: step 500, loss = 3.29 (382.2 examples/sec; 0.335 sec/batch)
2016-12-08 10:23:53.882285: step 510, loss = 3.08 (489.8 examples/sec; 0.261 sec/batch)
2016-12-08 10:23:56.821088: step 520, loss = 3.05 (354.5 examples/sec; 0.361 sec/batch)
2016-12-08 10:23:59.645069: step 530, loss = 3.20 (430.6 examples/sec; 0.297 sec/batch)
2016-12-08 10:24:02.548018: step 540, loss = 3.11 (382.7 examples/sec; 0.334 sec/batch)
2016-12-08 10:24:05.371259: step 550, loss = 3.17 (500.0 examples/sec; 0.256 sec/batch)
2016-12-08 10:24:08.275216: step 560, loss = 3.09 (449.1 examples/sec; 0.285 sec/batch)
2016-12-08 10:24:11.175955: step 570, loss = 2.89 (410.3 examples/sec; 0.312 sec/batch)
2016-12-08 10:24:14.198474: step 580, loss = 2.90 (409.3 examples/sec; 0.313 sec/batch)
2016-12-08 10:24:17.196789: step 590, loss = 2.95 (443.9 examples/sec; 0.288 sec/batch)
2016-12-08 10:24:19.909285: step 600, loss = 2.67 (461.1 examples/sec; 0.278 sec/batch)
2016-12-08 10:24:23.035100: step 610, loss = 3.09 (438.4 examples/sec; 0.292 sec/batch)
2016-12-08 10:24:26.115335: step 620, loss = 2.94 (383.8 examples/sec; 0.333 sec/batch)
2016-12-08 10:24:28.882504: step 630, loss = 2.96 (475.3 examples/sec; 0.269 sec/batch)
2016-12-08 10:24:31.836137: step 640, loss = 2.96 (392.9 examples/sec; 0.326 sec/batch)
2016-12-08 10:24:34.654145: step 650, loss = 2.96 (446.1 examples/sec; 0.287 sec/batch)
2016-12-08 10:24:37.384515: step 660, loss = 2.88 (496.2 examples/sec; 0.258 sec/batch)
2016-12-08 10:24:40.228801: step 670, loss = 2.96 (481.0 examples/sec; 0.266 sec/batch)
2016-12-08 10:24:43.037799: step 680, loss = 2.86 (437.7 examples/sec; 0.292 sec/batch)
2016-12-08 10:24:45.927582: step 690, loss = 2.91 (432.0 examples/sec; 0.296 sec/batch)
2016-12-08 10:24:48.736851: step 700, loss = 3.14 (482.4 examples/sec; 0.265 sec/batch)
2016-12-08 10:24:51.850608: step 710, loss = 3.06 (481.2 examples/sec; 0.266 sec/batch)
2016-12-08 10:24:54.448311: step 720, loss = 2.84 (480.6 examples/sec; 0.266 sec/batch)
2016-12-08 10:24:57.250175: step 730, loss = 2.82 (463.1 examples/sec; 0.276 sec/batch)
2016-12-08 10:25:00.149528: step 740, loss = 2.77 (416.1 examples/sec; 0.308 sec/batch)
2016-12-08 10:25:03.259518: step 750, loss = 2.82 (448.4 examples/sec; 0.285 sec/batch)
2016-12-08 10:25:05.958094: step 760, loss = 2.67 (439.2 examples/sec; 0.291 sec/batch)
2016-12-08 10:25:08.847555: step 770, loss = 2.68 (390.4 examples/sec; 0.328 sec/batch)
2016-12-08 10:25:11.827899: step 780, loss = 2.86 (443.6 examples/sec; 0.289 sec/batch)
2016-12-08 10:25:14.649270: step 790, loss = 2.81 (537.6 examples/sec; 0.238 sec/batch)
2016-12-08 10:25:17.441554: step 800, loss = 2.88 (468.8 examples/sec; 0.273 sec/batch)
2016-12-08 10:25:20.542559: step 810, loss = 2.67 (418.3 examples/sec; 0.306 sec/batch)
2016-12-08 10:25:23.337469: step 820, loss = 2.70 (490.2 examples/sec; 0.261 sec/batch)
2016-12-08 10:25:26.107000: step 830, loss = 2.75 (524.7 examples/sec; 0.244 sec/batch)
2016-12-08 10:25:29.038840: step 840, loss = 2.71 (467.3 examples/sec; 0.274 sec/batch)
2016-12-08 10:25:31.826692: step 850, loss = 2.63 (516.4 examples/sec; 0.248 sec/batch)
2016-12-08 10:25:34.563297: step 860, loss = 2.62 (546.4 examples/sec; 0.234 sec/batch)
2016-12-08 10:25:37.319588: step 870, loss = 2.71 (533.8 examples/sec; 0.240 sec/batch)
2016-12-08 10:25:40.163514: step 880, loss = 2.62 (396.9 examples/sec; 0.322 sec/batch)
2016-12-08 10:25:43.073196: step 890, loss = 2.69 (454.6 examples/sec; 0.282 sec/batch)
2016-12-08 10:25:45.979271: step 900, loss = 2.55 (424.4 examples/sec; 0.302 sec/batch)
2016-12-08 10:25:49.197440: step 910, loss = 2.42 (444.9 examples/sec; 0.288 sec/batch)
2016-12-08 10:25:52.012242: step 920, loss = 2.47 (467.9 examples/sec; 0.274 sec/batch)
2016-12-08 10:25:54.928931: step 930, loss = 2.49 (449.2 examples/sec; 0.285 sec/batch)
2016-12-08 10:25:57.829819: step 940, loss = 2.54 (462.6 examples/sec; 0.277 sec/batch)
2016-12-08 10:26:00.744068: step 950, loss = 2.56 (399.2 examples/sec; 0.321 sec/batch)
2016-12-08 10:26:03.525295: step 960, loss = 2.34 (428.3 examples/sec; 0.299 sec/batch)
2016-12-08 10:26:06.490617: step 970, loss = 2.45 (417.7 examples/sec; 0.306 sec/batch)
2016-12-08 10:26:09.418056: step 980, loss = 2.44 (437.7 examples/sec; 0.292 sec/batch)
2016-12-08 10:26:12.263750: step 990, loss = 2.38 (490.5 examples/sec; 0.261 sec/batch)
2016-12-08 10:26:15.105388: step 1000, loss = 2.37 (392.9 examples/sec; 0.326 sec/batch)
2016-12-08 10:26:18.902335: step 1010, loss = 2.61 (487.0 examples/sec; 0.263 sec/batch)
2016-12-08 10:26:21.682373: step 1020, loss = 2.47 (465.1 examples/sec; 0.275 sec/batch)
2016-12-08 10:26:24.507143: step 1030, loss = 2.33 (420.2 examples/sec; 0.305 sec/batch)
2016-12-08 10:26:27.276266: step 1040, loss = 2.71 (456.2 examples/sec; 0.281 sec/batch)
2016-12-08 10:26:30.072060: step 1050, loss = 2.32 (464.5 examples/sec; 0.276 sec/batch)
2016-12-08 10:26:32.902756: step 1060, loss = 2.30 (414.5 examples/sec; 0.309 sec/batch)
2016-12-08 10:26:35.910032: step 1070, loss = 2.26 (394.6 examples/sec; 0.324 sec/batch)
2016-12-08 10:26:38.667774: step 1080, loss = 2.53 (543.6 examples/sec; 0.235 sec/batch)
2016-12-08 10:26:41.460735: step 1090, loss = 2.27 (454.7 examples/sec; 0.282 sec/batch)
2016-12-08 10:26:44.447549: step 1100, loss = 2.52 (432.6 examples/sec; 0.296 sec/batch)
2016-12-08 10:26:47.708503: step 1110, loss = 2.34 (444.4 examples/sec; 0.288 sec/batch)
2016-12-08 10:26:50.405163: step 1120, loss = 2.19 (463.3 examples/sec; 0.276 sec/batch)
2016-12-08 10:26:53.170938: step 1130, loss = 2.48 (464.4 examples/sec; 0.276 sec/batch)
2016-12-08 10:26:55.947982: step 1140, loss = 2.32 (427.8 examples/sec; 0.299 sec/batch)
2016-12-08 10:26:58.821823: step 1150, loss = 2.43 (462.4 examples/sec; 0.277 sec/batch)
2016-12-08 10:27:01.764725: step 1160, loss = 2.26 (416.3 examples/sec; 0.307 sec/batch)
2016-12-08 10:27:04.692830: step 1170, loss = 2.26 (414.5 examples/sec; 0.309 sec/batch)
2016-12-08 10:27:07.543192: step 1180, loss = 2.23 (424.8 examples/sec; 0.301 sec/batch)
2016-12-08 10:27:10.289731: step 1190, loss = 2.19 (505.1 examples/sec; 0.253 sec/batch)
2016-12-08 10:27:13.101768: step 1200, loss = 2.21 (457.9 examples/sec; 0.280 sec/batch)
2016-12-08 10:27:16.286605: step 1210, loss = 2.12 (446.6 examples/sec; 0.287 sec/batch)
2016-12-08 10:27:19.108803: step 1220, loss = 2.28 (459.5 examples/sec; 0.279 sec/batch)
2016-12-08 10:27:22.078342: step 1230, loss = 2.13 (398.0 examples/sec; 0.322 sec/batch)
2016-12-08 10:27:24.898012: step 1240, loss = 2.44 (453.1 examples/sec; 0.283 sec/batch)
2016-12-08 10:27:27.755312: step 1250, loss = 2.17 (432.2 examples/sec; 0.296 sec/batch)
2016-12-08 10:27:30.536744: step 1260, loss = 2.33 (514.7 examples/sec; 0.249 sec/batch)
2016-12-08 10:27:33.374886: step 1270, loss = 2.62 (449.9 examples/sec; 0.284 sec/batch)
2016-12-08 10:27:36.147024: step 1280, loss = 2.12 (461.4 examples/sec; 0.277 sec/batch)
2016-12-08 10:27:39.219734: step 1290, loss = 2.23 (398.5 examples/sec; 0.321 sec/batch)
2016-12-08 10:27:42.075808: step 1300, loss = 2.22 (431.5 examples/sec; 0.297 sec/batch)
2016-12-08 10:27:45.226903: step 1310, loss = 2.20 (521.2 examples/sec; 0.246 sec/batch)
2016-12-08 10:27:48.046512: step 1320, loss = 2.21 (509.9 examples/sec; 0.251 sec/batch)
2016-12-08 10:27:51.040717: step 1330, loss = 2.07 (433.9 examples/sec; 0.295 sec/batch)
2016-12-08 10:27:53.965883: step 1340, loss = 2.32 (419.4 examples/sec; 0.305 sec/batch)
2016-12-08 10:27:56.751915: step 1350, loss = 2.08 (459.7 examples/sec; 0.278 sec/batch)
2016-12-08 10:27:59.569226: step 1360, loss = 2.11 (485.4 examples/sec; 0.264 sec/batch)
2016-12-08 10:28:02.374397: step 1370, loss = 2.09 (403.9 examples/sec; 0.317 sec/batch)
2016-12-08 10:28:05.092726: step 1380, loss = 2.15 (473.4 examples/sec; 0.270 sec/batch)
2016-12-08 10:28:07.913216: step 1390, loss = 2.01 (458.8 examples/sec; 0.279 sec/batch)
2016-12-08 10:28:10.879903: step 1400, loss = 2.00 (423.4 examples/sec; 0.302 sec/batch)
2016-12-08 10:28:14.128961: step 1410, loss = 2.08 (524.5 examples/sec; 0.244 sec/batch)
2016-12-08 10:28:16.935060: step 1420, loss = 1.94 (439.0 examples/sec; 0.292 sec/batch)
2016-12-08 10:28:19.799552: step 1430, loss = 2.10 (393.2 examples/sec; 0.326 sec/batch)
2016-12-08 10:28:22.512907: step 1440, loss = 1.97 (524.0 examples/sec; 0.244 sec/batch)
2016-12-08 10:28:25.248515: step 1450, loss = 1.92 (501.1 examples/sec; 0.255 sec/batch)
2016-12-08 10:28:28.115093: step 1460, loss = 1.98 (395.6 examples/sec; 0.324 sec/batch)
2016-12-08 10:28:30.981718: step 1470, loss = 2.27 (496.2 examples/sec; 0.258 sec/batch)
2016-12-08 10:28:33.828897: step 1480, loss = 1.86 (433.4 examples/sec; 0.295 sec/batch)
2016-12-08 10:28:36.462318: step 1490, loss = 1.93 (499.5 examples/sec; 0.256 sec/batch)
2016-12-08 10:28:39.316559: step 1500, loss = 1.89 (422.3 examples/sec; 0.303 sec/batch)
2016-12-08 10:28:42.404469: step 1510, loss = 1.97 (437.3 examples/sec; 0.293 sec/batch)
2016-12-08 10:28:45.217637: step 1520, loss = 1.99 (449.0 examples/sec; 0.285 sec/batch)
2016-12-08 10:28:47.997752: step 1530, loss = 1.96 (493.1 examples/sec; 0.260 sec/batch)
2016-12-08 10:28:50.803972: step 1540, loss = 1.89 (445.6 examples/sec; 0.287 sec/batch)
2016-12-08 10:28:53.733953: step 1550, loss = 1.93 (407.7 examples/sec; 0.314 sec/batch)
2016-12-08 10:28:56.460333: step 1560, loss = 1.91 (466.7 examples/sec; 0.274 sec/batch)
2016-12-08 10:28:59.374768: step 1570, loss = 1.80 (429.8 examples/sec; 0.298 sec/batch)
2016-12-08 10:29:02.212051: step 1580, loss = 2.07 (404.9 examples/sec; 0.316 sec/batch)
2016-12-08 10:29:05.008310: step 1590, loss = 1.80 (414.2 examples/sec; 0.309 sec/batch)
2016-12-08 10:29:07.762541: step 1600, loss = 1.75 (445.5 examples/sec; 0.287 sec/batch)
2016-12-08 10:29:10.911260: step 1610, loss = 1.70 (556.2 examples/sec; 0.230 sec/batch)
2016-12-08 10:29:13.969597: step 1620, loss = 1.76 (424.3 examples/sec; 0.302 sec/batch)
2016-12-08 10:29:16.732856: step 1630, loss = 1.98 (384.6 examples/sec; 0.333 sec/batch)
2016-12-08 10:29:19.599273: step 1640, loss = 1.86 (511.9 examples/sec; 0.250 sec/batch)
2016-12-08 10:29:22.447649: step 1650, loss = 1.80 (436.6 examples/sec; 0.293 sec/batch)
2016-12-08 10:29:25.341180: step 1660, loss = 1.80 (449.2 examples/sec; 0.285 sec/batch)
2016-12-08 10:29:28.280756: step 1670, loss = 1.92 (432.9 examples/sec; 0.296 sec/batch)
2016-12-08 10:29:31.197847: step 1680, loss = 1.91 (519.3 examples/sec; 0.246 sec/batch)
2016-12-08 10:29:34.043159: step 1690, loss = 1.86 (429.6 examples/sec; 0.298 sec/batch)
2016-12-08 10:29:36.881193: step 1700, loss = 1.77 (414.4 examples/sec; 0.309 sec/batch)
2016-12-08 10:29:39.991980: step 1710, loss = 1.95 (464.9 examples/sec; 0.275 sec/batch)
2016-12-08 10:29:42.821956: step 1720, loss = 1.64 (426.1 examples/sec; 0.300 sec/batch)
2016-12-08 10:29:45.784589: step 1730, loss = 1.76 (455.0 examples/sec; 0.281 sec/batch)
2016-12-08 10:29:48.699420: step 1740, loss = 1.70 (375.4 examples/sec; 0.341 sec/batch)
2016-12-08 10:29:51.531305: step 1750, loss = 1.63 (425.0 examples/sec; 0.301 sec/batch)
2016-12-08 10:29:54.359799: step 1760, loss = 1.84 (479.2 examples/sec; 0.267 sec/batch)
2016-12-08 10:29:57.140653: step 1770, loss = 1.77 (467.5 examples/sec; 0.274 sec/batch)
2016-12-08 10:30:00.125917: step 1780, loss = 2.08 (423.5 examples/sec; 0.302 sec/batch)
2016-12-08 10:30:02.965317: step 1790, loss = 1.91 (461.9 examples/sec; 0.277 sec/batch)
2016-12-08 10:30:05.799243: step 1800, loss = 1.64 (444.3 examples/sec; 0.288 sec/batch)
2016-12-08 10:30:08.798300: step 1810, loss = 1.81 (539.1 examples/sec; 0.237 sec/batch)
2016-12-08 10:30:11.862103: step 1820, loss = 1.64 (399.7 examples/sec; 0.320 sec/batch)
2016-12-08 10:30:14.696918: step 1830, loss = 1.82 (422.0 examples/sec; 0.303 sec/batch)
2016-12-08 10:30:17.572133: step 1840, loss = 1.80 (442.9 examples/sec; 0.289 sec/batch)
2016-12-08 10:30:20.433917: step 1850, loss = 1.80 (430.4 examples/sec; 0.297 sec/batch)
2016-12-08 10:30:23.334583: step 1860, loss = 1.82 (453.3 examples/sec; 0.282 sec/batch)
2016-12-08 10:30:26.174409: step 1870, loss = 1.80 (476.3 examples/sec; 0.269 sec/batch)
2016-12-08 10:30:29.040426: step 1880, loss = 1.75 (430.7 examples/sec; 0.297 sec/batch)
2016-12-08 10:30:31.877513: step 1890, loss = 1.62 (452.2 examples/sec; 0.283 sec/batch)
2016-12-08 10:30:34.674820: step 1900, loss = 1.68 (423.9 examples/sec; 0.302 sec/batch)
2016-12-08 10:30:37.802086: step 1910, loss = 1.55 (484.6 examples/sec; 0.264 sec/batch)
2016-12-08 10:30:40.542705: step 1920, loss = 1.69 (510.0 examples/sec; 0.251 sec/batch)
2016-12-08 10:30:43.299943: step 1930, loss = 1.67 (498.2 examples/sec; 0.257 sec/batch)
2016-12-08 10:30:46.114162: step 1940, loss = 1.87 (433.5 examples/sec; 0.295 sec/batch)
2016-12-08 10:30:49.029559: step 1950, loss = 1.64 (401.6 examples/sec; 0.319 sec/batch)
2016-12-08 10:30:51.855395: step 1960, loss = 1.60 (475.0 examples/sec; 0.269 sec/batch)
2016-12-08 10:30:54.754539: step 1970, loss = 1.66 (471.7 examples/sec; 0.271 sec/batch)
2016-12-08 10:30:57.402336: step 1980, loss = 1.53 (471.4 examples/sec; 0.272 sec/batch)
2016-12-08 10:31:00.276026: step 1990, loss = 1.53 (445.8 examples/sec; 0.287 sec/batch)
2016-12-08 10:31:03.073336: step 2000, loss = 1.59 (473.2 examples/sec; 0.270 sec/batch)
2016-12-08 10:31:07.055178: step 2010, loss = 1.49 (421.3 examples/sec; 0.304 sec/batch)
2016-12-08 10:31:09.907869: step 2020, loss = 1.51 (379.6 examples/sec; 0.337 sec/batch)
2016-12-08 10:31:12.908814: step 2030, loss = 1.54 (395.2 examples/sec; 0.324 sec/batch)
2016-12-08 10:31:15.767303: step 2040, loss = 1.47 (470.4 examples/sec; 0.272 sec/batch)
2016-12-08 10:31:18.528868: step 2050, loss = 1.76 (552.1 examples/sec; 0.232 sec/batch)
2016-12-08 10:31:21.391330: step 2060, loss = 1.46 (381.9 examples/sec; 0.335 sec/batch)
2016-12-08 10:31:24.224062: step 2070, loss = 1.71 (463.0 examples/sec; 0.276 sec/batch)
2016-12-08 10:31:27.048274: step 2080, loss = 1.73 (477.7 examples/sec; 0.268 sec/batch)
2016-12-08 10:31:29.684013: step 2090, loss = 1.53 (450.5 examples/sec; 0.284 sec/batch)
2016-12-08 10:31:32.535656: step 2100, loss = 1.53 (488.1 examples/sec; 0.262 sec/batch)
2016-12-08 10:31:35.650543: step 2110, loss = 1.63 (429.1 examples/sec; 0.298 sec/batch)
2016-12-08 10:31:38.428547: step 2120, loss = 1.73 (452.4 examples/sec; 0.283 sec/batch)
2016-12-08 10:31:41.242019: step 2130, loss = 1.54 (450.7 examples/sec; 0.284 sec/batch)
2016-12-08 10:31:43.932031: step 2140, loss = 1.59 (465.8 examples/sec; 0.275 sec/batch)
2016-12-08 10:31:46.864434: step 2150, loss = 1.64 (458.0 examples/sec; 0.279 sec/batch)
2016-12-08 10:31:49.763575: step 2160, loss = 1.70 (504.8 examples/sec; 0.254 sec/batch)
2016-12-08 10:31:52.662896: step 2170, loss = 1.40 (480.7 examples/sec; 0.266 sec/batch)
2016-12-08 10:31:55.495789: step 2180, loss = 1.65 (413.0 examples/sec; 0.310 sec/batch)
2016-12-08 10:31:58.290771: step 2190, loss = 1.60 (452.8 examples/sec; 0.283 sec/batch)
2016-12-08 10:32:01.120012: step 2200, loss = 1.59 (417.9 examples/sec; 0.306 sec/batch)
2016-12-08 10:32:04.201375: step 2210, loss = 1.61 (505.5 examples/sec; 0.253 sec/batch)
2016-12-08 10:32:06.947442: step 2220, loss = 1.55 (485.9 examples/sec; 0.263 sec/batch)
2016-12-08 10:32:09.662352: step 2230, loss = 1.72 (439.4 examples/sec; 0.291 sec/batch)
2016-12-08 10:32:12.475714: step 2240, loss = 1.50 (522.1 examples/sec; 0.245 sec/batch)
2016-12-08 10:32:15.358534: step 2250, loss = 1.63 (474.7 examples/sec; 0.270 sec/batch)
2016-12-08 10:32:18.199461: step 2260, loss = 1.54 (571.5 examples/sec; 0.224 sec/batch)
2016-12-08 10:32:21.173145: step 2270, loss = 1.41 (432.1 examples/sec; 0.296 sec/batch)
2016-12-08 10:32:24.154629: step 2280, loss = 1.65 (447.5 examples/sec; 0.286 sec/batch)
2016-12-08 10:32:27.016583: step 2290, loss = 1.64 (420.0 examples/sec; 0.305 sec/batch)
2016-12-08 10:32:29.825634: step 2300, loss = 1.50 (491.7 examples/sec; 0.260 sec/batch)
2016-12-08 10:32:32.889874: step 2310, loss = 1.51 (479.5 examples/sec; 0.267 sec/batch)
2016-12-08 10:32:35.801506: step 2320, loss = 1.53 (515.9 examples/sec; 0.248 sec/batch)
2016-12-08 10:32:38.671831: step 2330, loss = 1.56 (432.9 examples/sec; 0.296 sec/batch)
2016-12-08 10:32:41.566721: step 2340, loss = 1.64 (424.7 examples/sec; 0.301 sec/batch)
2016-12-08 10:32:44.372403: step 2350, loss = 1.63 (473.2 examples/sec; 0.271 sec/batch)
2016-12-08 10:32:47.102024: step 2360, loss = 1.57 (550.5 examples/sec; 0.233 sec/batch)
2016-12-08 10:32:49.899304: step 2370, loss = 1.41 (443.9 examples/sec; 0.288 sec/batch)
2016-12-08 10:32:52.746461: step 2380, loss = 1.24 (433.1 examples/sec; 0.296 sec/batch)
2016-12-08 10:32:55.626940: step 2390, loss = 1.40 (424.4 examples/sec; 0.302 sec/batch)
2016-12-08 10:32:58.537346: step 2400, loss = 1.50 (541.1 examples/sec; 0.237 sec/batch)
2016-12-08 10:33:01.619874: step 2410, loss = 1.38 (519.1 examples/sec; 0.247 sec/batch)
2016-12-08 10:33:04.568155: step 2420, loss = 1.38 (446.5 examples/sec; 0.287 sec/batch)
2016-12-08 10:33:07.508629: step 2430, loss = 1.36 (428.6 examples/sec; 0.299 sec/batch)
2016-12-08 10:33:10.332646: step 2440, loss = 1.37 (435.0 examples/sec; 0.294 sec/batch)
2016-12-08 10:33:13.173771: step 2450, loss = 1.59 (449.6 examples/sec; 0.285 sec/batch)
2016-12-08 10:33:16.069922: step 2460, loss = 1.59 (404.9 examples/sec; 0.316 sec/batch)
2016-12-08 10:33:18.810294: step 2470, loss = 1.51 (472.7 examples/sec; 0.271 sec/batch)
2016-12-08 10:33:21.623649: step 2480, loss = 1.38 (434.6 examples/sec; 0.295 sec/batch)
2016-12-08 10:33:24.725644: step 2490, loss = 1.37 (388.6 examples/sec; 0.329 sec/batch)
2016-12-08 10:33:27.724365: step 2500, loss = 1.48 (492.9 examples/sec; 0.260 sec/batch)
2016-12-08 10:33:30.796315: step 2510, loss = 1.34 (461.8 examples/sec; 0.277 sec/batch)
2016-12-08 10:33:33.602523: step 2520, loss = 1.53 (458.5 examples/sec; 0.279 sec/batch)
2016-12-08 10:33:36.387005: step 2530, loss = 1.36 (448.4 examples/sec; 0.285 sec/batch)
2016-12-08 10:33:39.225705: step 2540, loss = 1.23 (501.0 examples/sec; 0.255 sec/batch)
2016-12-08 10:33:42.213890: step 2550, loss = 1.47 (463.0 examples/sec; 0.276 sec/batch)
2016-12-08 10:33:45.155561: step 2560, loss = 1.95 (432.8 examples/sec; 0.296 sec/batch)
2016-12-08 10:33:47.977833: step 2570, loss = 1.26 (423.9 examples/sec; 0.302 sec/batch)
2016-12-08 10:33:50.909657: step 2580, loss = 1.37 (421.7 examples/sec; 0.304 sec/batch)
2016-12-08 10:33:54.039783: step 2590, loss = 1.53 (371.5 examples/sec; 0.345 sec/batch)
2016-12-08 10:33:56.815199: step 2600, loss = 1.39 (470.2 examples/sec; 0.272 sec/batch)
2016-12-08 10:33:59.871937: step 2610, loss = 1.36 (602.5 examples/sec; 0.212 sec/batch)
2016-12-08 10:34:02.724075: step 2620, loss = 1.33 (411.0 examples/sec; 0.311 sec/batch)
2016-12-08 10:34:05.552638: step 2630, loss = 1.51 (411.8 examples/sec; 0.311 sec/batch)
2016-12-08 10:34:08.496898: step 2640, loss = 1.44 (441.0 examples/sec; 0.290 sec/batch)
2016-12-08 10:34:11.399013: step 2650, loss = 1.41 (472.6 examples/sec; 0.271 sec/batch)
2016-12-08 10:34:14.251268: step 2660, loss = 1.31 (498.5 examples/sec; 0.257 sec/batch)
2016-12-08 10:34:17.090044: step 2670, loss = 1.24 (434.5 examples/sec; 0.295 sec/batch)
2016-12-08 10:34:19.759387: step 2680, loss = 1.28 (494.7 examples/sec; 0.259 sec/batch)
2016-12-08 10:34:22.190200: step 2690, loss = 1.34 (473.2 examples/sec; 0.270 sec/batch)
2016-12-08 10:34:24.992479: step 2700, loss = 1.45 (501.1 examples/sec; 0.255 sec/batch)
2016-12-08 10:34:28.144787: step 2710, loss = 1.37 (470.4 examples/sec; 0.272 sec/batch)
2016-12-08 10:34:31.013514: step 2720, loss = 1.44 (395.2 examples/sec; 0.324 sec/batch)
2016-12-08 10:34:33.865333: step 2730, loss = 1.43 (436.1 examples/sec; 0.294 sec/batch)
2016-12-08 10:34:36.740193: step 2740, loss = 1.33 (508.7 examples/sec; 0.252 sec/batch)
2016-12-08 10:34:39.470794: step 2750, loss = 1.33 (417.5 examples/sec; 0.307 sec/batch)
2016-12-08 10:34:42.337640: step 2760, loss = 1.20 (466.0 examples/sec; 0.275 sec/batch)
2016-12-08 10:34:45.229736: step 2770, loss = 1.35 (462.9 examples/sec; 0.277 sec/batch)
2016-12-08 10:34:48.326696: step 2780, loss = 1.41 (450.6 examples/sec; 0.284 sec/batch)
2016-12-08 10:34:51.131577: step 2790, loss = 1.33 (428.4 examples/sec; 0.299 sec/batch)
2016-12-08 10:34:54.117736: step 2800, loss = 1.40 (466.1 examples/sec; 0.275 sec/batch)
2016-12-08 10:34:57.286865: step 2810, loss = 1.40 (434.1 examples/sec; 0.295 sec/batch)
2016-12-08 10:35:00.232287: step 2820, loss = 1.43 (447.4 examples/sec; 0.286 sec/batch)
2016-12-08 10:35:03.165492: step 2830, loss = 1.55 (462.5 examples/sec; 0.277 sec/batch)
2016-12-08 10:35:05.957388: step 2840, loss = 1.29 (463.8 examples/sec; 0.276 sec/batch)
2016-12-08 10:35:08.807114: step 2850, loss = 1.17 (436.9 examples/sec; 0.293 sec/batch)
2016-12-08 10:35:11.803474: step 2860, loss = 1.37 (377.5 examples/sec; 0.339 sec/batch)
2016-12-08 10:35:14.619921: step 2870, loss = 1.31 (439.6 examples/sec; 0.291 sec/batch)
2016-12-08 10:35:17.597275: step 2880, loss = 1.45 (499.3 examples/sec; 0.256 sec/batch)
2016-12-08 10:35:20.414352: step 2890, loss = 1.40 (469.9 examples/sec; 0.272 sec/batch)
2016-12-08 10:35:23.199177: step 2900, loss = 1.35 (477.8 examples/sec; 0.268 sec/batch)
2016-12-08 10:35:26.398984: step 2910, loss = 1.41 (452.6 examples/sec; 0.283 sec/batch)
2016-12-08 10:35:29.217789: step 2920, loss = 1.43 (490.5 examples/sec; 0.261 sec/batch)
2016-12-08 10:35:32.159809: step 2930, loss = 1.15 (431.5 examples/sec; 0.297 sec/batch)
2016-12-08 10:35:35.026752: step 2940, loss = 1.37 (442.6 examples/sec; 0.289 sec/batch)
2016-12-08 10:35:37.822770: step 2950, loss = 1.38 (445.6 examples/sec; 0.287 sec/batch)
2016-12-08 10:35:40.505587: step 2960, loss = 1.21 (445.5 examples/sec; 0.287 sec/batch)
2016-12-08 10:35:43.323576: step 2970, loss = 1.39 (467.2 examples/sec; 0.274 sec/batch)
2016-12-08 10:35:46.071633: step 2980, loss = 1.32 (504.1 examples/sec; 0.254 sec/batch)
2016-12-08 10:35:48.889918: step 2990, loss = 1.37 (443.8 examples/sec; 0.288 sec/batch)
2016-12-08 10:35:51.833296: step 3000, loss = 1.36 (390.5 examples/sec; 0.328 sec/batch)
2016-12-08 10:35:55.699053: step 3010, loss = 1.35 (451.8 examples/sec; 0.283 sec/batch)
2016-12-08 10:35:58.562852: step 3020, loss = 1.12 (461.7 examples/sec; 0.277 sec/batch)
2016-12-08 10:36:01.383281: step 3030, loss = 1.38 (418.4 examples/sec; 0.306 sec/batch)
2016-12-08 10:36:04.136499: step 3040, loss = 1.19 (418.9 examples/sec; 0.306 sec/batch)
2016-12-08 10:36:06.920929: step 3050, loss = 1.28 (464.7 examples/sec; 0.275 sec/batch)
2016-12-08 10:36:09.569007: step 3060, loss = 1.37 (515.3 examples/sec; 0.248 sec/batch)
2016-12-08 10:36:12.274001: step 3070, loss = 1.21 (480.6 examples/sec; 0.266 sec/batch)
2016-12-08 10:36:15.104572: step 3080, loss = 1.27 (430.4 examples/sec; 0.297 sec/batch)
2016-12-08 10:36:18.104835: step 3090, loss = 1.36 (473.1 examples/sec; 0.271 sec/batch)
2016-12-08 10:36:20.922209: step 3100, loss = 1.37 (517.4 examples/sec; 0.247 sec/batch)
2016-12-08 10:36:24.130563: step 3110, loss = 1.36 (445.9 examples/sec; 0.287 sec/batch)
2016-12-08 10:36:26.947177: step 3120, loss = 1.30 (386.4 examples/sec; 0.331 sec/batch)
2016-12-08 10:36:29.675488: step 3130, loss = 1.32 (514.7 examples/sec; 0.249 sec/batch)
2016-12-08 10:36:32.454044: step 3140, loss = 1.15 (501.8 examples/sec; 0.255 sec/batch)
2016-12-08 10:36:35.287602: step 3150, loss = 1.30 (445.5 examples/sec; 0.287 sec/batch)
2016-12-08 10:36:38.017213: step 3160, loss = 1.31 (491.8 examples/sec; 0.260 sec/batch)
2016-12-08 10:36:40.855930: step 3170, loss = 1.21 (418.8 examples/sec; 0.306 sec/batch)
2016-12-08 10:36:43.762261: step 3180, loss = 1.09 (440.3 examples/sec; 0.291 sec/batch)
2016-12-08 10:36:46.519268: step 3190, loss = 1.33 (526.6 examples/sec; 0.243 sec/batch)
2016-12-08 10:36:49.202931: step 3200, loss = 1.22 (452.6 examples/sec; 0.283 sec/batch)
2016-12-08 10:36:52.312267: step 3210, loss = 1.15 (537.9 examples/sec; 0.238 sec/batch)
2016-12-08 10:36:55.301702: step 3220, loss = 1.27 (402.0 examples/sec; 0.318 sec/batch)
2016-12-08 10:36:58.104490: step 3230, loss = 1.28 (433.7 examples/sec; 0.295 sec/batch)
2016-12-08 10:37:00.829080: step 3240, loss = 1.29 (477.2 examples/sec; 0.268 sec/batch)
2016-12-08 10:37:03.615838: step 3250, loss = 1.09 (462.6 examples/sec; 0.277 sec/batch)
2016-12-08 10:37:06.348941: step 3260, loss = 1.09 (464.7 examples/sec; 0.275 sec/batch)
2016-12-08 10:37:09.076814: step 3270, loss = 1.44 (427.7 examples/sec; 0.299 sec/batch)
2016-12-08 10:37:11.806844: step 3280, loss = 1.16 (454.7 examples/sec; 0.282 sec/batch)
2016-12-08 10:37:14.486608: step 3290, loss = 1.15 (486.6 examples/sec; 0.263 sec/batch)
2016-12-08 10:37:17.151037: step 3300, loss = 1.12 (494.5 examples/sec; 0.259 sec/batch)
2016-12-08 10:37:20.121613: step 3310, loss = 0.92 (551.9 examples/sec; 0.232 sec/batch)
2016-12-08 10:37:22.912882: step 3320, loss = 1.23 (428.0 examples/sec; 0.299 sec/batch)
2016-12-08 10:37:25.573675: step 3330, loss = 1.10 (438.4 examples/sec; 0.292 sec/batch)
2016-12-08 10:37:28.378586: step 3340, loss = 1.12 (421.8 examples/sec; 0.303 sec/batch)
2016-12-08 10:37:31.295148: step 3350, loss = 1.08 (524.8 examples/sec; 0.244 sec/batch)
2016-12-08 10:37:34.078663: step 3360, loss = 1.13 (452.3 examples/sec; 0.283 sec/batch)
2016-12-08 10:37:36.921842: step 3370, loss = 1.32 (492.2 examples/sec; 0.260 sec/batch)
2016-12-08 10:37:39.655178: step 3380, loss = 1.17 (451.0 examples/sec; 0.284 sec/batch)
2016-12-08 10:37:42.515750: step 3390, loss = 1.07 (462.6 examples/sec; 0.277 sec/batch)
2016-12-08 10:37:45.271987: step 3400, loss = 1.08 (558.6 examples/sec; 0.229 sec/batch)
2016-12-08 10:37:48.352017: step 3410, loss = 1.09 (482.0 examples/sec; 0.266 sec/batch)
2016-12-08 10:37:51.236121: step 3420, loss = 1.23 (474.3 examples/sec; 0.270 sec/batch)
2016-12-08 10:37:54.027872: step 3430, loss = 1.27 (400.9 examples/sec; 0.319 sec/batch)
2016-12-08 10:37:56.841911: step 3440, loss = 1.13 (592.9 examples/sec; 0.216 sec/batch)
2016-12-08 10:37:59.599814: step 3450, loss = 1.24 (431.4 examples/sec; 0.297 sec/batch)
2016-12-08 10:38:02.380103: step 3460, loss = 1.22 (456.5 examples/sec; 0.280 sec/batch)
2016-12-08 10:38:05.148214: step 3470, loss = 1.09 (484.5 examples/sec; 0.264 sec/batch)
2016-12-08 10:38:07.994715: step 3480, loss = 1.05 (516.5 examples/sec; 0.248 sec/batch)
2016-12-08 10:38:10.775457: step 3490, loss = 1.16 (415.0 examples/sec; 0.308 sec/batch)
2016-12-08 10:38:13.507421: step 3500, loss = 1.13 (481.4 examples/sec; 0.266 sec/batch)
2016-12-08 10:38:16.448826: step 3510, loss = 1.19 (420.2 examples/sec; 0.305 sec/batch)
2016-12-08 10:38:19.112647: step 3520, loss = 1.07 (542.1 examples/sec; 0.236 sec/batch)
2016-12-08 10:38:21.949234: step 3530, loss = 1.10 (434.2 examples/sec; 0.295 sec/batch)
2016-12-08 10:38:24.841328: step 3540, loss = 1.00 (414.0 examples/sec; 0.309 sec/batch)
2016-12-08 10:38:27.705933: step 3550, loss = 1.09 (407.7 examples/sec; 0.314 sec/batch)
2016-12-08 10:38:30.441952: step 3560, loss = 1.17 (445.7 examples/sec; 0.287 sec/batch)
2016-12-08 10:38:33.297154: step 3570, loss = 1.15 (462.5 examples/sec; 0.277 sec/batch)
2016-12-08 10:38:35.990585: step 3580, loss = 1.14 (500.1 examples/sec; 0.256 sec/batch)
2016-12-08 10:38:38.674935: step 3590, loss = 1.07 (503.6 examples/sec; 0.254 sec/batch)
2016-12-08 10:38:41.602940: step 3600, loss = 1.17 (443.2 examples/sec; 0.289 sec/batch)
2016-12-08 10:38:44.661391: step 3610, loss = 0.97 (476.9 examples/sec; 0.268 sec/batch)
2016-12-08 10:38:47.544245: step 3620, loss = 1.15 (426.1 examples/sec; 0.300 sec/batch)
2016-12-08 10:38:50.421837: step 3630, loss = 1.09 (503.1 examples/sec; 0.254 sec/batch)
2016-12-08 10:38:53.136287: step 3640, loss = 1.20 (455.9 examples/sec; 0.281 sec/batch)
2016-12-08 10:38:55.986767: step 3650, loss = 1.01 (444.1 examples/sec; 0.288 sec/batch)
2016-12-08 10:38:58.815270: step 3660, loss = 1.19 (410.0 examples/sec; 0.312 sec/batch)
2016-12-08 10:39:01.626181: step 3670, loss = 1.14 (447.4 examples/sec; 0.286 sec/batch)
2016-12-08 10:39:04.489405: step 3680, loss = 1.22 (490.0 examples/sec; 0.261 sec/batch)
2016-12-08 10:39:07.267194: step 3690, loss = 1.16 (398.4 examples/sec; 0.321 sec/batch)
2016-12-08 10:39:09.965416: step 3700, loss = 0.99 (469.2 examples/sec; 0.273 sec/batch)
2016-12-08 10:39:13.137394: step 3710, loss = 1.15 (476.4 examples/sec; 0.269 sec/batch)
2016-12-08 10:39:15.990470: step 3720, loss = 0.98 (415.9 examples/sec; 0.308 sec/batch)
2016-12-08 10:39:18.825416: step 3730, loss = 1.31 (451.9 examples/sec; 0.283 sec/batch)
2016-12-08 10:39:21.552093: step 3740, loss = 1.13 (438.4 examples/sec; 0.292 sec/batch)
2016-12-08 10:39:24.348526: step 3750, loss = 1.06 (535.7 examples/sec; 0.239 sec/batch)
2016-12-08 10:39:27.022699: step 3760, loss = 1.23 (422.3 examples/sec; 0.303 sec/batch)
2016-12-08 10:39:29.774924: step 3770, loss = 1.02 (489.6 examples/sec; 0.261 sec/batch)
2016-12-08 10:39:32.469195: step 3780, loss = 1.08 (449.1 examples/sec; 0.285 sec/batch)
2016-12-08 10:39:35.426178: step 3790, loss = 1.02 (545.8 examples/sec; 0.235 sec/batch)
2016-12-08 10:39:38.163285: step 3800, loss = 1.11 (427.6 examples/sec; 0.299 sec/batch)
2016-12-08 10:39:41.400220: step 3810, loss = 1.17 (529.5 examples/sec; 0.242 sec/batch)
2016-12-08 10:39:44.173336: step 3820, loss = 1.29 (512.9 examples/sec; 0.250 sec/batch)
2016-12-08 10:39:46.994345: step 3830, loss = 1.12 (481.0 examples/sec; 0.266 sec/batch)
2016-12-08 10:39:49.850792: step 3840, loss = 1.21 (507.3 examples/sec; 0.252 sec/batch)
2016-12-08 10:39:52.738503: step 3850, loss = 1.31 (462.7 examples/sec; 0.277 sec/batch)
2016-12-08 10:39:55.543199: step 3860, loss = 1.25 (484.6 examples/sec; 0.264 sec/batch)
2016-12-08 10:39:58.410154: step 3870, loss = 1.05 (475.8 examples/sec; 0.269 sec/batch)
2016-12-08 10:40:01.225031: step 3880, loss = 1.15 (497.4 examples/sec; 0.257 sec/batch)
2016-12-08 10:40:04.128191: step 3890, loss = 1.28 (403.0 examples/sec; 0.318 sec/batch)
2016-12-08 10:40:06.974649: step 3900, loss = 1.15 (428.3 examples/sec; 0.299 sec/batch)
2016-12-08 10:40:10.106617: step 3910, loss = 1.13 (528.4 examples/sec; 0.242 sec/batch)
2016-12-08 10:40:12.958171: step 3920, loss = 1.19 (419.6 examples/sec; 0.305 sec/batch)
2016-12-08 10:40:15.837258: step 3930, loss = 1.16 (400.5 examples/sec; 0.320 sec/batch)
2016-12-08 10:40:18.563228: step 3940, loss = 1.11 (438.7 examples/sec; 0.292 sec/batch)
2016-12-08 10:40:21.460067: step 3950, loss = 1.08 (442.6 examples/sec; 0.289 sec/batch)
2016-12-08 10:40:24.365604: step 3960, loss = 1.01 (458.3 examples/sec; 0.279 sec/batch)
2016-12-08 10:40:27.165733: step 3970, loss = 1.02 (464.2 examples/sec; 0.276 sec/batch)
2016-12-08 10:40:29.980984: step 3980, loss = 1.13 (423.8 examples/sec; 0.302 sec/batch)
2016-12-08 10:40:32.658140: step 3990, loss = 1.19 (489.5 examples/sec; 0.261 sec/batch)
2016-12-08 10:40:35.358259: step 4000, loss = 1.05 (468.1 examples/sec; 0.273 sec/batch)
2016-12-08 10:40:39.022686: step 4010, loss = 1.21 (446.7 examples/sec; 0.287 sec/batch)
2016-12-08 10:40:41.918609: step 4020, loss = 1.18 (433.4 examples/sec; 0.295 sec/batch)
2016-12-08 10:40:44.705883: step 4030, loss = 1.19 (423.5 examples/sec; 0.302 sec/batch)
2016-12-08 10:40:47.583904: step 4040, loss = 1.20 (477.6 examples/sec; 0.268 sec/batch)
2016-12-08 10:40:50.447218: step 4050, loss = 1.07 (446.3 examples/sec; 0.287 sec/batch)
2016-12-08 10:40:53.314445: step 4060, loss = 1.06 (457.6 examples/sec; 0.280 sec/batch)
2016-12-08 10:40:56.053643: step 4070, loss = 1.47 (497.6 examples/sec; 0.257 sec/batch)
2016-12-08 10:40:58.913766: step 4080, loss = 1.15 (412.4 examples/sec; 0.310 sec/batch)
2016-12-08 10:41:01.875557: step 4090, loss = 1.27 (423.0 examples/sec; 0.303 sec/batch)
2016-12-08 10:41:04.830789: step 4100, loss = 1.04 (436.7 examples/sec; 0.293 sec/batch)
2016-12-08 10:41:08.050487: step 4110, loss = 1.01 (413.7 examples/sec; 0.309 sec/batch)
2016-12-08 10:41:10.862343: step 4120, loss = 1.06 (429.0 examples/sec; 0.298 sec/batch)
2016-12-08 10:41:13.576208: step 4130, loss = 1.13 (504.7 examples/sec; 0.254 sec/batch)
2016-12-08 10:41:16.394066: step 4140, loss = 1.07 (454.8 examples/sec; 0.281 sec/batch)
2016-12-08 10:41:19.093954: step 4150, loss = 1.06 (547.5 examples/sec; 0.234 sec/batch)
2016-12-08 10:41:21.951103: step 4160, loss = 1.08 (444.3 examples/sec; 0.288 sec/batch)
2016-12-08 10:41:24.847128: step 4170, loss = 1.02 (428.2 examples/sec; 0.299 sec/batch)
2016-12-08 10:41:27.576100: step 4180, loss = 1.03 (460.7 examples/sec; 0.278 sec/batch)
2016-12-08 10:41:30.497695: step 4190, loss = 1.23 (433.2 examples/sec; 0.295 sec/batch)
2016-12-08 10:41:33.309075: step 4200, loss = 1.13 (435.0 examples/sec; 0.294 sec/batch)
2016-12-08 10:41:36.358049: step 4210, loss = 1.09 (444.3 examples/sec; 0.288 sec/batch)
2016-12-08 10:41:39.274417: step 4220, loss = 0.96 (463.4 examples/sec; 0.276 sec/batch)
2016-12-08 10:41:42.072203: step 4230, loss = 0.99 (426.2 examples/sec; 0.300 sec/batch)
2016-12-08 10:41:44.863580: step 4240, loss = 0.98 (527.9 examples/sec; 0.242 sec/batch)
2016-12-08 10:41:47.640398: step 4250, loss = 1.09 (429.4 examples/sec; 0.298 sec/batch)
2016-12-08 10:41:50.607704: step 4260, loss = 1.07 (503.6 examples/sec; 0.254 sec/batch)
2016-12-08 10:41:53.334776: step 4270, loss = 0.98 (509.5 examples/sec; 0.251 sec/batch)
2016-12-08 10:41:56.083192: step 4280, loss = 1.11 (457.6 examples/sec; 0.280 sec/batch)
2016-12-08 10:41:58.922059: step 4290, loss = 1.11 (422.0 examples/sec; 0.303 sec/batch)
2016-12-08 10:42:01.800640: step 4300, loss = 1.06 (374.6 examples/sec; 0.342 sec/batch)
2016-12-08 10:42:04.948213: step 4310, loss = 1.11 (491.3 examples/sec; 0.261 sec/batch)
2016-12-08 10:42:07.831394: step 4320, loss = 1.08 (447.7 examples/sec; 0.286 sec/batch)
2016-12-08 10:42:10.660493: step 4330, loss = 1.03 (447.3 examples/sec; 0.286 sec/batch)
2016-12-08 10:42:13.543955: step 4340, loss = 1.00 (432.7 examples/sec; 0.296 sec/batch)
2016-12-08 10:42:16.288301: step 4350, loss = 1.08 (492.1 examples/sec; 0.260 sec/batch)
2016-12-08 10:42:19.050787: step 4360, loss = 1.18 (544.9 examples/sec; 0.235 sec/batch)
2016-12-08 10:42:21.866853: step 4370, loss = 1.28 (403.5 examples/sec; 0.317 sec/batch)
2016-12-08 10:42:24.810599: step 4380, loss = 1.06 (454.4 examples/sec; 0.282 sec/batch)
2016-12-08 10:42:27.481526: step 4390, loss = 1.27 (597.0 examples/sec; 0.214 sec/batch)
2016-12-08 10:42:30.405524: step 4400, loss = 0.93 (428.6 examples/sec; 0.299 sec/batch)
2016-12-08 10:42:33.644827: step 4410, loss = 0.97 (467.5 examples/sec; 0.274 sec/batch)
2016-12-08 10:42:36.389668: step 4420, loss = 1.07 (527.2 examples/sec; 0.243 sec/batch)
2016-12-08 10:42:39.363722: step 4430, loss = 1.02 (404.2 examples/sec; 0.317 sec/batch)
2016-12-08 10:42:42.196716: step 4440, loss = 0.99 (508.6 examples/sec; 0.252 sec/batch)
2016-12-08 10:42:45.180136: step 4450, loss = 1.10 (417.0 examples/sec; 0.307 sec/batch)
2016-12-08 10:42:48.037825: step 4460, loss = 1.04 (550.3 examples/sec; 0.233 sec/batch)
2016-12-08 10:42:50.760168: step 4470, loss = 1.16 (469.1 examples/sec; 0.273 sec/batch)
2016-12-08 10:42:53.673447: step 4480, loss = 0.94 (416.5 examples/sec; 0.307 sec/batch)
2016-12-08 10:42:56.603004: step 4490, loss = 0.94 (411.6 examples/sec; 0.311 sec/batch)
2016-12-08 10:42:59.345110: step 4500, loss = 1.00 (504.5 examples/sec; 0.254 sec/batch)
2016-12-08 10:43:02.405228: step 4510, loss = 1.00 (439.4 examples/sec; 0.291 sec/batch)
2016-12-08 10:43:05.309855: step 4520, loss = 0.99 (474.3 examples/sec; 0.270 sec/batch)
2016-12-08 10:43:08.104862: step 4530, loss = 0.97 (490.4 examples/sec; 0.261 sec/batch)
2016-12-08 10:43:11.018876: step 4540, loss = 0.91 (440.7 examples/sec; 0.290 sec/batch)
2016-12-08 10:43:13.757260: step 4550, loss = 1.01 (421.3 examples/sec; 0.304 sec/batch)
2016-12-08 10:43:16.639570: step 4560, loss = 1.07 (447.6 examples/sec; 0.286 sec/batch)
2016-12-08 10:43:19.341646: step 4570, loss = 1.07 (492.8 examples/sec; 0.260 sec/batch)
2016-12-08 10:43:22.205371: step 4580, loss = 0.95 (466.4 examples/sec; 0.274 sec/batch)
2016-12-08 10:43:24.975956: step 4590, loss = 1.07 (423.5 examples/sec; 0.302 sec/batch)
2016-12-08 10:43:27.850747: step 4600, loss = 0.96 (458.6 examples/sec; 0.279 sec/batch)
2016-12-08 10:43:31.001527: step 4610, loss = 1.08 (411.5 examples/sec; 0.311 sec/batch)
2016-12-08 10:43:33.943926: step 4620, loss = 0.95 (439.1 examples/sec; 0.291 sec/batch)
2016-12-08 10:43:36.786614: step 4630, loss = 0.95 (378.7 examples/sec; 0.338 sec/batch)
2016-12-08 10:43:39.619698: step 4640, loss = 0.89 (477.5 examples/sec; 0.268 sec/batch)
2016-12-08 10:43:42.408049: step 4650, loss = 0.95 (449.5 examples/sec; 0.285 sec/batch)
2016-12-08 10:43:45.279332: step 4660, loss = 0.97 (451.6 examples/sec; 0.283 sec/batch)
2016-12-08 10:43:47.927716: step 4670, loss = 0.94 (505.9 examples/sec; 0.253 sec/batch)
2016-12-08 10:43:50.966386: step 4680, loss = 0.87 (439.3 examples/sec; 0.291 sec/batch)
2016-12-08 10:43:53.831501: step 4690, loss = 1.20 (438.1 examples/sec; 0.292 sec/batch)
2016-12-08 10:43:56.644958: step 4700, loss = 0.97 (444.3 examples/sec; 0.288 sec/batch)
2016-12-08 10:43:59.751269: step 4710, loss = 1.18 (440.3 examples/sec; 0.291 sec/batch)
2016-12-08 10:44:02.581224: step 4720, loss = 1.07 (447.3 examples/sec; 0.286 sec/batch)
2016-12-08 10:44:05.255741: step 4730, loss = 0.96 (506.0 examples/sec; 0.253 sec/batch)
2016-12-08 10:44:08.083478: step 4740, loss = 1.23 (474.0 examples/sec; 0.270 sec/batch)
2016-12-08 10:44:10.972503: step 4750, loss = 1.03 (487.7 examples/sec; 0.262 sec/batch)
2016-12-08 10:44:13.882579: step 4760, loss = 0.89 (409.0 examples/sec; 0.313 sec/batch)
2016-12-08 10:44:16.589061: step 4770, loss = 0.99 (454.7 examples/sec; 0.282 sec/batch)
2016-12-08 10:44:19.373749: step 4780, loss = 1.00 (479.0 examples/sec; 0.267 sec/batch)
2016-12-08 10:44:22.128708: step 4790, loss = 1.10 (423.9 examples/sec; 0.302 sec/batch)
2016-12-08 10:44:24.958543: step 4800, loss = 1.00 (435.9 examples/sec; 0.294 sec/batch)
2016-12-08 10:44:28.106929: step 4810, loss = 1.02 (436.2 examples/sec; 0.293 sec/batch)
2016-12-08 10:44:30.782749: step 4820, loss = 0.93 (490.6 examples/sec; 0.261 sec/batch)
2016-12-08 10:44:33.624639: step 4830, loss = 1.03 (466.7 examples/sec; 0.274 sec/batch)
2016-12-08 10:44:36.268676: step 4840, loss = 1.17 (457.9 examples/sec; 0.280 sec/batch)
2016-12-08 10:44:39.140539: step 4850, loss = 0.90 (497.2 examples/sec; 0.257 sec/batch)
2016-12-08 10:44:41.945400: step 4860, loss = 0.84 (466.8 examples/sec; 0.274 sec/batch)
2016-12-08 10:44:44.720640: step 4870, loss = 1.08 (430.5 examples/sec; 0.297 sec/batch)
2016-12-08 10:44:47.492944: step 4880, loss = 0.88 (495.9 examples/sec; 0.258 sec/batch)
2016-12-08 10:44:50.388029: step 4890, loss = 1.20 (471.4 examples/sec; 0.272 sec/batch)
2016-12-08 10:44:53.291985: step 4900, loss = 1.13 (476.3 examples/sec; 0.269 sec/batch)
2016-12-08 10:44:56.452591: step 4910, loss = 1.05 (385.6 examples/sec; 0.332 sec/batch)
2016-12-08 10:44:59.362536: step 4920, loss = 0.95 (486.4 examples/sec; 0.263 sec/batch)
2016-12-08 10:45:02.427548: step 4930, loss = 1.03 (444.7 examples/sec; 0.288 sec/batch)
2016-12-08 10:45:05.234479: step 4940, loss = 0.87 (450.9 examples/sec; 0.284 sec/batch)
2016-12-08 10:45:08.134779: step 4950, loss = 1.00 (445.3 examples/sec; 0.287 sec/batch)
2016-12-08 10:45:10.966672: step 4960, loss = 0.95 (401.4 examples/sec; 0.319 sec/batch)
2016-12-08 10:45:13.776439: step 4970, loss = 1.12 (411.3 examples/sec; 0.311 sec/batch)
2016-12-08 10:45:16.569403: step 4980, loss = 0.78 (449.6 examples/sec; 0.285 sec/batch)
2016-12-08 10:45:19.455170: step 4990, loss = 0.96 (459.7 examples/sec; 0.278 sec/batch)
2016-12-08 10:45:22.237615: step 5000, loss = 1.10 (459.7 examples/sec; 0.278 sec/batch)
2016-12-08 10:45:25.913562: step 5010, loss = 1.01 (430.5 examples/sec; 0.297 sec/batch)
2016-12-08 10:45:28.718108: step 5020, loss = 0.97 (458.2 examples/sec; 0.279 sec/batch)
2016-12-08 10:45:31.489445: step 5030, loss = 1.04 (422.4 examples/sec; 0.303 sec/batch)
2016-12-08 10:45:34.329181: step 5040, loss = 0.90 (467.7 examples/sec; 0.274 sec/batch)
2016-12-08 10:45:37.321939: step 5050, loss = 0.97 (462.8 examples/sec; 0.277 sec/batch)
2016-12-08 10:45:40.072415: step 5060, loss = 1.00 (499.5 examples/sec; 0.256 sec/batch)
2016-12-08 10:45:42.700476: step 5070, loss = 1.06 (441.8 examples/sec; 0.290 sec/batch)
2016-12-08 10:45:45.453822: step 5080, loss = 1.15 (458.1 examples/sec; 0.279 sec/batch)
2016-12-08 10:45:48.243663: step 5090, loss = 1.16 (495.4 examples/sec; 0.258 sec/batch)
2016-12-08 10:45:50.904824: step 5100, loss = 0.98 (447.6 examples/sec; 0.286 sec/batch)
2016-12-08 10:45:54.070576: step 5110, loss = 0.99 (476.3 examples/sec; 0.269 sec/batch)
2016-12-08 10:45:57.034279: step 5120, loss = 0.93 (409.7 examples/sec; 0.312 sec/batch)
2016-12-08 10:45:59.767732: step 5130, loss = 0.87 (449.4 examples/sec; 0.285 sec/batch)
2016-12-08 10:46:02.637866: step 5140, loss = 1.05 (448.3 examples/sec; 0.286 sec/batch)
2016-12-08 10:46:05.529284: step 5150, loss = 0.96 (368.5 examples/sec; 0.347 sec/batch)
2016-12-08 10:46:08.500042: step 5160, loss = 1.11 (416.6 examples/sec; 0.307 sec/batch)
2016-12-08 10:46:11.311530: step 5170, loss = 1.07 (503.5 examples/sec; 0.254 sec/batch)
2016-12-08 10:46:14.154416: step 5180, loss = 1.03 (456.7 examples/sec; 0.280 sec/batch)
2016-12-08 10:46:16.983771: step 5190, loss = 1.17 (463.2 examples/sec; 0.276 sec/batch)
2016-12-08 10:46:19.764722: step 5200, loss = 0.95 (478.7 examples/sec; 0.267 sec/batch)
2016-12-08 10:46:22.833839: step 5210, loss = 0.99 (481.6 examples/sec; 0.266 sec/batch)
2016-12-08 10:46:25.787454: step 5220, loss = 1.01 (463.9 examples/sec; 0.276 sec/batch)
2016-12-08 10:46:28.498176: step 5230, loss = 0.99 (509.4 examples/sec; 0.251 sec/batch)
2016-12-08 10:46:31.280189: step 5240, loss = 0.99 (444.6 examples/sec; 0.288 sec/batch)
2016-12-08 10:46:34.046981: step 5250, loss = 1.01 (458.9 examples/sec; 0.279 sec/batch)
2016-12-08 10:46:36.735881: step 5260, loss = 0.98 (466.3 examples/sec; 0.274 sec/batch)
2016-12-08 10:46:39.502736: step 5270, loss = 1.02 (488.5 examples/sec; 0.262 sec/batch)
2016-12-08 10:46:42.290504: step 5280, loss = 0.88 (479.8 examples/sec; 0.267 sec/batch)
2016-12-08 10:46:44.998533: step 5290, loss = 1.00 (451.4 examples/sec; 0.284 sec/batch)
2016-12-08 10:46:47.670335: step 5300, loss = 1.22 (430.0 examples/sec; 0.298 sec/batch)
2016-12-08 10:46:50.718132: step 5310, loss = 0.94 (539.2 examples/sec; 0.237 sec/batch)
2016-12-08 10:46:53.618533: step 5320, loss = 0.80 (374.3 examples/sec; 0.342 sec/batch)
2016-12-08 10:46:56.368018: step 5330, loss = 0.97 (500.3 examples/sec; 0.256 sec/batch)
2016-12-08 10:46:59.110490: step 5340, loss = 0.98 (470.0 examples/sec; 0.272 sec/batch)
2016-12-08 10:47:02.016075: step 5350, loss = 1.02 (429.4 examples/sec; 0.298 sec/batch)
2016-12-08 10:47:04.583049: step 5360, loss = 0.88 (425.2 examples/sec; 0.301 sec/batch)
2016-12-08 10:47:07.327364: step 5370, loss = 0.99 (455.7 examples/sec; 0.281 sec/batch)
2016-12-08 10:47:10.079604: step 5380, loss = 1.18 (590.5 examples/sec; 0.217 sec/batch)
2016-12-08 10:47:12.969558: step 5390, loss = 0.86 (467.3 examples/sec; 0.274 sec/batch)
2016-12-08 10:47:15.832843: step 5400, loss = 0.89 (457.6 examples/sec; 0.280 sec/batch)
2016-12-08 10:47:18.913766: step 5410, loss = 0.91 (435.9 examples/sec; 0.294 sec/batch)
2016-12-08 10:47:21.730136: step 5420, loss = 0.94 (452.3 examples/sec; 0.283 sec/batch)
2016-12-08 10:47:24.569833: step 5430, loss = 0.93 (469.3 examples/sec; 0.273 sec/batch)
2016-12-08 10:47:27.468378: step 5440, loss = 1.08 (423.8 examples/sec; 0.302 sec/batch)
2016-12-08 10:47:30.239127: step 5450, loss = 0.95 (419.3 examples/sec; 0.305 sec/batch)
2016-12-08 10:47:33.041355: step 5460, loss = 0.93 (460.6 examples/sec; 0.278 sec/batch)
2016-12-08 10:47:35.960902: step 5470, loss = 1.10 (445.3 examples/sec; 0.287 sec/batch)
2016-12-08 10:47:38.573765: step 5480, loss = 1.08 (543.5 examples/sec; 0.236 sec/batch)
2016-12-08 10:47:41.513401: step 5490, loss = 0.94 (427.4 examples/sec; 0.299 sec/batch)
2016-12-08 10:47:44.296808: step 5500, loss = 0.97 (392.9 examples/sec; 0.326 sec/batch)
2016-12-08 10:47:47.388909: step 5510, loss = 1.00 (425.7 examples/sec; 0.301 sec/batch)
2016-12-08 10:47:50.369926: step 5520, loss = 1.06 (360.0 examples/sec; 0.356 sec/batch)
2016-12-08 10:47:53.305236: step 5530, loss = 0.91 (456.4 examples/sec; 0.280 sec/batch)
2016-12-08 10:47:55.956795: step 5540, loss = 0.98 (512.1 examples/sec; 0.250 sec/batch)
2016-12-08 10:47:58.810635: step 5550, loss = 0.96 (401.7 examples/sec; 0.319 sec/batch)
2016-12-08 10:48:01.662102: step 5560, loss = 1.03 (466.9 examples/sec; 0.274 sec/batch)
2016-12-08 10:48:04.344179: step 5570, loss = 1.15 (490.1 examples/sec; 0.261 sec/batch)
2016-12-08 10:48:06.990976: step 5580, loss = 1.10 (508.4 examples/sec; 0.252 sec/batch)
2016-12-08 10:48:09.744362: step 5590, loss = 1.05 (461.7 examples/sec; 0.277 sec/batch)
2016-12-08 10:48:12.501195: step 5600, loss = 1.08 (458.8 examples/sec; 0.279 sec/batch)
2016-12-08 10:48:15.475342: step 5610, loss = 0.99 (466.8 examples/sec; 0.274 sec/batch)
2016-12-08 10:48:18.338407: step 5620, loss = 1.03 (422.5 examples/sec; 0.303 sec/batch)
2016-12-08 10:48:20.992520: step 5630, loss = 0.91 (459.9 examples/sec; 0.278 sec/batch)
2016-12-08 10:48:23.841772: step 5640, loss = 1.03 (458.8 examples/sec; 0.279 sec/batch)
2016-12-08 10:48:26.489897: step 5650, loss = 1.10 (495.6 examples/sec; 0.258 sec/batch)
2016-12-08 10:48:29.177569: step 5660, loss = 1.00 (536.0 examples/sec; 0.239 sec/batch)
2016-12-08 10:48:32.131659: step 5670, loss = 0.96 (436.1 examples/sec; 0.293 sec/batch)
2016-12-08 10:48:34.998374: step 5680, loss = 0.94 (435.6 examples/sec; 0.294 sec/batch)
2016-12-08 10:48:37.736106: step 5690, loss = 1.05 (472.2 examples/sec; 0.271 sec/batch)
2016-12-08 10:48:40.475086: step 5700, loss = 0.98 (470.9 examples/sec; 0.272 sec/batch)
2016-12-08 10:48:43.583337: step 5710, loss = 1.00 (502.4 examples/sec; 0.255 sec/batch)
2016-12-08 10:48:46.450693: step 5720, loss = 1.29 (442.5 examples/sec; 0.289 sec/batch)
2016-12-08 10:48:49.290888: step 5730, loss = 1.10 (493.7 examples/sec; 0.259 sec/batch)
2016-12-08 10:48:52.159746: step 5740, loss = 1.00 (428.8 examples/sec; 0.299 sec/batch)
2016-12-08 10:48:54.927071: step 5750, loss = 0.91 (450.2 examples/sec; 0.284 sec/batch)
2016-12-08 10:48:57.634943: step 5760, loss = 0.88 (477.8 examples/sec; 0.268 sec/batch)
2016-12-08 10:49:00.402071: step 5770, loss = 1.05 (396.9 examples/sec; 0.322 sec/batch)
2016-12-08 10:49:03.240267: step 5780, loss = 0.96 (435.9 examples/sec; 0.294 sec/batch)
2016-12-08 10:49:06.008545: step 5790, loss = 1.04 (535.1 examples/sec; 0.239 sec/batch)
2016-12-08 10:49:08.852538: step 5800, loss = 0.98 (480.5 examples/sec; 0.266 sec/batch)
2016-12-08 10:49:12.026893: step 5810, loss = 1.05 (409.5 examples/sec; 0.313 sec/batch)
2016-12-08 10:49:14.776897: step 5820, loss = 0.98 (390.5 examples/sec; 0.328 sec/batch)
2016-12-08 10:49:17.601798: step 5830, loss = 1.05 (448.3 examples/sec; 0.286 sec/batch)
2016-12-08 10:49:20.428366: step 5840, loss = 0.99 (425.1 examples/sec; 0.301 sec/batch)
2016-12-08 10:49:23.228636: step 5850, loss = 1.04 (434.4 examples/sec; 0.295 sec/batch)
2016-12-08 10:49:26.013991: step 5860, loss = 0.83 (455.0 examples/sec; 0.281 sec/batch)
2016-12-08 10:49:28.906869: step 5870, loss = 0.92 (402.6 examples/sec; 0.318 sec/batch)
2016-12-08 10:49:31.676770: step 5880, loss = 0.98 (490.5 examples/sec; 0.261 sec/batch)
2016-12-08 10:49:34.393841: step 5890, loss = 0.91 (505.4 examples/sec; 0.253 sec/batch)
2016-12-08 10:49:37.271302: step 5900, loss = 1.03 (449.1 examples/sec; 0.285 sec/batch)
2016-12-08 10:49:40.292496: step 5910, loss = 0.95 (552.4 examples/sec; 0.232 sec/batch)
2016-12-08 10:49:43.180409: step 5920, loss = 0.89 (452.7 examples/sec; 0.283 sec/batch)
2016-12-08 10:49:45.900595: step 5930, loss = 0.86 (436.0 examples/sec; 0.294 sec/batch)
2016-12-08 10:49:48.585976: step 5940, loss = 0.99 (550.9 examples/sec; 0.232 sec/batch)
2016-12-08 10:49:51.364634: step 5950, loss = 0.77 (516.7 examples/sec; 0.248 sec/batch)
2016-12-08 10:49:54.140765: step 5960, loss = 1.01 (511.1 examples/sec; 0.250 sec/batch)
2016-12-08 10:49:57.028442: step 5970, loss = 0.89 (455.0 examples/sec; 0.281 sec/batch)
2016-12-08 10:49:59.743162: step 5980, loss = 1.09 (485.9 examples/sec; 0.263 sec/batch)
2016-12-08 10:50:02.502291: step 5990, loss = 0.89 (411.0 examples/sec; 0.311 sec/batch)
2016-12-08 10:50:05.314704: step 6000, loss = 1.08 (470.3 examples/sec; 0.272 sec/batch)
2016-12-08 10:50:09.028054: step 6010, loss = 1.08 (464.0 examples/sec; 0.276 sec/batch)
2016-12-08 10:50:11.760705: step 6020, loss = 1.06 (468.8 examples/sec; 0.273 sec/batch)
2016-12-08 10:50:14.676938: step 6030, loss = 1.03 (371.1 examples/sec; 0.345 sec/batch)
2016-12-08 10:50:17.497060: step 6040, loss = 1.03 (484.7 examples/sec; 0.264 sec/batch)
2016-12-08 10:50:20.097934: step 6050, loss = 0.99 (472.7 examples/sec; 0.271 sec/batch)
2016-12-08 10:50:22.981899: step 6060, loss = 1.00 (403.8 examples/sec; 0.317 sec/batch)
2016-12-08 10:50:25.782745: step 6070, loss = 0.82 (508.9 examples/sec; 0.252 sec/batch)
2016-12-08 10:50:28.601689: step 6080, loss = 0.93 (494.3 examples/sec; 0.259 sec/batch)
2016-12-08 10:50:31.412688: step 6090, loss = 0.85 (525.6 examples/sec; 0.244 sec/batch)
2016-12-08 10:50:34.194684: step 6100, loss = 0.96 (442.1 examples/sec; 0.290 sec/batch)
2016-12-08 10:50:37.256743: step 6110, loss = 0.86 (503.3 examples/sec; 0.254 sec/batch)
2016-12-08 10:50:40.060448: step 6120, loss = 0.85 (535.4 examples/sec; 0.239 sec/batch)
2016-12-08 10:50:42.756980: step 6130, loss = 0.92 (437.3 examples/sec; 0.293 sec/batch)
2016-12-08 10:50:45.443829: step 6140, loss = 1.04 (482.9 examples/sec; 0.265 sec/batch)
2016-12-08 10:50:48.298558: step 6150, loss = 0.89 (423.7 examples/sec; 0.302 sec/batch)
2016-12-08 10:50:51.033680: step 6160, loss = 1.12 (478.3 examples/sec; 0.268 sec/batch)
2016-12-08 10:50:53.789564: step 6170, loss = 1.07 (496.9 examples/sec; 0.258 sec/batch)
2016-12-08 10:50:56.579210: step 6180, loss = 0.99 (468.5 examples/sec; 0.273 sec/batch)
2016-12-08 10:50:59.370446: step 6190, loss = 0.94 (522.1 examples/sec; 0.245 sec/batch)
2016-12-08 10:51:02.185948: step 6200, loss = 0.92 (431.8 examples/sec; 0.296 sec/batch)
2016-12-08 10:51:05.399018: step 6210, loss = 1.28 (446.9 examples/sec; 0.286 sec/batch)
2016-12-08 10:51:08.106380: step 6220, loss = 0.86 (551.9 examples/sec; 0.232 sec/batch)
2016-12-08 10:51:10.830591: step 6230, loss = 0.83 (510.9 examples/sec; 0.251 sec/batch)
2016-12-08 10:51:13.566826: step 6240, loss = 0.84 (471.5 examples/sec; 0.271 sec/batch)
2016-12-08 10:51:16.345303: step 6250, loss = 0.80 (521.9 examples/sec; 0.245 sec/batch)
2016-12-08 10:51:19.176294: step 6260, loss = 0.81 (437.1 examples/sec; 0.293 sec/batch)
2016-12-08 10:51:22.051524: step 6270, loss = 0.97 (441.2 examples/sec; 0.290 sec/batch)
2016-12-08 10:51:24.751951: step 6280, loss = 0.93 (426.4 examples/sec; 0.300 sec/batch)
2016-12-08 10:51:27.588067: step 6290, loss = 0.71 (372.5 examples/sec; 0.344 sec/batch)
2016-12-08 10:51:30.298638: step 6300, loss = 0.90 (525.1 examples/sec; 0.244 sec/batch)
2016-12-08 10:51:33.479932: step 6310, loss = 1.06 (448.6 examples/sec; 0.285 sec/batch)
2016-12-08 10:51:36.337098: step 6320, loss = 0.94 (507.4 examples/sec; 0.252 sec/batch)
2016-12-08 10:51:39.168941: step 6330, loss = 0.89 (403.2 examples/sec; 0.317 sec/batch)
2016-12-08 10:51:41.830886: step 6340, loss = 0.98 (444.6 examples/sec; 0.288 sec/batch)
2016-12-08 10:51:44.509472: step 6350, loss = 0.94 (520.1 examples/sec; 0.246 sec/batch)
2016-12-08 10:51:47.414674: step 6360, loss = 0.84 (432.3 examples/sec; 0.296 sec/batch)
2016-12-08 10:51:50.331879: step 6370, loss = 0.93 (450.1 examples/sec; 0.284 sec/batch)
2016-12-08 10:51:53.116364: step 6380, loss = 0.92 (474.9 examples/sec; 0.270 sec/batch)
2016-12-08 10:51:55.978076: step 6390, loss = 1.06 (446.7 examples/sec; 0.287 sec/batch)
2016-12-08 10:51:58.847953: step 6400, loss = 0.83 (463.7 examples/sec; 0.276 sec/batch)
2016-12-08 10:52:02.033506: step 6410, loss = 0.97 (405.1 examples/sec; 0.316 sec/batch)
2016-12-08 10:52:04.852956: step 6420, loss = 0.86 (522.2 examples/sec; 0.245 sec/batch)
2016-12-08 10:52:07.668576: step 6430, loss = 1.02 (459.5 examples/sec; 0.279 sec/batch)
2016-12-08 10:52:10.407435: step 6440, loss = 0.86 (528.9 examples/sec; 0.242 sec/batch)
2016-12-08 10:52:13.208031: step 6450, loss = 0.73 (475.6 examples/sec; 0.269 sec/batch)
2016-12-08 10:52:15.991893: step 6460, loss = 0.85 (437.2 examples/sec; 0.293 sec/batch)
2016-12-08 10:52:18.759217: step 6470, loss = 0.97 (455.3 examples/sec; 0.281 sec/batch)
2016-12-08 10:52:21.495081: step 6480, loss = 0.88 (515.3 examples/sec; 0.248 sec/batch)
2016-12-08 10:52:24.143424: step 6490, loss = 0.87 (520.1 examples/sec; 0.246 sec/batch)
2016-12-08 10:52:27.001111: step 6500, loss = 1.17 (428.0 examples/sec; 0.299 sec/batch)
2016-12-08 10:52:30.214059: step 6510, loss = 0.85 (425.9 examples/sec; 0.301 sec/batch)
2016-12-08 10:52:33.117292: step 6520, loss = 0.85 (435.7 examples/sec; 0.294 sec/batch)
2016-12-08 10:52:35.971410: step 6530, loss = 1.06 (456.2 examples/sec; 0.281 sec/batch)
2016-12-08 10:52:38.731111: step 6540, loss = 0.82 (515.4 examples/sec; 0.248 sec/batch)
2016-12-08 10:52:41.656240: step 6550, loss = 0.91 (448.4 examples/sec; 0.285 sec/batch)
2016-12-08 10:52:44.490794: step 6560, loss = 1.00 (493.8 examples/sec; 0.259 sec/batch)
2016-12-08 10:52:47.233308: step 6570, loss = 0.93 (479.1 examples/sec; 0.267 sec/batch)
2016-12-08 10:52:49.967560: step 6580, loss = 1.05 (475.5 examples/sec; 0.269 sec/batch)
2016-12-08 10:52:52.697134: step 6590, loss = 0.93 (419.8 examples/sec; 0.305 sec/batch)
2016-12-08 10:52:55.585933: step 6600, loss = 0.99 (387.1 examples/sec; 0.331 sec/batch)
2016-12-08 10:52:58.731380: step 6610, loss = 1.09 (410.2 examples/sec; 0.312 sec/batch)
2016-12-08 10:53:01.646882: step 6620, loss = 1.05 (440.9 examples/sec; 0.290 sec/batch)
2016-12-08 10:53:04.467259: step 6630, loss = 1.13 (449.3 examples/sec; 0.285 sec/batch)
2016-12-08 10:53:07.292956: step 6640, loss = 0.92 (483.6 examples/sec; 0.265 sec/batch)
2016-12-08 10:53:10.091554: step 6650, loss = 0.78 (484.1 examples/sec; 0.264 sec/batch)
2016-12-08 10:53:12.990913: step 6660, loss = 0.99 (458.0 examples/sec; 0.279 sec/batch)
2016-12-08 10:53:15.745782: step 6670, loss = 0.95 (534.9 examples/sec; 0.239 sec/batch)
2016-12-08 10:53:18.651051: step 6680, loss = 0.98 (350.0 examples/sec; 0.366 sec/batch)
2016-12-08 10:53:21.491612: step 6690, loss = 0.85 (388.2 examples/sec; 0.330 sec/batch)
2016-12-08 10:53:24.239311: step 6700, loss = 1.02 (413.0 examples/sec; 0.310 sec/batch)
2016-12-08 10:53:27.273214: step 6710, loss = 0.73 (560.3 examples/sec; 0.228 sec/batch)
2016-12-08 10:53:30.084250: step 6720, loss = 1.04 (422.6 examples/sec; 0.303 sec/batch)
2016-12-08 10:53:32.797704: step 6730, loss = 0.88 (530.9 examples/sec; 0.241 sec/batch)
2016-12-08 10:53:35.515145: step 6740, loss = 0.91 (503.4 examples/sec; 0.254 sec/batch)
2016-12-08 10:53:38.178526: step 6750, loss = 0.81 (507.1 examples/sec; 0.252 sec/batch)
2016-12-08 10:53:40.948325: step 6760, loss = 0.93 (524.8 examples/sec; 0.244 sec/batch)
2016-12-08 10:53:43.697671: step 6770, loss = 0.97 (481.8 examples/sec; 0.266 sec/batch)
2016-12-08 10:53:46.377179: step 6780, loss = 0.89 (526.0 examples/sec; 0.243 sec/batch)
2016-12-08 10:53:49.339866: step 6790, loss = 0.87 (409.8 examples/sec; 0.312 sec/batch)
2016-12-08 10:53:52.000344: step 6800, loss = 1.01 (472.0 examples/sec; 0.271 sec/batch)
2016-12-08 10:53:55.053729: step 6810, loss = 0.89 (535.5 examples/sec; 0.239 sec/batch)
2016-12-08 10:53:57.688303: step 6820, loss = 1.04 (430.7 examples/sec; 0.297 sec/batch)
2016-12-08 10:54:00.479101: step 6830, loss = 0.96 (385.4 examples/sec; 0.332 sec/batch)
2016-12-08 10:54:03.135663: step 6840, loss = 1.14 (511.7 examples/sec; 0.250 sec/batch)
2016-12-08 10:54:06.002375: step 6850, loss = 1.23 (513.1 examples/sec; 0.249 sec/batch)
2016-12-08 10:54:09.000645: step 6860, loss = 0.88 (398.9 examples/sec; 0.321 sec/batch)
2016-12-08 10:54:11.873457: step 6870, loss = 1.01 (463.6 examples/sec; 0.276 sec/batch)
2016-12-08 10:54:14.773822: step 6880, loss = 1.13 (356.5 examples/sec; 0.359 sec/batch)
2016-12-08 10:54:17.484840: step 6890, loss = 0.88 (451.6 examples/sec; 0.283 sec/batch)
2016-12-08 10:54:20.242980: step 6900, loss = 0.92 (436.2 examples/sec; 0.293 sec/batch)
2016-12-08 10:54:23.303087: step 6910, loss = 1.04 (488.2 examples/sec; 0.262 sec/batch)
2016-12-08 10:54:26.178615: step 6920, loss = 0.92 (470.9 examples/sec; 0.272 sec/batch)
2016-12-08 10:54:28.886246: step 6930, loss = 1.00 (462.7 examples/sec; 0.277 sec/batch)
2016-12-08 10:54:31.712963: step 6940, loss = 1.04 (461.5 examples/sec; 0.277 sec/batch)
2016-12-08 10:54:34.602084: step 6950, loss = 0.79 (492.3 examples/sec; 0.260 sec/batch)
2016-12-08 10:54:37.481735: step 6960, loss = 0.88 (440.3 examples/sec; 0.291 sec/batch)
2016-12-08 10:54:40.293485: step 6970, loss = 1.09 (467.4 examples/sec; 0.274 sec/batch)
2016-12-08 10:54:43.167134: step 6980, loss = 0.81 (464.1 examples/sec; 0.276 sec/batch)
2016-12-08 10:54:46.015672: step 6990, loss = 0.83 (409.2 examples/sec; 0.313 sec/batch)
2016-12-08 10:54:48.807350: step 7000, loss = 0.78 (392.5 examples/sec; 0.326 sec/batch)
2016-12-08 10:54:52.686074: step 7010, loss = 0.82 (435.6 examples/sec; 0.294 sec/batch)
2016-12-08 10:54:55.532549: step 7020, loss = 0.84 (412.9 examples/sec; 0.310 sec/batch)
2016-12-08 10:54:58.293369: step 7030, loss = 0.97 (481.5 examples/sec; 0.266 sec/batch)
2016-12-08 10:55:01.029145: step 7040, loss = 0.93 (498.2 examples/sec; 0.257 sec/batch)
2016-12-08 10:55:04.002861: step 7050, loss = 0.88 (452.8 examples/sec; 0.283 sec/batch)
2016-12-08 10:55:06.803037: step 7060, loss = 0.95 (469.8 examples/sec; 0.272 sec/batch)
2016-12-08 10:55:09.530542: step 7070, loss = 0.85 (414.3 examples/sec; 0.309 sec/batch)
2016-12-08 10:55:12.467784: step 7080, loss = 0.86 (420.5 examples/sec; 0.304 sec/batch)
2016-12-08 10:55:15.324114: step 7090, loss = 0.91 (454.9 examples/sec; 0.281 sec/batch)
2016-12-08 10:55:18.210727: step 7100, loss = 0.82 (442.4 examples/sec; 0.289 sec/batch)
2016-12-08 10:55:21.178128: step 7110, loss = 0.94 (437.9 examples/sec; 0.292 sec/batch)
2016-12-08 10:55:24.073113: step 7120, loss = 0.98 (404.8 examples/sec; 0.316 sec/batch)
2016-12-08 10:55:26.971334: step 7130, loss = 0.82 (416.1 examples/sec; 0.308 sec/batch)
2016-12-08 10:55:29.641481: step 7140, loss = 0.99 (472.9 examples/sec; 0.271 sec/batch)
2016-12-08 10:55:32.640238: step 7150, loss = 0.83 (439.7 examples/sec; 0.291 sec/batch)
2016-12-08 10:55:35.383528: step 7160, loss = 0.78 (442.1 examples/sec; 0.290 sec/batch)
2016-12-08 10:55:38.110233: step 7170, loss = 1.05 (568.9 examples/sec; 0.225 sec/batch)
2016-12-08 10:55:40.660610: step 7180, loss = 1.02 (478.3 examples/sec; 0.268 sec/batch)
2016-12-08 10:55:43.394441: step 7190, loss = 0.93 (439.0 examples/sec; 0.292 sec/batch)
2016-12-08 10:55:46.177416: step 7200, loss = 0.86 (452.0 examples/sec; 0.283 sec/batch)
2016-12-08 10:55:49.350717: step 7210, loss = 0.95 (483.5 examples/sec; 0.265 sec/batch)
2016-12-08 10:55:52.171560: step 7220, loss = 1.21 (518.3 examples/sec; 0.247 sec/batch)
2016-12-08 10:55:54.974929: step 7230, loss = 0.82 (466.9 examples/sec; 0.274 sec/batch)
2016-12-08 10:55:57.670115: step 7240, loss = 0.89 (476.0 examples/sec; 0.269 sec/batch)
2016-12-08 10:56:00.432024: step 7250, loss = 1.20 (444.5 examples/sec; 0.288 sec/batch)
2016-12-08 10:56:03.286164: step 7260, loss = 0.86 (502.6 examples/sec; 0.255 sec/batch)
2016-12-08 10:56:06.087363: step 7270, loss = 0.93 (498.9 examples/sec; 0.257 sec/batch)
2016-12-08 10:56:08.871117: step 7280, loss = 0.99 (464.0 examples/sec; 0.276 sec/batch)
2016-12-08 10:56:11.673132: step 7290, loss = 0.93 (393.2 examples/sec; 0.326 sec/batch)
2016-12-08 10:56:14.493214: step 7300, loss = 0.87 (469.4 examples/sec; 0.273 sec/batch)
2016-12-08 10:56:17.588881: step 7310, loss = 0.99 (425.9 examples/sec; 0.301 sec/batch)
2016-12-08 10:56:20.297378: step 7320, loss = 0.97 (414.5 examples/sec; 0.309 sec/batch)
2016-12-08 10:56:23.128105: step 7330, loss = 0.94 (455.8 examples/sec; 0.281 sec/batch)
2016-12-08 10:56:25.992026: step 7340, loss = 1.13 (395.0 examples/sec; 0.324 sec/batch)
2016-12-08 10:56:28.624728: step 7350, loss = 0.80 (516.8 examples/sec; 0.248 sec/batch)
2016-12-08 10:56:31.304725: step 7360, loss = 1.09 (445.5 examples/sec; 0.287 sec/batch)
2016-12-08 10:56:33.986328: step 7370, loss = 0.93 (516.5 examples/sec; 0.248 sec/batch)
2016-12-08 10:56:36.803530: step 7380, loss = 0.98 (431.6 examples/sec; 0.297 sec/batch)
2016-12-08 10:56:39.632551: step 7390, loss = 0.83 (434.7 examples/sec; 0.294 sec/batch)
2016-12-08 10:56:42.505356: step 7400, loss = 0.88 (441.1 examples/sec; 0.290 sec/batch)
2016-12-08 10:56:45.701638: step 7410, loss = 0.77 (455.0 examples/sec; 0.281 sec/batch)
2016-12-08 10:56:48.597495: step 7420, loss = 1.03 (431.5 examples/sec; 0.297 sec/batch)
2016-12-08 10:56:51.429619: step 7430, loss = 1.04 (442.4 examples/sec; 0.289 sec/batch)
2016-12-08 10:56:54.168373: step 7440, loss = 1.08 (508.0 examples/sec; 0.252 sec/batch)
2016-12-08 10:56:57.000153: step 7450, loss = 0.86 (466.8 examples/sec; 0.274 sec/batch)
2016-12-08 10:56:59.808242: step 7460, loss = 0.75 (509.0 examples/sec; 0.251 sec/batch)
2016-12-08 10:57:02.695864: step 7470, loss = 0.98 (423.7 examples/sec; 0.302 sec/batch)
2016-12-08 10:57:05.531076: step 7480, loss = 0.85 (413.0 examples/sec; 0.310 sec/batch)
2016-12-08 10:57:08.493084: step 7490, loss = 0.92 (390.3 examples/sec; 0.328 sec/batch)
2016-12-08 10:57:11.337605: step 7500, loss = 0.76 (471.9 examples/sec; 0.271 sec/batch)
2016-12-08 10:57:14.317804: step 7510, loss = 1.06 (505.2 examples/sec; 0.253 sec/batch)
2016-12-08 10:57:17.161558: step 7520, loss = 0.75 (461.8 examples/sec; 0.277 sec/batch)
2016-12-08 10:57:19.907422: step 7530, loss = 1.15 (496.8 examples/sec; 0.258 sec/batch)
2016-12-08 10:57:22.758829: step 7540, loss = 0.85 (450.0 examples/sec; 0.284 sec/batch)
2016-12-08 10:57:25.434881: step 7550, loss = 0.83 (482.3 examples/sec; 0.265 sec/batch)
2016-12-08 10:57:28.168856: step 7560, loss = 1.03 (502.8 examples/sec; 0.255 sec/batch)
2016-12-08 10:57:30.958381: step 7570, loss = 0.99 (442.6 examples/sec; 0.289 sec/batch)
2016-12-08 10:57:33.705567: step 7580, loss = 0.99 (463.9 examples/sec; 0.276 sec/batch)
2016-12-08 10:57:36.505042: step 7590, loss = 0.91 (475.1 examples/sec; 0.269 sec/batch)
2016-12-08 10:57:39.299303: step 7600, loss = 0.80 (543.0 examples/sec; 0.236 sec/batch)
2016-12-08 10:57:42.441406: step 7610, loss = 0.86 (415.4 examples/sec; 0.308 sec/batch)
2016-12-08 10:57:45.124780: step 7620, loss = 0.93 (426.8 examples/sec; 0.300 sec/batch)
2016-12-08 10:57:47.737268: step 7630, loss = 1.01 (560.5 examples/sec; 0.228 sec/batch)
2016-12-08 10:57:50.440340: step 7640, loss = 0.98 (509.6 examples/sec; 0.251 sec/batch)
2016-12-08 10:57:53.309427: step 7650, loss = 0.84 (474.0 examples/sec; 0.270 sec/batch)
2016-12-08 10:57:56.031543: step 7660, loss = 0.87 (451.1 examples/sec; 0.284 sec/batch)
2016-12-08 10:57:58.799850: step 7670, loss = 0.96 (466.7 examples/sec; 0.274 sec/batch)
2016-12-08 10:58:01.538184: step 7680, loss = 0.79 (530.6 examples/sec; 0.241 sec/batch)
2016-12-08 10:58:04.330635: step 7690, loss = 1.05 (473.7 examples/sec; 0.270 sec/batch)
2016-12-08 10:58:07.237699: step 7700, loss = 0.97 (381.8 examples/sec; 0.335 sec/batch)
2016-12-08 10:58:10.197956: step 7710, loss = 1.05 (472.9 examples/sec; 0.271 sec/batch)
2016-12-08 10:58:12.962298: step 7720, loss = 0.66 (420.9 examples/sec; 0.304 sec/batch)
2016-12-08 10:58:15.745530: step 7730, loss = 0.87 (502.3 examples/sec; 0.255 sec/batch)
2016-12-08 10:58:18.512654: step 7740, loss = 0.95 (478.4 examples/sec; 0.268 sec/batch)
2016-12-08 10:58:21.278471: step 7750, loss = 0.93 (453.0 examples/sec; 0.283 sec/batch)
2016-12-08 10:58:24.067555: step 7760, loss = 0.77 (531.4 examples/sec; 0.241 sec/batch)
2016-12-08 10:58:26.811620: step 7770, loss = 1.09 (435.9 examples/sec; 0.294 sec/batch)
2016-12-08 10:58:29.576082: step 7780, loss = 0.96 (517.4 examples/sec; 0.247 sec/batch)
2016-12-08 10:58:32.460287: step 7790, loss = 1.16 (386.1 examples/sec; 0.332 sec/batch)
2016-12-08 10:58:35.202131: step 7800, loss = 0.85 (528.3 examples/sec; 0.242 sec/batch)
2016-12-08 10:58:38.221474: step 7810, loss = 0.80 (461.6 examples/sec; 0.277 sec/batch)
2016-12-08 10:58:41.056879: step 7820, loss = 0.93 (437.3 examples/sec; 0.293 sec/batch)
2016-12-08 10:58:43.990584: step 7830, loss = 0.80 (420.4 examples/sec; 0.304 sec/batch)
2016-12-08 10:58:46.803644: step 7840, loss = 0.80 (446.0 examples/sec; 0.287 sec/batch)
2016-12-08 10:58:49.654195: step 7850, loss = 0.92 (392.3 examples/sec; 0.326 sec/batch)
2016-12-08 10:58:52.369293: step 7860, loss = 0.98 (489.9 examples/sec; 0.261 sec/batch)
2016-12-08 10:58:55.098979: step 7870, loss = 1.00 (480.0 examples/sec; 0.267 sec/batch)
2016-12-08 10:58:57.972567: step 7880, loss = 0.88 (415.9 examples/sec; 0.308 sec/batch)
2016-12-08 10:59:00.801750: step 7890, loss = 0.84 (492.1 examples/sec; 0.260 sec/batch)
2016-12-08 10:59:03.529611: step 7900, loss = 0.86 (402.6 examples/sec; 0.318 sec/batch)
2016-12-08 10:59:06.610701: step 7910, loss = 0.81 (469.9 examples/sec; 0.272 sec/batch)
2016-12-08 10:59:09.432604: step 7920, loss = 0.98 (462.0 examples/sec; 0.277 sec/batch)
2016-12-08 10:59:12.306947: step 7930, loss = 1.00 (438.9 examples/sec; 0.292 sec/batch)
2016-12-08 10:59:15.142872: step 7940, loss = 0.80 (415.8 examples/sec; 0.308 sec/batch)
2016-12-08 10:59:17.978465: step 7950, loss = 0.84 (420.0 examples/sec; 0.305 sec/batch)
2016-12-08 10:59:20.858069: step 7960, loss = 0.98 (428.8 examples/sec; 0.298 sec/batch)
2016-12-08 10:59:23.776736: step 7970, loss = 0.80 (421.4 examples/sec; 0.304 sec/batch)
2016-12-08 10:59:26.502624: step 7980, loss = 0.70 (462.0 examples/sec; 0.277 sec/batch)
2016-12-08 10:59:29.287457: step 7990, loss = 0.93 (409.5 examples/sec; 0.313 sec/batch)
2016-12-08 10:59:32.089516: step 8000, loss = 1.01 (438.1 examples/sec; 0.292 sec/batch)
2016-12-08 10:59:36.125139: step 8010, loss = 1.03 (438.5 examples/sec; 0.292 sec/batch)
2016-12-08 10:59:38.966860: step 8020, loss = 0.81 (447.7 examples/sec; 0.286 sec/batch)
2016-12-08 10:59:41.705508: step 8030, loss = 1.32 (422.5 examples/sec; 0.303 sec/batch)
2016-12-08 10:59:44.306464: step 8040, loss = 0.88 (536.8 examples/sec; 0.238 sec/batch)
2016-12-08 10:59:47.121892: step 8050, loss = 0.96 (479.0 examples/sec; 0.267 sec/batch)
2016-12-08 10:59:49.814632: step 8060, loss = 0.96 (447.6 examples/sec; 0.286 sec/batch)
2016-12-08 10:59:52.668056: step 8070, loss = 0.92 (443.6 examples/sec; 0.289 sec/batch)
2016-12-08 10:59:55.451571: step 8080, loss = 0.82 (464.1 examples/sec; 0.276 sec/batch)
2016-12-08 10:59:58.388959: step 8090, loss = 0.79 (470.8 examples/sec; 0.272 sec/batch)
2016-12-08 11:00:01.225008: step 8100, loss = 1.01 (492.8 examples/sec; 0.260 sec/batch)
2016-12-08 11:00:04.322680: step 8110, loss = 0.85 (494.8 examples/sec; 0.259 sec/batch)
2016-12-08 11:00:07.046507: step 8120, loss = 0.84 (436.1 examples/sec; 0.294 sec/batch)
2016-12-08 11:00:09.932621: step 8130, loss = 1.01 (435.3 examples/sec; 0.294 sec/batch)
2016-12-08 11:00:12.824529: step 8140, loss = 0.73 (445.9 examples/sec; 0.287 sec/batch)
2016-12-08 11:00:15.636023: step 8150, loss = 0.94 (441.9 examples/sec; 0.290 sec/batch)
2016-12-08 11:00:18.525438: step 8160, loss = 0.90 (485.1 examples/sec; 0.264 sec/batch)
2016-12-08 11:00:21.313886: step 8170, loss = 1.04 (475.0 examples/sec; 0.269 sec/batch)
2016-12-08 11:00:24.091242: step 8180, loss = 0.80 (519.8 examples/sec; 0.246 sec/batch)
2016-12-08 11:00:26.890977: step 8190, loss = 0.83 (424.1 examples/sec; 0.302 sec/batch)
2016-12-08 11:00:29.729800: step 8200, loss = 1.03 (468.0 examples/sec; 0.274 sec/batch)
2016-12-08 11:00:32.828537: step 8210, loss = 1.03 (479.5 examples/sec; 0.267 sec/batch)
2016-12-08 11:00:35.627973: step 8220, loss = 0.86 (426.5 examples/sec; 0.300 sec/batch)
2016-12-08 11:00:38.443005: step 8230, loss = 0.87 (442.8 examples/sec; 0.289 sec/batch)
2016-12-08 11:00:41.173308: step 8240, loss = 0.93 (433.4 examples/sec; 0.295 sec/batch)
2016-12-08 11:00:43.792114: step 8250, loss = 0.95 (495.9 examples/sec; 0.258 sec/batch)
2016-12-08 11:00:46.438889: step 8260, loss = 0.78 (451.4 examples/sec; 0.284 sec/batch)
2016-12-08 11:00:49.298347: step 8270, loss = 1.03 (442.1 examples/sec; 0.290 sec/batch)
2016-12-08 11:00:52.086311: step 8280, loss = 1.01 (425.4 examples/sec; 0.301 sec/batch)
2016-12-08 11:00:54.843933: step 8290, loss = 0.83 (485.8 examples/sec; 0.264 sec/batch)
2016-12-08 11:00:57.639312: step 8300, loss = 0.84 (480.2 examples/sec; 0.267 sec/batch)
2016-12-08 11:01:00.773554: step 8310, loss = 0.80 (431.9 examples/sec; 0.296 sec/batch)
2016-12-08 11:01:03.617761: step 8320, loss = 0.97 (504.6 examples/sec; 0.254 sec/batch)
2016-12-08 11:01:06.578418: step 8330, loss = 0.83 (401.6 examples/sec; 0.319 sec/batch)
2016-12-08 11:01:09.469058: step 8340, loss = 1.01 (435.4 examples/sec; 0.294 sec/batch)
2016-12-08 11:01:12.242436: step 8350, loss = 0.99 (427.7 examples/sec; 0.299 sec/batch)
2016-12-08 11:01:15.035640: step 8360, loss = 0.95 (511.2 examples/sec; 0.250 sec/batch)
2016-12-08 11:01:17.872964: step 8370, loss = 1.04 (509.8 examples/sec; 0.251 sec/batch)
2016-12-08 11:01:20.606277: step 8380, loss = 0.86 (429.2 examples/sec; 0.298 sec/batch)
2016-12-08 11:01:23.310822: step 8390, loss = 0.86 (534.0 examples/sec; 0.240 sec/batch)
2016-12-08 11:01:26.104073: step 8400, loss = 0.97 (535.7 examples/sec; 0.239 sec/batch)
2016-12-08 11:01:29.212859: step 8410, loss = 1.13 (437.7 examples/sec; 0.292 sec/batch)
2016-12-08 11:01:32.035910: step 8420, loss = 0.86 (460.5 examples/sec; 0.278 sec/batch)
2016-12-08 11:01:34.823243: step 8430, loss = 0.84 (503.2 examples/sec; 0.254 sec/batch)
2016-12-08 11:01:37.505242: step 8440, loss = 1.03 (492.8 examples/sec; 0.260 sec/batch)
2016-12-08 11:01:40.406416: step 8450, loss = 0.78 (455.8 examples/sec; 0.281 sec/batch)
2016-12-08 11:01:43.238249: step 8460, loss = 0.71 (478.8 examples/sec; 0.267 sec/batch)
2016-12-08 11:01:46.079240: step 8470, loss = 0.82 (484.2 examples/sec; 0.264 sec/batch)
2016-12-08 11:01:48.997586: step 8480, loss = 0.93 (462.9 examples/sec; 0.277 sec/batch)
2016-12-08 11:01:51.775626: step 8490, loss = 0.89 (458.4 examples/sec; 0.279 sec/batch)
2016-12-08 11:01:54.517899: step 8500, loss = 0.84 (495.1 examples/sec; 0.259 sec/batch)
2016-12-08 11:01:57.526981: step 8510, loss = 1.05 (446.7 examples/sec; 0.287 sec/batch)
2016-12-08 11:02:00.417491: step 8520, loss = 0.87 (445.4 examples/sec; 0.287 sec/batch)
2016-12-08 11:02:03.195318: step 8530, loss = 0.78 (562.2 examples/sec; 0.228 sec/batch)
2016-12-08 11:02:05.961267: step 8540, loss = 0.91 (439.6 examples/sec; 0.291 sec/batch)
2016-12-08 11:02:08.747938: step 8550, loss = 0.95 (460.9 examples/sec; 0.278 sec/batch)
2016-12-08 11:02:11.473717: step 8560, loss = 0.83 (473.9 examples/sec; 0.270 sec/batch)
2016-12-08 11:02:14.343478: step 8570, loss = 1.07 (470.6 examples/sec; 0.272 sec/batch)
2016-12-08 11:02:17.089252: step 8580, loss = 1.02 (425.5 examples/sec; 0.301 sec/batch)
2016-12-08 11:02:19.990936: step 8590, loss = 0.94 (457.2 examples/sec; 0.280 sec/batch)
2016-12-08 11:02:22.784840: step 8600, loss = 0.93 (477.3 examples/sec; 0.268 sec/batch)
2016-12-08 11:02:25.745717: step 8610, loss = 0.78 (456.6 examples/sec; 0.280 sec/batch)
2016-12-08 11:02:28.486708: step 8620, loss = 0.83 (548.3 examples/sec; 0.233 sec/batch)
2016-12-08 11:02:31.190349: step 8630, loss = 0.90 (468.9 examples/sec; 0.273 sec/batch)
2016-12-08 11:02:33.942166: step 8640, loss = 1.20 (485.2 examples/sec; 0.264 sec/batch)
2016-12-08 11:02:36.682490: step 8650, loss = 0.81 (459.4 examples/sec; 0.279 sec/batch)
2016-12-08 11:02:39.510625: step 8660, loss = 0.92 (386.2 examples/sec; 0.331 sec/batch)
2016-12-08 11:02:42.213280: step 8670, loss = 0.78 (535.9 examples/sec; 0.239 sec/batch)
2016-12-08 11:02:44.883635: step 8680, loss = 0.82 (454.1 examples/sec; 0.282 sec/batch)
2016-12-08 11:02:47.576299: step 8690, loss = 0.97 (442.0 examples/sec; 0.290 sec/batch)
2016-12-08 11:02:50.238926: step 8700, loss = 1.03 (521.2 examples/sec; 0.246 sec/batch)
2016-12-08 11:02:53.190260: step 8710, loss = 0.87 (479.3 examples/sec; 0.267 sec/batch)
2016-12-08 11:02:55.982926: step 8720, loss = 0.93 (375.7 examples/sec; 0.341 sec/batch)
2016-12-08 11:02:58.743215: step 8730, loss = 0.84 (431.5 examples/sec; 0.297 sec/batch)
2016-12-08 11:03:01.612948: step 8740, loss = 1.02 (509.8 examples/sec; 0.251 sec/batch)
2016-12-08 11:03:04.274015: step 8750, loss = 0.84 (476.6 examples/sec; 0.269 sec/batch)
2016-12-08 11:03:07.251862: step 8760, loss = 0.92 (472.8 examples/sec; 0.271 sec/batch)
2016-12-08 11:03:09.947068: step 8770, loss = 0.85 (429.8 examples/sec; 0.298 sec/batch)
2016-12-08 11:03:12.613001: step 8780, loss = 1.05 (486.6 examples/sec; 0.263 sec/batch)
2016-12-08 11:03:15.413386: step 8790, loss = 0.83 (468.5 examples/sec; 0.273 sec/batch)
2016-12-08 11:03:18.275633: step 8800, loss = 0.83 (457.5 examples/sec; 0.280 sec/batch)
2016-12-08 11:03:21.517863: step 8810, loss = 0.98 (391.1 examples/sec; 0.327 sec/batch)
2016-12-08 11:03:24.343362: step 8820, loss = 0.93 (523.5 examples/sec; 0.245 sec/batch)
2016-12-08 11:03:27.020272: step 8830, loss = 0.85 (453.2 examples/sec; 0.282 sec/batch)
2016-12-08 11:03:29.753669: step 8840, loss = 0.83 (413.5 examples/sec; 0.310 sec/batch)
2016-12-08 11:03:32.498177: step 8850, loss = 1.06 (447.3 examples/sec; 0.286 sec/batch)
2016-12-08 11:03:35.219863: step 8860, loss = 0.93 (433.6 examples/sec; 0.295 sec/batch)
2016-12-08 11:03:38.031105: step 8870, loss = 0.87 (410.8 examples/sec; 0.312 sec/batch)
2016-12-08 11:03:40.874224: step 8880, loss = 0.95 (493.5 examples/sec; 0.259 sec/batch)
2016-12-08 11:03:43.625140: step 8890, loss = 0.99 (438.4 examples/sec; 0.292 sec/batch)
2016-12-08 11:03:46.490819: step 8900, loss = 0.93 (436.9 examples/sec; 0.293 sec/batch)
2016-12-08 11:03:49.639433: step 8910, loss = 0.84 (444.3 examples/sec; 0.288 sec/batch)
2016-12-08 11:03:52.435221: step 8920, loss = 0.77 (489.8 examples/sec; 0.261 sec/batch)
2016-12-08 11:03:54.923373: step 8930, loss = 0.84 (522.9 examples/sec; 0.245 sec/batch)
2016-12-08 11:03:57.707015: step 8940, loss = 0.82 (464.8 examples/sec; 0.275 sec/batch)
2016-12-08 11:04:00.481478: step 8950, loss = 0.96 (458.5 examples/sec; 0.279 sec/batch)
2016-12-08 11:04:03.151546: step 8960, loss = 0.97 (514.3 examples/sec; 0.249 sec/batch)
2016-12-08 11:04:05.746959: step 8970, loss = 1.03 (439.7 examples/sec; 0.291 sec/batch)
2016-12-08 11:04:08.604299: step 8980, loss = 0.83 (526.3 examples/sec; 0.243 sec/batch)
2016-12-08 11:04:11.416633: step 8990, loss = 0.83 (495.1 examples/sec; 0.259 sec/batch)
2016-12-08 11:04:14.206870: step 9000, loss = 0.94 (477.1 examples/sec; 0.268 sec/batch)
2016-12-08 11:04:17.697981: step 9010, loss = 0.86 (500.4 examples/sec; 0.256 sec/batch)
2016-12-08 11:04:20.435523: step 9020, loss = 0.84 (393.5 examples/sec; 0.325 sec/batch)
2016-12-08 11:04:23.298884: step 9030, loss = 0.96 (437.0 examples/sec; 0.293 sec/batch)
2016-12-08 11:04:26.089325: step 9040, loss = 0.98 (453.3 examples/sec; 0.282 sec/batch)
2016-12-08 11:04:28.852583: step 9050, loss = 0.93 (558.7 examples/sec; 0.229 sec/batch)
2016-12-08 11:04:31.628530: step 9060, loss = 1.08 (450.9 examples/sec; 0.284 sec/batch)
2016-12-08 11:04:34.536181: step 9070, loss = 0.89 (493.5 examples/sec; 0.259 sec/batch)
2016-12-08 11:04:37.380394: step 9080, loss = 0.74 (464.3 examples/sec; 0.276 sec/batch)
2016-12-08 11:04:40.223541: step 9090, loss = 0.76 (430.4 examples/sec; 0.297 sec/batch)
2016-12-08 11:04:42.951134: step 9100, loss = 0.94 (473.4 examples/sec; 0.270 sec/batch)
2016-12-08 11:04:46.091702: step 9110, loss = 0.79 (442.5 examples/sec; 0.289 sec/batch)
2016-12-08 11:04:48.965036: step 9120, loss = 0.93 (480.3 examples/sec; 0.266 sec/batch)
2016-12-08 11:04:51.685923: step 9130, loss = 0.94 (478.6 examples/sec; 0.267 sec/batch)
2016-12-08 11:04:54.513273: step 9140, loss = 0.98 (530.2 examples/sec; 0.241 sec/batch)
2016-12-08 11:04:57.239944: step 9150, loss = 0.87 (482.4 examples/sec; 0.265 sec/batch)
2016-12-08 11:05:00.150840: step 9160, loss = 0.97 (487.4 examples/sec; 0.263 sec/batch)
2016-12-08 11:05:02.986949: step 9170, loss = 1.02 (489.5 examples/sec; 0.261 sec/batch)
2016-12-08 11:05:05.760740: step 9180, loss = 0.77 (440.8 examples/sec; 0.290 sec/batch)
2016-12-08 11:05:08.730889: step 9190, loss = 0.81 (407.6 examples/sec; 0.314 sec/batch)
2016-12-08 11:05:11.585841: step 9200, loss = 1.19 (474.8 examples/sec; 0.270 sec/batch)
2016-12-08 11:05:14.651583: step 9210, loss = 0.95 (407.3 examples/sec; 0.314 sec/batch)
2016-12-08 11:05:17.524331: step 9220, loss = 1.03 (454.8 examples/sec; 0.281 sec/batch)
2016-12-08 11:05:20.339873: step 9230, loss = 0.75 (466.3 examples/sec; 0.274 sec/batch)
2016-12-08 11:05:23.255028: step 9240, loss = 0.75 (446.4 examples/sec; 0.287 sec/batch)
2016-12-08 11:05:26.116908: step 9250, loss = 0.75 (396.6 examples/sec; 0.323 sec/batch)
2016-12-08 11:05:28.894129: step 9260, loss = 0.86 (418.7 examples/sec; 0.306 sec/batch)
2016-12-08 11:05:31.530944: step 9270, loss = 0.86 (450.8 examples/sec; 0.284 sec/batch)
2016-12-08 11:05:34.268973: step 9280, loss = 0.93 (467.2 examples/sec; 0.274 sec/batch)
2016-12-08 11:05:37.221438: step 9290, loss = 0.78 (419.3 examples/sec; 0.305 sec/batch)
2016-12-08 11:05:40.107823: step 9300, loss = 0.96 (452.7 examples/sec; 0.283 sec/batch)
2016-12-08 11:05:43.054487: step 9310, loss = 1.10 (385.6 examples/sec; 0.332 sec/batch)
2016-12-08 11:05:45.784264: step 9320, loss = 1.03 (458.3 examples/sec; 0.279 sec/batch)
2016-12-08 11:05:48.538586: step 9330, loss = 0.78 (493.2 examples/sec; 0.260 sec/batch)
2016-12-08 11:05:51.320318: step 9340, loss = 1.09 (442.7 examples/sec; 0.289 sec/batch)
2016-12-08 11:05:54.164116: step 9350, loss = 0.86 (470.4 examples/sec; 0.272 sec/batch)
2016-12-08 11:05:57.036902: step 9360, loss = 0.84 (405.8 examples/sec; 0.315 sec/batch)
2016-12-08 11:05:59.841081: step 9370, loss = 1.00 (452.9 examples/sec; 0.283 sec/batch)
2016-12-08 11:06:02.485860: step 9380, loss = 0.83 (540.0 examples/sec; 0.237 sec/batch)
2016-12-08 11:06:05.267393: step 9390, loss = 0.80 (449.0 examples/sec; 0.285 sec/batch)
2016-12-08 11:06:08.020491: step 9400, loss = 1.00 (467.9 examples/sec; 0.274 sec/batch)
2016-12-08 11:06:11.076975: step 9410, loss = 0.85 (464.4 examples/sec; 0.276 sec/batch)
2016-12-08 11:06:13.875011: step 9420, loss = 0.80 (568.6 examples/sec; 0.225 sec/batch)
2016-12-08 11:06:16.681981: step 9430, loss = 1.01 (469.0 examples/sec; 0.273 sec/batch)
2016-12-08 11:06:19.395850: step 9440, loss = 0.63 (485.4 examples/sec; 0.264 sec/batch)
2016-12-08 11:06:22.193790: step 9450, loss = 0.73 (463.1 examples/sec; 0.276 sec/batch)
2016-12-08 11:06:24.972500: step 9460, loss = 0.88 (483.4 examples/sec; 0.265 sec/batch)
2016-12-08 11:06:27.748854: step 9470, loss = 1.17 (425.4 examples/sec; 0.301 sec/batch)
2016-12-08 11:06:30.370076: step 9480, loss = 1.10 (562.8 examples/sec; 0.227 sec/batch)
2016-12-08 11:06:33.085269: step 9490, loss = 0.92 (492.2 examples/sec; 0.260 sec/batch)
2016-12-08 11:06:35.912606: step 9500, loss = 0.77 (452.2 examples/sec; 0.283 sec/batch)
2016-12-08 11:06:38.862827: step 9510, loss = 0.95 (525.8 examples/sec; 0.243 sec/batch)
2016-12-08 11:06:41.654230: step 9520, loss = 0.78 (474.9 examples/sec; 0.270 sec/batch)
2016-12-08 11:06:44.517012: step 9530, loss = 0.98 (449.5 examples/sec; 0.285 sec/batch)
2016-12-08 11:06:47.350733: step 9540, loss = 1.11 (439.7 examples/sec; 0.291 sec/batch)
2016-12-08 11:06:50.193639: step 9550, loss = 0.99 (426.3 examples/sec; 0.300 sec/batch)
2016-12-08 11:06:53.010611: step 9560, loss = 0.77 (459.5 examples/sec; 0.279 sec/batch)
2016-12-08 11:06:55.936532: step 9570, loss = 1.05 (542.3 examples/sec; 0.236 sec/batch)
2016-12-08 11:06:58.751275: step 9580, loss = 0.95 (407.9 examples/sec; 0.314 sec/batch)
2016-12-08 11:07:01.593089: step 9590, loss = 1.02 (439.0 examples/sec; 0.292 sec/batch)
2016-12-08 11:07:04.401387: step 9600, loss = 0.93 (445.2 examples/sec; 0.287 sec/batch)
2016-12-08 11:07:07.542309: step 9610, loss = 0.85 (442.3 examples/sec; 0.289 sec/batch)
2016-12-08 11:07:10.330873: step 9620, loss = 0.85 (472.3 examples/sec; 0.271 sec/batch)
2016-12-08 11:07:13.209928: step 9630, loss = 0.83 (452.6 examples/sec; 0.283 sec/batch)
2016-12-08 11:07:16.193334: step 9640, loss = 1.12 (455.4 examples/sec; 0.281 sec/batch)
2016-12-08 11:07:19.029147: step 9650, loss = 0.91 (518.2 examples/sec; 0.247 sec/batch)
2016-12-08 11:07:21.919458: step 9660, loss = 0.77 (406.3 examples/sec; 0.315 sec/batch)
2016-12-08 11:07:24.750488: step 9670, loss = 0.84 (432.1 examples/sec; 0.296 sec/batch)
2016-12-08 11:07:27.424736: step 9680, loss = 1.05 (467.7 examples/sec; 0.274 sec/batch)
2016-12-08 11:07:30.229385: step 9690, loss = 0.97 (495.1 examples/sec; 0.259 sec/batch)
2016-12-08 11:07:33.008588: step 9700, loss = 0.86 (415.9 examples/sec; 0.308 sec/batch)
2016-12-08 11:07:36.050913: step 9710, loss = 1.00 (436.3 examples/sec; 0.293 sec/batch)
2016-12-08 11:07:38.880650: step 9720, loss = 0.92 (383.9 examples/sec; 0.333 sec/batch)
2016-12-08 11:07:41.582093: step 9730, loss = 0.93 (423.7 examples/sec; 0.302 sec/batch)
2016-12-08 11:07:44.352471: step 9740, loss = 0.93 (454.8 examples/sec; 0.281 sec/batch)
2016-12-08 11:07:47.112728: step 9750, loss = 0.91 (489.1 examples/sec; 0.262 sec/batch)
2016-12-08 11:07:49.919654: step 9760, loss = 0.79 (453.4 examples/sec; 0.282 sec/batch)
2016-12-08 11:07:52.596225: step 9770, loss = 0.96 (445.9 examples/sec; 0.287 sec/batch)
2016-12-08 11:07:55.371134: step 9780, loss = 0.87 (493.2 examples/sec; 0.260 sec/batch)
2016-12-08 11:07:58.129814: step 9790, loss = 0.84 (492.7 examples/sec; 0.260 sec/batch)
2016-12-08 11:08:01.136229: step 9800, loss = 0.89 (466.2 examples/sec; 0.275 sec/batch)
2016-12-08 11:08:04.345103: step 9810, loss = 0.99 (459.2 examples/sec; 0.279 sec/batch)
2016-12-08 11:08:07.031974: step 9820, loss = 0.79 (514.7 examples/sec; 0.249 sec/batch)
2016-12-08 11:08:09.753395: step 9830, loss = 0.94 (393.8 examples/sec; 0.325 sec/batch)
2016-12-08 11:08:12.494547: step 9840, loss = 0.84 (523.2 examples/sec; 0.245 sec/batch)
2016-12-08 11:08:15.294428: step 9850, loss = 0.96 (524.0 examples/sec; 0.244 sec/batch)
2016-12-08 11:08:18.060982: step 9860, loss = 0.90 (462.3 examples/sec; 0.277 sec/batch)
2016-12-08 11:08:20.911230: step 9870, loss = 1.07 (402.3 examples/sec; 0.318 sec/batch)
2016-12-08 11:08:23.746884: step 9880, loss = 0.75 (407.6 examples/sec; 0.314 sec/batch)
2016-12-08 11:08:26.576916: step 9890, loss = 0.97 (425.4 examples/sec; 0.301 sec/batch)
2016-12-08 11:08:29.421578: step 9900, loss = 0.88 (479.3 examples/sec; 0.267 sec/batch)
2016-12-08 11:08:32.580562: step 9910, loss = 0.85 (411.1 examples/sec; 0.311 sec/batch)
2016-12-08 11:08:35.374546: step 9920, loss = 0.87 (500.8 examples/sec; 0.256 sec/batch)
2016-12-08 11:08:38.208263: step 9930, loss = 0.89 (437.0 examples/sec; 0.293 sec/batch)
2016-12-08 11:08:41.094578: step 9940, loss = 1.07 (376.4 examples/sec; 0.340 sec/batch)
2016-12-08 11:08:43.874605: step 9950, loss = 0.91 (469.5 examples/sec; 0.273 sec/batch)
2016-12-08 11:08:46.742568: step 9960, loss = 0.81 (440.6 examples/sec; 0.291 sec/batch)
2016-12-08 11:08:49.560627: step 9970, loss = 1.06 (438.3 examples/sec; 0.292 sec/batch)
2016-12-08 11:08:52.437748: step 9980, loss = 1.05 (455.6 examples/sec; 0.281 sec/batch)
2016-12-08 11:08:55.412342: step 9990, loss = 0.84 (447.2 examples/sec; 0.286 sec/batch)
2016-12-08 11:08:58.312109: step 10000, loss = 0.80 (502.9 examples/sec; 0.255 sec/batch)
2016-12-08 11:09:01.951145: step 10010, loss = 1.03 (471.7 examples/sec; 0.271 sec/batch)
2016-12-08 11:09:04.781942: step 10020, loss = 0.74 (456.9 examples/sec; 0.280 sec/batch)
2016-12-08 11:09:07.506603: step 10030, loss = 0.86 (472.8 examples/sec; 0.271 sec/batch)
2016-12-08 11:09:10.264159: step 10040, loss = 0.73 (445.0 examples/sec; 0.288 sec/batch)
2016-12-08 11:09:12.994202: step 10050, loss = 0.88 (440.6 examples/sec; 0.291 sec/batch)
2016-12-08 11:09:15.785254: step 10060, loss = 1.00 (459.5 examples/sec; 0.279 sec/batch)
2016-12-08 11:09:18.350724: step 10070, loss = 0.90 (438.6 examples/sec; 0.292 sec/batch)
2016-12-08 11:09:20.990529: step 10080, loss = 0.93 (534.0 examples/sec; 0.240 sec/batch)
2016-12-08 11:09:23.927564: step 10090, loss = 0.83 (441.8 examples/sec; 0.290 sec/batch)
2016-12-08 11:09:26.660277: step 10100, loss = 0.83 (462.1 examples/sec; 0.277 sec/batch)
2016-12-08 11:09:29.636721: step 10110, loss = 0.74 (460.2 examples/sec; 0.278 sec/batch)
2016-12-08 11:09:32.430891: step 10120, loss = 0.84 (489.1 examples/sec; 0.262 sec/batch)
2016-12-08 11:09:35.264679: step 10130, loss = 0.96 (500.8 examples/sec; 0.256 sec/batch)
2016-12-08 11:09:37.865139: step 10140, loss = 0.88 (545.3 examples/sec; 0.235 sec/batch)
2016-12-08 11:09:40.548793: step 10150, loss = 0.98 (461.5 examples/sec; 0.277 sec/batch)
2016-12-08 11:09:43.264793: step 10160, loss = 0.84 (493.0 examples/sec; 0.260 sec/batch)
2016-12-08 11:09:46.182598: step 10170, loss = 0.90 (496.1 examples/sec; 0.258 sec/batch)
2016-12-08 11:09:49.039556: step 10180, loss = 0.73 (515.0 examples/sec; 0.249 sec/batch)
2016-12-08 11:09:51.907667: step 10190, loss = 0.91 (452.9 examples/sec; 0.283 sec/batch)
2016-12-08 11:09:54.740393: step 10200, loss = 0.90 (390.9 examples/sec; 0.327 sec/batch)
2016-12-08 11:09:57.820066: step 10210, loss = 0.89 (432.8 examples/sec; 0.296 sec/batch)
2016-12-08 11:10:00.554397: step 10220, loss = 0.83 (492.7 examples/sec; 0.260 sec/batch)
2016-12-08 11:10:03.392575: step 10230, loss = 0.63 (398.5 examples/sec; 0.321 sec/batch)
2016-12-08 11:10:06.056499: step 10240, loss = 0.83 (482.6 examples/sec; 0.265 sec/batch)
2016-12-08 11:10:08.815194: step 10250, loss = 0.75 (454.7 examples/sec; 0.281 sec/batch)
2016-12-08 11:10:11.610268: step 10260, loss = 0.90 (435.3 examples/sec; 0.294 sec/batch)
2016-12-08 11:10:14.347199: step 10270, loss = 0.86 (489.8 examples/sec; 0.261 sec/batch)
2016-12-08 11:10:17.143132: step 10280, loss = 0.71 (437.6 examples/sec; 0.292 sec/batch)
2016-12-08 11:10:19.898274: step 10290, loss = 0.82 (415.7 examples/sec; 0.308 sec/batch)
2016-12-08 11:10:22.719459: step 10300, loss = 1.14 (482.1 examples/sec; 0.266 sec/batch)
2016-12-08 11:10:25.753840: step 10310, loss = 1.08 (439.6 examples/sec; 0.291 sec/batch)
2016-12-08 11:10:28.559378: step 10320, loss = 0.90 (526.5 examples/sec; 0.243 sec/batch)
2016-12-08 11:10:31.226100: step 10330, loss = 0.95 (468.8 examples/sec; 0.273 sec/batch)
2016-12-08 11:10:33.925440: step 10340, loss = 0.80 (467.8 examples/sec; 0.274 sec/batch)
2016-12-08 11:10:36.729932: step 10350, loss = 0.77 (423.0 examples/sec; 0.303 sec/batch)
2016-12-08 11:10:39.438502: step 10360, loss = 1.20 (535.5 examples/sec; 0.239 sec/batch)
2016-12-08 11:10:42.122713: step 10370, loss = 0.72 (436.0 examples/sec; 0.294 sec/batch)
2016-12-08 11:10:45.173102: step 10380, loss = 0.79 (431.8 examples/sec; 0.296 sec/batch)
2016-12-08 11:10:47.956638: step 10390, loss = 0.95 (435.0 examples/sec; 0.294 sec/batch)
2016-12-08 11:10:50.798097: step 10400, loss = 1.04 (475.1 examples/sec; 0.269 sec/batch)
2016-12-08 11:10:53.899488: step 10410, loss = 1.01 (419.6 examples/sec; 0.305 sec/batch)
2016-12-08 11:10:56.761838: step 10420, loss = 0.89 (437.1 examples/sec; 0.293 sec/batch)
2016-12-08 11:10:59.515011: step 10430, loss = 0.80 (496.9 examples/sec; 0.258 sec/batch)
2016-12-08 11:11:02.128536: step 10440, loss = 0.91 (486.6 examples/sec; 0.263 sec/batch)
2016-12-08 11:11:04.845604: step 10450, loss = 0.98 (501.2 examples/sec; 0.255 sec/batch)
2016-12-08 11:11:07.658647: step 10460, loss = 1.02 (475.0 examples/sec; 0.269 sec/batch)
2016-12-08 11:11:10.490062: step 10470, loss = 0.86 (433.9 examples/sec; 0.295 sec/batch)
2016-12-08 11:11:13.227220: step 10480, loss = 0.87 (428.7 examples/sec; 0.299 sec/batch)
2016-12-08 11:11:15.918814: step 10490, loss = 0.99 (426.7 examples/sec; 0.300 sec/batch)
2016-12-08 11:11:18.720603: step 10500, loss = 0.81 (442.8 examples/sec; 0.289 sec/batch)
2016-12-08 11:11:21.749837: step 10510, loss = 0.92 (400.8 examples/sec; 0.319 sec/batch)
2016-12-08 11:11:24.491980: step 10520, loss = 0.88 (460.1 examples/sec; 0.278 sec/batch)
2016-12-08 11:11:27.153113: step 10530, loss = 0.82 (471.2 examples/sec; 0.272 sec/batch)
2016-12-08 11:11:29.915209: step 10540, loss = 0.98 (485.8 examples/sec; 0.263 sec/batch)
2016-12-08 11:11:32.652052: step 10550, loss = 0.83 (457.0 examples/sec; 0.280 sec/batch)
2016-12-08 11:11:35.613633: step 10560, loss = 1.00 (409.1 examples/sec; 0.313 sec/batch)
2016-12-08 11:11:38.483814: step 10570, loss = 0.95 (466.7 examples/sec; 0.274 sec/batch)
2016-12-08 11:11:41.215848: step 10580, loss = 0.83 (545.0 examples/sec; 0.235 sec/batch)
2016-12-08 11:11:43.903137: step 10590, loss = 0.87 (473.6 examples/sec; 0.270 sec/batch)
2016-12-08 11:11:46.691858: step 10600, loss = 1.13 (512.7 examples/sec; 0.250 sec/batch)
2016-12-08 11:11:49.790134: step 10610, loss = 0.89 (457.4 examples/sec; 0.280 sec/batch)
2016-12-08 11:11:52.526107: step 10620, loss = 0.97 (467.5 examples/sec; 0.274 sec/batch)
2016-12-08 11:11:55.196341: step 10630, loss = 0.98 (448.0 examples/sec; 0.286 sec/batch)
2016-12-08 11:11:58.216843: step 10640, loss = 0.86 (387.2 examples/sec; 0.331 sec/batch)
2016-12-08 11:12:00.875896: step 10650, loss = 0.86 (381.6 examples/sec; 0.335 sec/batch)
2016-12-08 11:12:03.781784: step 10660, loss = 1.16 (432.6 examples/sec; 0.296 sec/batch)
2016-12-08 11:12:06.577306: step 10670, loss = 0.90 (503.0 examples/sec; 0.254 sec/batch)
2016-12-08 11:12:09.283664: step 10680, loss = 0.83 (434.5 examples/sec; 0.295 sec/batch)
2016-12-08 11:12:11.913999: step 10690, loss = 0.80 (426.8 examples/sec; 0.300 sec/batch)
2016-12-08 11:12:14.719057: step 10700, loss = 0.86 (436.7 examples/sec; 0.293 sec/batch)
2016-12-08 11:12:17.799970: step 10710, loss = 0.83 (427.5 examples/sec; 0.299 sec/batch)
2016-12-08 11:12:20.474785: step 10720, loss = 0.69 (458.9 examples/sec; 0.279 sec/batch)
2016-12-08 11:12:23.109281: step 10730, loss = 0.75 (464.5 examples/sec; 0.276 sec/batch)
2016-12-08 11:12:26.040473: step 10740, loss = 1.02 (432.2 examples/sec; 0.296 sec/batch)
2016-12-08 11:12:28.849361: step 10750, loss = 0.81 (470.7 examples/sec; 0.272 sec/batch)
2016-12-08 11:12:31.456223: step 10760, loss = 0.86 (553.7 examples/sec; 0.231 sec/batch)
2016-12-08 11:12:34.242816: step 10770, loss = 0.89 (519.4 examples/sec; 0.246 sec/batch)
2016-12-08 11:12:37.162556: step 10780, loss = 0.96 (484.0 examples/sec; 0.264 sec/batch)
2016-12-08 11:12:39.995082: step 10790, loss = 0.77 (424.2 examples/sec; 0.302 sec/batch)
2016-12-08 11:12:42.653697: step 10800, loss = 0.95 (441.0 examples/sec; 0.290 sec/batch)
2016-12-08 11:12:45.808481: step 10810, loss = 0.90 (426.7 examples/sec; 0.300 sec/batch)
2016-12-08 11:12:48.587094: step 10820, loss = 0.95 (451.8 examples/sec; 0.283 sec/batch)
2016-12-08 11:12:51.354632: step 10830, loss = 0.74 (426.3 examples/sec; 0.300 sec/batch)
2016-12-08 11:12:54.054935: step 10840, loss = 0.74 (440.9 examples/sec; 0.290 sec/batch)
2016-12-08 11:12:56.833900: step 10850, loss = 0.84 (445.0 examples/sec; 0.288 sec/batch)
2016-12-08 11:12:59.684045: step 10860, loss = 0.76 (418.5 examples/sec; 0.306 sec/batch)
2016-12-08 11:13:02.394359: step 10870, loss = 0.95 (508.0 examples/sec; 0.252 sec/batch)
2016-12-08 11:13:05.204385: step 10880, loss = 0.88 (430.6 examples/sec; 0.297 sec/batch)
2016-12-08 11:13:07.959717: step 10890, loss = 0.89 (431.6 examples/sec; 0.297 sec/batch)
2016-12-08 11:13:10.746530: step 10900, loss = 1.04 (448.8 examples/sec; 0.285 sec/batch)
2016-12-08 11:13:13.882366: step 10910, loss = 1.00 (496.9 examples/sec; 0.258 sec/batch)
2016-12-08 11:13:16.634062: step 10920, loss = 0.98 (470.5 examples/sec; 0.272 sec/batch)
2016-12-08 11:13:19.444423: step 10930, loss = 0.82 (430.5 examples/sec; 0.297 sec/batch)
2016-12-08 11:13:22.218522: step 10940, loss = 0.71 (439.6 examples/sec; 0.291 sec/batch)
2016-12-08 11:13:24.934818: step 10950, loss = 0.73 (491.1 examples/sec; 0.261 sec/batch)
2016-12-08 11:13:27.821524: step 10960, loss = 0.84 (453.3 examples/sec; 0.282 sec/batch)
2016-12-08 11:13:30.661017: step 10970, loss = 0.94 (446.0 examples/sec; 0.287 sec/batch)
2016-12-08 11:13:33.403934: step 10980, loss = 0.98 (430.6 examples/sec; 0.297 sec/batch)
2016-12-08 11:13:36.261771: step 10990, loss = 0.84 (509.8 examples/sec; 0.251 sec/batch)
2016-12-08 11:13:38.952778: step 11000, loss = 0.84 (438.7 examples/sec; 0.292 sec/batch)
2016-12-08 11:13:42.677185: step 11010, loss = 0.78 (421.4 examples/sec; 0.304 sec/batch)
2016-12-08 11:13:45.401059: step 11020, loss = 0.83 (491.5 examples/sec; 0.260 sec/batch)
2016-12-08 11:13:48.069697: step 11030, loss = 0.90 (432.1 examples/sec; 0.296 sec/batch)
2016-12-08 11:13:50.795429: step 11040, loss = 0.87 (466.5 examples/sec; 0.274 sec/batch)
2016-12-08 11:13:53.527955: step 11050, loss = 0.84 (473.7 examples/sec; 0.270 sec/batch)
2016-12-08 11:13:56.193362: step 11060, loss = 0.80 (499.3 examples/sec; 0.256 sec/batch)
2016-12-08 11:13:58.921050: step 11070, loss = 1.01 (534.8 examples/sec; 0.239 sec/batch)
2016-12-08 11:14:01.745984: step 11080, loss = 0.78 (453.0 examples/sec; 0.283 sec/batch)
2016-12-08 11:14:04.459442: step 11090, loss = 1.04 (433.5 examples/sec; 0.295 sec/batch)
2016-12-08 11:14:07.256905: step 11100, loss = 0.94 (491.2 examples/sec; 0.261 sec/batch)
2016-12-08 11:14:10.278394: step 11110, loss = 0.85 (454.2 examples/sec; 0.282 sec/batch)
2016-12-08 11:14:12.960726: step 11120, loss = 0.97 (456.2 examples/sec; 0.281 sec/batch)
2016-12-08 11:14:15.881724: step 11130, loss = 0.97 (405.1 examples/sec; 0.316 sec/batch)
2016-12-08 11:14:18.619198: step 11140, loss = 0.92 (421.6 examples/sec; 0.304 sec/batch)
2016-12-08 11:14:21.337893: step 11150, loss = 0.75 (452.5 examples/sec; 0.283 sec/batch)
2016-12-08 11:14:24.078868: step 11160, loss = 0.70 (542.9 examples/sec; 0.236 sec/batch)
2016-12-08 11:14:26.779370: step 11170, loss = 0.92 (453.2 examples/sec; 0.282 sec/batch)
2016-12-08 11:14:29.574609: step 11180, loss = 0.95 (465.2 examples/sec; 0.275 sec/batch)
2016-12-08 11:14:32.253799: step 11190, loss = 0.84 (425.2 examples/sec; 0.301 sec/batch)
2016-12-08 11:14:35.034406: step 11200, loss = 0.91 (439.0 examples/sec; 0.292 sec/batch)
2016-12-08 11:14:38.081324: step 11210, loss = 0.98 (501.4 examples/sec; 0.255 sec/batch)
2016-12-08 11:14:40.860112: step 11220, loss = 0.76 (458.6 examples/sec; 0.279 sec/batch)
2016-12-08 11:14:43.687455: step 11230, loss = 1.13 (443.9 examples/sec; 0.288 sec/batch)
2016-12-08 11:14:46.560271: step 11240, loss = 0.81 (498.0 examples/sec; 0.257 sec/batch)
2016-12-08 11:14:49.320124: step 11250, loss = 0.75 (453.5 examples/sec; 0.282 sec/batch)
2016-12-08 11:14:52.084637: step 11260, loss = 0.77 (397.1 examples/sec; 0.322 sec/batch)
2016-12-08 11:14:54.780543: step 11270, loss = 0.77 (481.0 examples/sec; 0.266 sec/batch)
2016-12-08 11:14:57.522226: step 11280, loss = 0.86 (447.6 examples/sec; 0.286 sec/batch)
2016-12-08 11:15:00.385595: step 11290, loss = 0.85 (496.3 examples/sec; 0.258 sec/batch)
2016-12-08 11:15:03.115092: step 11300, loss = 1.01 (501.1 examples/sec; 0.255 sec/batch)
2016-12-08 11:15:06.310144: step 11310, loss = 0.80 (422.0 examples/sec; 0.303 sec/batch)
2016-12-08 11:15:09.076399: step 11320, loss = 0.89 (459.6 examples/sec; 0.279 sec/batch)
2016-12-08 11:15:12.019156: step 11330, loss = 0.87 (449.0 examples/sec; 0.285 sec/batch)
2016-12-08 11:15:14.911365: step 11340, loss = 0.89 (416.1 examples/sec; 0.308 sec/batch)
2016-12-08 11:15:17.639305: step 11350, loss = 0.87 (422.8 examples/sec; 0.303 sec/batch)
2016-12-08 11:15:20.306593: step 11360, loss = 0.80 (529.2 examples/sec; 0.242 sec/batch)
2016-12-08 11:15:23.189384: step 11370, loss = 0.83 (444.3 examples/sec; 0.288 sec/batch)
2016-12-08 11:15:26.085010: step 11380, loss = 0.96 (455.5 examples/sec; 0.281 sec/batch)
2016-12-08 11:15:28.830828: step 11390, loss = 0.78 (445.9 examples/sec; 0.287 sec/batch)
2016-12-08 11:15:31.797960: step 11400, loss = 0.95 (442.9 examples/sec; 0.289 sec/batch)
2016-12-08 11:15:34.901688: step 11410, loss = 0.90 (454.9 examples/sec; 0.281 sec/batch)
2016-12-08 11:15:37.607664: step 11420, loss = 0.83 (552.5 examples/sec; 0.232 sec/batch)
2016-12-08 11:15:40.424705: step 11430, loss = 0.71 (458.7 examples/sec; 0.279 sec/batch)
2016-12-08 11:15:43.123591: step 11440, loss = 0.93 (478.2 examples/sec; 0.268 sec/batch)
2016-12-08 11:15:45.936972: step 11450, loss = 0.94 (397.0 examples/sec; 0.322 sec/batch)
2016-12-08 11:15:48.696221: step 11460, loss = 0.83 (457.4 examples/sec; 0.280 sec/batch)
2016-12-08 11:15:51.558107: step 11470, loss = 0.90 (469.8 examples/sec; 0.272 sec/batch)
2016-12-08 11:15:54.273388: step 11480, loss = 0.80 (524.0 examples/sec; 0.244 sec/batch)
2016-12-08 11:15:56.967153: step 11490, loss = 0.84 (446.6 examples/sec; 0.287 sec/batch)
2016-12-08 11:15:59.758895: step 11500, loss = 0.90 (481.6 examples/sec; 0.266 sec/batch)
2016-12-08 11:16:02.978905: step 11510, loss = 0.69 (436.7 examples/sec; 0.293 sec/batch)
2016-12-08 11:16:05.813009: step 11520, loss = 0.92 (454.9 examples/sec; 0.281 sec/batch)
2016-12-08 11:16:08.641853: step 11530, loss = 0.69 (500.6 examples/sec; 0.256 sec/batch)
2016-12-08 11:16:11.341414: step 11540, loss = 0.74 (476.4 examples/sec; 0.269 sec/batch)
2016-12-08 11:16:14.193766: step 11550, loss = 0.77 (460.0 examples/sec; 0.278 sec/batch)
2016-12-08 11:16:17.093398: step 11560, loss = 0.75 (471.3 examples/sec; 0.272 sec/batch)
2016-12-08 11:16:19.839706: step 11570, loss = 0.80 (432.1 examples/sec; 0.296 sec/batch)
2016-12-08 11:16:22.579398: step 11580, loss = 0.75 (470.1 examples/sec; 0.272 sec/batch)
2016-12-08 11:16:25.322249: step 11590, loss = 1.01 (471.6 examples/sec; 0.271 sec/batch)
2016-12-08 11:16:28.055453: step 11600, loss = 0.70 (510.0 examples/sec; 0.251 sec/batch)
2016-12-08 11:16:31.236703: step 11610, loss = 0.73 (416.5 examples/sec; 0.307 sec/batch)
2016-12-08 11:16:33.967063: step 11620, loss = 1.10 (460.7 examples/sec; 0.278 sec/batch)
2016-12-08 11:16:36.639735: step 11630, loss = 0.81 (489.7 examples/sec; 0.261 sec/batch)
2016-12-08 11:16:39.383577: step 11640, loss = 0.69 (423.4 examples/sec; 0.302 sec/batch)
2016-12-08 11:16:42.031978: step 11650, loss = 0.86 (498.1 examples/sec; 0.257 sec/batch)
2016-12-08 11:16:44.675394: step 11660, loss = 0.72 (457.6 examples/sec; 0.280 sec/batch)
2016-12-08 11:16:47.437294: step 11670, loss = 0.91 (459.1 examples/sec; 0.279 sec/batch)
2016-12-08 11:16:50.205411: step 11680, loss = 1.04 (404.9 examples/sec; 0.316 sec/batch)
2016-12-08 11:16:52.999023: step 11690, loss = 0.88 (483.8 examples/sec; 0.265 sec/batch)
2016-12-08 11:16:55.748138: step 11700, loss = 0.87 (451.1 examples/sec; 0.284 sec/batch)
2016-12-08 11:16:58.838814: step 11710, loss = 0.85 (522.0 examples/sec; 0.245 sec/batch)
2016-12-08 11:17:01.531208: step 11720, loss = 0.92 (446.7 examples/sec; 0.287 sec/batch)
2016-12-08 11:17:04.305387: step 11730, loss = 0.86 (500.9 examples/sec; 0.256 sec/batch)
2016-12-08 11:17:07.072555: step 11740, loss = 0.85 (457.7 examples/sec; 0.280 sec/batch)
2016-12-08 11:17:09.814922: step 11750, loss = 0.73 (468.1 examples/sec; 0.273 sec/batch)
2016-12-08 11:17:12.618098: step 11760, loss = 0.64 (485.0 examples/sec; 0.264 sec/batch)
2016-12-08 11:17:15.374289: step 11770, loss = 0.73 (480.7 examples/sec; 0.266 sec/batch)
2016-12-08 11:17:18.170407: step 11780, loss = 0.92 (484.4 examples/sec; 0.264 sec/batch)
2016-12-08 11:17:20.880299: step 11790, loss = 0.86 (522.0 examples/sec; 0.245 sec/batch)
2016-12-08 11:17:23.655666: step 11800, loss = 0.93 (428.0 examples/sec; 0.299 sec/batch)
2016-12-08 11:17:26.605755: step 11810, loss = 1.09 (499.5 examples/sec; 0.256 sec/batch)
2016-12-08 11:17:29.320959: step 11820, loss = 0.70 (413.5 examples/sec; 0.310 sec/batch)
2016-12-08 11:17:32.058859: step 11830, loss = 1.16 (476.7 examples/sec; 0.269 sec/batch)
2016-12-08 11:17:34.719286: step 11840, loss = 0.77 (462.7 examples/sec; 0.277 sec/batch)
2016-12-08 11:17:37.383711: step 11850, loss = 0.87 (492.5 examples/sec; 0.260 sec/batch)
2016-12-08 11:17:40.208538: step 11860, loss = 0.80 (503.7 examples/sec; 0.254 sec/batch)
2016-12-08 11:17:43.006959: step 11870, loss = 0.92 (476.9 examples/sec; 0.268 sec/batch)
2016-12-08 11:17:45.749672: step 11880, loss = 0.87 (487.1 examples/sec; 0.263 sec/batch)
2016-12-08 11:17:48.537277: step 11890, loss = 0.79 (399.7 examples/sec; 0.320 sec/batch)
2016-12-08 11:17:51.459827: step 11900, loss = 0.85 (434.8 examples/sec; 0.294 sec/batch)
2016-12-08 11:17:54.559158: step 11910, loss = 0.94 (463.2 examples/sec; 0.276 sec/batch)
2016-12-08 11:17:57.289228: step 11920, loss = 0.85 (450.9 examples/sec; 0.284 sec/batch)
2016-12-08 11:18:00.163904: step 11930, loss = 0.99 (422.8 examples/sec; 0.303 sec/batch)
2016-12-08 11:18:02.832547: step 11940, loss = 0.82 (556.6 examples/sec; 0.230 sec/batch)
2016-12-08 11:18:05.567106: step 11950, loss = 0.86 (522.8 examples/sec; 0.245 sec/batch)
2016-12-08 11:18:08.270905: step 11960, loss = 0.81 (490.3 examples/sec; 0.261 sec/batch)
2016-12-08 11:18:10.971658: step 11970, loss = 0.78 (454.6 examples/sec; 0.282 sec/batch)
2016-12-08 11:18:13.663316: step 11980, loss = 0.79 (429.9 examples/sec; 0.298 sec/batch)
2016-12-08 11:18:16.448116: step 11990, loss = 0.82 (426.6 examples/sec; 0.300 sec/batch)
2016-12-08 11:18:19.353466: step 12000, loss = 1.02 (486.2 examples/sec; 0.263 sec/batch)
2016-12-08 11:18:23.238686: step 12010, loss = 0.83 (463.2 examples/sec; 0.276 sec/batch)
2016-12-08 11:18:25.931993: step 12020, loss = 0.90 (531.5 examples/sec; 0.241 sec/batch)
2016-12-08 11:18:28.650586: step 12030, loss = 0.82 (519.8 examples/sec; 0.246 sec/batch)
2016-12-08 11:18:31.422555: step 12040, loss = 0.94 (463.3 examples/sec; 0.276 sec/batch)
2016-12-08 11:18:34.254442: step 12050, loss = 0.71 (465.3 examples/sec; 0.275 sec/batch)
2016-12-08 11:18:37.106114: step 12060, loss = 0.72 (449.8 examples/sec; 0.285 sec/batch)
2016-12-08 11:18:39.819775: step 12070, loss = 0.96 (435.9 examples/sec; 0.294 sec/batch)
2016-12-08 11:18:42.606297: step 12080, loss = 0.95 (430.4 examples/sec; 0.297 sec/batch)
2016-12-08 11:18:45.390443: step 12090, loss = 1.02 (496.6 examples/sec; 0.258 sec/batch)
2016-12-08 11:18:48.283458: step 12100, loss = 0.75 (544.6 examples/sec; 0.235 sec/batch)
2016-12-08 11:18:51.490946: step 12110, loss = 0.87 (429.5 examples/sec; 0.298 sec/batch)
2016-12-08 11:18:54.196799: step 12120, loss = 0.99 (435.2 examples/sec; 0.294 sec/batch)
2016-12-08 11:18:56.989905: step 12130, loss = 0.69 (448.4 examples/sec; 0.285 sec/batch)
2016-12-08 11:18:59.798935: step 12140, loss = 0.85 (474.6 examples/sec; 0.270 sec/batch)
2016-12-08 11:19:02.397613: step 12150, loss = 1.03 (485.4 examples/sec; 0.264 sec/batch)
2016-12-08 11:19:05.079182: step 12160, loss = 0.84 (485.0 examples/sec; 0.264 sec/batch)
2016-12-08 11:19:07.905592: step 12170, loss = 0.91 (458.7 examples/sec; 0.279 sec/batch)
2016-12-08 11:19:10.704883: step 12180, loss = 0.93 (412.7 examples/sec; 0.310 sec/batch)
2016-12-08 11:19:13.466467: step 12190, loss = 0.74 (433.9 examples/sec; 0.295 sec/batch)
2016-12-08 11:19:16.436095: step 12200, loss = 0.87 (447.7 examples/sec; 0.286 sec/batch)
2016-12-08 11:19:19.618106: step 12210, loss = 1.07 (485.9 examples/sec; 0.263 sec/batch)
2016-12-08 11:19:22.522501: step 12220, loss = 0.83 (426.6 examples/sec; 0.300 sec/batch)
2016-12-08 11:19:25.171680: step 12230, loss = 0.90 (581.6 examples/sec; 0.220 sec/batch)
2016-12-08 11:19:27.833221: step 12240, loss = 0.91 (440.5 examples/sec; 0.291 sec/batch)
2016-12-08 11:19:30.731347: step 12250, loss = 1.16 (403.3 examples/sec; 0.317 sec/batch)
2016-12-08 11:19:33.504049: step 12260, loss = 0.84 (437.2 examples/sec; 0.293 sec/batch)
2016-12-08 11:19:36.272899: step 12270, loss = 0.91 (440.3 examples/sec; 0.291 sec/batch)
2016-12-08 11:19:39.038757: step 12280, loss = 0.83 (563.4 examples/sec; 0.227 sec/batch)
2016-12-08 11:19:41.775185: step 12290, loss = 0.92 (438.2 examples/sec; 0.292 sec/batch)
2016-12-08 11:19:44.474719: step 12300, loss = 0.83 (475.8 examples/sec; 0.269 sec/batch)
2016-12-08 11:19:47.560384: step 12310, loss = 0.93 (440.3 examples/sec; 0.291 sec/batch)
2016-12-08 11:19:50.262128: step 12320, loss = 0.95 (431.0 examples/sec; 0.297 sec/batch)
2016-12-08 11:19:52.997988: step 12330, loss = 0.72 (486.3 examples/sec; 0.263 sec/batch)
2016-12-08 11:19:55.673125: step 12340, loss = 0.94 (428.5 examples/sec; 0.299 sec/batch)
2016-12-08 11:19:58.325171: step 12350, loss = 0.90 (488.8 examples/sec; 0.262 sec/batch)
2016-12-08 11:20:01.139933: step 12360, loss = 0.80 (567.9 examples/sec; 0.225 sec/batch)
2016-12-08 11:20:03.824526: step 12370, loss = 0.95 (565.8 examples/sec; 0.226 sec/batch)
2016-12-08 11:20:06.597990: step 12380, loss = 0.75 (515.8 examples/sec; 0.248 sec/batch)
2016-12-08 11:20:09.129993: step 12390, loss = 0.84 (501.4 examples/sec; 0.255 sec/batch)
2016-12-08 11:20:11.644755: step 12400, loss = 0.86 (428.5 examples/sec; 0.299 sec/batch)
2016-12-08 11:20:14.645997: step 12410, loss = 0.94 (496.3 examples/sec; 0.258 sec/batch)
2016-12-08 11:20:17.404859: step 12420, loss = 0.65 (394.0 examples/sec; 0.325 sec/batch)
2016-12-08 11:20:20.219921: step 12430, loss = 0.79 (456.3 examples/sec; 0.281 sec/batch)
2016-12-08 11:20:23.108196: step 12440, loss = 1.20 (468.3 examples/sec; 0.273 sec/batch)
2016-12-08 11:20:25.833938: step 12450, loss = 0.81 (466.6 examples/sec; 0.274 sec/batch)
2016-12-08 11:20:28.569839: step 12460, loss = 0.90 (467.8 examples/sec; 0.274 sec/batch)
2016-12-08 11:20:31.424613: step 12470, loss = 0.94 (455.3 examples/sec; 0.281 sec/batch)
2016-12-08 11:20:34.286338: step 12480, loss = 0.77 (387.9 examples/sec; 0.330 sec/batch)
2016-12-08 11:20:36.997789: step 12490, loss = 0.91 (487.9 examples/sec; 0.262 sec/batch)
2016-12-08 11:20:39.783686: step 12500, loss = 0.94 (426.4 examples/sec; 0.300 sec/batch)
2016-12-08 11:20:42.911970: step 12510, loss = 0.84 (459.3 examples/sec; 0.279 sec/batch)
2016-12-08 11:20:45.699229: step 12520, loss = 0.79 (423.4 examples/sec; 0.302 sec/batch)
2016-12-08 11:20:48.536695: step 12530, loss = 0.89 (500.5 examples/sec; 0.256 sec/batch)
2016-12-08 11:20:51.316237: step 12540, loss = 0.89 (488.1 examples/sec; 0.262 sec/batch)
2016-12-08 11:20:54.195307: step 12550, loss = 0.84 (418.3 examples/sec; 0.306 sec/batch)
2016-12-08 11:20:57.072547: step 12560, loss = 0.67 (472.3 examples/sec; 0.271 sec/batch)
2016-12-08 11:20:59.793001: step 12570, loss = 0.91 (454.4 examples/sec; 0.282 sec/batch)
2016-12-08 11:21:02.774903: step 12580, loss = 0.71 (428.3 examples/sec; 0.299 sec/batch)
2016-12-08 11:21:05.726132: step 12590, loss = 0.88 (441.5 examples/sec; 0.290 sec/batch)
2016-12-08 11:21:08.632682: step 12600, loss = 0.85 (429.1 examples/sec; 0.298 sec/batch)
2016-12-08 11:21:11.592994: step 12610, loss = 0.84 (512.7 examples/sec; 0.250 sec/batch)
