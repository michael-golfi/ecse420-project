Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
WARNING:tensorflow:From cifar10_train.py:81 in train.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2016-03-02.
Instructions for updating:
Please use tf.global_variables instead.
2016-12-08 06:21:25.003454: step 0, loss = 4.68 (25.4 examples/sec; 5.031 sec/batch)
2016-12-08 06:21:33.364223: step 10, loss = 4.62 (164.3 examples/sec; 0.779 sec/batch)
2016-12-08 06:21:40.972531: step 20, loss = 4.45 (167.0 examples/sec; 0.767 sec/batch)
2016-12-08 06:21:48.455716: step 30, loss = 4.43 (169.1 examples/sec; 0.757 sec/batch)
2016-12-08 06:21:55.852008: step 40, loss = 4.46 (164.9 examples/sec; 0.776 sec/batch)
2016-12-08 06:22:02.310938: step 50, loss = 4.29 (196.2 examples/sec; 0.652 sec/batch)
2016-12-08 06:22:09.477915: step 60, loss = 4.30 (187.0 examples/sec; 0.684 sec/batch)
2016-12-08 06:22:16.627364: step 70, loss = 4.14 (179.0 examples/sec; 0.715 sec/batch)
2016-12-08 06:22:23.201623: step 80, loss = 4.17 (193.7 examples/sec; 0.661 sec/batch)
2016-12-08 06:22:30.304330: step 90, loss = 4.15 (173.1 examples/sec; 0.740 sec/batch)
2016-12-08 06:22:37.345916: step 100, loss = 4.18 (188.3 examples/sec; 0.680 sec/batch)
2016-12-08 06:22:44.344348: step 110, loss = 4.08 (180.4 examples/sec; 0.710 sec/batch)
2016-12-08 06:22:51.319714: step 120, loss = 4.09 (172.1 examples/sec; 0.744 sec/batch)
2016-12-08 06:22:58.417810: step 130, loss = 4.04 (187.4 examples/sec; 0.683 sec/batch)
2016-12-08 06:23:05.088497: step 140, loss = 3.94 (205.3 examples/sec; 0.624 sec/batch)
2016-12-08 06:23:11.998880: step 150, loss = 3.86 (188.8 examples/sec; 0.678 sec/batch)
2016-12-08 06:23:18.660878: step 160, loss = 4.20 (191.9 examples/sec; 0.667 sec/batch)
2016-12-08 06:23:25.560172: step 170, loss = 3.86 (195.0 examples/sec; 0.656 sec/batch)
2016-12-08 06:23:32.619446: step 180, loss = 3.79 (179.1 examples/sec; 0.715 sec/batch)
2016-12-08 06:23:39.459354: step 190, loss = 3.77 (164.6 examples/sec; 0.778 sec/batch)
2016-12-08 06:23:46.187935: step 200, loss = 3.77 (191.7 examples/sec; 0.668 sec/batch)
2016-12-08 06:23:53.917743: step 210, loss = 3.78 (218.6 examples/sec; 0.585 sec/batch)
2016-12-08 06:24:00.634346: step 220, loss = 3.85 (209.8 examples/sec; 0.610 sec/batch)
2016-12-08 06:24:06.623202: step 230, loss = 3.67 (189.1 examples/sec; 0.677 sec/batch)
2016-12-08 06:24:13.771592: step 240, loss = 3.68 (176.5 examples/sec; 0.725 sec/batch)
2016-12-08 06:24:20.572931: step 250, loss = 3.60 (209.4 examples/sec; 0.611 sec/batch)
2016-12-08 06:24:27.712759: step 260, loss = 3.61 (172.9 examples/sec; 0.741 sec/batch)
2016-12-08 06:24:34.758888: step 270, loss = 3.49 (181.2 examples/sec; 0.707 sec/batch)
2016-12-08 06:24:41.704429: step 280, loss = 3.69 (206.1 examples/sec; 0.621 sec/batch)
2016-12-08 06:24:48.594392: step 290, loss = 3.76 (173.8 examples/sec; 0.737 sec/batch)
2016-12-08 06:24:55.356521: step 300, loss = 3.65 (184.7 examples/sec; 0.693 sec/batch)
2016-12-08 06:25:02.486177: step 310, loss = 3.83 (214.5 examples/sec; 0.597 sec/batch)
2016-12-08 06:25:09.119459: step 320, loss = 3.60 (178.3 examples/sec; 0.718 sec/batch)
2016-12-08 06:25:15.918685: step 330, loss = 3.35 (234.2 examples/sec; 0.547 sec/batch)
2016-12-08 06:25:22.910465: step 340, loss = 3.44 (161.2 examples/sec; 0.794 sec/batch)
2016-12-08 06:25:29.676300: step 350, loss = 3.47 (191.1 examples/sec; 0.670 sec/batch)
2016-12-08 06:25:36.821121: step 360, loss = 3.37 (204.1 examples/sec; 0.627 sec/batch)
2016-12-08 06:25:43.762045: step 370, loss = 3.52 (186.0 examples/sec; 0.688 sec/batch)
2016-12-08 06:25:50.215602: step 380, loss = 3.48 (229.7 examples/sec; 0.557 sec/batch)
2016-12-08 06:25:56.949423: step 390, loss = 3.48 (162.9 examples/sec; 0.786 sec/batch)
2016-12-08 06:26:04.241803: step 400, loss = 3.50 (163.4 examples/sec; 0.784 sec/batch)
2016-12-08 06:26:12.082550: step 410, loss = 3.42 (163.5 examples/sec; 0.783 sec/batch)
2016-12-08 06:26:18.914447: step 420, loss = 3.25 (190.2 examples/sec; 0.673 sec/batch)
2016-12-08 06:26:25.308246: step 430, loss = 3.34 (196.8 examples/sec; 0.650 sec/batch)
2016-12-08 06:26:32.212723: step 440, loss = 3.33 (187.4 examples/sec; 0.683 sec/batch)
2016-12-08 06:26:39.228424: step 450, loss = 3.33 (189.7 examples/sec; 0.675 sec/batch)
2016-12-08 06:26:45.978852: step 460, loss = 3.07 (182.2 examples/sec; 0.702 sec/batch)
2016-12-08 06:26:52.682468: step 470, loss = 3.17 (186.9 examples/sec; 0.685 sec/batch)
2016-12-08 06:26:59.605169: step 480, loss = 3.41 (180.5 examples/sec; 0.709 sec/batch)
2016-12-08 06:27:06.607357: step 490, loss = 3.32 (195.9 examples/sec; 0.653 sec/batch)
2016-12-08 06:27:13.636683: step 500, loss = 3.14 (196.2 examples/sec; 0.653 sec/batch)
2016-12-08 06:27:21.172731: step 510, loss = 3.10 (192.6 examples/sec; 0.665 sec/batch)
2016-12-08 06:27:27.828879: step 520, loss = 3.43 (160.9 examples/sec; 0.796 sec/batch)
2016-12-08 06:27:35.020904: step 530, loss = 2.97 (166.3 examples/sec; 0.770 sec/batch)
2016-12-08 06:27:41.888344: step 540, loss = 3.11 (220.7 examples/sec; 0.580 sec/batch)
2016-12-08 06:27:48.377283: step 550, loss = 2.96 (195.0 examples/sec; 0.657 sec/batch)
2016-12-08 06:27:55.354341: step 560, loss = 3.10 (160.2 examples/sec; 0.799 sec/batch)
2016-12-08 06:28:02.134752: step 570, loss = 2.97 (173.1 examples/sec; 0.739 sec/batch)
2016-12-08 06:28:08.960601: step 580, loss = 2.93 (204.7 examples/sec; 0.625 sec/batch)
2016-12-08 06:28:15.670206: step 590, loss = 3.08 (197.0 examples/sec; 0.650 sec/batch)
2016-12-08 06:28:22.301129: step 600, loss = 3.08 (191.2 examples/sec; 0.669 sec/batch)
2016-12-08 06:28:30.163301: step 610, loss = 3.09 (202.1 examples/sec; 0.633 sec/batch)
2016-12-08 06:28:37.242879: step 620, loss = 2.96 (172.4 examples/sec; 0.742 sec/batch)
2016-12-08 06:28:44.265819: step 630, loss = 3.00 (185.6 examples/sec; 0.690 sec/batch)
2016-12-08 06:28:51.224234: step 640, loss = 2.78 (170.6 examples/sec; 0.750 sec/batch)
2016-12-08 06:28:57.998881: step 650, loss = 2.90 (177.3 examples/sec; 0.722 sec/batch)
2016-12-08 06:29:04.938341: step 660, loss = 2.79 (195.7 examples/sec; 0.654 sec/batch)
2016-12-08 06:29:11.465530: step 670, loss = 2.99 (182.5 examples/sec; 0.701 sec/batch)
2016-12-08 06:29:17.941871: step 680, loss = 3.05 (229.1 examples/sec; 0.559 sec/batch)
2016-12-08 06:29:24.776323: step 690, loss = 2.87 (175.2 examples/sec; 0.730 sec/batch)
2016-12-08 06:29:31.730103: step 700, loss = 2.88 (197.6 examples/sec; 0.648 sec/batch)
2016-12-08 06:29:39.279167: step 710, loss = 2.92 (182.0 examples/sec; 0.703 sec/batch)
2016-12-08 06:29:46.320740: step 720, loss = 2.95 (187.3 examples/sec; 0.683 sec/batch)
2016-12-08 06:29:53.321620: step 730, loss = 2.72 (192.3 examples/sec; 0.666 sec/batch)
2016-12-08 06:30:00.382011: step 740, loss = 2.85 (170.4 examples/sec; 0.751 sec/batch)
2016-12-08 06:30:07.614445: step 750, loss = 2.75 (171.9 examples/sec; 0.745 sec/batch)
2016-12-08 06:30:14.563386: step 760, loss = 2.77 (172.9 examples/sec; 0.740 sec/batch)
2016-12-08 06:30:21.214435: step 770, loss = 2.76 (213.2 examples/sec; 0.600 sec/batch)
2016-12-08 06:30:28.023580: step 780, loss = 2.78 (184.4 examples/sec; 0.694 sec/batch)
2016-12-08 06:30:34.966008: step 790, loss = 2.61 (219.3 examples/sec; 0.584 sec/batch)
2016-12-08 06:30:41.691742: step 800, loss = 2.50 (184.6 examples/sec; 0.693 sec/batch)
2016-12-08 06:30:49.384987: step 810, loss = 2.70 (175.7 examples/sec; 0.728 sec/batch)
2016-12-08 06:30:56.462025: step 820, loss = 2.61 (181.9 examples/sec; 0.704 sec/batch)
2016-12-08 06:31:03.172151: step 830, loss = 2.63 (188.8 examples/sec; 0.678 sec/batch)
2016-12-08 06:31:09.922166: step 840, loss = 2.66 (189.6 examples/sec; 0.675 sec/batch)
2016-12-08 06:31:16.577141: step 850, loss = 2.86 (176.4 examples/sec; 0.726 sec/batch)
2016-12-08 06:31:23.287387: step 860, loss = 2.83 (190.1 examples/sec; 0.673 sec/batch)
2016-12-08 06:31:30.434175: step 870, loss = 2.55 (175.1 examples/sec; 0.731 sec/batch)
2016-12-08 06:31:37.195279: step 880, loss = 2.54 (180.9 examples/sec; 0.708 sec/batch)
2016-12-08 06:31:44.349982: step 890, loss = 2.49 (168.7 examples/sec; 0.759 sec/batch)
2016-12-08 06:31:51.107492: step 900, loss = 2.60 (223.3 examples/sec; 0.573 sec/batch)
2016-12-08 06:31:58.648708: step 910, loss = 2.65 (184.3 examples/sec; 0.695 sec/batch)
2016-12-08 06:32:04.991953: step 920, loss = 2.61 (195.0 examples/sec; 0.656 sec/batch)
2016-12-08 06:32:12.006265: step 930, loss = 2.46 (199.4 examples/sec; 0.642 sec/batch)
2016-12-08 06:32:18.719366: step 940, loss = 2.47 (196.9 examples/sec; 0.650 sec/batch)
2016-12-08 06:32:25.428208: step 950, loss = 2.55 (202.4 examples/sec; 0.632 sec/batch)
2016-12-08 06:32:32.443333: step 960, loss = 2.57 (196.4 examples/sec; 0.652 sec/batch)
2016-12-08 06:32:39.343236: step 970, loss = 2.53 (171.0 examples/sec; 0.749 sec/batch)
2016-12-08 06:32:46.209654: step 980, loss = 2.45 (194.7 examples/sec; 0.657 sec/batch)
2016-12-08 06:32:53.120677: step 990, loss = 2.41 (186.4 examples/sec; 0.687 sec/batch)
2016-12-08 06:33:00.251464: step 1000, loss = 2.51 (191.8 examples/sec; 0.667 sec/batch)
2016-12-08 06:33:08.312648: step 1010, loss = 2.46 (188.9 examples/sec; 0.678 sec/batch)
2016-12-08 06:33:15.145875: step 1020, loss = 2.42 (173.8 examples/sec; 0.736 sec/batch)
2016-12-08 06:33:21.769483: step 1030, loss = 2.16 (238.1 examples/sec; 0.537 sec/batch)
2016-12-08 06:33:28.641788: step 1040, loss = 2.25 (182.7 examples/sec; 0.701 sec/batch)
2016-12-08 06:33:35.755751: step 1050, loss = 2.42 (186.5 examples/sec; 0.686 sec/batch)
2016-12-08 06:33:42.602999: step 1060, loss = 2.27 (183.3 examples/sec; 0.698 sec/batch)
2016-12-08 06:33:49.238423: step 1070, loss = 2.37 (188.2 examples/sec; 0.680 sec/batch)
2016-12-08 06:33:55.944957: step 1080, loss = 2.43 (216.6 examples/sec; 0.591 sec/batch)
2016-12-08 06:34:02.638625: step 1090, loss = 2.26 (196.5 examples/sec; 0.652 sec/batch)
2016-12-08 06:34:09.576475: step 1100, loss = 2.29 (205.2 examples/sec; 0.624 sec/batch)
2016-12-08 06:34:16.826622: step 1110, loss = 2.63 (175.2 examples/sec; 0.731 sec/batch)
2016-12-08 06:34:23.693115: step 1120, loss = 2.51 (175.7 examples/sec; 0.729 sec/batch)
2016-12-08 06:34:30.605672: step 1130, loss = 2.20 (196.4 examples/sec; 0.652 sec/batch)
2016-12-08 06:34:37.633601: step 1140, loss = 2.26 (162.7 examples/sec; 0.786 sec/batch)
2016-12-08 06:34:44.367948: step 1150, loss = 2.27 (188.5 examples/sec; 0.679 sec/batch)
2016-12-08 06:34:51.035428: step 1160, loss = 2.20 (197.1 examples/sec; 0.649 sec/batch)
2016-12-08 06:34:57.801036: step 1170, loss = 2.28 (187.0 examples/sec; 0.684 sec/batch)
2016-12-08 06:35:04.674620: step 1180, loss = 2.26 (189.3 examples/sec; 0.676 sec/batch)
2016-12-08 06:35:11.772725: step 1190, loss = 2.09 (172.4 examples/sec; 0.742 sec/batch)
2016-12-08 06:35:18.623541: step 1200, loss = 2.40 (192.4 examples/sec; 0.665 sec/batch)
2016-12-08 06:35:26.168299: step 1210, loss = 1.98 (188.5 examples/sec; 0.679 sec/batch)
2016-12-08 06:35:32.685889: step 1220, loss = 2.20 (214.1 examples/sec; 0.598 sec/batch)
2016-12-08 06:35:39.552236: step 1230, loss = 2.23 (167.5 examples/sec; 0.764 sec/batch)
2016-12-08 06:35:46.318945: step 1240, loss = 2.24 (170.6 examples/sec; 0.750 sec/batch)
2016-12-08 06:35:53.151086: step 1250, loss = 2.33 (172.0 examples/sec; 0.744 sec/batch)
2016-12-08 06:35:59.610262: step 1260, loss = 2.01 (181.7 examples/sec; 0.704 sec/batch)
2016-12-08 06:36:06.541949: step 1270, loss = 2.20 (178.4 examples/sec; 0.718 sec/batch)
2016-12-08 06:36:12.961871: step 1280, loss = 2.03 (196.5 examples/sec; 0.651 sec/batch)
2016-12-08 06:36:19.648631: step 1290, loss = 2.32 (212.1 examples/sec; 0.603 sec/batch)
2016-12-08 06:36:26.464768: step 1300, loss = 2.49 (176.9 examples/sec; 0.724 sec/batch)
2016-12-08 06:36:33.815567: step 1310, loss = 1.96 (180.8 examples/sec; 0.708 sec/batch)
2016-12-08 06:36:40.238638: step 1320, loss = 2.00 (202.0 examples/sec; 0.634 sec/batch)
2016-12-08 06:36:47.230097: step 1330, loss = 2.08 (206.1 examples/sec; 0.621 sec/batch)
2016-12-08 06:36:53.902290: step 1340, loss = 2.07 (190.2 examples/sec; 0.673 sec/batch)
2016-12-08 06:37:00.578738: step 1350, loss = 2.09 (205.0 examples/sec; 0.625 sec/batch)
2016-12-08 06:37:07.081149: step 1360, loss = 1.98 (217.9 examples/sec; 0.588 sec/batch)
2016-12-08 06:37:14.059252: step 1370, loss = 2.11 (175.5 examples/sec; 0.729 sec/batch)
2016-12-08 06:37:21.210982: step 1380, loss = 2.04 (211.7 examples/sec; 0.605 sec/batch)
2016-12-08 06:37:28.352391: step 1390, loss = 1.86 (182.5 examples/sec; 0.701 sec/batch)
2016-12-08 06:37:34.844942: step 1400, loss = 2.04 (197.7 examples/sec; 0.647 sec/batch)
2016-12-08 06:37:42.224580: step 1410, loss = 2.05 (186.2 examples/sec; 0.687 sec/batch)
2016-12-08 06:37:48.776689: step 1420, loss = 2.11 (205.2 examples/sec; 0.624 sec/batch)
2016-12-08 06:37:55.565949: step 1430, loss = 1.99 (176.1 examples/sec; 0.727 sec/batch)
2016-12-08 06:38:02.789622: step 1440, loss = 2.12 (181.5 examples/sec; 0.705 sec/batch)
2016-12-08 06:38:09.514955: step 1450, loss = 2.05 (192.5 examples/sec; 0.665 sec/batch)
2016-12-08 06:38:16.131892: step 1460, loss = 1.97 (222.4 examples/sec; 0.576 sec/batch)
2016-12-08 06:38:22.764442: step 1470, loss = 1.91 (211.1 examples/sec; 0.606 sec/batch)
2016-12-08 06:38:29.526430: step 1480, loss = 1.66 (203.9 examples/sec; 0.628 sec/batch)
2016-12-08 06:38:36.535988: step 1490, loss = 1.90 (181.7 examples/sec; 0.704 sec/batch)
2016-12-08 06:38:43.209297: step 1500, loss = 1.87 (167.8 examples/sec; 0.763 sec/batch)
2016-12-08 06:38:50.983024: step 1510, loss = 1.78 (164.7 examples/sec; 0.777 sec/batch)
2016-12-08 06:38:57.775532: step 1520, loss = 1.90 (199.4 examples/sec; 0.642 sec/batch)
2016-12-08 06:39:04.416666: step 1530, loss = 1.88 (158.9 examples/sec; 0.805 sec/batch)
2016-12-08 06:39:11.170595: step 1540, loss = 1.76 (167.2 examples/sec; 0.765 sec/batch)
2016-12-08 06:39:18.189077: step 1550, loss = 1.76 (189.2 examples/sec; 0.677 sec/batch)
2016-12-08 06:39:24.731832: step 1560, loss = 1.99 (200.4 examples/sec; 0.639 sec/batch)
2016-12-08 06:39:31.180311: step 1570, loss = 1.86 (187.8 examples/sec; 0.682 sec/batch)
2016-12-08 06:39:38.263398: step 1580, loss = 1.84 (160.4 examples/sec; 0.798 sec/batch)
2016-12-08 06:39:45.027751: step 1590, loss = 1.91 (201.1 examples/sec; 0.636 sec/batch)
2016-12-08 06:39:51.746818: step 1600, loss = 1.81 (173.3 examples/sec; 0.739 sec/batch)
2016-12-08 06:39:59.186855: step 1610, loss = 1.93 (174.8 examples/sec; 0.732 sec/batch)
2016-12-08 06:40:05.234503: step 1620, loss = 1.84 (188.5 examples/sec; 0.679 sec/batch)
2016-12-08 06:40:12.246790: step 1630, loss = 1.88 (158.1 examples/sec; 0.810 sec/batch)
2016-12-08 06:40:18.861006: step 1640, loss = 1.83 (169.0 examples/sec; 0.757 sec/batch)
2016-12-08 06:40:25.296771: step 1650, loss = 1.78 (172.8 examples/sec; 0.741 sec/batch)
2016-12-08 06:40:32.040463: step 1660, loss = 1.78 (177.6 examples/sec; 0.721 sec/batch)
2016-12-08 06:40:39.123841: step 1670, loss = 1.85 (184.0 examples/sec; 0.696 sec/batch)
2016-12-08 06:40:45.687388: step 1680, loss = 2.11 (201.8 examples/sec; 0.634 sec/batch)
2016-12-08 06:40:52.665660: step 1690, loss = 1.90 (187.5 examples/sec; 0.683 sec/batch)
2016-12-08 06:40:59.408208: step 1700, loss = 1.81 (181.9 examples/sec; 0.704 sec/batch)
2016-12-08 06:41:07.176961: step 1710, loss = 1.86 (178.8 examples/sec; 0.716 sec/batch)
2016-12-08 06:41:13.535016: step 1720, loss = 1.76 (188.0 examples/sec; 0.681 sec/batch)
2016-12-08 06:41:20.304729: step 1730, loss = 1.88 (212.7 examples/sec; 0.602 sec/batch)
2016-12-08 06:41:27.058803: step 1740, loss = 1.82 (175.5 examples/sec; 0.729 sec/batch)
2016-12-08 06:41:33.714360: step 1750, loss = 1.71 (180.9 examples/sec; 0.708 sec/batch)
2016-12-08 06:41:40.365066: step 1760, loss = 1.67 (190.0 examples/sec; 0.674 sec/batch)
2016-12-08 06:41:47.003204: step 1770, loss = 1.79 (210.5 examples/sec; 0.608 sec/batch)
2016-12-08 06:41:53.645983: step 1780, loss = 1.70 (197.3 examples/sec; 0.649 sec/batch)
2016-12-08 06:42:00.488537: step 1790, loss = 1.71 (174.6 examples/sec; 0.733 sec/batch)
2016-12-08 06:42:07.411723: step 1800, loss = 1.64 (174.9 examples/sec; 0.732 sec/batch)
2016-12-08 06:42:14.633807: step 1810, loss = 1.56 (197.8 examples/sec; 0.647 sec/batch)
2016-12-08 06:42:21.814585: step 1820, loss = 1.68 (184.8 examples/sec; 0.693 sec/batch)
2016-12-08 06:42:28.640810: step 1830, loss = 1.80 (190.7 examples/sec; 0.671 sec/batch)
2016-12-08 06:42:35.808331: step 1840, loss = 1.86 (176.5 examples/sec; 0.725 sec/batch)
2016-12-08 06:42:42.466316: step 1850, loss = 1.72 (175.2 examples/sec; 0.730 sec/batch)
2016-12-08 06:42:49.636427: step 1860, loss = 1.78 (178.5 examples/sec; 0.717 sec/batch)
2016-12-08 06:42:56.521888: step 1870, loss = 1.69 (198.0 examples/sec; 0.646 sec/batch)
2016-12-08 06:43:03.712681: step 1880, loss = 1.70 (179.5 examples/sec; 0.713 sec/batch)
2016-12-08 06:43:10.919069: step 1890, loss = 1.55 (165.2 examples/sec; 0.775 sec/batch)
2016-12-08 06:43:17.989417: step 1900, loss = 1.83 (191.5 examples/sec; 0.668 sec/batch)
2016-12-08 06:43:25.535748: step 1910, loss = 1.79 (194.7 examples/sec; 0.657 sec/batch)
2016-12-08 06:43:32.223568: step 1920, loss = 1.80 (173.6 examples/sec; 0.737 sec/batch)
2016-12-08 06:43:39.154909: step 1930, loss = 1.54 (186.9 examples/sec; 0.685 sec/batch)
2016-12-08 06:43:46.227721: step 1940, loss = 1.93 (172.5 examples/sec; 0.742 sec/batch)
2016-12-08 06:43:53.249792: step 1950, loss = 1.49 (188.8 examples/sec; 0.678 sec/batch)
2016-12-08 06:44:00.197147: step 1960, loss = 1.93 (175.6 examples/sec; 0.729 sec/batch)
2016-12-08 06:44:06.799950: step 1970, loss = 1.64 (229.0 examples/sec; 0.559 sec/batch)
2016-12-08 06:44:12.806264: step 1980, loss = 1.56 (214.0 examples/sec; 0.598 sec/batch)
2016-12-08 06:44:19.675697: step 1990, loss = 1.45 (161.3 examples/sec; 0.794 sec/batch)
2016-12-08 06:44:26.681985: step 2000, loss = 1.80 (181.0 examples/sec; 0.707 sec/batch)
2016-12-08 06:44:34.644644: step 2010, loss = 1.72 (174.8 examples/sec; 0.732 sec/batch)
2016-12-08 06:44:41.828026: step 2020, loss = 1.62 (190.6 examples/sec; 0.671 sec/batch)
2016-12-08 06:44:48.977700: step 2030, loss = 1.49 (196.0 examples/sec; 0.653 sec/batch)
2016-12-08 06:44:56.030441: step 2040, loss = 1.58 (177.8 examples/sec; 0.720 sec/batch)
2016-12-08 06:45:03.079996: step 2050, loss = 1.58 (176.4 examples/sec; 0.726 sec/batch)
2016-12-08 06:45:09.780779: step 2060, loss = 1.63 (188.8 examples/sec; 0.678 sec/batch)
2016-12-08 06:45:16.599976: step 2070, loss = 1.61 (180.6 examples/sec; 0.709 sec/batch)
2016-12-08 06:45:23.091583: step 2080, loss = 1.68 (215.4 examples/sec; 0.594 sec/batch)
2016-12-08 06:45:29.647324: step 2090, loss = 1.53 (171.9 examples/sec; 0.745 sec/batch)
2016-12-08 06:45:36.364084: step 2100, loss = 1.55 (187.7 examples/sec; 0.682 sec/batch)
2016-12-08 06:45:43.487461: step 2110, loss = 1.77 (170.2 examples/sec; 0.752 sec/batch)
2016-12-08 06:45:50.024260: step 2120, loss = 1.91 (208.8 examples/sec; 0.613 sec/batch)
2016-12-08 06:45:56.722780: step 2130, loss = 1.51 (189.6 examples/sec; 0.675 sec/batch)
2016-12-08 06:46:03.624000: step 2140, loss = 1.67 (162.9 examples/sec; 0.786 sec/batch)
2016-12-08 06:46:10.507553: step 2150, loss = 1.39 (156.7 examples/sec; 0.817 sec/batch)
2016-12-08 06:46:17.225734: step 2160, loss = 1.51 (179.3 examples/sec; 0.714 sec/batch)
2016-12-08 06:46:24.088178: step 2170, loss = 1.46 (209.8 examples/sec; 0.610 sec/batch)
2016-12-08 06:46:31.089538: step 2180, loss = 1.75 (193.6 examples/sec; 0.661 sec/batch)
2016-12-08 06:46:38.233469: step 2190, loss = 1.58 (169.7 examples/sec; 0.754 sec/batch)
2016-12-08 06:46:45.052321: step 2200, loss = 1.53 (222.7 examples/sec; 0.575 sec/batch)
2016-12-08 06:46:52.692620: step 2210, loss = 1.37 (177.7 examples/sec; 0.720 sec/batch)
2016-12-08 06:46:59.906554: step 2220, loss = 1.28 (198.3 examples/sec; 0.646 sec/batch)
2016-12-08 06:47:06.806073: step 2230, loss = 1.54 (193.4 examples/sec; 0.662 sec/batch)
2016-12-08 06:47:13.637683: step 2240, loss = 1.36 (176.0 examples/sec; 0.727 sec/batch)
2016-12-08 06:47:20.815346: step 2250, loss = 1.48 (188.3 examples/sec; 0.680 sec/batch)
2016-12-08 06:47:27.241202: step 2260, loss = 1.34 (199.1 examples/sec; 0.643 sec/batch)
2016-12-08 06:47:34.200054: step 2270, loss = 1.52 (177.5 examples/sec; 0.721 sec/batch)
2016-12-08 06:47:41.248005: step 2280, loss = 1.58 (185.1 examples/sec; 0.691 sec/batch)
2016-12-08 06:47:47.971926: step 2290, loss = 1.31 (172.8 examples/sec; 0.741 sec/batch)
2016-12-08 06:47:54.563457: step 2300, loss = 1.40 (199.8 examples/sec; 0.641 sec/batch)
2016-12-08 06:48:02.231443: step 2310, loss = 1.49 (171.7 examples/sec; 0.746 sec/batch)
2016-12-08 06:48:09.001056: step 2320, loss = 1.54 (178.6 examples/sec; 0.717 sec/batch)
2016-12-08 06:48:16.111952: step 2330, loss = 1.43 (195.4 examples/sec; 0.655 sec/batch)
2016-12-08 06:48:22.977079: step 2340, loss = 1.63 (205.3 examples/sec; 0.623 sec/batch)
2016-12-08 06:48:29.526555: step 2350, loss = 1.39 (185.2 examples/sec; 0.691 sec/batch)
2016-12-08 06:48:36.230061: step 2360, loss = 1.41 (211.6 examples/sec; 0.605 sec/batch)
2016-12-08 06:48:42.681281: step 2370, loss = 1.52 (189.7 examples/sec; 0.675 sec/batch)
2016-12-08 06:48:49.350292: step 2380, loss = 1.47 (221.4 examples/sec; 0.578 sec/batch)
2016-12-08 06:48:56.310512: step 2390, loss = 1.61 (180.8 examples/sec; 0.708 sec/batch)
2016-12-08 06:49:03.322620: step 2400, loss = 1.56 (174.0 examples/sec; 0.735 sec/batch)
2016-12-08 06:49:10.484097: step 2410, loss = 1.48 (208.3 examples/sec; 0.614 sec/batch)
2016-12-08 06:49:17.148681: step 2420, loss = 1.52 (169.4 examples/sec; 0.755 sec/batch)
2016-12-08 06:49:23.847981: step 2430, loss = 1.36 (181.8 examples/sec; 0.704 sec/batch)
2016-12-08 06:49:30.527220: step 2440, loss = 1.37 (164.4 examples/sec; 0.779 sec/batch)
2016-12-08 06:49:37.360666: step 2450, loss = 1.48 (173.9 examples/sec; 0.736 sec/batch)
2016-12-08 06:49:44.036176: step 2460, loss = 1.45 (175.0 examples/sec; 0.732 sec/batch)
2016-12-08 06:49:51.275812: step 2470, loss = 1.47 (203.9 examples/sec; 0.628 sec/batch)
2016-12-08 06:49:58.156648: step 2480, loss = 1.48 (188.4 examples/sec; 0.679 sec/batch)
2016-12-08 06:50:05.152131: step 2490, loss = 1.32 (188.4 examples/sec; 0.680 sec/batch)
2016-12-08 06:50:11.738197: step 2500, loss = 1.51 (213.8 examples/sec; 0.599 sec/batch)
2016-12-08 06:50:19.331387: step 2510, loss = 1.38 (181.0 examples/sec; 0.707 sec/batch)
2016-12-08 06:50:26.069919: step 2520, loss = 1.48 (174.0 examples/sec; 0.736 sec/batch)
2016-12-08 06:50:32.512452: step 2530, loss = 1.45 (259.2 examples/sec; 0.494 sec/batch)
2016-12-08 06:50:38.919750: step 2540, loss = 1.33 (224.9 examples/sec; 0.569 sec/batch)
2016-12-08 06:50:45.802586: step 2550, loss = 1.43 (219.3 examples/sec; 0.584 sec/batch)
2016-12-08 06:50:52.517517: step 2560, loss = 1.32 (192.6 examples/sec; 0.664 sec/batch)
2016-12-08 06:50:59.330444: step 2570, loss = 1.42 (164.9 examples/sec; 0.776 sec/batch)
2016-12-08 06:51:05.703666: step 2580, loss = 1.33 (185.2 examples/sec; 0.691 sec/batch)
2016-12-08 06:51:12.645803: step 2590, loss = 1.23 (184.1 examples/sec; 0.695 sec/batch)
2016-12-08 06:51:19.500249: step 2600, loss = 1.50 (183.4 examples/sec; 0.698 sec/batch)
2016-12-08 06:51:26.545314: step 2610, loss = 1.52 (182.7 examples/sec; 0.700 sec/batch)
2016-12-08 06:51:33.129060: step 2620, loss = 1.43 (179.0 examples/sec; 0.715 sec/batch)
2016-12-08 06:51:39.752719: step 2630, loss = 1.39 (185.5 examples/sec; 0.690 sec/batch)
2016-12-08 06:51:46.214709: step 2640, loss = 1.32 (183.8 examples/sec; 0.697 sec/batch)
2016-12-08 06:51:53.426598: step 2650, loss = 1.36 (172.3 examples/sec; 0.743 sec/batch)
2016-12-08 06:52:00.132249: step 2660, loss = 1.39 (163.8 examples/sec; 0.781 sec/batch)
2016-12-08 06:52:06.951377: step 2670, loss = 1.22 (197.1 examples/sec; 0.649 sec/batch)
2016-12-08 06:52:13.341322: step 2680, loss = 1.33 (199.1 examples/sec; 0.643 sec/batch)
2016-12-08 06:52:20.095654: step 2690, loss = 1.18 (196.5 examples/sec; 0.651 sec/batch)
2016-12-08 06:52:26.892828: step 2700, loss = 1.38 (177.1 examples/sec; 0.723 sec/batch)
2016-12-08 06:52:34.307969: step 2710, loss = 1.36 (171.2 examples/sec; 0.748 sec/batch)
2016-12-08 06:52:41.551951: step 2720, loss = 1.36 (167.6 examples/sec; 0.764 sec/batch)
2016-12-08 06:52:48.445967: step 2730, loss = 1.48 (188.6 examples/sec; 0.679 sec/batch)
2016-12-08 06:52:54.990219: step 2740, loss = 1.56 (196.8 examples/sec; 0.651 sec/batch)
2016-12-08 06:53:02.214826: step 2750, loss = 1.36 (168.5 examples/sec; 0.760 sec/batch)
2016-12-08 06:53:08.844456: step 2760, loss = 1.19 (188.3 examples/sec; 0.680 sec/batch)
2016-12-08 06:53:15.552278: step 2770, loss = 1.43 (198.7 examples/sec; 0.644 sec/batch)
2016-12-08 06:53:22.285076: step 2780, loss = 1.37 (189.1 examples/sec; 0.677 sec/batch)
2016-12-08 06:53:29.211089: step 2790, loss = 1.46 (193.2 examples/sec; 0.662 sec/batch)
2016-12-08 06:53:35.996183: step 2800, loss = 1.27 (187.8 examples/sec; 0.682 sec/batch)
2016-12-08 06:53:43.290722: step 2810, loss = 1.21 (171.0 examples/sec; 0.748 sec/batch)
2016-12-08 06:53:49.857134: step 2820, loss = 1.26 (186.5 examples/sec; 0.686 sec/batch)
2016-12-08 06:53:56.381581: step 2830, loss = 1.23 (232.8 examples/sec; 0.550 sec/batch)
2016-12-08 06:54:03.080481: step 2840, loss = 1.27 (182.0 examples/sec; 0.703 sec/batch)
2016-12-08 06:54:09.898064: step 2850, loss = 1.32 (177.5 examples/sec; 0.721 sec/batch)
2016-12-08 06:54:16.494127: step 2860, loss = 1.30 (200.1 examples/sec; 0.640 sec/batch)
2016-12-08 06:54:22.962198: step 2870, loss = 1.22 (195.3 examples/sec; 0.656 sec/batch)
2016-12-08 06:54:29.918857: step 2880, loss = 1.30 (198.8 examples/sec; 0.644 sec/batch)
2016-12-08 06:54:36.958599: step 2890, loss = 0.94 (167.5 examples/sec; 0.764 sec/batch)
2016-12-08 06:54:43.945767: step 2900, loss = 1.14 (168.4 examples/sec; 0.760 sec/batch)
2016-12-08 06:54:51.644959: step 2910, loss = 1.24 (220.4 examples/sec; 0.581 sec/batch)
2016-12-08 06:54:58.415020: step 2920, loss = 1.29 (174.9 examples/sec; 0.732 sec/batch)
2016-12-08 06:55:05.513445: step 2930, loss = 1.32 (189.2 examples/sec; 0.677 sec/batch)
2016-12-08 06:55:11.968906: step 2940, loss = 1.48 (200.3 examples/sec; 0.639 sec/batch)
2016-12-08 06:55:18.691433: step 2950, loss = 1.31 (192.6 examples/sec; 0.664 sec/batch)
2016-12-08 06:55:25.693018: step 2960, loss = 1.19 (172.5 examples/sec; 0.742 sec/batch)
2016-12-08 06:55:32.370519: step 2970, loss = 1.39 (182.8 examples/sec; 0.700 sec/batch)
2016-12-08 06:55:39.352344: step 2980, loss = 1.16 (177.6 examples/sec; 0.721 sec/batch)
2016-12-08 06:55:45.909997: step 2990, loss = 1.17 (185.6 examples/sec; 0.690 sec/batch)
2016-12-08 06:55:52.928210: step 3000, loss = 1.26 (172.4 examples/sec; 0.742 sec/batch)
2016-12-08 06:56:01.025906: step 3010, loss = 1.23 (211.0 examples/sec; 0.607 sec/batch)
2016-12-08 06:56:07.853990: step 3020, loss = 1.31 (188.1 examples/sec; 0.681 sec/batch)
2016-12-08 06:56:14.866152: step 3030, loss = 1.28 (190.1 examples/sec; 0.673 sec/batch)
2016-12-08 06:56:21.134015: step 3040, loss = 1.27 (171.6 examples/sec; 0.746 sec/batch)
2016-12-08 06:56:28.230661: step 3050, loss = 1.09 (207.3 examples/sec; 0.618 sec/batch)
2016-12-08 06:56:34.841103: step 3060, loss = 1.47 (200.4 examples/sec; 0.639 sec/batch)
2016-12-08 06:56:41.340573: step 3070, loss = 1.41 (195.3 examples/sec; 0.655 sec/batch)
2016-12-08 06:56:48.235598: step 3080, loss = 1.26 (193.1 examples/sec; 0.663 sec/batch)
2016-12-08 06:56:54.968136: step 3090, loss = 1.14 (178.3 examples/sec; 0.718 sec/batch)
2016-12-08 06:57:01.954452: step 3100, loss = 1.20 (207.4 examples/sec; 0.617 sec/batch)
2016-12-08 06:57:09.080715: step 3110, loss = 1.22 (219.2 examples/sec; 0.584 sec/batch)
2016-12-08 06:57:15.699072: step 3120, loss = 1.20 (206.3 examples/sec; 0.620 sec/batch)
2016-12-08 06:57:22.616099: step 3130, loss = 1.34 (219.9 examples/sec; 0.582 sec/batch)
2016-12-08 06:57:29.601040: step 3140, loss = 1.24 (198.8 examples/sec; 0.644 sec/batch)
2016-12-08 06:57:36.341116: step 3150, loss = 1.27 (218.9 examples/sec; 0.585 sec/batch)
2016-12-08 06:57:43.177689: step 3160, loss = 1.25 (186.0 examples/sec; 0.688 sec/batch)
2016-12-08 06:57:49.816610: step 3170, loss = 1.13 (186.1 examples/sec; 0.688 sec/batch)
2016-12-08 06:57:56.405247: step 3180, loss = 1.32 (203.8 examples/sec; 0.628 sec/batch)
2016-12-08 06:58:03.169365: step 3190, loss = 1.33 (185.0 examples/sec; 0.692 sec/batch)
2016-12-08 06:58:09.817445: step 3200, loss = 1.17 (184.8 examples/sec; 0.693 sec/batch)
2016-12-08 06:58:17.527585: step 3210, loss = 1.29 (197.3 examples/sec; 0.649 sec/batch)
2016-12-08 06:58:24.137842: step 3220, loss = 1.14 (187.7 examples/sec; 0.682 sec/batch)
2016-12-08 06:58:30.914165: step 3230, loss = 1.22 (177.3 examples/sec; 0.722 sec/batch)
2016-12-08 06:58:37.687573: step 3240, loss = 1.31 (187.6 examples/sec; 0.682 sec/batch)
2016-12-08 06:58:44.675679: step 3250, loss = 1.13 (190.5 examples/sec; 0.672 sec/batch)
2016-12-08 06:58:51.581237: step 3260, loss = 1.42 (198.0 examples/sec; 0.646 sec/batch)
2016-12-08 06:58:58.228164: step 3270, loss = 1.15 (184.8 examples/sec; 0.693 sec/batch)
2016-12-08 06:59:05.342098: step 3280, loss = 1.19 (188.8 examples/sec; 0.678 sec/batch)
2016-12-08 06:59:11.672340: step 3290, loss = 1.30 (183.0 examples/sec; 0.700 sec/batch)
2016-12-08 06:59:18.301727: step 3300, loss = 1.42 (217.3 examples/sec; 0.589 sec/batch)
2016-12-08 06:59:25.509423: step 3310, loss = 1.11 (208.7 examples/sec; 0.613 sec/batch)
2016-12-08 06:59:32.381214: step 3320, loss = 1.24 (165.1 examples/sec; 0.775 sec/batch)
2016-12-08 06:59:39.009807: step 3330, loss = 1.02 (199.2 examples/sec; 0.643 sec/batch)
2016-12-08 06:59:45.776828: step 3340, loss = 1.31 (196.6 examples/sec; 0.651 sec/batch)
2016-12-08 06:59:52.838688: step 3350, loss = 1.29 (180.9 examples/sec; 0.708 sec/batch)
2016-12-08 06:59:59.367162: step 3360, loss = 1.11 (174.0 examples/sec; 0.735 sec/batch)
2016-12-08 07:00:05.918077: step 3370, loss = 1.19 (188.2 examples/sec; 0.680 sec/batch)
2016-12-08 07:00:12.345102: step 3380, loss = 1.39 (215.4 examples/sec; 0.594 sec/batch)
2016-12-08 07:00:18.968373: step 3390, loss = 1.23 (218.2 examples/sec; 0.587 sec/batch)
2016-12-08 07:00:25.791000: step 3400, loss = 1.02 (172.1 examples/sec; 0.744 sec/batch)
2016-12-08 07:00:33.522117: step 3410, loss = 1.24 (196.1 examples/sec; 0.653 sec/batch)
2016-12-08 07:00:40.317455: step 3420, loss = 1.19 (200.8 examples/sec; 0.637 sec/batch)
2016-12-08 07:00:46.948605: step 3430, loss = 1.27 (186.8 examples/sec; 0.685 sec/batch)
2016-12-08 07:00:54.105965: step 3440, loss = 1.09 (157.8 examples/sec; 0.811 sec/batch)
2016-12-08 07:01:00.925166: step 3450, loss = 0.88 (187.0 examples/sec; 0.684 sec/batch)
2016-12-08 07:01:07.440887: step 3460, loss = 1.28 (175.2 examples/sec; 0.731 sec/batch)
2016-12-08 07:01:14.254400: step 3470, loss = 1.17 (201.4 examples/sec; 0.636 sec/batch)
2016-12-08 07:01:20.969760: step 3480, loss = 1.01 (219.7 examples/sec; 0.583 sec/batch)
2016-12-08 07:01:27.262673: step 3490, loss = 1.22 (176.7 examples/sec; 0.724 sec/batch)
2016-12-08 07:01:34.129728: step 3500, loss = 1.06 (194.6 examples/sec; 0.658 sec/batch)
2016-12-08 07:01:41.538969: step 3510, loss = 1.08 (198.2 examples/sec; 0.646 sec/batch)
2016-12-08 07:01:48.175003: step 3520, loss = 1.16 (222.4 examples/sec; 0.576 sec/batch)
2016-12-08 07:01:54.992232: step 3530, loss = 1.15 (195.2 examples/sec; 0.656 sec/batch)
2016-12-08 07:02:01.471989: step 3540, loss = 1.33 (186.4 examples/sec; 0.687 sec/batch)
2016-12-08 07:02:08.421711: step 3550, loss = 1.24 (178.0 examples/sec; 0.719 sec/batch)
2016-12-08 07:02:14.984072: step 3560, loss = 1.11 (168.6 examples/sec; 0.759 sec/batch)
2016-12-08 07:02:21.921934: step 3570, loss = 1.23 (183.3 examples/sec; 0.698 sec/batch)
2016-12-08 07:02:28.168508: step 3580, loss = 1.09 (200.4 examples/sec; 0.639 sec/batch)
2016-12-08 07:02:34.950957: step 3590, loss = 0.97 (214.9 examples/sec; 0.596 sec/batch)
2016-12-08 07:02:41.713903: step 3600, loss = 1.38 (177.7 examples/sec; 0.720 sec/batch)
2016-12-08 07:02:49.289533: step 3610, loss = 1.15 (185.4 examples/sec; 0.690 sec/batch)
2016-12-08 07:02:56.060423: step 3620, loss = 1.13 (202.0 examples/sec; 0.634 sec/batch)
2016-12-08 07:03:02.846232: step 3630, loss = 1.23 (180.1 examples/sec; 0.711 sec/batch)
2016-12-08 07:03:09.689965: step 3640, loss = 1.20 (198.5 examples/sec; 0.645 sec/batch)
2016-12-08 07:03:16.696221: step 3650, loss = 0.95 (218.9 examples/sec; 0.585 sec/batch)
2016-12-08 07:03:23.441224: step 3660, loss = 1.05 (196.8 examples/sec; 0.650 sec/batch)
2016-12-08 07:03:30.216577: step 3670, loss = 1.24 (199.3 examples/sec; 0.642 sec/batch)
2016-12-08 07:03:36.620679: step 3680, loss = 1.40 (187.4 examples/sec; 0.683 sec/batch)
2016-12-08 07:03:43.291887: step 3690, loss = 1.01 (196.4 examples/sec; 0.652 sec/batch)
2016-12-08 07:03:50.322812: step 3700, loss = 1.17 (190.3 examples/sec; 0.673 sec/batch)
2016-12-08 07:03:57.913497: step 3710, loss = 1.16 (168.4 examples/sec; 0.760 sec/batch)
2016-12-08 07:04:04.399566: step 3720, loss = 1.29 (221.1 examples/sec; 0.579 sec/batch)
2016-12-08 07:04:11.131439: step 3730, loss = 1.14 (212.8 examples/sec; 0.601 sec/batch)
2016-12-08 07:04:17.823834: step 3740, loss = 1.05 (207.9 examples/sec; 0.616 sec/batch)
2016-12-08 07:04:24.768247: step 3750, loss = 1.17 (176.9 examples/sec; 0.723 sec/batch)
2016-12-08 07:04:31.610711: step 3760, loss = 0.93 (188.7 examples/sec; 0.678 sec/batch)
2016-12-08 07:04:38.979929: step 3770, loss = 1.39 (188.3 examples/sec; 0.680 sec/batch)
2016-12-08 07:04:45.524703: step 3780, loss = 1.08 (202.6 examples/sec; 0.632 sec/batch)
2016-12-08 07:04:52.360177: step 3790, loss = 1.09 (203.2 examples/sec; 0.630 sec/batch)
2016-12-08 07:04:58.753091: step 3800, loss = 1.22 (210.0 examples/sec; 0.610 sec/batch)
2016-12-08 07:05:06.247726: step 3810, loss = 0.96 (199.7 examples/sec; 0.641 sec/batch)
2016-12-08 07:05:12.849781: step 3820, loss = 1.12 (193.7 examples/sec; 0.661 sec/batch)
2016-12-08 07:05:19.525666: step 3830, loss = 1.01 (188.5 examples/sec; 0.679 sec/batch)
2016-12-08 07:05:26.220680: step 3840, loss = 0.90 (175.8 examples/sec; 0.728 sec/batch)
2016-12-08 07:05:33.038868: step 3850, loss = 1.23 (175.1 examples/sec; 0.731 sec/batch)
2016-12-08 07:05:39.597755: step 3860, loss = 1.03 (174.0 examples/sec; 0.736 sec/batch)
2016-12-08 07:05:46.170577: step 3870, loss = 1.33 (204.7 examples/sec; 0.625 sec/batch)
2016-12-08 07:05:52.869325: step 3880, loss = 1.11 (236.3 examples/sec; 0.542 sec/batch)
2016-12-08 07:05:59.603176: step 3890, loss = 0.96 (194.5 examples/sec; 0.658 sec/batch)
2016-12-08 07:06:06.284093: step 3900, loss = 1.21 (195.2 examples/sec; 0.656 sec/batch)
2016-12-08 07:06:13.501327: step 3910, loss = 1.21 (192.7 examples/sec; 0.664 sec/batch)
2016-12-08 07:06:20.628269: step 3920, loss = 1.09 (184.0 examples/sec; 0.696 sec/batch)
2016-12-08 07:06:27.235832: step 3930, loss = 0.99 (200.2 examples/sec; 0.639 sec/batch)
2016-12-08 07:06:34.071019: step 3940, loss = 1.19 (192.7 examples/sec; 0.664 sec/batch)
2016-12-08 07:06:40.303542: step 3950, loss = 1.44 (231.1 examples/sec; 0.554 sec/batch)
2016-12-08 07:06:46.692420: step 3960, loss = 1.11 (214.1 examples/sec; 0.598 sec/batch)
2016-12-08 07:06:53.482467: step 3970, loss = 1.19 (172.5 examples/sec; 0.742 sec/batch)
2016-12-08 07:06:59.976365: step 3980, loss = 1.11 (195.2 examples/sec; 0.656 sec/batch)
2016-12-08 07:07:06.948178: step 3990, loss = 1.04 (202.4 examples/sec; 0.632 sec/batch)
2016-12-08 07:07:13.589234: step 4000, loss = 1.07 (211.6 examples/sec; 0.605 sec/batch)
2016-12-08 07:07:21.304459: step 4010, loss = 1.18 (190.4 examples/sec; 0.672 sec/batch)
2016-12-08 07:07:28.208825: step 4020, loss = 1.10 (192.2 examples/sec; 0.666 sec/batch)
2016-12-08 07:07:35.438449: step 4030, loss = 1.25 (174.7 examples/sec; 0.733 sec/batch)
2016-12-08 07:07:41.945145: step 4040, loss = 1.05 (175.6 examples/sec; 0.729 sec/batch)
2016-12-08 07:07:48.373351: step 4050, loss = 1.06 (200.8 examples/sec; 0.637 sec/batch)
2016-12-08 07:07:54.912175: step 4060, loss = 1.00 (188.7 examples/sec; 0.678 sec/batch)
2016-12-08 07:08:01.762342: step 4070, loss = 1.11 (214.2 examples/sec; 0.598 sec/batch)
2016-12-08 07:08:08.416764: step 4080, loss = 1.10 (189.0 examples/sec; 0.677 sec/batch)
2016-12-08 07:08:15.214619: step 4090, loss = 0.89 (174.9 examples/sec; 0.732 sec/batch)
2016-12-08 07:08:21.920338: step 4100, loss = 1.60 (174.0 examples/sec; 0.736 sec/batch)
2016-12-08 07:08:29.698651: step 4110, loss = 1.00 (198.5 examples/sec; 0.645 sec/batch)
2016-12-08 07:08:36.343157: step 4120, loss = 0.92 (184.6 examples/sec; 0.693 sec/batch)
2016-12-08 07:08:42.836424: step 4130, loss = 1.08 (191.6 examples/sec; 0.668 sec/batch)
2016-12-08 07:08:48.948103: step 4140, loss = 0.99 (219.5 examples/sec; 0.583 sec/batch)
2016-12-08 07:08:55.720127: step 4150, loss = 0.99 (187.1 examples/sec; 0.684 sec/batch)
2016-12-08 07:09:02.024180: step 4160, loss = 1.10 (177.8 examples/sec; 0.720 sec/batch)
2016-12-08 07:09:08.840898: step 4170, loss = 1.16 (207.9 examples/sec; 0.616 sec/batch)
2016-12-08 07:09:15.662416: step 4180, loss = 1.10 (166.4 examples/sec; 0.769 sec/batch)
2016-12-08 07:09:22.253714: step 4190, loss = 1.14 (226.5 examples/sec; 0.565 sec/batch)
2016-12-08 07:09:28.819899: step 4200, loss = 0.96 (196.9 examples/sec; 0.650 sec/batch)
2016-12-08 07:09:36.192521: step 4210, loss = 1.09 (185.5 examples/sec; 0.690 sec/batch)
2016-12-08 07:09:43.105039: step 4220, loss = 1.29 (160.0 examples/sec; 0.800 sec/batch)
2016-12-08 07:09:49.926866: step 4230, loss = 1.16 (182.9 examples/sec; 0.700 sec/batch)
2016-12-08 07:09:56.400546: step 4240, loss = 1.24 (183.7 examples/sec; 0.697 sec/batch)
2016-12-08 07:10:03.098679: step 4250, loss = 1.32 (200.6 examples/sec; 0.638 sec/batch)
2016-12-08 07:10:10.141941: step 4260, loss = 1.05 (183.8 examples/sec; 0.696 sec/batch)
2016-12-08 07:10:16.780590: step 4270, loss = 1.02 (217.1 examples/sec; 0.590 sec/batch)
2016-12-08 07:10:23.544797: step 4280, loss = 1.06 (192.2 examples/sec; 0.666 sec/batch)
2016-12-08 07:10:30.674956: step 4290, loss = 1.18 (165.2 examples/sec; 0.775 sec/batch)
2016-12-08 07:10:37.301329: step 4300, loss = 1.02 (185.0 examples/sec; 0.692 sec/batch)
2016-12-08 07:10:44.836922: step 4310, loss = 0.98 (167.3 examples/sec; 0.765 sec/batch)
2016-12-08 07:10:51.562580: step 4320, loss = 0.91 (171.3 examples/sec; 0.747 sec/batch)
2016-12-08 07:10:58.278339: step 4330, loss = 1.06 (196.7 examples/sec; 0.651 sec/batch)
2016-12-08 07:11:04.749761: step 4340, loss = 1.05 (205.6 examples/sec; 0.622 sec/batch)
2016-12-08 07:11:11.542293: step 4350, loss = 1.08 (210.0 examples/sec; 0.609 sec/batch)
2016-12-08 07:11:18.071840: step 4360, loss = 1.09 (182.7 examples/sec; 0.701 sec/batch)
2016-12-08 07:11:25.007607: step 4370, loss = 0.92 (184.5 examples/sec; 0.694 sec/batch)
2016-12-08 07:11:31.784075: step 4380, loss = 1.11 (217.9 examples/sec; 0.588 sec/batch)
2016-12-08 07:11:38.378618: step 4390, loss = 1.20 (228.4 examples/sec; 0.561 sec/batch)
2016-12-08 07:11:45.064436: step 4400, loss = 1.00 (196.2 examples/sec; 0.653 sec/batch)
2016-12-08 07:11:52.611141: step 4410, loss = 0.89 (179.3 examples/sec; 0.714 sec/batch)
2016-12-08 07:11:59.463965: step 4420, loss = 1.04 (175.8 examples/sec; 0.728 sec/batch)
2016-12-08 07:12:06.057791: step 4430, loss = 1.02 (207.8 examples/sec; 0.616 sec/batch)
2016-12-08 07:12:12.985286: step 4440, loss = 1.03 (176.9 examples/sec; 0.724 sec/batch)
2016-12-08 07:12:19.927144: step 4450, loss = 1.12 (201.3 examples/sec; 0.636 sec/batch)
2016-12-08 07:12:26.526784: step 4460, loss = 0.98 (188.2 examples/sec; 0.680 sec/batch)
2016-12-08 07:12:33.302235: step 4470, loss = 1.01 (158.5 examples/sec; 0.807 sec/batch)
2016-12-08 07:12:40.435203: step 4480, loss = 1.08 (171.2 examples/sec; 0.748 sec/batch)
2016-12-08 07:12:46.818487: step 4490, loss = 1.29 (197.7 examples/sec; 0.647 sec/batch)
2016-12-08 07:12:53.545715: step 4500, loss = 0.95 (166.9 examples/sec; 0.767 sec/batch)
2016-12-08 07:13:00.915288: step 4510, loss = 1.05 (225.9 examples/sec; 0.567 sec/batch)
2016-12-08 07:13:07.371466: step 4520, loss = 1.07 (180.5 examples/sec; 0.709 sec/batch)
2016-12-08 07:13:13.853365: step 4530, loss = 0.96 (203.4 examples/sec; 0.629 sec/batch)
2016-12-08 07:13:20.579546: step 4540, loss = 1.16 (195.5 examples/sec; 0.655 sec/batch)
2016-12-08 07:13:27.538029: step 4550, loss = 1.10 (201.3 examples/sec; 0.636 sec/batch)
2016-12-08 07:13:34.180350: step 4560, loss = 1.19 (219.2 examples/sec; 0.584 sec/batch)
2016-12-08 07:13:40.473848: step 4570, loss = 1.02 (206.1 examples/sec; 0.621 sec/batch)
2016-12-08 07:13:46.807785: step 4580, loss = 0.99 (202.1 examples/sec; 0.633 sec/batch)
2016-12-08 07:13:52.959740: step 4590, loss = 1.04 (209.6 examples/sec; 0.611 sec/batch)
2016-12-08 07:13:59.246544: step 4600, loss = 0.92 (190.8 examples/sec; 0.671 sec/batch)
2016-12-08 07:14:06.853676: step 4610, loss = 1.06 (191.8 examples/sec; 0.667 sec/batch)
2016-12-08 07:14:13.526678: step 4620, loss = 0.79 (191.1 examples/sec; 0.670 sec/batch)
2016-12-08 07:14:20.228781: step 4630, loss = 1.15 (199.6 examples/sec; 0.641 sec/batch)
2016-12-08 07:14:27.001707: step 4640, loss = 0.92 (208.7 examples/sec; 0.613 sec/batch)
2016-12-08 07:14:33.774392: step 4650, loss = 1.17 (196.7 examples/sec; 0.651 sec/batch)
2016-12-08 07:14:40.681320: step 4660, loss = 0.95 (183.0 examples/sec; 0.699 sec/batch)
2016-12-08 07:14:47.444595: step 4670, loss = 1.03 (213.8 examples/sec; 0.599 sec/batch)
2016-12-08 07:14:53.912269: step 4680, loss = 0.94 (197.9 examples/sec; 0.647 sec/batch)
2016-12-08 07:15:00.551885: step 4690, loss = 1.01 (212.3 examples/sec; 0.603 sec/batch)
2016-12-08 07:15:07.405919: step 4700, loss = 0.91 (181.7 examples/sec; 0.704 sec/batch)
2016-12-08 07:15:14.822034: step 4710, loss = 0.85 (231.6 examples/sec; 0.553 sec/batch)
2016-12-08 07:15:21.522659: step 4720, loss = 1.04 (161.1 examples/sec; 0.795 sec/batch)
2016-12-08 07:15:28.021321: step 4730, loss = 1.13 (230.9 examples/sec; 0.554 sec/batch)
2016-12-08 07:15:34.613755: step 4740, loss = 0.84 (175.6 examples/sec; 0.729 sec/batch)
2016-12-08 07:15:41.592798: step 4750, loss = 1.13 (181.1 examples/sec; 0.707 sec/batch)
2016-12-08 07:15:47.965269: step 4760, loss = 1.21 (201.5 examples/sec; 0.635 sec/batch)
2016-12-08 07:15:54.734557: step 4770, loss = 1.04 (201.0 examples/sec; 0.637 sec/batch)
2016-12-08 07:16:01.663359: step 4780, loss = 1.07 (193.6 examples/sec; 0.661 sec/batch)
2016-12-08 07:16:08.158190: step 4790, loss = 0.94 (180.4 examples/sec; 0.710 sec/batch)
2016-12-08 07:16:15.113085: step 4800, loss = 0.94 (189.6 examples/sec; 0.675 sec/batch)
2016-12-08 07:16:22.620523: step 4810, loss = 0.98 (175.2 examples/sec; 0.731 sec/batch)
2016-12-08 07:16:29.549739: step 4820, loss = 0.99 (210.3 examples/sec; 0.609 sec/batch)
2016-12-08 07:16:36.509758: step 4830, loss = 1.07 (195.6 examples/sec; 0.654 sec/batch)
2016-12-08 07:16:43.327089: step 4840, loss = 1.01 (178.5 examples/sec; 0.717 sec/batch)
2016-12-08 07:16:50.330559: step 4850, loss = 1.13 (198.4 examples/sec; 0.645 sec/batch)
2016-12-08 07:16:57.069326: step 4860, loss = 1.02 (178.6 examples/sec; 0.717 sec/batch)
2016-12-08 07:17:04.078844: step 4870, loss = 1.09 (192.8 examples/sec; 0.664 sec/batch)
2016-12-08 07:17:10.712862: step 4880, loss = 1.12 (191.2 examples/sec; 0.670 sec/batch)
2016-12-08 07:17:17.546813: step 4890, loss = 0.96 (171.1 examples/sec; 0.748 sec/batch)
2016-12-08 07:17:24.324065: step 4900, loss = 0.84 (172.5 examples/sec; 0.742 sec/batch)
2016-12-08 07:17:31.738662: step 4910, loss = 1.01 (212.9 examples/sec; 0.601 sec/batch)
2016-12-08 07:17:38.642136: step 4920, loss = 1.07 (199.5 examples/sec; 0.642 sec/batch)
2016-12-08 07:17:45.081483: step 4930, loss = 0.96 (215.9 examples/sec; 0.593 sec/batch)
2016-12-08 07:17:51.767459: step 4940, loss = 0.98 (175.8 examples/sec; 0.728 sec/batch)
2016-12-08 07:17:58.340969: step 4950, loss = 0.85 (188.3 examples/sec; 0.680 sec/batch)
2016-12-08 07:18:05.014013: step 4960, loss = 1.06 (214.7 examples/sec; 0.596 sec/batch)
2016-12-08 07:18:11.646171: step 4970, loss = 1.01 (207.2 examples/sec; 0.618 sec/batch)
2016-12-08 07:18:18.724026: step 4980, loss = 1.23 (171.5 examples/sec; 0.746 sec/batch)
2016-12-08 07:18:25.461304: step 4990, loss = 1.19 (181.0 examples/sec; 0.707 sec/batch)
2016-12-08 07:18:32.008401: step 5000, loss = 1.16 (192.9 examples/sec; 0.664 sec/batch)
2016-12-08 07:18:39.966880: step 5010, loss = 1.05 (199.5 examples/sec; 0.642 sec/batch)
2016-12-08 07:18:46.827640: step 5020, loss = 0.89 (191.7 examples/sec; 0.668 sec/batch)
2016-12-08 07:18:53.528063: step 5030, loss = 0.97 (182.7 examples/sec; 0.701 sec/batch)
2016-12-08 07:19:00.209694: step 5040, loss = 1.19 (185.9 examples/sec; 0.689 sec/batch)
2016-12-08 07:19:07.436914: step 5050, loss = 1.03 (190.5 examples/sec; 0.672 sec/batch)
2016-12-08 07:19:14.573273: step 5060, loss = 1.05 (174.7 examples/sec; 0.733 sec/batch)
2016-12-08 07:19:21.133202: step 5070, loss = 1.16 (209.9 examples/sec; 0.610 sec/batch)
2016-12-08 07:19:27.932827: step 5080, loss = 1.12 (188.3 examples/sec; 0.680 sec/batch)
2016-12-08 07:19:34.518026: step 5090, loss = 1.08 (190.5 examples/sec; 0.672 sec/batch)
2016-12-08 07:19:41.183362: step 5100, loss = 1.18 (187.8 examples/sec; 0.682 sec/batch)
2016-12-08 07:19:48.843560: step 5110, loss = 1.02 (165.6 examples/sec; 0.773 sec/batch)
2016-12-08 07:19:55.522544: step 5120, loss = 1.01 (187.1 examples/sec; 0.684 sec/batch)
2016-12-08 07:20:02.296977: step 5130, loss = 1.01 (168.6 examples/sec; 0.759 sec/batch)
2016-12-08 07:20:09.160851: step 5140, loss = 1.05 (187.6 examples/sec; 0.682 sec/batch)
2016-12-08 07:20:15.182638: step 5150, loss = 1.01 (212.4 examples/sec; 0.603 sec/batch)
2016-12-08 07:20:21.596716: step 5160, loss = 0.98 (195.5 examples/sec; 0.655 sec/batch)
2016-12-08 07:20:28.347702: step 5170, loss = 0.98 (181.1 examples/sec; 0.707 sec/batch)
2016-12-08 07:20:35.320065: step 5180, loss = 0.95 (185.4 examples/sec; 0.690 sec/batch)
2016-12-08 07:20:41.991486: step 5190, loss = 1.12 (203.7 examples/sec; 0.628 sec/batch)
2016-12-08 07:20:48.882945: step 5200, loss = 1.12 (191.9 examples/sec; 0.667 sec/batch)
2016-12-08 07:20:55.858142: step 5210, loss = 1.08 (199.3 examples/sec; 0.642 sec/batch)
2016-12-08 07:21:02.561883: step 5220, loss = 0.79 (188.0 examples/sec; 0.681 sec/batch)
2016-12-08 07:21:09.151896: step 5230, loss = 0.77 (192.3 examples/sec; 0.666 sec/batch)
