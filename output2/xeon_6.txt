Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
WARNING:tensorflow:From cifar10_train.py:81 in train.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2016-03-02.
Instructions for updating:
Please use tf.global_variables instead.
2016-12-08 09:21:24.070932: step 0, loss = 4.67 (29.1 examples/sec; 4.397 sec/batch)
2016-12-08 09:21:28.448874: step 10, loss = 4.63 (429.1 examples/sec; 0.298 sec/batch)
2016-12-08 09:21:31.763869: step 20, loss = 4.69 (416.2 examples/sec; 0.308 sec/batch)
2016-12-08 09:21:35.127100: step 30, loss = 4.36 (394.0 examples/sec; 0.325 sec/batch)
2016-12-08 09:21:38.534280: step 40, loss = 4.25 (381.3 examples/sec; 0.336 sec/batch)
2016-12-08 09:21:41.888119: step 50, loss = 4.38 (399.1 examples/sec; 0.321 sec/batch)
2016-12-08 09:21:45.209150: step 60, loss = 4.24 (449.2 examples/sec; 0.285 sec/batch)
2016-12-08 09:21:48.774536: step 70, loss = 4.30 (349.3 examples/sec; 0.366 sec/batch)
2016-12-08 09:21:52.095975: step 80, loss = 4.41 (350.4 examples/sec; 0.365 sec/batch)
2016-12-08 09:21:55.477442: step 90, loss = 4.11 (341.9 examples/sec; 0.374 sec/batch)
2016-12-08 09:21:58.770581: step 100, loss = 4.19 (378.2 examples/sec; 0.338 sec/batch)
2016-12-08 09:22:02.517410: step 110, loss = 4.08 (372.4 examples/sec; 0.344 sec/batch)
2016-12-08 09:22:05.938872: step 120, loss = 4.11 (388.7 examples/sec; 0.329 sec/batch)
2016-12-08 09:22:09.180814: step 130, loss = 4.10 (382.9 examples/sec; 0.334 sec/batch)
2016-12-08 09:22:12.273543: step 140, loss = 4.30 (415.3 examples/sec; 0.308 sec/batch)
2016-12-08 09:22:15.664008: step 150, loss = 3.96 (395.2 examples/sec; 0.324 sec/batch)
2016-12-08 09:22:18.917902: step 160, loss = 4.05 (386.2 examples/sec; 0.331 sec/batch)
2016-12-08 09:22:22.264744: step 170, loss = 3.87 (420.6 examples/sec; 0.304 sec/batch)
2016-12-08 09:22:25.557952: step 180, loss = 4.00 (378.3 examples/sec; 0.338 sec/batch)
2016-12-08 09:22:28.907826: step 190, loss = 3.82 (403.5 examples/sec; 0.317 sec/batch)
2016-12-08 09:22:32.323979: step 200, loss = 3.96 (389.6 examples/sec; 0.329 sec/batch)
2016-12-08 09:22:36.077577: step 210, loss = 3.68 (372.8 examples/sec; 0.343 sec/batch)
2016-12-08 09:22:39.476906: step 220, loss = 3.93 (387.7 examples/sec; 0.330 sec/batch)
2016-12-08 09:22:42.761171: step 230, loss = 3.70 (400.4 examples/sec; 0.320 sec/batch)
2016-12-08 09:22:46.069578: step 240, loss = 3.75 (409.9 examples/sec; 0.312 sec/batch)
2016-12-08 09:22:49.437372: step 250, loss = 3.68 (421.3 examples/sec; 0.304 sec/batch)
2016-12-08 09:22:52.943918: step 260, loss = 3.69 (356.3 examples/sec; 0.359 sec/batch)
2016-12-08 09:22:56.372260: step 270, loss = 3.62 (419.8 examples/sec; 0.305 sec/batch)
2016-12-08 09:22:59.811875: step 280, loss = 3.55 (363.1 examples/sec; 0.352 sec/batch)
2016-12-08 09:23:03.124624: step 290, loss = 3.65 (373.6 examples/sec; 0.343 sec/batch)
2016-12-08 09:23:06.652172: step 300, loss = 3.46 (367.9 examples/sec; 0.348 sec/batch)
2016-12-08 09:23:10.461383: step 310, loss = 3.67 (385.0 examples/sec; 0.333 sec/batch)
2016-12-08 09:23:13.791819: step 320, loss = 3.48 (387.1 examples/sec; 0.331 sec/batch)
2016-12-08 09:23:17.095177: step 330, loss = 3.77 (403.4 examples/sec; 0.317 sec/batch)
2016-12-08 09:23:20.433173: step 340, loss = 3.41 (310.2 examples/sec; 0.413 sec/batch)
2016-12-08 09:23:23.582235: step 350, loss = 3.50 (423.4 examples/sec; 0.302 sec/batch)
2016-12-08 09:23:26.881698: step 360, loss = 3.46 (359.9 examples/sec; 0.356 sec/batch)
2016-12-08 09:23:30.302721: step 370, loss = 3.57 (339.0 examples/sec; 0.378 sec/batch)
2016-12-08 09:23:33.544866: step 380, loss = 3.62 (388.8 examples/sec; 0.329 sec/batch)
2016-12-08 09:23:36.822948: step 390, loss = 3.36 (379.8 examples/sec; 0.337 sec/batch)
2016-12-08 09:23:40.058721: step 400, loss = 3.32 (357.6 examples/sec; 0.358 sec/batch)
2016-12-08 09:23:43.671844: step 410, loss = 3.34 (382.7 examples/sec; 0.334 sec/batch)
2016-12-08 09:23:46.845242: step 420, loss = 3.35 (369.9 examples/sec; 0.346 sec/batch)
2016-12-08 09:23:50.229052: step 430, loss = 3.29 (365.7 examples/sec; 0.350 sec/batch)
2016-12-08 09:23:53.663463: step 440, loss = 3.28 (380.4 examples/sec; 0.337 sec/batch)
2016-12-08 09:23:57.005647: step 450, loss = 3.42 (372.6 examples/sec; 0.344 sec/batch)
2016-12-08 09:24:00.455939: step 460, loss = 3.30 (355.0 examples/sec; 0.361 sec/batch)
2016-12-08 09:24:03.716980: step 470, loss = 3.27 (388.2 examples/sec; 0.330 sec/batch)
2016-12-08 09:24:06.996671: step 480, loss = 3.25 (392.8 examples/sec; 0.326 sec/batch)
2016-12-08 09:24:10.311646: step 490, loss = 3.18 (446.5 examples/sec; 0.287 sec/batch)
2016-12-08 09:24:13.680410: step 500, loss = 2.99 (369.1 examples/sec; 0.347 sec/batch)
2016-12-08 09:24:17.551423: step 510, loss = 3.10 (392.7 examples/sec; 0.326 sec/batch)
2016-12-08 09:24:20.864918: step 520, loss = 3.00 (406.8 examples/sec; 0.315 sec/batch)
2016-12-08 09:24:24.110069: step 530, loss = 3.18 (416.1 examples/sec; 0.308 sec/batch)
2016-12-08 09:24:27.547472: step 540, loss = 3.04 (368.2 examples/sec; 0.348 sec/batch)
2016-12-08 09:24:30.955555: step 550, loss = 3.08 (355.9 examples/sec; 0.360 sec/batch)
2016-12-08 09:24:34.306035: step 560, loss = 3.04 (452.3 examples/sec; 0.283 sec/batch)
2016-12-08 09:24:37.604463: step 570, loss = 2.96 (379.7 examples/sec; 0.337 sec/batch)
2016-12-08 09:24:41.049919: step 580, loss = 2.98 (417.2 examples/sec; 0.307 sec/batch)
2016-12-08 09:24:44.313884: step 590, loss = 3.04 (413.9 examples/sec; 0.309 sec/batch)
2016-12-08 09:24:47.697938: step 600, loss = 2.92 (353.9 examples/sec; 0.362 sec/batch)
2016-12-08 09:24:51.351038: step 610, loss = 2.90 (382.3 examples/sec; 0.335 sec/batch)
2016-12-08 09:24:54.723443: step 620, loss = 2.94 (360.7 examples/sec; 0.355 sec/batch)
2016-12-08 09:24:58.068002: step 630, loss = 3.25 (410.3 examples/sec; 0.312 sec/batch)
2016-12-08 09:25:01.308592: step 640, loss = 3.03 (431.8 examples/sec; 0.296 sec/batch)
2016-12-08 09:25:04.629044: step 650, loss = 2.89 (483.8 examples/sec; 0.265 sec/batch)
2016-12-08 09:25:07.850202: step 660, loss = 2.85 (375.7 examples/sec; 0.341 sec/batch)
2016-12-08 09:25:11.205908: step 670, loss = 2.95 (424.9 examples/sec; 0.301 sec/batch)
2016-12-08 09:25:14.461067: step 680, loss = 2.98 (393.5 examples/sec; 0.325 sec/batch)
2016-12-08 09:25:17.643060: step 690, loss = 2.99 (385.4 examples/sec; 0.332 sec/batch)
2016-12-08 09:25:20.832891: step 700, loss = 2.87 (382.5 examples/sec; 0.335 sec/batch)
2016-12-08 09:25:24.554715: step 710, loss = 2.80 (380.3 examples/sec; 0.337 sec/batch)
2016-12-08 09:25:27.968699: step 720, loss = 2.79 (403.0 examples/sec; 0.318 sec/batch)
2016-12-08 09:25:31.090130: step 730, loss = 3.11 (419.6 examples/sec; 0.305 sec/batch)
2016-12-08 09:25:34.337336: step 740, loss = 3.08 (438.9 examples/sec; 0.292 sec/batch)
2016-12-08 09:25:37.558654: step 750, loss = 2.84 (402.0 examples/sec; 0.318 sec/batch)
2016-12-08 09:25:40.987139: step 760, loss = 2.86 (372.6 examples/sec; 0.344 sec/batch)
2016-12-08 09:25:44.233809: step 770, loss = 2.71 (408.6 examples/sec; 0.313 sec/batch)
2016-12-08 09:25:47.543001: step 780, loss = 2.96 (354.9 examples/sec; 0.361 sec/batch)
2016-12-08 09:25:50.661327: step 790, loss = 2.73 (468.7 examples/sec; 0.273 sec/batch)
2016-12-08 09:25:54.033726: step 800, loss = 2.74 (410.3 examples/sec; 0.312 sec/batch)
2016-12-08 09:25:57.539463: step 810, loss = 2.82 (427.8 examples/sec; 0.299 sec/batch)
2016-12-08 09:26:00.870183: step 820, loss = 2.76 (445.0 examples/sec; 0.288 sec/batch)
2016-12-08 09:26:04.273695: step 830, loss = 2.63 (382.4 examples/sec; 0.335 sec/batch)
2016-12-08 09:26:07.737273: step 840, loss = 2.51 (349.6 examples/sec; 0.366 sec/batch)
2016-12-08 09:26:11.112658: step 850, loss = 2.65 (344.0 examples/sec; 0.372 sec/batch)
2016-12-08 09:26:14.538103: step 860, loss = 2.66 (406.7 examples/sec; 0.315 sec/batch)
2016-12-08 09:26:17.846210: step 870, loss = 2.60 (398.7 examples/sec; 0.321 sec/batch)
2016-12-08 09:26:21.204864: step 880, loss = 2.49 (472.3 examples/sec; 0.271 sec/batch)
2016-12-08 09:26:24.379088: step 890, loss = 2.46 (402.8 examples/sec; 0.318 sec/batch)
2016-12-08 09:26:27.610861: step 900, loss = 2.43 (379.3 examples/sec; 0.337 sec/batch)
2016-12-08 09:26:31.274951: step 910, loss = 2.62 (419.1 examples/sec; 0.305 sec/batch)
2016-12-08 09:26:34.505791: step 920, loss = 2.49 (408.0 examples/sec; 0.314 sec/batch)
2016-12-08 09:26:37.716875: step 930, loss = 2.55 (392.6 examples/sec; 0.326 sec/batch)
2016-12-08 09:26:40.820677: step 940, loss = 2.43 (385.8 examples/sec; 0.332 sec/batch)
2016-12-08 09:26:44.008453: step 950, loss = 2.43 (435.7 examples/sec; 0.294 sec/batch)
2016-12-08 09:26:47.266794: step 960, loss = 2.57 (392.3 examples/sec; 0.326 sec/batch)
2016-12-08 09:26:50.473376: step 970, loss = 2.81 (421.6 examples/sec; 0.304 sec/batch)
2016-12-08 09:26:53.764013: step 980, loss = 2.51 (380.3 examples/sec; 0.337 sec/batch)
2016-12-08 09:26:57.095557: step 990, loss = 2.51 (385.7 examples/sec; 0.332 sec/batch)
2016-12-08 09:27:00.410816: step 1000, loss = 2.41 (453.0 examples/sec; 0.283 sec/batch)
2016-12-08 09:27:04.594567: step 1010, loss = 2.52 (423.4 examples/sec; 0.302 sec/batch)
2016-12-08 09:27:07.823378: step 1020, loss = 2.27 (383.1 examples/sec; 0.334 sec/batch)
2016-12-08 09:27:11.236124: step 1030, loss = 2.27 (389.7 examples/sec; 0.328 sec/batch)
2016-12-08 09:27:14.408261: step 1040, loss = 2.23 (399.2 examples/sec; 0.321 sec/batch)
2016-12-08 09:27:17.731028: step 1050, loss = 2.46 (354.9 examples/sec; 0.361 sec/batch)
2016-12-08 09:27:21.217020: step 1060, loss = 2.34 (356.2 examples/sec; 0.359 sec/batch)
2016-12-08 09:27:24.762310: step 1070, loss = 2.20 (363.1 examples/sec; 0.352 sec/batch)
2016-12-08 09:27:28.035166: step 1080, loss = 2.38 (405.0 examples/sec; 0.316 sec/batch)
2016-12-08 09:27:31.193348: step 1090, loss = 2.39 (479.2 examples/sec; 0.267 sec/batch)
2016-12-08 09:27:34.404076: step 1100, loss = 2.51 (384.4 examples/sec; 0.333 sec/batch)
2016-12-08 09:27:37.981110: step 1110, loss = 2.43 (387.2 examples/sec; 0.331 sec/batch)
2016-12-08 09:27:41.389539: step 1120, loss = 2.39 (331.1 examples/sec; 0.387 sec/batch)
2016-12-08 09:27:44.795997: step 1130, loss = 2.28 (399.6 examples/sec; 0.320 sec/batch)
2016-12-08 09:27:48.114612: step 1140, loss = 2.31 (409.5 examples/sec; 0.313 sec/batch)
2016-12-08 09:27:51.582727: step 1150, loss = 2.27 (391.9 examples/sec; 0.327 sec/batch)
2016-12-08 09:27:54.914789: step 1160, loss = 2.19 (341.2 examples/sec; 0.375 sec/batch)
2016-12-08 09:27:58.198784: step 1170, loss = 2.29 (378.0 examples/sec; 0.339 sec/batch)
2016-12-08 09:28:01.589489: step 1180, loss = 2.20 (371.6 examples/sec; 0.344 sec/batch)
2016-12-08 09:28:04.944923: step 1190, loss = 2.28 (391.5 examples/sec; 0.327 sec/batch)
2016-12-08 09:28:08.355935: step 1200, loss = 2.36 (405.2 examples/sec; 0.316 sec/batch)
2016-12-08 09:28:11.931408: step 1210, loss = 2.25 (390.9 examples/sec; 0.327 sec/batch)
2016-12-08 09:28:15.284340: step 1220, loss = 2.17 (369.1 examples/sec; 0.347 sec/batch)
2016-12-08 09:28:18.744478: step 1230, loss = 2.36 (349.1 examples/sec; 0.367 sec/batch)
2016-12-08 09:28:22.022068: step 1240, loss = 2.15 (360.0 examples/sec; 0.356 sec/batch)
2016-12-08 09:28:25.290494: step 1250, loss = 2.31 (388.3 examples/sec; 0.330 sec/batch)
2016-12-08 09:28:28.556613: step 1260, loss = 2.08 (354.9 examples/sec; 0.361 sec/batch)
2016-12-08 09:28:31.827362: step 1270, loss = 2.02 (395.5 examples/sec; 0.324 sec/batch)
2016-12-08 09:28:35.181870: step 1280, loss = 2.17 (382.7 examples/sec; 0.334 sec/batch)
2016-12-08 09:28:38.485113: step 1290, loss = 2.43 (393.7 examples/sec; 0.325 sec/batch)
2016-12-08 09:28:41.981648: step 1300, loss = 1.97 (353.3 examples/sec; 0.362 sec/batch)
2016-12-08 09:28:45.726747: step 1310, loss = 2.01 (382.2 examples/sec; 0.335 sec/batch)
2016-12-08 09:28:48.961867: step 1320, loss = 2.37 (397.4 examples/sec; 0.322 sec/batch)
2016-12-08 09:28:52.222078: step 1330, loss = 2.01 (333.5 examples/sec; 0.384 sec/batch)
2016-12-08 09:28:55.665378: step 1340, loss = 2.25 (368.6 examples/sec; 0.347 sec/batch)
2016-12-08 09:28:59.000821: step 1350, loss = 2.06 (418.7 examples/sec; 0.306 sec/batch)
2016-12-08 09:29:02.352031: step 1360, loss = 2.16 (399.0 examples/sec; 0.321 sec/batch)
2016-12-08 09:29:05.611105: step 1370, loss = 2.00 (410.5 examples/sec; 0.312 sec/batch)
2016-12-08 09:29:08.851135: step 1380, loss = 2.32 (378.1 examples/sec; 0.339 sec/batch)
2016-12-08 09:29:12.195239: step 1390, loss = 2.18 (372.1 examples/sec; 0.344 sec/batch)
2016-12-08 09:29:15.529562: step 1400, loss = 2.06 (397.1 examples/sec; 0.322 sec/batch)
2016-12-08 09:29:18.981376: step 1410, loss = 2.23 (377.0 examples/sec; 0.340 sec/batch)
2016-12-08 09:29:22.390715: step 1420, loss = 2.10 (317.1 examples/sec; 0.404 sec/batch)
2016-12-08 09:29:25.751620: step 1430, loss = 1.87 (361.5 examples/sec; 0.354 sec/batch)
2016-12-08 09:29:29.072207: step 1440, loss = 2.14 (394.0 examples/sec; 0.325 sec/batch)
2016-12-08 09:29:32.288491: step 1450, loss = 2.07 (374.0 examples/sec; 0.342 sec/batch)
2016-12-08 09:29:35.632077: step 1460, loss = 1.84 (348.4 examples/sec; 0.367 sec/batch)
2016-12-08 09:29:38.852439: step 1470, loss = 1.91 (393.2 examples/sec; 0.326 sec/batch)
2016-12-08 09:29:42.107938: step 1480, loss = 2.02 (418.3 examples/sec; 0.306 sec/batch)
2016-12-08 09:29:45.513107: step 1490, loss = 2.29 (403.5 examples/sec; 0.317 sec/batch)
2016-12-08 09:29:48.886979: step 1500, loss = 2.03 (386.4 examples/sec; 0.331 sec/batch)
2016-12-08 09:29:52.507894: step 1510, loss = 2.29 (364.7 examples/sec; 0.351 sec/batch)
2016-12-08 09:29:55.977741: step 1520, loss = 1.79 (356.2 examples/sec; 0.359 sec/batch)
2016-12-08 09:29:59.484654: step 1530, loss = 1.97 (389.9 examples/sec; 0.328 sec/batch)
2016-12-08 09:30:02.863292: step 1540, loss = 1.98 (393.1 examples/sec; 0.326 sec/batch)
2016-12-08 09:30:05.956798: step 1550, loss = 1.90 (406.6 examples/sec; 0.315 sec/batch)
2016-12-08 09:30:09.118592: step 1560, loss = 1.88 (386.7 examples/sec; 0.331 sec/batch)
2016-12-08 09:30:12.443295: step 1570, loss = 2.10 (406.2 examples/sec; 0.315 sec/batch)
2016-12-08 09:30:15.737068: step 1580, loss = 1.87 (372.0 examples/sec; 0.344 sec/batch)
2016-12-08 09:30:18.981309: step 1590, loss = 2.06 (396.0 examples/sec; 0.323 sec/batch)
2016-12-08 09:30:22.283437: step 1600, loss = 1.95 (383.8 examples/sec; 0.333 sec/batch)
2016-12-08 09:30:25.913961: step 1610, loss = 1.97 (470.4 examples/sec; 0.272 sec/batch)
2016-12-08 09:30:29.260913: step 1620, loss = 1.92 (393.6 examples/sec; 0.325 sec/batch)
2016-12-08 09:30:32.539257: step 1630, loss = 1.82 (402.2 examples/sec; 0.318 sec/batch)
2016-12-08 09:30:35.789659: step 1640, loss = 1.81 (376.2 examples/sec; 0.340 sec/batch)
2016-12-08 09:30:38.958104: step 1650, loss = 1.77 (381.2 examples/sec; 0.336 sec/batch)
2016-12-08 09:30:42.218812: step 1660, loss = 1.94 (402.9 examples/sec; 0.318 sec/batch)
2016-12-08 09:30:45.507141: step 1670, loss = 1.76 (372.0 examples/sec; 0.344 sec/batch)
2016-12-08 09:30:48.754475: step 1680, loss = 1.72 (366.9 examples/sec; 0.349 sec/batch)
2016-12-08 09:30:51.984580: step 1690, loss = 1.89 (379.3 examples/sec; 0.337 sec/batch)
2016-12-08 09:30:55.155177: step 1700, loss = 1.85 (380.3 examples/sec; 0.337 sec/batch)
2016-12-08 09:30:58.751348: step 1710, loss = 1.84 (395.2 examples/sec; 0.324 sec/batch)
2016-12-08 09:31:02.074759: step 1720, loss = 1.84 (424.2 examples/sec; 0.302 sec/batch)
2016-12-08 09:31:05.323190: step 1730, loss = 1.76 (371.6 examples/sec; 0.344 sec/batch)
2016-12-08 09:31:08.665560: step 1740, loss = 1.88 (396.4 examples/sec; 0.323 sec/batch)
2016-12-08 09:31:11.824791: step 1750, loss = 1.85 (472.1 examples/sec; 0.271 sec/batch)
2016-12-08 09:31:15.000504: step 1760, loss = 1.66 (370.1 examples/sec; 0.346 sec/batch)
2016-12-08 09:31:18.262070: step 1770, loss = 1.85 (386.7 examples/sec; 0.331 sec/batch)
2016-12-08 09:31:21.592617: step 1780, loss = 1.84 (433.1 examples/sec; 0.296 sec/batch)
2016-12-08 09:31:24.959836: step 1790, loss = 1.70 (419.0 examples/sec; 0.305 sec/batch)
2016-12-08 09:31:28.215473: step 1800, loss = 1.68 (390.5 examples/sec; 0.328 sec/batch)
2016-12-08 09:31:31.827177: step 1810, loss = 1.76 (376.3 examples/sec; 0.340 sec/batch)
2016-12-08 09:31:34.993667: step 1820, loss = 1.84 (346.0 examples/sec; 0.370 sec/batch)
2016-12-08 09:31:38.215662: step 1830, loss = 1.88 (362.0 examples/sec; 0.354 sec/batch)
2016-12-08 09:31:41.633465: step 1840, loss = 1.82 (353.9 examples/sec; 0.362 sec/batch)
2016-12-08 09:31:44.941268: step 1850, loss = 1.66 (374.9 examples/sec; 0.341 sec/batch)
2016-12-08 09:31:48.104013: step 1860, loss = 1.80 (397.1 examples/sec; 0.322 sec/batch)
2016-12-08 09:31:51.401549: step 1870, loss = 1.76 (378.2 examples/sec; 0.338 sec/batch)
2016-12-08 09:31:54.694765: step 1880, loss = 1.64 (441.1 examples/sec; 0.290 sec/batch)
2016-12-08 09:31:57.817222: step 1890, loss = 1.84 (346.4 examples/sec; 0.369 sec/batch)
2016-12-08 09:32:01.110641: step 1900, loss = 1.64 (351.1 examples/sec; 0.365 sec/batch)
2016-12-08 09:32:05.025877: step 1910, loss = 1.43 (400.8 examples/sec; 0.319 sec/batch)
2016-12-08 09:32:08.247417: step 1920, loss = 1.57 (464.1 examples/sec; 0.276 sec/batch)
2016-12-08 09:32:11.364027: step 1930, loss = 1.49 (437.6 examples/sec; 0.292 sec/batch)
2016-12-08 09:32:14.725839: step 1940, loss = 1.67 (381.1 examples/sec; 0.336 sec/batch)
2016-12-08 09:32:18.067002: step 1950, loss = 1.66 (405.4 examples/sec; 0.316 sec/batch)
2016-12-08 09:32:21.438075: step 1960, loss = 1.86 (386.8 examples/sec; 0.331 sec/batch)
2016-12-08 09:32:24.764837: step 1970, loss = 1.58 (348.4 examples/sec; 0.367 sec/batch)
2016-12-08 09:32:28.088372: step 1980, loss = 1.70 (384.0 examples/sec; 0.333 sec/batch)
2016-12-08 09:32:31.338662: step 1990, loss = 1.58 (461.7 examples/sec; 0.277 sec/batch)
2016-12-08 09:32:34.501425: step 2000, loss = 1.73 (372.9 examples/sec; 0.343 sec/batch)
2016-12-08 09:32:38.694195: step 2010, loss = 1.57 (331.8 examples/sec; 0.386 sec/batch)
2016-12-08 09:32:42.184461: step 2020, loss = 1.80 (355.4 examples/sec; 0.360 sec/batch)
2016-12-08 09:32:45.578962: step 2030, loss = 1.72 (387.9 examples/sec; 0.330 sec/batch)
2016-12-08 09:32:48.832782: step 2040, loss = 1.64 (364.6 examples/sec; 0.351 sec/batch)
2016-12-08 09:32:52.046370: step 2050, loss = 1.72 (445.7 examples/sec; 0.287 sec/batch)
2016-12-08 09:32:55.269699: step 2060, loss = 1.60 (414.6 examples/sec; 0.309 sec/batch)
2016-12-08 09:32:58.537021: step 2070, loss = 1.57 (399.4 examples/sec; 0.320 sec/batch)
2016-12-08 09:33:01.775808: step 2080, loss = 1.53 (357.3 examples/sec; 0.358 sec/batch)
2016-12-08 09:33:04.993359: step 2090, loss = 1.70 (355.0 examples/sec; 0.361 sec/batch)
2016-12-08 09:33:08.242727: step 2100, loss = 1.54 (390.5 examples/sec; 0.328 sec/batch)
2016-12-08 09:33:11.847023: step 2110, loss = 1.64 (391.9 examples/sec; 0.327 sec/batch)
2016-12-08 09:33:15.269996: step 2120, loss = 1.48 (362.5 examples/sec; 0.353 sec/batch)
2016-12-08 09:33:18.526398: step 2130, loss = 1.62 (441.3 examples/sec; 0.290 sec/batch)
2016-12-08 09:33:21.621695: step 2140, loss = 1.62 (440.0 examples/sec; 0.291 sec/batch)
2016-12-08 09:33:24.822730: step 2150, loss = 1.60 (401.4 examples/sec; 0.319 sec/batch)
2016-12-08 09:33:28.157637: step 2160, loss = 1.68 (457.8 examples/sec; 0.280 sec/batch)
2016-12-08 09:33:31.485233: step 2170, loss = 1.69 (410.6 examples/sec; 0.312 sec/batch)
2016-12-08 09:33:34.669425: step 2180, loss = 1.61 (415.9 examples/sec; 0.308 sec/batch)
2016-12-08 09:33:37.765464: step 2190, loss = 1.57 (410.7 examples/sec; 0.312 sec/batch)
2016-12-08 09:33:40.883988: step 2200, loss = 1.92 (406.3 examples/sec; 0.315 sec/batch)
2016-12-08 09:33:44.593256: step 2210, loss = 1.40 (388.1 examples/sec; 0.330 sec/batch)
2016-12-08 09:33:47.862110: step 2220, loss = 1.43 (354.7 examples/sec; 0.361 sec/batch)
2016-12-08 09:33:51.117818: step 2230, loss = 1.51 (350.2 examples/sec; 0.365 sec/batch)
2016-12-08 09:33:54.375901: step 2240, loss = 1.46 (409.9 examples/sec; 0.312 sec/batch)
2016-12-08 09:33:57.661535: step 2250, loss = 1.58 (412.9 examples/sec; 0.310 sec/batch)
2016-12-08 09:34:00.826142: step 2260, loss = 1.53 (389.1 examples/sec; 0.329 sec/batch)
2016-12-08 09:34:04.056938: step 2270, loss = 1.45 (439.3 examples/sec; 0.291 sec/batch)
2016-12-08 09:34:07.308127: step 2280, loss = 1.49 (426.3 examples/sec; 0.300 sec/batch)
2016-12-08 09:34:10.378280: step 2290, loss = 1.43 (463.6 examples/sec; 0.276 sec/batch)
2016-12-08 09:34:13.546124: step 2300, loss = 1.51 (426.9 examples/sec; 0.300 sec/batch)
2016-12-08 09:34:17.284562: step 2310, loss = 1.47 (408.0 examples/sec; 0.314 sec/batch)
2016-12-08 09:34:20.610621: step 2320, loss = 1.50 (444.0 examples/sec; 0.288 sec/batch)
2016-12-08 09:34:24.020888: step 2330, loss = 1.88 (366.9 examples/sec; 0.349 sec/batch)
2016-12-08 09:34:27.337009: step 2340, loss = 1.51 (368.3 examples/sec; 0.348 sec/batch)
2016-12-08 09:34:30.698812: step 2350, loss = 1.45 (361.1 examples/sec; 0.354 sec/batch)
2016-12-08 09:34:33.918713: step 2360, loss = 1.46 (400.0 examples/sec; 0.320 sec/batch)
2016-12-08 09:34:37.280586: step 2370, loss = 1.62 (382.3 examples/sec; 0.335 sec/batch)
2016-12-08 09:34:40.651071: step 2380, loss = 1.49 (415.1 examples/sec; 0.308 sec/batch)
2016-12-08 09:34:43.894487: step 2390, loss = 1.51 (406.1 examples/sec; 0.315 sec/batch)
2016-12-08 09:34:47.321766: step 2400, loss = 1.37 (436.8 examples/sec; 0.293 sec/batch)
2016-12-08 09:34:50.942905: step 2410, loss = 1.32 (374.4 examples/sec; 0.342 sec/batch)
2016-12-08 09:34:54.250000: step 2420, loss = 1.26 (347.1 examples/sec; 0.369 sec/batch)
2016-12-08 09:34:57.618503: step 2430, loss = 1.49 (382.6 examples/sec; 0.335 sec/batch)
2016-12-08 09:35:00.920661: step 2440, loss = 1.57 (358.3 examples/sec; 0.357 sec/batch)
2016-12-08 09:35:04.306311: step 2450, loss = 1.50 (426.5 examples/sec; 0.300 sec/batch)
2016-12-08 09:35:07.489416: step 2460, loss = 1.43 (382.8 examples/sec; 0.334 sec/batch)
2016-12-08 09:35:10.726146: step 2470, loss = 1.66 (417.4 examples/sec; 0.307 sec/batch)
2016-12-08 09:35:14.016200: step 2480, loss = 1.42 (366.4 examples/sec; 0.349 sec/batch)
2016-12-08 09:35:17.281079: step 2490, loss = 1.33 (351.4 examples/sec; 0.364 sec/batch)
2016-12-08 09:35:20.522222: step 2500, loss = 1.48 (405.7 examples/sec; 0.316 sec/batch)
2016-12-08 09:35:24.108179: step 2510, loss = 1.50 (351.4 examples/sec; 0.364 sec/batch)
2016-12-08 09:35:27.484011: step 2520, loss = 1.40 (364.4 examples/sec; 0.351 sec/batch)
2016-12-08 09:35:30.807959: step 2530, loss = 1.60 (348.3 examples/sec; 0.368 sec/batch)
2016-12-08 09:35:34.045463: step 2540, loss = 1.50 (410.7 examples/sec; 0.312 sec/batch)
2016-12-08 09:35:37.326046: step 2550, loss = 1.42 (398.8 examples/sec; 0.321 sec/batch)
2016-12-08 09:35:40.549367: step 2560, loss = 1.37 (402.9 examples/sec; 0.318 sec/batch)
2016-12-08 09:35:43.855743: step 2570, loss = 1.29 (341.0 examples/sec; 0.375 sec/batch)
2016-12-08 09:35:47.300080: step 2580, loss = 1.47 (398.2 examples/sec; 0.321 sec/batch)
2016-12-08 09:35:50.518116: step 2590, loss = 1.45 (348.6 examples/sec; 0.367 sec/batch)
2016-12-08 09:35:53.757744: step 2600, loss = 1.37 (390.4 examples/sec; 0.328 sec/batch)
2016-12-08 09:35:57.171605: step 2610, loss = 1.46 (463.7 examples/sec; 0.276 sec/batch)
2016-12-08 09:36:00.209894: step 2620, loss = 1.53 (453.3 examples/sec; 0.282 sec/batch)
2016-12-08 09:36:03.525848: step 2630, loss = 1.23 (381.2 examples/sec; 0.336 sec/batch)
2016-12-08 09:36:06.930387: step 2640, loss = 1.42 (384.0 examples/sec; 0.333 sec/batch)
2016-12-08 09:36:10.221248: step 2650, loss = 1.56 (385.6 examples/sec; 0.332 sec/batch)
2016-12-08 09:36:13.402532: step 2660, loss = 1.73 (393.5 examples/sec; 0.325 sec/batch)
2016-12-08 09:36:16.687541: step 2670, loss = 1.54 (405.9 examples/sec; 0.315 sec/batch)
2016-12-08 09:36:19.973896: step 2680, loss = 1.39 (440.0 examples/sec; 0.291 sec/batch)
2016-12-08 09:36:23.264580: step 2690, loss = 1.39 (353.8 examples/sec; 0.362 sec/batch)
2016-12-08 09:36:26.541744: step 2700, loss = 1.38 (398.1 examples/sec; 0.322 sec/batch)
2016-12-08 09:36:30.024016: step 2710, loss = 1.42 (393.2 examples/sec; 0.326 sec/batch)
2016-12-08 09:36:33.333200: step 2720, loss = 1.33 (427.3 examples/sec; 0.300 sec/batch)
2016-12-08 09:36:36.544204: step 2730, loss = 1.24 (361.7 examples/sec; 0.354 sec/batch)
2016-12-08 09:36:39.938183: step 2740, loss = 1.23 (418.0 examples/sec; 0.306 sec/batch)
2016-12-08 09:36:43.314025: step 2750, loss = 1.39 (380.7 examples/sec; 0.336 sec/batch)
2016-12-08 09:36:46.458182: step 2760, loss = 1.45 (439.5 examples/sec; 0.291 sec/batch)
2016-12-08 09:36:49.747355: step 2770, loss = 1.29 (371.1 examples/sec; 0.345 sec/batch)
2016-12-08 09:36:53.186664: step 2780, loss = 1.24 (369.8 examples/sec; 0.346 sec/batch)
2016-12-08 09:36:56.473145: step 2790, loss = 1.38 (427.3 examples/sec; 0.300 sec/batch)
2016-12-08 09:36:59.931702: step 2800, loss = 1.37 (349.7 examples/sec; 0.366 sec/batch)
2016-12-08 09:37:03.607951: step 2810, loss = 1.27 (377.7 examples/sec; 0.339 sec/batch)
2016-12-08 09:37:06.877564: step 2820, loss = 1.16 (373.2 examples/sec; 0.343 sec/batch)
2016-12-08 09:37:10.223972: step 2830, loss = 1.34 (415.0 examples/sec; 0.308 sec/batch)
2016-12-08 09:37:13.257782: step 2840, loss = 1.37 (399.4 examples/sec; 0.320 sec/batch)
2016-12-08 09:37:16.361006: step 2850, loss = 1.20 (397.1 examples/sec; 0.322 sec/batch)
2016-12-08 09:37:19.708838: step 2860, loss = 1.29 (362.0 examples/sec; 0.354 sec/batch)
2016-12-08 09:37:22.938088: step 2870, loss = 1.41 (352.3 examples/sec; 0.363 sec/batch)
2016-12-08 09:37:26.134874: step 2880, loss = 1.29 (448.1 examples/sec; 0.286 sec/batch)
2016-12-08 09:37:29.554049: step 2890, loss = 1.31 (377.4 examples/sec; 0.339 sec/batch)
2016-12-08 09:37:32.853662: step 2900, loss = 1.27 (406.4 examples/sec; 0.315 sec/batch)
2016-12-08 09:37:36.561380: step 2910, loss = 1.27 (457.8 examples/sec; 0.280 sec/batch)
2016-12-08 09:37:39.948277: step 2920, loss = 1.56 (387.9 examples/sec; 0.330 sec/batch)
2016-12-08 09:37:43.326651: step 2930, loss = 1.16 (418.5 examples/sec; 0.306 sec/batch)
2016-12-08 09:37:46.636845: step 2940, loss = 1.29 (363.6 examples/sec; 0.352 sec/batch)
2016-12-08 09:37:49.898676: step 2950, loss = 1.18 (376.9 examples/sec; 0.340 sec/batch)
2016-12-08 09:37:52.983988: step 2960, loss = 1.33 (413.2 examples/sec; 0.310 sec/batch)
2016-12-08 09:37:56.422789: step 2970, loss = 1.41 (360.1 examples/sec; 0.355 sec/batch)
2016-12-08 09:37:59.689749: step 2980, loss = 1.27 (408.0 examples/sec; 0.314 sec/batch)
2016-12-08 09:38:02.971772: step 2990, loss = 1.42 (351.2 examples/sec; 0.364 sec/batch)
2016-12-08 09:38:06.303686: step 3000, loss = 1.17 (423.0 examples/sec; 0.303 sec/batch)
2016-12-08 09:38:10.643582: step 3010, loss = 1.29 (365.3 examples/sec; 0.350 sec/batch)
2016-12-08 09:38:13.772789: step 3020, loss = 1.17 (359.4 examples/sec; 0.356 sec/batch)
2016-12-08 09:38:17.126653: step 3030, loss = 1.23 (417.4 examples/sec; 0.307 sec/batch)
2016-12-08 09:38:20.459422: step 3040, loss = 1.30 (409.1 examples/sec; 0.313 sec/batch)
2016-12-08 09:38:23.885363: step 3050, loss = 1.47 (382.1 examples/sec; 0.335 sec/batch)
2016-12-08 09:38:27.156383: step 3060, loss = 1.30 (380.2 examples/sec; 0.337 sec/batch)
2016-12-08 09:38:30.177856: step 3070, loss = 1.17 (390.9 examples/sec; 0.327 sec/batch)
2016-12-08 09:38:33.474918: step 3080, loss = 1.41 (401.7 examples/sec; 0.319 sec/batch)
2016-12-08 09:38:36.909845: step 3090, loss = 1.30 (367.1 examples/sec; 0.349 sec/batch)
2016-12-08 09:38:40.148398: step 3100, loss = 1.38 (356.1 examples/sec; 0.359 sec/batch)
2016-12-08 09:38:43.769369: step 3110, loss = 1.34 (370.8 examples/sec; 0.345 sec/batch)
2016-12-08 09:38:47.029923: step 3120, loss = 1.41 (382.5 examples/sec; 0.335 sec/batch)
2016-12-08 09:38:50.436153: step 3130, loss = 1.30 (400.1 examples/sec; 0.320 sec/batch)
2016-12-08 09:38:53.552883: step 3140, loss = 1.18 (415.0 examples/sec; 0.308 sec/batch)
2016-12-08 09:38:56.876686: step 3150, loss = 1.50 (371.4 examples/sec; 0.345 sec/batch)
2016-12-08 09:39:00.103238: step 3160, loss = 1.17 (380.0 examples/sec; 0.337 sec/batch)
2016-12-08 09:39:03.172484: step 3170, loss = 1.42 (431.1 examples/sec; 0.297 sec/batch)
2016-12-08 09:39:06.486600: step 3180, loss = 1.08 (338.4 examples/sec; 0.378 sec/batch)
2016-12-08 09:39:09.736239: step 3190, loss = 1.34 (389.0 examples/sec; 0.329 sec/batch)
2016-12-08 09:39:12.940551: step 3200, loss = 1.32 (420.0 examples/sec; 0.305 sec/batch)
2016-12-08 09:39:16.677773: step 3210, loss = 1.47 (351.5 examples/sec; 0.364 sec/batch)
2016-12-08 09:39:19.888198: step 3220, loss = 1.14 (391.9 examples/sec; 0.327 sec/batch)
2016-12-08 09:39:23.162508: step 3230, loss = 1.30 (409.9 examples/sec; 0.312 sec/batch)
2016-12-08 09:39:26.492799: step 3240, loss = 1.24 (375.0 examples/sec; 0.341 sec/batch)
2016-12-08 09:39:29.760316: step 3250, loss = 1.15 (361.4 examples/sec; 0.354 sec/batch)
2016-12-08 09:39:32.964891: step 3260, loss = 1.37 (365.0 examples/sec; 0.351 sec/batch)
2016-12-08 09:39:36.154455: step 3270, loss = 1.17 (355.8 examples/sec; 0.360 sec/batch)
2016-12-08 09:39:39.507938: step 3280, loss = 1.36 (383.3 examples/sec; 0.334 sec/batch)
2016-12-08 09:39:42.933565: step 3290, loss = 1.16 (370.0 examples/sec; 0.346 sec/batch)
2016-12-08 09:39:46.014879: step 3300, loss = 1.39 (361.9 examples/sec; 0.354 sec/batch)
2016-12-08 09:39:49.587224: step 3310, loss = 1.26 (410.6 examples/sec; 0.312 sec/batch)
2016-12-08 09:39:52.814591: step 3320, loss = 1.24 (373.3 examples/sec; 0.343 sec/batch)
2016-12-08 09:39:55.944773: step 3330, loss = 1.30 (410.2 examples/sec; 0.312 sec/batch)
2016-12-08 09:39:59.028652: step 3340, loss = 1.06 (393.1 examples/sec; 0.326 sec/batch)
2016-12-08 09:40:02.324432: step 3350, loss = 1.11 (350.9 examples/sec; 0.365 sec/batch)
2016-12-08 09:40:05.676867: step 3360, loss = 1.09 (394.6 examples/sec; 0.324 sec/batch)
2016-12-08 09:40:08.931895: step 3370, loss = 1.20 (369.8 examples/sec; 0.346 sec/batch)
2016-12-08 09:40:12.254964: step 3380, loss = 1.28 (375.7 examples/sec; 0.341 sec/batch)
2016-12-08 09:40:15.525885: step 3390, loss = 1.18 (383.1 examples/sec; 0.334 sec/batch)
2016-12-08 09:40:18.886160: step 3400, loss = 1.11 (401.8 examples/sec; 0.319 sec/batch)
2016-12-08 09:40:22.371355: step 3410, loss = 1.16 (412.8 examples/sec; 0.310 sec/batch)
2016-12-08 09:40:25.633646: step 3420, loss = 1.19 (385.6 examples/sec; 0.332 sec/batch)
2016-12-08 09:40:28.757157: step 3430, loss = 1.17 (384.3 examples/sec; 0.333 sec/batch)
2016-12-08 09:40:31.990279: step 3440, loss = 1.21 (368.9 examples/sec; 0.347 sec/batch)
2016-12-08 09:40:35.215068: step 3450, loss = 1.23 (355.2 examples/sec; 0.360 sec/batch)
2016-12-08 09:40:38.616522: step 3460, loss = 1.46 (367.1 examples/sec; 0.349 sec/batch)
2016-12-08 09:40:41.944223: step 3470, loss = 1.11 (390.1 examples/sec; 0.328 sec/batch)
2016-12-08 09:40:45.250135: step 3480, loss = 1.09 (374.8 examples/sec; 0.341 sec/batch)
2016-12-08 09:40:48.383474: step 3490, loss = 1.25 (455.6 examples/sec; 0.281 sec/batch)
2016-12-08 09:40:51.563783: step 3500, loss = 1.38 (336.3 examples/sec; 0.381 sec/batch)
2016-12-08 09:40:55.143499: step 3510, loss = 1.22 (490.4 examples/sec; 0.261 sec/batch)
2016-12-08 09:40:58.262648: step 3520, loss = 1.14 (355.5 examples/sec; 0.360 sec/batch)
2016-12-08 09:41:01.534210: step 3530, loss = 1.30 (367.0 examples/sec; 0.349 sec/batch)
2016-12-08 09:41:04.839993: step 3540, loss = 1.16 (371.4 examples/sec; 0.345 sec/batch)
2016-12-08 09:41:08.256977: step 3550, loss = 1.07 (360.6 examples/sec; 0.355 sec/batch)
2016-12-08 09:41:11.629554: step 3560, loss = 1.27 (382.6 examples/sec; 0.335 sec/batch)
2016-12-08 09:41:14.960310: step 3570, loss = 1.17 (392.5 examples/sec; 0.326 sec/batch)
2016-12-08 09:41:18.131013: step 3580, loss = 1.22 (336.5 examples/sec; 0.380 sec/batch)
2016-12-08 09:41:21.530469: step 3590, loss = 1.10 (434.0 examples/sec; 0.295 sec/batch)
2016-12-08 09:41:24.883663: step 3600, loss = 1.16 (353.7 examples/sec; 0.362 sec/batch)
2016-12-08 09:41:28.556747: step 3610, loss = 1.17 (334.0 examples/sec; 0.383 sec/batch)
2016-12-08 09:41:31.808308: step 3620, loss = 1.32 (392.0 examples/sec; 0.326 sec/batch)
2016-12-08 09:41:34.975473: step 3630, loss = 1.21 (423.0 examples/sec; 0.303 sec/batch)
2016-12-08 09:41:38.331556: step 3640, loss = 1.10 (365.4 examples/sec; 0.350 sec/batch)
2016-12-08 09:41:41.712300: step 3650, loss = 1.26 (381.0 examples/sec; 0.336 sec/batch)
2016-12-08 09:41:44.928671: step 3660, loss = 1.00 (438.0 examples/sec; 0.292 sec/batch)
2016-12-08 09:41:48.238415: step 3670, loss = 1.10 (374.6 examples/sec; 0.342 sec/batch)
2016-12-08 09:41:51.457318: step 3680, loss = 1.22 (376.7 examples/sec; 0.340 sec/batch)
2016-12-08 09:41:54.729754: step 3690, loss = 1.19 (365.0 examples/sec; 0.351 sec/batch)
2016-12-08 09:41:57.986764: step 3700, loss = 1.01 (424.5 examples/sec; 0.302 sec/batch)
2016-12-08 09:42:01.532250: step 3710, loss = 1.17 (394.2 examples/sec; 0.325 sec/batch)
2016-12-08 09:42:04.783067: step 3720, loss = 1.12 (409.1 examples/sec; 0.313 sec/batch)
2016-12-08 09:42:08.163040: step 3730, loss = 1.11 (394.3 examples/sec; 0.325 sec/batch)
2016-12-08 09:42:11.478479: step 3740, loss = 0.86 (387.2 examples/sec; 0.331 sec/batch)
2016-12-08 09:42:14.824552: step 3750, loss = 1.09 (377.8 examples/sec; 0.339 sec/batch)
2016-12-08 09:42:18.188404: step 3760, loss = 1.42 (367.9 examples/sec; 0.348 sec/batch)
2016-12-08 09:42:21.531792: step 3770, loss = 1.15 (373.1 examples/sec; 0.343 sec/batch)
2016-12-08 09:42:24.885038: step 3780, loss = 0.99 (384.7 examples/sec; 0.333 sec/batch)
2016-12-08 09:42:28.166067: step 3790, loss = 1.16 (447.7 examples/sec; 0.286 sec/batch)
2016-12-08 09:42:31.415881: step 3800, loss = 1.24 (448.1 examples/sec; 0.286 sec/batch)
2016-12-08 09:42:34.969645: step 3810, loss = 1.15 (427.7 examples/sec; 0.299 sec/batch)
2016-12-08 09:42:38.164995: step 3820, loss = 1.31 (393.2 examples/sec; 0.326 sec/batch)
2016-12-08 09:42:41.498587: step 3830, loss = 1.16 (370.4 examples/sec; 0.346 sec/batch)
2016-12-08 09:42:44.743344: step 3840, loss = 0.99 (471.6 examples/sec; 0.271 sec/batch)
2016-12-08 09:42:47.972894: step 3850, loss = 1.17 (392.9 examples/sec; 0.326 sec/batch)
2016-12-08 09:42:51.265471: step 3860, loss = 1.14 (399.8 examples/sec; 0.320 sec/batch)
2016-12-08 09:42:54.571017: step 3870, loss = 1.08 (369.0 examples/sec; 0.347 sec/batch)
2016-12-08 09:42:57.866045: step 3880, loss = 1.13 (364.0 examples/sec; 0.352 sec/batch)
2016-12-08 09:43:01.214850: step 3890, loss = 1.01 (382.4 examples/sec; 0.335 sec/batch)
2016-12-08 09:43:04.635729: step 3900, loss = 0.90 (319.6 examples/sec; 0.400 sec/batch)
2016-12-08 09:43:08.128281: step 3910, loss = 1.19 (374.5 examples/sec; 0.342 sec/batch)
2016-12-08 09:43:11.654657: step 3920, loss = 1.00 (336.1 examples/sec; 0.381 sec/batch)
2016-12-08 09:43:14.923316: step 3930, loss = 1.19 (442.7 examples/sec; 0.289 sec/batch)
2016-12-08 09:43:17.885447: step 3940, loss = 1.05 (467.2 examples/sec; 0.274 sec/batch)
2016-12-08 09:43:21.245382: step 3950, loss = 1.17 (384.7 examples/sec; 0.333 sec/batch)
2016-12-08 09:43:24.430689: step 3960, loss = 1.12 (363.9 examples/sec; 0.352 sec/batch)
2016-12-08 09:43:27.882721: step 3970, loss = 1.06 (387.4 examples/sec; 0.330 sec/batch)
2016-12-08 09:43:31.077218: step 3980, loss = 1.14 (387.8 examples/sec; 0.330 sec/batch)
2016-12-08 09:43:34.244833: step 3990, loss = 0.88 (415.8 examples/sec; 0.308 sec/batch)
2016-12-08 09:43:37.448826: step 4000, loss = 0.97 (370.9 examples/sec; 0.345 sec/batch)
2016-12-08 09:43:41.647108: step 4010, loss = 1.30 (379.5 examples/sec; 0.337 sec/batch)
2016-12-08 09:43:44.902134: step 4020, loss = 1.10 (346.1 examples/sec; 0.370 sec/batch)
2016-12-08 09:43:48.395319: step 4030, loss = 1.07 (320.1 examples/sec; 0.400 sec/batch)
2016-12-08 09:43:51.606187: step 4040, loss = 1.24 (417.0 examples/sec; 0.307 sec/batch)
2016-12-08 09:43:54.901929: step 4050, loss = 1.04 (409.4 examples/sec; 0.313 sec/batch)
2016-12-08 09:43:58.187836: step 4060, loss = 1.40 (388.1 examples/sec; 0.330 sec/batch)
2016-12-08 09:44:01.537371: step 4070, loss = 1.14 (417.1 examples/sec; 0.307 sec/batch)
2016-12-08 09:44:04.758581: step 4080, loss = 1.10 (431.3 examples/sec; 0.297 sec/batch)
2016-12-08 09:44:07.943954: step 4090, loss = 1.13 (385.6 examples/sec; 0.332 sec/batch)
2016-12-08 09:44:11.051177: step 4100, loss = 1.01 (403.5 examples/sec; 0.317 sec/batch)
2016-12-08 09:44:14.669889: step 4110, loss = 0.93 (387.9 examples/sec; 0.330 sec/batch)
2016-12-08 09:44:17.890342: step 4120, loss = 1.17 (406.0 examples/sec; 0.315 sec/batch)
2016-12-08 09:44:21.353304: step 4130, loss = 1.01 (396.1 examples/sec; 0.323 sec/batch)
2016-12-08 09:44:24.516751: step 4140, loss = 0.97 (427.9 examples/sec; 0.299 sec/batch)
2016-12-08 09:44:27.727511: step 4150, loss = 1.12 (415.7 examples/sec; 0.308 sec/batch)
2016-12-08 09:44:30.984563: step 4160, loss = 1.07 (409.5 examples/sec; 0.313 sec/batch)
2016-12-08 09:44:34.034073: step 4170, loss = 0.98 (443.7 examples/sec; 0.288 sec/batch)
2016-12-08 09:44:37.294915: step 4180, loss = 1.07 (407.6 examples/sec; 0.314 sec/batch)
2016-12-08 09:44:40.443434: step 4190, loss = 1.06 (409.4 examples/sec; 0.313 sec/batch)
2016-12-08 09:44:43.710085: step 4200, loss = 1.01 (412.7 examples/sec; 0.310 sec/batch)
2016-12-08 09:44:47.268235: step 4210, loss = 1.09 (452.3 examples/sec; 0.283 sec/batch)
2016-12-08 09:44:50.399254: step 4220, loss = 1.15 (357.6 examples/sec; 0.358 sec/batch)
2016-12-08 09:44:53.672710: step 4230, loss = 0.89 (402.6 examples/sec; 0.318 sec/batch)
2016-12-08 09:44:56.745703: step 4240, loss = 1.07 (357.4 examples/sec; 0.358 sec/batch)
2016-12-08 09:45:00.042624: step 4250, loss = 1.13 (415.5 examples/sec; 0.308 sec/batch)
2016-12-08 09:45:03.190601: step 4260, loss = 1.10 (487.2 examples/sec; 0.263 sec/batch)
2016-12-08 09:45:05.982752: step 4270, loss = 1.12 (482.7 examples/sec; 0.265 sec/batch)
2016-12-08 09:45:09.005994: step 4280, loss = 1.13 (402.9 examples/sec; 0.318 sec/batch)
2016-12-08 09:45:12.352046: step 4290, loss = 1.06 (434.3 examples/sec; 0.295 sec/batch)
2016-12-08 09:45:15.406111: step 4300, loss = 1.01 (417.7 examples/sec; 0.306 sec/batch)
2016-12-08 09:45:19.224383: step 4310, loss = 0.94 (403.6 examples/sec; 0.317 sec/batch)
2016-12-08 09:45:22.540938: step 4320, loss = 1.01 (414.0 examples/sec; 0.309 sec/batch)
2016-12-08 09:45:25.741237: step 4330, loss = 1.14 (420.4 examples/sec; 0.304 sec/batch)
2016-12-08 09:45:29.059910: step 4340, loss = 1.07 (424.2 examples/sec; 0.302 sec/batch)
2016-12-08 09:45:32.274978: step 4350, loss = 1.16 (409.5 examples/sec; 0.313 sec/batch)
2016-12-08 09:45:35.538833: step 4360, loss = 1.04 (414.7 examples/sec; 0.309 sec/batch)
2016-12-08 09:45:38.903080: step 4370, loss = 1.16 (345.5 examples/sec; 0.371 sec/batch)
2016-12-08 09:45:42.217009: step 4380, loss = 1.13 (367.1 examples/sec; 0.349 sec/batch)
2016-12-08 09:45:45.220518: step 4390, loss = 1.08 (417.9 examples/sec; 0.306 sec/batch)
2016-12-08 09:45:48.433617: step 4400, loss = 0.93 (399.6 examples/sec; 0.320 sec/batch)
2016-12-08 09:45:51.921311: step 4410, loss = 0.89 (408.4 examples/sec; 0.313 sec/batch)
2016-12-08 09:45:55.147523: step 4420, loss = 1.03 (381.2 examples/sec; 0.336 sec/batch)
2016-12-08 09:45:58.304414: step 4430, loss = 1.16 (431.3 examples/sec; 0.297 sec/batch)
2016-12-08 09:46:01.532483: step 4440, loss = 0.96 (470.5 examples/sec; 0.272 sec/batch)
2016-12-08 09:46:04.489417: step 4450, loss = 1.14 (392.8 examples/sec; 0.326 sec/batch)
2016-12-08 09:46:07.547367: step 4460, loss = 1.00 (451.7 examples/sec; 0.283 sec/batch)
2016-12-08 09:46:10.935388: step 4470, loss = 1.27 (413.7 examples/sec; 0.309 sec/batch)
2016-12-08 09:46:14.241779: step 4480, loss = 0.99 (349.1 examples/sec; 0.367 sec/batch)
2016-12-08 09:46:17.395536: step 4490, loss = 1.01 (379.1 examples/sec; 0.338 sec/batch)
2016-12-08 09:46:20.625769: step 4500, loss = 1.01 (421.8 examples/sec; 0.303 sec/batch)
2016-12-08 09:46:24.202475: step 4510, loss = 1.04 (384.9 examples/sec; 0.333 sec/batch)
2016-12-08 09:46:27.320841: step 4520, loss = 1.04 (394.3 examples/sec; 0.325 sec/batch)
2016-12-08 09:46:30.717513: step 4530, loss = 0.97 (397.6 examples/sec; 0.322 sec/batch)
2016-12-08 09:46:33.798083: step 4540, loss = 0.82 (416.4 examples/sec; 0.307 sec/batch)
2016-12-08 09:46:36.942021: step 4550, loss = 0.98 (410.5 examples/sec; 0.312 sec/batch)
2016-12-08 09:46:40.253172: step 4560, loss = 0.98 (356.5 examples/sec; 0.359 sec/batch)
2016-12-08 09:46:43.425707: step 4570, loss = 0.95 (436.2 examples/sec; 0.293 sec/batch)
2016-12-08 09:46:46.548630: step 4580, loss = 1.33 (425.9 examples/sec; 0.301 sec/batch)
2016-12-08 09:46:49.782715: step 4590, loss = 1.20 (357.5 examples/sec; 0.358 sec/batch)
2016-12-08 09:46:52.943325: step 4600, loss = 1.21 (367.7 examples/sec; 0.348 sec/batch)
2016-12-08 09:46:56.352119: step 4610, loss = 0.96 (453.2 examples/sec; 0.282 sec/batch)
2016-12-08 09:46:59.508024: step 4620, loss = 1.07 (406.2 examples/sec; 0.315 sec/batch)
2016-12-08 09:47:02.924850: step 4630, loss = 1.22 (361.7 examples/sec; 0.354 sec/batch)
2016-12-08 09:47:06.161999: step 4640, loss = 0.91 (440.0 examples/sec; 0.291 sec/batch)
2016-12-08 09:47:09.543148: step 4650, loss = 1.03 (371.4 examples/sec; 0.345 sec/batch)
2016-12-08 09:47:12.958097: step 4660, loss = 1.04 (381.4 examples/sec; 0.336 sec/batch)
2016-12-08 09:47:16.370822: step 4670, loss = 1.07 (353.6 examples/sec; 0.362 sec/batch)
2016-12-08 09:47:19.518195: step 4680, loss = 1.12 (426.2 examples/sec; 0.300 sec/batch)
2016-12-08 09:47:22.521760: step 4690, loss = 0.93 (399.2 examples/sec; 0.321 sec/batch)
2016-12-08 09:47:25.738353: step 4700, loss = 1.05 (408.6 examples/sec; 0.313 sec/batch)
2016-12-08 09:47:29.124149: step 4710, loss = 1.04 (388.5 examples/sec; 0.329 sec/batch)
2016-12-08 09:47:32.270581: step 4720, loss = 1.08 (426.6 examples/sec; 0.300 sec/batch)
2016-12-08 09:47:35.439636: step 4730, loss = 1.07 (411.1 examples/sec; 0.311 sec/batch)
2016-12-08 09:47:38.611889: step 4740, loss = 0.92 (403.4 examples/sec; 0.317 sec/batch)
2016-12-08 09:47:41.616313: step 4750, loss = 0.94 (401.3 examples/sec; 0.319 sec/batch)
2016-12-08 09:47:44.868720: step 4760, loss = 1.22 (376.4 examples/sec; 0.340 sec/batch)
2016-12-08 09:47:48.156444: step 4770, loss = 0.89 (360.7 examples/sec; 0.355 sec/batch)
2016-12-08 09:47:51.387390: step 4780, loss = 1.01 (378.8 examples/sec; 0.338 sec/batch)
2016-12-08 09:47:54.696657: step 4790, loss = 1.01 (354.1 examples/sec; 0.362 sec/batch)
2016-12-08 09:47:57.823496: step 4800, loss = 1.02 (443.1 examples/sec; 0.289 sec/batch)
2016-12-08 09:48:01.325774: step 4810, loss = 1.05 (425.6 examples/sec; 0.301 sec/batch)
2016-12-08 09:48:04.469256: step 4820, loss = 1.20 (421.5 examples/sec; 0.304 sec/batch)
2016-12-08 09:48:07.787179: step 4830, loss = 1.10 (350.9 examples/sec; 0.365 sec/batch)
2016-12-08 09:48:10.998793: step 4840, loss = 1.01 (373.1 examples/sec; 0.343 sec/batch)
2016-12-08 09:48:14.339413: step 4850, loss = 1.03 (358.8 examples/sec; 0.357 sec/batch)
2016-12-08 09:48:17.553371: step 4860, loss = 1.16 (435.0 examples/sec; 0.294 sec/batch)
2016-12-08 09:48:20.750193: step 4870, loss = 1.07 (394.6 examples/sec; 0.324 sec/batch)
2016-12-08 09:48:23.977599: step 4880, loss = 0.94 (386.8 examples/sec; 0.331 sec/batch)
2016-12-08 09:48:27.059907: step 4890, loss = 0.91 (363.2 examples/sec; 0.352 sec/batch)
2016-12-08 09:48:30.396352: step 4900, loss = 1.05 (371.3 examples/sec; 0.345 sec/batch)
2016-12-08 09:48:34.036207: step 4910, loss = 1.12 (354.0 examples/sec; 0.362 sec/batch)
2016-12-08 09:48:37.310678: step 4920, loss = 1.08 (343.1 examples/sec; 0.373 sec/batch)
2016-12-08 09:48:40.555607: step 4930, loss = 0.91 (391.0 examples/sec; 0.327 sec/batch)
2016-12-08 09:48:43.876170: step 4940, loss = 0.95 (373.4 examples/sec; 0.343 sec/batch)
2016-12-08 09:48:47.152852: step 4950, loss = 0.99 (364.3 examples/sec; 0.351 sec/batch)
2016-12-08 09:48:50.583990: step 4960, loss = 1.11 (446.5 examples/sec; 0.287 sec/batch)
2016-12-08 09:48:53.525406: step 4970, loss = 1.02 (434.8 examples/sec; 0.294 sec/batch)
2016-12-08 09:48:56.703983: step 4980, loss = 0.97 (379.6 examples/sec; 0.337 sec/batch)
2016-12-08 09:48:59.812913: step 4990, loss = 1.10 (382.6 examples/sec; 0.335 sec/batch)
2016-12-08 09:49:03.239408: step 5000, loss = 1.03 (371.1 examples/sec; 0.345 sec/batch)
2016-12-08 09:49:07.591606: step 5010, loss = 1.01 (381.8 examples/sec; 0.335 sec/batch)
2016-12-08 09:49:10.734181: step 5020, loss = 0.99 (406.2 examples/sec; 0.315 sec/batch)
2016-12-08 09:49:13.883256: step 5030, loss = 1.12 (382.5 examples/sec; 0.335 sec/batch)
2016-12-08 09:49:17.218568: step 5040, loss = 0.96 (366.9 examples/sec; 0.349 sec/batch)
2016-12-08 09:49:20.403438: step 5050, loss = 1.29 (475.6 examples/sec; 0.269 sec/batch)
2016-12-08 09:49:23.656748: step 5060, loss = 1.20 (370.7 examples/sec; 0.345 sec/batch)
2016-12-08 09:49:26.723657: step 5070, loss = 0.99 (331.6 examples/sec; 0.386 sec/batch)
2016-12-08 09:49:29.880915: step 5080, loss = 0.96 (351.6 examples/sec; 0.364 sec/batch)
2016-12-08 09:49:33.023460: step 5090, loss = 0.95 (393.3 examples/sec; 0.325 sec/batch)
2016-12-08 09:49:36.372874: step 5100, loss = 0.93 (344.5 examples/sec; 0.372 sec/batch)
2016-12-08 09:49:39.933714: step 5110, loss = 1.17 (370.9 examples/sec; 0.345 sec/batch)
2016-12-08 09:49:42.928287: step 5120, loss = 1.01 (396.2 examples/sec; 0.323 sec/batch)
2016-12-08 09:49:46.160886: step 5130, loss = 0.91 (376.2 examples/sec; 0.340 sec/batch)
2016-12-08 09:49:49.409446: step 5140, loss = 1.05 (392.2 examples/sec; 0.326 sec/batch)
2016-12-08 09:49:52.556585: step 5150, loss = 0.97 (373.1 examples/sec; 0.343 sec/batch)
2016-12-08 09:49:55.883552: step 5160, loss = 0.94 (391.4 examples/sec; 0.327 sec/batch)
2016-12-08 09:49:58.932011: step 5170, loss = 1.04 (420.9 examples/sec; 0.304 sec/batch)
2016-12-08 09:50:01.907132: step 5180, loss = 1.09 (408.1 examples/sec; 0.314 sec/batch)
2016-12-08 09:50:05.161218: step 5190, loss = 0.94 (395.7 examples/sec; 0.323 sec/batch)
2016-12-08 09:50:08.538701: step 5200, loss = 1.19 (432.0 examples/sec; 0.296 sec/batch)
2016-12-08 09:50:11.910697: step 5210, loss = 0.97 (447.0 examples/sec; 0.286 sec/batch)
2016-12-08 09:50:15.242489: step 5220, loss = 0.96 (461.7 examples/sec; 0.277 sec/batch)
2016-12-08 09:50:18.369778: step 5230, loss = 0.94 (435.2 examples/sec; 0.294 sec/batch)
2016-12-08 09:50:21.658103: step 5240, loss = 0.93 (352.0 examples/sec; 0.364 sec/batch)
2016-12-08 09:50:24.918111: step 5250, loss = 0.99 (379.0 examples/sec; 0.338 sec/batch)
2016-12-08 09:50:28.013124: step 5260, loss = 0.95 (425.9 examples/sec; 0.301 sec/batch)
2016-12-08 09:50:31.284256: step 5270, loss = 1.02 (353.7 examples/sec; 0.362 sec/batch)
2016-12-08 09:50:34.648306: step 5280, loss = 1.04 (380.1 examples/sec; 0.337 sec/batch)
2016-12-08 09:50:37.805496: step 5290, loss = 1.12 (338.5 examples/sec; 0.378 sec/batch)
2016-12-08 09:50:41.009445: step 5300, loss = 1.24 (389.7 examples/sec; 0.328 sec/batch)
2016-12-08 09:50:44.545319: step 5310, loss = 0.96 (419.0 examples/sec; 0.305 sec/batch)
2016-12-08 09:50:47.821936: step 5320, loss = 0.82 (417.2 examples/sec; 0.307 sec/batch)
2016-12-08 09:50:51.026650: step 5330, loss = 0.91 (433.2 examples/sec; 0.295 sec/batch)
2016-12-08 09:50:54.232770: step 5340, loss = 1.05 (410.9 examples/sec; 0.311 sec/batch)
2016-12-08 09:50:57.451228: step 5350, loss = 0.91 (367.7 examples/sec; 0.348 sec/batch)
2016-12-08 09:51:00.741311: step 5360, loss = 1.00 (419.9 examples/sec; 0.305 sec/batch)
2016-12-08 09:51:04.134309: step 5370, loss = 1.21 (383.7 examples/sec; 0.334 sec/batch)
2016-12-08 09:51:07.299212: step 5380, loss = 1.05 (389.4 examples/sec; 0.329 sec/batch)
2016-12-08 09:51:10.464585: step 5390, loss = 1.04 (397.9 examples/sec; 0.322 sec/batch)
2016-12-08 09:51:13.724149: step 5400, loss = 0.95 (423.2 examples/sec; 0.302 sec/batch)
2016-12-08 09:51:17.211346: step 5410, loss = 0.98 (402.3 examples/sec; 0.318 sec/batch)
2016-12-08 09:51:20.475638: step 5420, loss = 0.99 (429.0 examples/sec; 0.298 sec/batch)
2016-12-08 09:51:23.722599: step 5430, loss = 0.98 (371.1 examples/sec; 0.345 sec/batch)
2016-12-08 09:51:27.209585: step 5440, loss = 1.15 (362.9 examples/sec; 0.353 sec/batch)
2016-12-08 09:51:30.416813: step 5450, loss = 1.05 (489.4 examples/sec; 0.262 sec/batch)
2016-12-08 09:51:33.738428: step 5460, loss = 1.06 (362.0 examples/sec; 0.354 sec/batch)
2016-12-08 09:51:36.877326: step 5470, loss = 1.00 (428.3 examples/sec; 0.299 sec/batch)
2016-12-08 09:51:40.253799: step 5480, loss = 1.05 (384.9 examples/sec; 0.333 sec/batch)
2016-12-08 09:51:43.597032: step 5490, loss = 1.06 (340.3 examples/sec; 0.376 sec/batch)
2016-12-08 09:51:46.802476: step 5500, loss = 0.89 (366.2 examples/sec; 0.350 sec/batch)
2016-12-08 09:51:50.235917: step 5510, loss = 0.85 (395.3 examples/sec; 0.324 sec/batch)
2016-12-08 09:51:53.586818: step 5520, loss = 1.00 (447.8 examples/sec; 0.286 sec/batch)
2016-12-08 09:51:56.767941: step 5530, loss = 0.97 (371.6 examples/sec; 0.344 sec/batch)
2016-12-08 09:52:00.069759: step 5540, loss = 0.81 (364.8 examples/sec; 0.351 sec/batch)
2016-12-08 09:52:03.351425: step 5550, loss = 0.78 (412.4 examples/sec; 0.310 sec/batch)
2016-12-08 09:52:06.615308: step 5560, loss = 0.94 (352.1 examples/sec; 0.364 sec/batch)
2016-12-08 09:52:09.827572: step 5570, loss = 0.96 (364.0 examples/sec; 0.352 sec/batch)
2016-12-08 09:52:12.951567: step 5580, loss = 1.01 (413.2 examples/sec; 0.310 sec/batch)
2016-12-08 09:52:16.324018: step 5590, loss = 1.09 (382.7 examples/sec; 0.334 sec/batch)
2016-12-08 09:52:19.518339: step 5600, loss = 0.96 (430.8 examples/sec; 0.297 sec/batch)
2016-12-08 09:52:23.176339: step 5610, loss = 0.92 (393.2 examples/sec; 0.326 sec/batch)
2016-12-08 09:52:26.602878: step 5620, loss = 0.89 (370.4 examples/sec; 0.346 sec/batch)
2016-12-08 09:52:29.892764: step 5630, loss = 1.02 (413.3 examples/sec; 0.310 sec/batch)
2016-12-08 09:52:32.967107: step 5640, loss = 0.95 (415.1 examples/sec; 0.308 sec/batch)
2016-12-08 09:52:35.985307: step 5650, loss = 0.99 (454.8 examples/sec; 0.281 sec/batch)
2016-12-08 09:52:39.103425: step 5660, loss = 0.99 (384.4 examples/sec; 0.333 sec/batch)
2016-12-08 09:52:42.263287: step 5670, loss = 0.99 (422.9 examples/sec; 0.303 sec/batch)
2016-12-08 09:52:45.457699: step 5680, loss = 0.95 (358.0 examples/sec; 0.358 sec/batch)
2016-12-08 09:52:48.666529: step 5690, loss = 0.98 (372.3 examples/sec; 0.344 sec/batch)
2016-12-08 09:52:51.906780: step 5700, loss = 0.81 (434.4 examples/sec; 0.295 sec/batch)
2016-12-08 09:52:55.457783: step 5710, loss = 0.94 (391.2 examples/sec; 0.327 sec/batch)
2016-12-08 09:52:58.739581: step 5720, loss = 0.85 (408.0 examples/sec; 0.314 sec/batch)
2016-12-08 09:53:01.924344: step 5730, loss = 0.98 (364.2 examples/sec; 0.351 sec/batch)
2016-12-08 09:53:05.181384: step 5740, loss = 1.12 (376.0 examples/sec; 0.340 sec/batch)
2016-12-08 09:53:08.485278: step 5750, loss = 1.07 (443.8 examples/sec; 0.288 sec/batch)
2016-12-08 09:53:11.750196: step 5760, loss = 0.72 (423.6 examples/sec; 0.302 sec/batch)
2016-12-08 09:53:14.822602: step 5770, loss = 0.97 (380.5 examples/sec; 0.336 sec/batch)
2016-12-08 09:53:17.849676: step 5780, loss = 0.83 (402.1 examples/sec; 0.318 sec/batch)
2016-12-08 09:53:21.001382: step 5790, loss = 0.95 (411.6 examples/sec; 0.311 sec/batch)
2016-12-08 09:53:24.117266: step 5800, loss = 1.05 (423.6 examples/sec; 0.302 sec/batch)
2016-12-08 09:53:27.653402: step 5810, loss = 0.88 (372.2 examples/sec; 0.344 sec/batch)
2016-12-08 09:53:30.861681: step 5820, loss = 0.96 (399.9 examples/sec; 0.320 sec/batch)
2016-12-08 09:53:34.043087: step 5830, loss = 1.10 (368.2 examples/sec; 0.348 sec/batch)
2016-12-08 09:53:37.357855: step 5840, loss = 0.89 (322.9 examples/sec; 0.396 sec/batch)
2016-12-08 09:53:40.730273: step 5850, loss = 0.88 (391.8 examples/sec; 0.327 sec/batch)
2016-12-08 09:53:44.068013: step 5860, loss = 1.02 (376.5 examples/sec; 0.340 sec/batch)
2016-12-08 09:53:47.151840: step 5870, loss = 1.03 (442.2 examples/sec; 0.289 sec/batch)
2016-12-08 09:53:50.364782: step 5880, loss = 1.04 (406.0 examples/sec; 0.315 sec/batch)
2016-12-08 09:53:53.542611: step 5890, loss = 1.11 (460.3 examples/sec; 0.278 sec/batch)
2016-12-08 09:53:56.909340: step 5900, loss = 1.07 (343.3 examples/sec; 0.373 sec/batch)
2016-12-08 09:54:00.578154: step 5910, loss = 1.09 (386.2 examples/sec; 0.331 sec/batch)
2016-12-08 09:54:03.886366: step 5920, loss = 0.87 (381.0 examples/sec; 0.336 sec/batch)
2016-12-08 09:54:07.069920: step 5930, loss = 1.07 (382.9 examples/sec; 0.334 sec/batch)
2016-12-08 09:54:10.158821: step 5940, loss = 0.96 (447.5 examples/sec; 0.286 sec/batch)
2016-12-08 09:54:13.351889: step 5950, loss = 1.15 (401.9 examples/sec; 0.319 sec/batch)
2016-12-08 09:54:16.588459: step 5960, loss = 0.94 (363.1 examples/sec; 0.353 sec/batch)
2016-12-08 09:54:19.902701: step 5970, loss = 0.99 (347.1 examples/sec; 0.369 sec/batch)
2016-12-08 09:54:23.193741: step 5980, loss = 0.79 (446.9 examples/sec; 0.286 sec/batch)
2016-12-08 09:54:26.496482: step 5990, loss = 1.11 (390.9 examples/sec; 0.327 sec/batch)
2016-12-08 09:54:29.796036: step 6000, loss = 1.18 (379.8 examples/sec; 0.337 sec/batch)
2016-12-08 09:54:34.120141: step 6010, loss = 0.85 (372.6 examples/sec; 0.344 sec/batch)
2016-12-08 09:54:37.566129: step 6020, loss = 0.94 (351.5 examples/sec; 0.364 sec/batch)
2016-12-08 09:54:40.847550: step 6030, loss = 0.79 (366.4 examples/sec; 0.349 sec/batch)
2016-12-08 09:54:44.086118: step 6040, loss = 0.91 (422.0 examples/sec; 0.303 sec/batch)
2016-12-08 09:54:47.364396: step 6050, loss = 0.92 (412.7 examples/sec; 0.310 sec/batch)
2016-12-08 09:54:50.650101: step 6060, loss = 1.21 (365.4 examples/sec; 0.350 sec/batch)
2016-12-08 09:54:53.898986: step 6070, loss = 1.12 (410.4 examples/sec; 0.312 sec/batch)
2016-12-08 09:54:57.087219: step 6080, loss = 1.00 (422.5 examples/sec; 0.303 sec/batch)
2016-12-08 09:55:00.230084: step 6090, loss = 0.84 (396.4 examples/sec; 0.323 sec/batch)
2016-12-08 09:55:03.557505: step 6100, loss = 1.16 (385.6 examples/sec; 0.332 sec/batch)
2016-12-08 09:55:07.228941: step 6110, loss = 1.23 (428.5 examples/sec; 0.299 sec/batch)
2016-12-08 09:55:10.574157: step 6120, loss = 0.92 (399.9 examples/sec; 0.320 sec/batch)
2016-12-08 09:55:13.912631: step 6130, loss = 0.94 (380.6 examples/sec; 0.336 sec/batch)
2016-12-08 09:55:17.071958: step 6140, loss = 1.01 (421.2 examples/sec; 0.304 sec/batch)
2016-12-08 09:55:20.357397: step 6150, loss = 1.00 (365.4 examples/sec; 0.350 sec/batch)
2016-12-08 09:55:23.601221: step 6160, loss = 1.01 (334.9 examples/sec; 0.382 sec/batch)
2016-12-08 09:55:26.714874: step 6170, loss = 0.85 (411.7 examples/sec; 0.311 sec/batch)
2016-12-08 09:55:30.004210: step 6180, loss = 0.85 (355.1 examples/sec; 0.360 sec/batch)
2016-12-08 09:55:33.205168: step 6190, loss = 0.94 (434.9 examples/sec; 0.294 sec/batch)
2016-12-08 09:55:36.496494: step 6200, loss = 0.94 (356.4 examples/sec; 0.359 sec/batch)
2016-12-08 09:55:40.148792: step 6210, loss = 1.08 (438.7 examples/sec; 0.292 sec/batch)
2016-12-08 09:55:43.460161: step 6220, loss = 0.92 (350.1 examples/sec; 0.366 sec/batch)
2016-12-08 09:55:46.458711: step 6230, loss = 0.99 (440.9 examples/sec; 0.290 sec/batch)
2016-12-08 09:55:49.673855: step 6240, loss = 0.84 (384.1 examples/sec; 0.333 sec/batch)
2016-12-08 09:55:52.841514: step 6250, loss = 0.99 (456.8 examples/sec; 0.280 sec/batch)
2016-12-08 09:55:56.213997: step 6260, loss = 1.03 (383.7 examples/sec; 0.334 sec/batch)
2016-12-08 09:55:59.482747: step 6270, loss = 0.91 (386.4 examples/sec; 0.331 sec/batch)
2016-12-08 09:56:02.609148: step 6280, loss = 0.79 (379.4 examples/sec; 0.337 sec/batch)
2016-12-08 09:56:05.878975: step 6290, loss = 0.94 (435.6 examples/sec; 0.294 sec/batch)
2016-12-08 09:56:09.116781: step 6300, loss = 1.06 (408.0 examples/sec; 0.314 sec/batch)
2016-12-08 09:56:12.768896: step 6310, loss = 0.84 (395.6 examples/sec; 0.324 sec/batch)
2016-12-08 09:56:15.985836: step 6320, loss = 1.16 (428.9 examples/sec; 0.298 sec/batch)
2016-12-08 09:56:19.321270: step 6330, loss = 1.00 (393.4 examples/sec; 0.325 sec/batch)
2016-12-08 09:56:22.502661: step 6340, loss = 0.95 (360.3 examples/sec; 0.355 sec/batch)
2016-12-08 09:56:25.814985: step 6350, loss = 0.98 (380.4 examples/sec; 0.336 sec/batch)
2016-12-08 09:56:29.124119: step 6360, loss = 0.93 (409.0 examples/sec; 0.313 sec/batch)
2016-12-08 09:56:32.298404: step 6370, loss = 1.05 (400.7 examples/sec; 0.319 sec/batch)
2016-12-08 09:56:35.509841: step 6380, loss = 0.87 (373.5 examples/sec; 0.343 sec/batch)
2016-12-08 09:56:38.799959: step 6390, loss = 1.02 (437.0 examples/sec; 0.293 sec/batch)
2016-12-08 09:56:42.198954: step 6400, loss = 0.88 (420.6 examples/sec; 0.304 sec/batch)
2016-12-08 09:56:46.008927: step 6410, loss = 0.98 (381.7 examples/sec; 0.335 sec/batch)
2016-12-08 09:56:49.348805: step 6420, loss = 0.91 (408.4 examples/sec; 0.313 sec/batch)
2016-12-08 09:56:52.595066: step 6430, loss = 0.96 (463.3 examples/sec; 0.276 sec/batch)
2016-12-08 09:56:56.035446: step 6440, loss = 1.15 (386.2 examples/sec; 0.331 sec/batch)
2016-12-08 09:56:59.270934: step 6450, loss = 0.92 (389.3 examples/sec; 0.329 sec/batch)
2016-12-08 09:57:02.484304: step 6460, loss = 0.97 (386.7 examples/sec; 0.331 sec/batch)
2016-12-08 09:57:05.704501: step 6470, loss = 0.90 (424.3 examples/sec; 0.302 sec/batch)
2016-12-08 09:57:09.044652: step 6480, loss = 0.91 (377.9 examples/sec; 0.339 sec/batch)
2016-12-08 09:57:12.211693: step 6490, loss = 1.09 (421.8 examples/sec; 0.303 sec/batch)
2016-12-08 09:57:15.467111: step 6500, loss = 0.80 (487.1 examples/sec; 0.263 sec/batch)
2016-12-08 09:57:19.077517: step 6510, loss = 0.87 (389.5 examples/sec; 0.329 sec/batch)
2016-12-08 09:57:22.278848: step 6520, loss = 0.82 (432.5 examples/sec; 0.296 sec/batch)
2016-12-08 09:57:25.413892: step 6530, loss = 1.07 (447.5 examples/sec; 0.286 sec/batch)
2016-12-08 09:57:28.583440: step 6540, loss = 1.11 (391.0 examples/sec; 0.327 sec/batch)
2016-12-08 09:57:31.720862: step 6550, loss = 0.80 (367.2 examples/sec; 0.349 sec/batch)
2016-12-08 09:57:35.060630: step 6560, loss = 0.93 (370.3 examples/sec; 0.346 sec/batch)
2016-12-08 09:57:38.348699: step 6570, loss = 0.78 (343.5 examples/sec; 0.373 sec/batch)
2016-12-08 09:57:41.417488: step 6580, loss = 1.07 (415.5 examples/sec; 0.308 sec/batch)
2016-12-08 09:57:44.797320: step 6590, loss = 0.84 (371.7 examples/sec; 0.344 sec/batch)
2016-12-08 09:57:47.983279: step 6600, loss = 1.11 (451.7 examples/sec; 0.283 sec/batch)
2016-12-08 09:57:51.598251: step 6610, loss = 0.91 (438.9 examples/sec; 0.292 sec/batch)
2016-12-08 09:57:54.869100: step 6620, loss = 0.81 (377.6 examples/sec; 0.339 sec/batch)
2016-12-08 09:57:58.126940: step 6630, loss = 0.91 (453.3 examples/sec; 0.282 sec/batch)
2016-12-08 09:58:01.247999: step 6640, loss = 0.99 (411.7 examples/sec; 0.311 sec/batch)
2016-12-08 09:58:04.408224: step 6650, loss = 0.91 (371.8 examples/sec; 0.344 sec/batch)
2016-12-08 09:58:07.546283: step 6660, loss = 0.81 (430.9 examples/sec; 0.297 sec/batch)
2016-12-08 09:58:10.820404: step 6670, loss = 0.75 (396.7 examples/sec; 0.323 sec/batch)
2016-12-08 09:58:13.955145: step 6680, loss = 0.91 (391.1 examples/sec; 0.327 sec/batch)
2016-12-08 09:58:17.276584: step 6690, loss = 1.00 (433.7 examples/sec; 0.295 sec/batch)
2016-12-08 09:58:20.377883: step 6700, loss = 1.03 (432.2 examples/sec; 0.296 sec/batch)
2016-12-08 09:58:24.027365: step 6710, loss = 0.83 (332.4 examples/sec; 0.385 sec/batch)
2016-12-08 09:58:27.357084: step 6720, loss = 0.93 (357.4 examples/sec; 0.358 sec/batch)
2016-12-08 09:58:30.699509: step 6730, loss = 0.89 (411.9 examples/sec; 0.311 sec/batch)
2016-12-08 09:58:34.031150: step 6740, loss = 0.94 (373.2 examples/sec; 0.343 sec/batch)
2016-12-08 09:58:37.252461: step 6750, loss = 0.92 (361.8 examples/sec; 0.354 sec/batch)
2016-12-08 09:58:40.358059: step 6760, loss = 0.98 (439.3 examples/sec; 0.291 sec/batch)
2016-12-08 09:58:43.541527: step 6770, loss = 0.86 (418.0 examples/sec; 0.306 sec/batch)
2016-12-08 09:58:46.769044: step 6780, loss = 1.07 (395.6 examples/sec; 0.324 sec/batch)
2016-12-08 09:58:50.006363: step 6790, loss = 0.92 (361.7 examples/sec; 0.354 sec/batch)
2016-12-08 09:58:53.405155: step 6800, loss = 1.06 (458.9 examples/sec; 0.279 sec/batch)
2016-12-08 09:58:57.050008: step 6810, loss = 0.99 (381.3 examples/sec; 0.336 sec/batch)
2016-12-08 09:59:00.334358: step 6820, loss = 0.81 (356.2 examples/sec; 0.359 sec/batch)
2016-12-08 09:59:03.547399: step 6830, loss = 0.81 (454.7 examples/sec; 0.282 sec/batch)
2016-12-08 09:59:06.866883: step 6840, loss = 0.98 (376.8 examples/sec; 0.340 sec/batch)
2016-12-08 09:59:10.255107: step 6850, loss = 0.94 (362.9 examples/sec; 0.353 sec/batch)
2016-12-08 09:59:13.579576: step 6860, loss = 1.04 (357.3 examples/sec; 0.358 sec/batch)
2016-12-08 09:59:16.881069: step 6870, loss = 0.97 (391.8 examples/sec; 0.327 sec/batch)
2016-12-08 09:59:20.060997: step 6880, loss = 1.02 (387.1 examples/sec; 0.331 sec/batch)
2016-12-08 09:59:23.266222: step 6890, loss = 1.05 (405.6 examples/sec; 0.316 sec/batch)
2016-12-08 09:59:26.480588: step 6900, loss = 0.93 (402.1 examples/sec; 0.318 sec/batch)
2016-12-08 09:59:29.899292: step 6910, loss = 0.94 (370.1 examples/sec; 0.346 sec/batch)
2016-12-08 09:59:33.057703: step 6920, loss = 0.85 (398.8 examples/sec; 0.321 sec/batch)
2016-12-08 09:59:36.219523: step 6930, loss = 0.98 (430.8 examples/sec; 0.297 sec/batch)
2016-12-08 09:59:39.439588: step 6940, loss = 0.88 (422.0 examples/sec; 0.303 sec/batch)
2016-12-08 09:59:42.600406: step 6950, loss = 0.97 (407.5 examples/sec; 0.314 sec/batch)
2016-12-08 09:59:45.806247: step 6960, loss = 0.91 (391.5 examples/sec; 0.327 sec/batch)
2016-12-08 09:59:48.873352: step 6970, loss = 0.99 (409.8 examples/sec; 0.312 sec/batch)
2016-12-08 09:59:52.161650: step 6980, loss = 0.83 (390.0 examples/sec; 0.328 sec/batch)
2016-12-08 09:59:55.250002: step 6990, loss = 0.98 (470.8 examples/sec; 0.272 sec/batch)
2016-12-08 09:59:58.144078: step 7000, loss = 0.99 (380.9 examples/sec; 0.336 sec/batch)
2016-12-08 10:00:02.198368: step 7010, loss = 1.07 (448.0 examples/sec; 0.286 sec/batch)
2016-12-08 10:00:05.437814: step 7020, loss = 0.84 (347.1 examples/sec; 0.369 sec/batch)
2016-12-08 10:00:08.583149: step 7030, loss = 0.90 (399.7 examples/sec; 0.320 sec/batch)
2016-12-08 10:00:11.868653: step 7040, loss = 0.85 (402.1 examples/sec; 0.318 sec/batch)
2016-12-08 10:00:15.175445: step 7050, loss = 0.85 (389.4 examples/sec; 0.329 sec/batch)
2016-12-08 10:00:18.142095: step 7060, loss = 0.84 (460.9 examples/sec; 0.278 sec/batch)
2016-12-08 10:00:21.265367: step 7070, loss = 0.94 (390.4 examples/sec; 0.328 sec/batch)
2016-12-08 10:00:24.495748: step 7080, loss = 0.92 (405.5 examples/sec; 0.316 sec/batch)
2016-12-08 10:00:27.841085: step 7090, loss = 0.99 (385.6 examples/sec; 0.332 sec/batch)
2016-12-08 10:00:31.064793: step 7100, loss = 0.92 (390.9 examples/sec; 0.327 sec/batch)
2016-12-08 10:00:34.597493: step 7110, loss = 0.95 (427.8 examples/sec; 0.299 sec/batch)
2016-12-08 10:00:37.883858: step 7120, loss = 1.04 (324.1 examples/sec; 0.395 sec/batch)
2016-12-08 10:00:41.045862: step 7130, loss = 0.92 (437.3 examples/sec; 0.293 sec/batch)
2016-12-08 10:00:44.189297: step 7140, loss = 0.76 (379.8 examples/sec; 0.337 sec/batch)
2016-12-08 10:00:47.318176: step 7150, loss = 0.97 (406.2 examples/sec; 0.315 sec/batch)
2016-12-08 10:00:50.567312: step 7160, loss = 0.93 (441.4 examples/sec; 0.290 sec/batch)
2016-12-08 10:00:53.694854: step 7170, loss = 1.00 (417.5 examples/sec; 0.307 sec/batch)
2016-12-08 10:00:56.860810: step 7180, loss = 1.05 (399.5 examples/sec; 0.320 sec/batch)
2016-12-08 10:01:00.066216: step 7190, loss = 0.82 (407.9 examples/sec; 0.314 sec/batch)
2016-12-08 10:01:03.186781: step 7200, loss = 0.91 (371.6 examples/sec; 0.344 sec/batch)
2016-12-08 10:01:06.770174: step 7210, loss = 0.91 (374.4 examples/sec; 0.342 sec/batch)
2016-12-08 10:01:09.683963: step 7220, loss = 0.88 (443.3 examples/sec; 0.289 sec/batch)
2016-12-08 10:01:12.871694: step 7230, loss = 0.79 (396.1 examples/sec; 0.323 sec/batch)
2016-12-08 10:01:16.027911: step 7240, loss = 0.97 (373.1 examples/sec; 0.343 sec/batch)
2016-12-08 10:01:19.321754: step 7250, loss = 1.05 (408.4 examples/sec; 0.313 sec/batch)
2016-12-08 10:01:22.627466: step 7260, loss = 0.91 (369.3 examples/sec; 0.347 sec/batch)
2016-12-08 10:01:25.772106: step 7270, loss = 0.89 (410.5 examples/sec; 0.312 sec/batch)
2016-12-08 10:01:29.302134: step 7280, loss = 0.93 (368.1 examples/sec; 0.348 sec/batch)
2016-12-08 10:01:32.490253: step 7290, loss = 0.79 (389.7 examples/sec; 0.328 sec/batch)
2016-12-08 10:01:35.641824: step 7300, loss = 0.81 (389.7 examples/sec; 0.328 sec/batch)
2016-12-08 10:01:39.208361: step 7310, loss = 0.95 (375.2 examples/sec; 0.341 sec/batch)
2016-12-08 10:01:42.345657: step 7320, loss = 0.97 (430.7 examples/sec; 0.297 sec/batch)
2016-12-08 10:01:45.379791: step 7330, loss = 0.83 (383.3 examples/sec; 0.334 sec/batch)
2016-12-08 10:01:48.646368: step 7340, loss = 0.93 (357.9 examples/sec; 0.358 sec/batch)
2016-12-08 10:01:51.952151: step 7350, loss = 0.90 (438.3 examples/sec; 0.292 sec/batch)
2016-12-08 10:01:55.207353: step 7360, loss = 0.87 (373.1 examples/sec; 0.343 sec/batch)
2016-12-08 10:01:58.466458: step 7370, loss = 0.84 (399.8 examples/sec; 0.320 sec/batch)
2016-12-08 10:02:01.686350: step 7380, loss = 0.98 (448.1 examples/sec; 0.286 sec/batch)
2016-12-08 10:02:04.877436: step 7390, loss = 0.91 (376.0 examples/sec; 0.340 sec/batch)
2016-12-08 10:02:08.089119: step 7400, loss = 0.93 (474.2 examples/sec; 0.270 sec/batch)
2016-12-08 10:02:11.648439: step 7410, loss = 0.85 (446.5 examples/sec; 0.287 sec/batch)
2016-12-08 10:02:14.702035: step 7420, loss = 0.92 (413.8 examples/sec; 0.309 sec/batch)
2016-12-08 10:02:18.018755: step 7430, loss = 1.10 (356.3 examples/sec; 0.359 sec/batch)
2016-12-08 10:02:21.160468: step 7440, loss = 0.90 (417.3 examples/sec; 0.307 sec/batch)
2016-12-08 10:02:24.443261: step 7450, loss = 0.86 (404.7 examples/sec; 0.316 sec/batch)
2016-12-08 10:02:27.881502: step 7460, loss = 1.01 (341.5 examples/sec; 0.375 sec/batch)
2016-12-08 10:02:31.269095: step 7470, loss = 0.83 (453.2 examples/sec; 0.282 sec/batch)
2016-12-08 10:02:34.647182: step 7480, loss = 0.91 (384.3 examples/sec; 0.333 sec/batch)
2016-12-08 10:02:37.901988: step 7490, loss = 1.02 (371.6 examples/sec; 0.344 sec/batch)
2016-12-08 10:02:41.115155: step 7500, loss = 0.83 (390.9 examples/sec; 0.327 sec/batch)
2016-12-08 10:02:44.558258: step 7510, loss = 0.82 (398.7 examples/sec; 0.321 sec/batch)
2016-12-08 10:02:47.774552: step 7520, loss = 1.00 (403.4 examples/sec; 0.317 sec/batch)
2016-12-08 10:02:51.038210: step 7530, loss = 0.80 (395.1 examples/sec; 0.324 sec/batch)
2016-12-08 10:02:54.269799: step 7540, loss = 0.97 (345.9 examples/sec; 0.370 sec/batch)
2016-12-08 10:02:57.457434: step 7550, loss = 0.95 (387.3 examples/sec; 0.330 sec/batch)
2016-12-08 10:03:00.816503: step 7560, loss = 1.17 (371.4 examples/sec; 0.345 sec/batch)
2016-12-08 10:03:03.969416: step 7570, loss = 1.06 (444.6 examples/sec; 0.288 sec/batch)
2016-12-08 10:03:07.279632: step 7580, loss = 0.96 (348.2 examples/sec; 0.368 sec/batch)
2016-12-08 10:03:10.448180: step 7590, loss = 0.93 (394.5 examples/sec; 0.324 sec/batch)
2016-12-08 10:03:13.806424: step 7600, loss = 0.87 (393.0 examples/sec; 0.326 sec/batch)
2016-12-08 10:03:17.410198: step 7610, loss = 1.07 (416.3 examples/sec; 0.307 sec/batch)
2016-12-08 10:03:20.714583: step 7620, loss = 0.84 (398.3 examples/sec; 0.321 sec/batch)
2016-12-08 10:03:23.935864: step 7630, loss = 1.03 (434.0 examples/sec; 0.295 sec/batch)
2016-12-08 10:03:26.979414: step 7640, loss = 1.02 (423.1 examples/sec; 0.303 sec/batch)
2016-12-08 10:03:30.296032: step 7650, loss = 0.87 (400.8 examples/sec; 0.319 sec/batch)
2016-12-08 10:03:33.496278: step 7660, loss = 0.92 (343.3 examples/sec; 0.373 sec/batch)
2016-12-08 10:03:36.813283: step 7670, loss = 0.64 (356.5 examples/sec; 0.359 sec/batch)
2016-12-08 10:03:40.086016: step 7680, loss = 0.83 (438.3 examples/sec; 0.292 sec/batch)
2016-12-08 10:03:43.131876: step 7690, loss = 0.90 (521.2 examples/sec; 0.246 sec/batch)
2016-12-08 10:03:46.386143: step 7700, loss = 0.98 (417.4 examples/sec; 0.307 sec/batch)
2016-12-08 10:03:49.987893: step 7710, loss = 1.03 (405.1 examples/sec; 0.316 sec/batch)
2016-12-08 10:03:53.179072: step 7720, loss = 0.91 (449.0 examples/sec; 0.285 sec/batch)
2016-12-08 10:03:56.499331: step 7730, loss = 0.85 (381.7 examples/sec; 0.335 sec/batch)
2016-12-08 10:03:59.801628: step 7740, loss = 0.82 (380.5 examples/sec; 0.336 sec/batch)
2016-12-08 10:04:03.110962: step 7750, loss = 0.73 (364.0 examples/sec; 0.352 sec/batch)
2016-12-08 10:04:06.390162: step 7760, loss = 0.89 (404.7 examples/sec; 0.316 sec/batch)
2016-12-08 10:04:09.683017: step 7770, loss = 0.92 (401.9 examples/sec; 0.318 sec/batch)
2016-12-08 10:04:12.914235: step 7780, loss = 0.88 (445.1 examples/sec; 0.288 sec/batch)
2016-12-08 10:04:16.152151: step 7790, loss = 1.12 (347.1 examples/sec; 0.369 sec/batch)
2016-12-08 10:04:19.478285: step 7800, loss = 0.87 (464.8 examples/sec; 0.275 sec/batch)
2016-12-08 10:04:23.012582: step 7810, loss = 0.90 (345.9 examples/sec; 0.370 sec/batch)
2016-12-08 10:04:26.515721: step 7820, loss = 0.89 (368.8 examples/sec; 0.347 sec/batch)
2016-12-08 10:04:29.832874: step 7830, loss = 1.01 (361.3 examples/sec; 0.354 sec/batch)
2016-12-08 10:04:33.069691: step 7840, loss = 1.27 (369.5 examples/sec; 0.346 sec/batch)
2016-12-08 10:04:36.423644: step 7850, loss = 0.90 (355.0 examples/sec; 0.361 sec/batch)
2016-12-08 10:04:39.756834: step 7860, loss = 0.96 (356.5 examples/sec; 0.359 sec/batch)
2016-12-08 10:04:42.961720: step 7870, loss = 0.89 (411.2 examples/sec; 0.311 sec/batch)
2016-12-08 10:04:46.208769: step 7880, loss = 0.94 (424.3 examples/sec; 0.302 sec/batch)
2016-12-08 10:04:49.378956: step 7890, loss = 1.06 (375.1 examples/sec; 0.341 sec/batch)
2016-12-08 10:04:52.564387: step 7900, loss = 0.87 (380.4 examples/sec; 0.336 sec/batch)
2016-12-08 10:04:55.957522: step 7910, loss = 0.83 (334.5 examples/sec; 0.383 sec/batch)
2016-12-08 10:04:59.216949: step 7920, loss = 1.00 (336.3 examples/sec; 0.381 sec/batch)
2016-12-08 10:05:02.441116: step 7930, loss = 1.07 (394.9 examples/sec; 0.324 sec/batch)
2016-12-08 10:05:05.624479: step 7940, loss = 0.86 (397.8 examples/sec; 0.322 sec/batch)
2016-12-08 10:05:08.798163: step 7950, loss = 0.99 (392.0 examples/sec; 0.327 sec/batch)
2016-12-08 10:05:12.229890: step 7960, loss = 0.95 (374.7 examples/sec; 0.342 sec/batch)
2016-12-08 10:05:15.429329: step 7970, loss = 0.92 (390.3 examples/sec; 0.328 sec/batch)
2016-12-08 10:05:18.749061: step 7980, loss = 1.02 (383.3 examples/sec; 0.334 sec/batch)
2016-12-08 10:05:22.074329: step 7990, loss = 1.01 (396.3 examples/sec; 0.323 sec/batch)
2016-12-08 10:05:25.322802: step 8000, loss = 0.87 (360.3 examples/sec; 0.355 sec/batch)
2016-12-08 10:05:29.721692: step 8010, loss = 0.89 (344.1 examples/sec; 0.372 sec/batch)
2016-12-08 10:05:32.734554: step 8020, loss = 0.95 (392.1 examples/sec; 0.326 sec/batch)
2016-12-08 10:05:35.995482: step 8030, loss = 1.02 (399.8 examples/sec; 0.320 sec/batch)
2016-12-08 10:05:39.196473: step 8040, loss = 1.06 (362.3 examples/sec; 0.353 sec/batch)
2016-12-08 10:05:42.254454: step 8050, loss = 0.83 (383.5 examples/sec; 0.334 sec/batch)
2016-12-08 10:05:45.340352: step 8060, loss = 1.04 (383.8 examples/sec; 0.333 sec/batch)
2016-12-08 10:05:48.792688: step 8070, loss = 1.04 (373.0 examples/sec; 0.343 sec/batch)
2016-12-08 10:05:51.821448: step 8080, loss = 1.11 (394.6 examples/sec; 0.324 sec/batch)
2016-12-08 10:05:55.015993: step 8090, loss = 0.85 (388.3 examples/sec; 0.330 sec/batch)
2016-12-08 10:05:58.324650: step 8100, loss = 0.75 (357.0 examples/sec; 0.359 sec/batch)
2016-12-08 10:06:01.964473: step 8110, loss = 0.85 (367.8 examples/sec; 0.348 sec/batch)
2016-12-08 10:06:05.159482: step 8120, loss = 0.87 (421.8 examples/sec; 0.303 sec/batch)
2016-12-08 10:06:08.354365: step 8130, loss = 0.81 (369.8 examples/sec; 0.346 sec/batch)
2016-12-08 10:06:11.605755: step 8140, loss = 0.97 (344.3 examples/sec; 0.372 sec/batch)
2016-12-08 10:06:14.799135: step 8150, loss = 0.78 (383.2 examples/sec; 0.334 sec/batch)
2016-12-08 10:06:18.016995: step 8160, loss = 0.97 (397.6 examples/sec; 0.322 sec/batch)
2016-12-08 10:06:21.095207: step 8170, loss = 1.12 (381.4 examples/sec; 0.336 sec/batch)
2016-12-08 10:06:24.097306: step 8180, loss = 0.80 (445.5 examples/sec; 0.287 sec/batch)
2016-12-08 10:06:27.227492: step 8190, loss = 1.10 (401.3 examples/sec; 0.319 sec/batch)
2016-12-08 10:06:30.515976: step 8200, loss = 0.69 (379.7 examples/sec; 0.337 sec/batch)
2016-12-08 10:06:34.192560: step 8210, loss = 0.93 (406.2 examples/sec; 0.315 sec/batch)
2016-12-08 10:06:37.469751: step 8220, loss = 0.98 (395.5 examples/sec; 0.324 sec/batch)
2016-12-08 10:06:40.648643: step 8230, loss = 0.92 (431.4 examples/sec; 0.297 sec/batch)
2016-12-08 10:06:43.676059: step 8240, loss = 0.90 (395.9 examples/sec; 0.323 sec/batch)
2016-12-08 10:06:46.842411: step 8250, loss = 1.15 (380.7 examples/sec; 0.336 sec/batch)
2016-12-08 10:06:50.104679: step 8260, loss = 1.08 (370.8 examples/sec; 0.345 sec/batch)
2016-12-08 10:06:53.281683: step 8270, loss = 0.99 (410.0 examples/sec; 0.312 sec/batch)
2016-12-08 10:06:56.368200: step 8280, loss = 0.94 (389.0 examples/sec; 0.329 sec/batch)
2016-12-08 10:06:59.529825: step 8290, loss = 0.96 (441.5 examples/sec; 0.290 sec/batch)
2016-12-08 10:07:02.898507: step 8300, loss = 0.79 (421.7 examples/sec; 0.304 sec/batch)
2016-12-08 10:07:06.455115: step 8310, loss = 1.01 (380.7 examples/sec; 0.336 sec/batch)
2016-12-08 10:07:09.801652: step 8320, loss = 0.85 (366.8 examples/sec; 0.349 sec/batch)
2016-12-08 10:07:12.952020: step 8330, loss = 0.82 (372.4 examples/sec; 0.344 sec/batch)
2016-12-08 10:07:16.235723: step 8340, loss = 0.95 (422.5 examples/sec; 0.303 sec/batch)
2016-12-08 10:07:19.533861: step 8350, loss = 0.90 (398.2 examples/sec; 0.321 sec/batch)
2016-12-08 10:07:22.595862: step 8360, loss = 0.80 (401.9 examples/sec; 0.318 sec/batch)
2016-12-08 10:07:25.823337: step 8370, loss = 0.89 (399.9 examples/sec; 0.320 sec/batch)
2016-12-08 10:07:29.000019: step 8380, loss = 0.99 (453.0 examples/sec; 0.283 sec/batch)
2016-12-08 10:07:32.118203: step 8390, loss = 0.77 (403.8 examples/sec; 0.317 sec/batch)
2016-12-08 10:07:35.387937: step 8400, loss = 0.82 (422.3 examples/sec; 0.303 sec/batch)
2016-12-08 10:07:38.902398: step 8410, loss = 0.96 (375.4 examples/sec; 0.341 sec/batch)
2016-12-08 10:07:42.198759: step 8420, loss = 0.96 (356.2 examples/sec; 0.359 sec/batch)
2016-12-08 10:07:45.308526: step 8430, loss = 0.90 (370.9 examples/sec; 0.345 sec/batch)
2016-12-08 10:07:48.596780: step 8440, loss = 0.88 (373.5 examples/sec; 0.343 sec/batch)
2016-12-08 10:07:51.760247: step 8450, loss = 0.84 (431.0 examples/sec; 0.297 sec/batch)
2016-12-08 10:07:55.026255: step 8460, loss = 0.90 (457.3 examples/sec; 0.280 sec/batch)
2016-12-08 10:07:58.265330: step 8470, loss = 0.97 (451.1 examples/sec; 0.284 sec/batch)
2016-12-08 10:08:01.518209: step 8480, loss = 0.85 (420.0 examples/sec; 0.305 sec/batch)
2016-12-08 10:08:04.911184: step 8490, loss = 0.89 (389.3 examples/sec; 0.329 sec/batch)
2016-12-08 10:08:08.363138: step 8500, loss = 1.03 (424.0 examples/sec; 0.302 sec/batch)
2016-12-08 10:08:11.849264: step 8510, loss = 1.20 (400.2 examples/sec; 0.320 sec/batch)
2016-12-08 10:08:15.305951: step 8520, loss = 0.80 (357.6 examples/sec; 0.358 sec/batch)
2016-12-08 10:08:18.479752: step 8530, loss = 0.79 (410.5 examples/sec; 0.312 sec/batch)
2016-12-08 10:08:21.631208: step 8540, loss = 0.93 (375.4 examples/sec; 0.341 sec/batch)
2016-12-08 10:08:24.616363: step 8550, loss = 0.97 (395.3 examples/sec; 0.324 sec/batch)
2016-12-08 10:08:27.806889: step 8560, loss = 0.94 (448.8 examples/sec; 0.285 sec/batch)
2016-12-08 10:08:31.025834: step 8570, loss = 0.99 (411.4 examples/sec; 0.311 sec/batch)
2016-12-08 10:08:34.341164: step 8580, loss = 0.86 (461.1 examples/sec; 0.278 sec/batch)
2016-12-08 10:08:37.525077: step 8590, loss = 0.87 (361.1 examples/sec; 0.354 sec/batch)
2016-12-08 10:08:40.659978: step 8600, loss = 0.98 (381.1 examples/sec; 0.336 sec/batch)
2016-12-08 10:08:44.368373: step 8610, loss = 0.90 (381.7 examples/sec; 0.335 sec/batch)
2016-12-08 10:08:47.583330: step 8620, loss = 1.06 (366.0 examples/sec; 0.350 sec/batch)
2016-12-08 10:08:50.830073: step 8630, loss = 0.98 (365.3 examples/sec; 0.350 sec/batch)
2016-12-08 10:08:54.120281: step 8640, loss = 0.84 (388.2 examples/sec; 0.330 sec/batch)
2016-12-08 10:08:57.342795: step 8650, loss = 0.94 (442.0 examples/sec; 0.290 sec/batch)
2016-12-08 10:09:00.689011: step 8660, loss = 1.12 (382.4 examples/sec; 0.335 sec/batch)
2016-12-08 10:09:03.864460: step 8670, loss = 0.89 (413.0 examples/sec; 0.310 sec/batch)
2016-12-08 10:09:06.950778: step 8680, loss = 1.01 (429.1 examples/sec; 0.298 sec/batch)
2016-12-08 10:09:10.162641: step 8690, loss = 1.09 (399.3 examples/sec; 0.321 sec/batch)
2016-12-08 10:09:13.432027: step 8700, loss = 0.86 (434.1 examples/sec; 0.295 sec/batch)
2016-12-08 10:09:16.847639: step 8710, loss = 0.85 (371.1 examples/sec; 0.345 sec/batch)
2016-12-08 10:09:20.110914: step 8720, loss = 0.87 (376.0 examples/sec; 0.340 sec/batch)
2016-12-08 10:09:23.284829: step 8730, loss = 0.81 (467.7 examples/sec; 0.274 sec/batch)
2016-12-08 10:09:26.557664: step 8740, loss = 0.89 (372.9 examples/sec; 0.343 sec/batch)
2016-12-08 10:09:29.594675: step 8750, loss = 0.81 (433.5 examples/sec; 0.295 sec/batch)
2016-12-08 10:09:32.824335: step 8760, loss = 0.83 (422.7 examples/sec; 0.303 sec/batch)
2016-12-08 10:09:36.082066: step 8770, loss = 1.13 (406.5 examples/sec; 0.315 sec/batch)
2016-12-08 10:09:39.284035: step 8780, loss = 1.02 (404.4 examples/sec; 0.317 sec/batch)
2016-12-08 10:09:42.445771: step 8790, loss = 0.94 (479.7 examples/sec; 0.267 sec/batch)
2016-12-08 10:09:45.620915: step 8800, loss = 0.89 (390.0 examples/sec; 0.328 sec/batch)
2016-12-08 10:09:49.265602: step 8810, loss = 0.86 (387.4 examples/sec; 0.330 sec/batch)
2016-12-08 10:09:52.525877: step 8820, loss = 0.98 (369.2 examples/sec; 0.347 sec/batch)
2016-12-08 10:09:55.740196: step 8830, loss = 0.96 (371.2 examples/sec; 0.345 sec/batch)
2016-12-08 10:09:58.790734: step 8840, loss = 0.79 (422.0 examples/sec; 0.303 sec/batch)
2016-12-08 10:10:02.104633: step 8850, loss = 0.87 (360.2 examples/sec; 0.355 sec/batch)
2016-12-08 10:10:05.441238: step 8860, loss = 0.84 (409.0 examples/sec; 0.313 sec/batch)
2016-12-08 10:10:08.667109: step 8870, loss = 0.77 (393.6 examples/sec; 0.325 sec/batch)
2016-12-08 10:10:11.805566: step 8880, loss = 1.04 (385.0 examples/sec; 0.333 sec/batch)
2016-12-08 10:10:15.268708: step 8890, loss = 0.96 (375.0 examples/sec; 0.341 sec/batch)
2016-12-08 10:10:18.522519: step 8900, loss = 0.89 (395.3 examples/sec; 0.324 sec/batch)
2016-12-08 10:10:22.044100: step 8910, loss = 0.83 (441.4 examples/sec; 0.290 sec/batch)
2016-12-08 10:10:25.243854: step 8920, loss = 0.84 (409.6 examples/sec; 0.313 sec/batch)
2016-12-08 10:10:28.422593: step 8930, loss = 0.98 (410.8 examples/sec; 0.312 sec/batch)
2016-12-08 10:10:31.737278: step 8940, loss = 0.71 (440.0 examples/sec; 0.291 sec/batch)
2016-12-08 10:10:34.915891: step 8950, loss = 0.83 (454.7 examples/sec; 0.281 sec/batch)
2016-12-08 10:10:38.164588: step 8960, loss = 0.89 (430.1 examples/sec; 0.298 sec/batch)
2016-12-08 10:10:41.383627: step 8970, loss = 0.81 (348.0 examples/sec; 0.368 sec/batch)
2016-12-08 10:10:44.412431: step 8980, loss = 0.87 (419.2 examples/sec; 0.305 sec/batch)
2016-12-08 10:10:47.690933: step 8990, loss = 0.80 (391.1 examples/sec; 0.327 sec/batch)
2016-12-08 10:10:50.882396: step 9000, loss = 0.82 (384.2 examples/sec; 0.333 sec/batch)
2016-12-08 10:10:55.031866: step 9010, loss = 1.04 (461.3 examples/sec; 0.277 sec/batch)
2016-12-08 10:10:58.361600: step 9020, loss = 0.85 (398.9 examples/sec; 0.321 sec/batch)
2016-12-08 10:11:01.768570: step 9030, loss = 0.81 (368.0 examples/sec; 0.348 sec/batch)
2016-12-08 10:11:04.908656: step 9040, loss = 0.88 (412.6 examples/sec; 0.310 sec/batch)
2016-12-08 10:11:08.000232: step 9050, loss = 0.83 (432.4 examples/sec; 0.296 sec/batch)
2016-12-08 10:11:11.090676: step 9060, loss = 1.06 (451.6 examples/sec; 0.283 sec/batch)
2016-12-08 10:11:14.327574: step 9070, loss = 0.73 (403.5 examples/sec; 0.317 sec/batch)
2016-12-08 10:11:17.620795: step 9080, loss = 0.90 (421.4 examples/sec; 0.304 sec/batch)
2016-12-08 10:11:20.899646: step 9090, loss = 0.86 (368.9 examples/sec; 0.347 sec/batch)
2016-12-08 10:11:24.068912: step 9100, loss = 0.89 (386.4 examples/sec; 0.331 sec/batch)
2016-12-08 10:11:27.854056: step 9110, loss = 0.93 (384.3 examples/sec; 0.333 sec/batch)
2016-12-08 10:11:31.014787: step 9120, loss = 1.02 (421.1 examples/sec; 0.304 sec/batch)
2016-12-08 10:11:34.146528: step 9130, loss = 0.84 (424.3 examples/sec; 0.302 sec/batch)
2016-12-08 10:11:37.441082: step 9140, loss = 0.83 (366.9 examples/sec; 0.349 sec/batch)
2016-12-08 10:11:40.616488: step 9150, loss = 1.04 (384.7 examples/sec; 0.333 sec/batch)
2016-12-08 10:11:43.858213: step 9160, loss = 0.85 (375.7 examples/sec; 0.341 sec/batch)
2016-12-08 10:11:46.934488: step 9170, loss = 0.87 (388.1 examples/sec; 0.330 sec/batch)
2016-12-08 10:11:50.273750: step 9180, loss = 0.94 (383.4 examples/sec; 0.334 sec/batch)
2016-12-08 10:11:53.508552: step 9190, loss = 0.80 (371.4 examples/sec; 0.345 sec/batch)
2016-12-08 10:11:56.849374: step 9200, loss = 0.95 (360.8 examples/sec; 0.355 sec/batch)
2016-12-08 10:12:00.467951: step 9210, loss = 0.97 (357.6 examples/sec; 0.358 sec/batch)
2016-12-08 10:12:03.553814: step 9220, loss = 1.03 (438.9 examples/sec; 0.292 sec/batch)
2016-12-08 10:12:06.800757: step 9230, loss = 0.99 (352.4 examples/sec; 0.363 sec/batch)
2016-12-08 10:12:09.998880: step 9240, loss = 0.84 (343.9 examples/sec; 0.372 sec/batch)
2016-12-08 10:12:13.250958: step 9250, loss = 0.80 (400.6 examples/sec; 0.320 sec/batch)
2016-12-08 10:12:16.435883: step 9260, loss = 0.77 (442.7 examples/sec; 0.289 sec/batch)
2016-12-08 10:12:19.601170: step 9270, loss = 0.83 (425.0 examples/sec; 0.301 sec/batch)
2016-12-08 10:12:22.785859: step 9280, loss = 0.85 (419.2 examples/sec; 0.305 sec/batch)
2016-12-08 10:12:25.953432: step 9290, loss = 0.81 (381.6 examples/sec; 0.335 sec/batch)
2016-12-08 10:12:29.103824: step 9300, loss = 0.83 (373.0 examples/sec; 0.343 sec/batch)
2016-12-08 10:12:32.583053: step 9310, loss = 0.97 (485.6 examples/sec; 0.264 sec/batch)
2016-12-08 10:12:35.806381: step 9320, loss = 0.94 (357.5 examples/sec; 0.358 sec/batch)
2016-12-08 10:12:39.016682: step 9330, loss = 1.07 (402.8 examples/sec; 0.318 sec/batch)
2016-12-08 10:12:42.078230: step 9340, loss = 1.10 (399.9 examples/sec; 0.320 sec/batch)
2016-12-08 10:12:45.084228: step 9350, loss = 0.99 (374.2 examples/sec; 0.342 sec/batch)
2016-12-08 10:12:48.308634: step 9360, loss = 0.72 (406.3 examples/sec; 0.315 sec/batch)
2016-12-08 10:12:51.401357: step 9370, loss = 0.92 (449.1 examples/sec; 0.285 sec/batch)
2016-12-08 10:12:54.747699: step 9380, loss = 0.75 (382.3 examples/sec; 0.335 sec/batch)
2016-12-08 10:12:57.974025: step 9390, loss = 0.78 (419.7 examples/sec; 0.305 sec/batch)
2016-12-08 10:13:01.242870: step 9400, loss = 1.04 (389.0 examples/sec; 0.329 sec/batch)
2016-12-08 10:13:05.042416: step 9410, loss = 0.81 (442.9 examples/sec; 0.289 sec/batch)
2016-12-08 10:13:08.124266: step 9420, loss = 1.06 (433.5 examples/sec; 0.295 sec/batch)
2016-12-08 10:13:11.390963: step 9430, loss = 0.98 (415.0 examples/sec; 0.308 sec/batch)
2016-12-08 10:13:14.489868: step 9440, loss = 0.97 (400.8 examples/sec; 0.319 sec/batch)
2016-12-08 10:13:17.671638: step 9450, loss = 0.89 (446.1 examples/sec; 0.287 sec/batch)
2016-12-08 10:13:20.770555: step 9460, loss = 0.92 (372.3 examples/sec; 0.344 sec/batch)
2016-12-08 10:13:24.133928: step 9470, loss = 0.73 (362.7 examples/sec; 0.353 sec/batch)
2016-12-08 10:13:27.386925: step 9480, loss = 0.90 (390.9 examples/sec; 0.327 sec/batch)
2016-12-08 10:13:30.536930: step 9490, loss = 0.84 (388.1 examples/sec; 0.330 sec/batch)
2016-12-08 10:13:33.860950: step 9500, loss = 0.70 (374.0 examples/sec; 0.342 sec/batch)
2016-12-08 10:13:37.521928: step 9510, loss = 0.85 (361.2 examples/sec; 0.354 sec/batch)
2016-12-08 10:13:40.650243: step 9520, loss = 0.95 (349.5 examples/sec; 0.366 sec/batch)
2016-12-08 10:13:43.813218: step 9530, loss = 0.93 (421.6 examples/sec; 0.304 sec/batch)
2016-12-08 10:13:47.197167: step 9540, loss = 0.96 (416.4 examples/sec; 0.307 sec/batch)
2016-12-08 10:13:50.511475: step 9550, loss = 0.96 (384.0 examples/sec; 0.333 sec/batch)
2016-12-08 10:13:53.718798: step 9560, loss = 0.77 (356.1 examples/sec; 0.359 sec/batch)
2016-12-08 10:13:56.970840: step 9570, loss = 0.83 (446.6 examples/sec; 0.287 sec/batch)
2016-12-08 10:14:00.239565: step 9580, loss = 0.79 (397.2 examples/sec; 0.322 sec/batch)
2016-12-08 10:14:03.687351: step 9590, loss = 0.80 (391.6 examples/sec; 0.327 sec/batch)
2016-12-08 10:14:06.908630: step 9600, loss = 0.92 (386.1 examples/sec; 0.331 sec/batch)
2016-12-08 10:14:10.495490: step 9610, loss = 0.87 (399.7 examples/sec; 0.320 sec/batch)
2016-12-08 10:14:13.863849: step 9620, loss = 0.94 (342.6 examples/sec; 0.374 sec/batch)
2016-12-08 10:14:17.150152: step 9630, loss = 0.79 (372.2 examples/sec; 0.344 sec/batch)
2016-12-08 10:14:20.409803: step 9640, loss = 0.87 (387.3 examples/sec; 0.330 sec/batch)
2016-12-08 10:14:23.620328: step 9650, loss = 0.95 (372.7 examples/sec; 0.343 sec/batch)
2016-12-08 10:14:26.853270: step 9660, loss = 0.82 (372.1 examples/sec; 0.344 sec/batch)
2016-12-08 10:14:29.950210: step 9670, loss = 0.82 (366.1 examples/sec; 0.350 sec/batch)
2016-12-08 10:14:33.416086: step 9680, loss = 0.98 (323.4 examples/sec; 0.396 sec/batch)
2016-12-08 10:14:36.644010: step 9690, loss = 0.82 (475.0 examples/sec; 0.269 sec/batch)
2016-12-08 10:14:39.806511: step 9700, loss = 0.94 (386.6 examples/sec; 0.331 sec/batch)
2016-12-08 10:14:43.138172: step 9710, loss = 0.81 (386.0 examples/sec; 0.332 sec/batch)
2016-12-08 10:14:46.271375: step 9720, loss = 0.78 (449.0 examples/sec; 0.285 sec/batch)
2016-12-08 10:14:49.370378: step 9730, loss = 0.93 (440.4 examples/sec; 0.291 sec/batch)
2016-12-08 10:14:52.431535: step 9740, loss = 0.88 (444.3 examples/sec; 0.288 sec/batch)
2016-12-08 10:14:55.630531: step 9750, loss = 0.82 (383.0 examples/sec; 0.334 sec/batch)
2016-12-08 10:14:59.053135: step 9760, loss = 1.16 (431.1 examples/sec; 0.297 sec/batch)
2016-12-08 10:15:02.149943: step 9770, loss = 0.82 (352.9 examples/sec; 0.363 sec/batch)
2016-12-08 10:15:05.549829: step 9780, loss = 0.84 (383.2 examples/sec; 0.334 sec/batch)
2016-12-08 10:15:08.824601: step 9790, loss = 1.04 (372.4 examples/sec; 0.344 sec/batch)
2016-12-08 10:15:11.939391: step 9800, loss = 0.85 (462.9 examples/sec; 0.277 sec/batch)
2016-12-08 10:15:15.371278: step 9810, loss = 0.88 (365.3 examples/sec; 0.350 sec/batch)
2016-12-08 10:15:18.702431: step 9820, loss = 0.84 (385.9 examples/sec; 0.332 sec/batch)
2016-12-08 10:15:21.795235: step 9830, loss = 1.01 (462.4 examples/sec; 0.277 sec/batch)
2016-12-08 10:15:24.874961: step 9840, loss = 1.02 (444.5 examples/sec; 0.288 sec/batch)
2016-12-08 10:15:28.098415: step 9850, loss = 0.98 (395.8 examples/sec; 0.323 sec/batch)
2016-12-08 10:15:31.185707: step 9860, loss = 0.81 (404.1 examples/sec; 0.317 sec/batch)
2016-12-08 10:15:34.288713: step 9870, loss = 0.85 (406.5 examples/sec; 0.315 sec/batch)
2016-12-08 10:15:37.449935: step 9880, loss = 0.89 (404.8 examples/sec; 0.316 sec/batch)
2016-12-08 10:15:40.708086: step 9890, loss = 0.96 (368.9 examples/sec; 0.347 sec/batch)
2016-12-08 10:15:43.918431: step 9900, loss = 0.95 (369.0 examples/sec; 0.347 sec/batch)
2016-12-08 10:15:47.513212: step 9910, loss = 1.08 (439.0 examples/sec; 0.292 sec/batch)
2016-12-08 10:15:50.636307: step 9920, loss = 1.09 (374.1 examples/sec; 0.342 sec/batch)
2016-12-08 10:15:53.976247: step 9930, loss = 0.92 (391.8 examples/sec; 0.327 sec/batch)
2016-12-08 10:15:57.196542: step 9940, loss = 0.88 (466.9 examples/sec; 0.274 sec/batch)
2016-12-08 10:16:00.248463: step 9950, loss = 0.82 (473.3 examples/sec; 0.270 sec/batch)
2016-12-08 10:16:03.476803: step 9960, loss = 1.00 (415.7 examples/sec; 0.308 sec/batch)
2016-12-08 10:16:06.532238: step 9970, loss = 0.93 (320.3 examples/sec; 0.400 sec/batch)
2016-12-08 10:16:09.878982: step 9980, loss = 0.77 (360.8 examples/sec; 0.355 sec/batch)
2016-12-08 10:16:13.019450: step 9990, loss = 0.90 (416.0 examples/sec; 0.308 sec/batch)
2016-12-08 10:16:16.143801: step 10000, loss = 1.01 (372.4 examples/sec; 0.344 sec/batch)
2016-12-08 10:16:20.374546: step 10010, loss = 1.04 (411.1 examples/sec; 0.311 sec/batch)
2016-12-08 10:16:23.442970: step 10020, loss = 0.84 (456.6 examples/sec; 0.280 sec/batch)
2016-12-08 10:16:26.679566: step 10030, loss = 1.18 (413.5 examples/sec; 0.310 sec/batch)
2016-12-08 10:16:29.958809: step 10040, loss = 0.90 (408.5 examples/sec; 0.313 sec/batch)
2016-12-08 10:16:33.320290: step 10050, loss = 0.91 (433.2 examples/sec; 0.295 sec/batch)
2016-12-08 10:16:36.555349: step 10060, loss = 1.16 (407.3 examples/sec; 0.314 sec/batch)
2016-12-08 10:16:39.894294: step 10070, loss = 0.93 (356.3 examples/sec; 0.359 sec/batch)
2016-12-08 10:16:43.041390: step 10080, loss = 1.09 (393.1 examples/sec; 0.326 sec/batch)
2016-12-08 10:16:46.358808: step 10090, loss = 0.95 (423.8 examples/sec; 0.302 sec/batch)
2016-12-08 10:16:49.614406: step 10100, loss = 1.10 (329.4 examples/sec; 0.389 sec/batch)
2016-12-08 10:16:53.076492: step 10110, loss = 0.79 (403.4 examples/sec; 0.317 sec/batch)
2016-12-08 10:16:56.313027: step 10120, loss = 0.81 (401.6 examples/sec; 0.319 sec/batch)
2016-12-08 10:16:59.491841: step 10130, loss = 0.87 (363.4 examples/sec; 0.352 sec/batch)
2016-12-08 10:17:02.821861: step 10140, loss = 0.93 (389.2 examples/sec; 0.329 sec/batch)
2016-12-08 10:17:05.930324: step 10150, loss = 0.81 (418.9 examples/sec; 0.306 sec/batch)
2016-12-08 10:17:09.081257: step 10160, loss = 0.93 (365.1 examples/sec; 0.351 sec/batch)
2016-12-08 10:17:12.199649: step 10170, loss = 0.91 (427.9 examples/sec; 0.299 sec/batch)
2016-12-08 10:17:15.462943: step 10180, loss = 0.89 (381.3 examples/sec; 0.336 sec/batch)
2016-12-08 10:17:18.802894: step 10190, loss = 0.88 (393.4 examples/sec; 0.325 sec/batch)
2016-12-08 10:17:22.079562: step 10200, loss = 0.70 (354.8 examples/sec; 0.361 sec/batch)
2016-12-08 10:17:25.608994: step 10210, loss = 0.92 (452.2 examples/sec; 0.283 sec/batch)
2016-12-08 10:17:28.908889: step 10220, loss = 0.83 (365.1 examples/sec; 0.351 sec/batch)
2016-12-08 10:17:32.155152: step 10230, loss = 1.04 (384.3 examples/sec; 0.333 sec/batch)
2016-12-08 10:17:35.649532: step 10240, loss = 0.96 (417.2 examples/sec; 0.307 sec/batch)
2016-12-08 10:17:38.722868: step 10250, loss = 0.86 (389.3 examples/sec; 0.329 sec/batch)
2016-12-08 10:17:41.937006: step 10260, loss = 0.87 (367.6 examples/sec; 0.348 sec/batch)
2016-12-08 10:17:45.078955: step 10270, loss = 0.99 (426.9 examples/sec; 0.300 sec/batch)
2016-12-08 10:17:48.350612: step 10280, loss = 0.96 (450.5 examples/sec; 0.284 sec/batch)
2016-12-08 10:17:51.699996: step 10290, loss = 0.87 (365.5 examples/sec; 0.350 sec/batch)
2016-12-08 10:17:54.863357: step 10300, loss = 0.77 (449.7 examples/sec; 0.285 sec/batch)
2016-12-08 10:17:58.490281: step 10310, loss = 0.89 (367.4 examples/sec; 0.348 sec/batch)
2016-12-08 10:18:01.590511: step 10320, loss = 0.90 (387.9 examples/sec; 0.330 sec/batch)
2016-12-08 10:18:05.012933: step 10330, loss = 0.85 (366.4 examples/sec; 0.349 sec/batch)
2016-12-08 10:18:08.145974: step 10340, loss = 1.19 (415.6 examples/sec; 0.308 sec/batch)
2016-12-08 10:18:11.255027: step 10350, loss = 0.99 (410.4 examples/sec; 0.312 sec/batch)
2016-12-08 10:18:14.406493: step 10360, loss = 0.79 (386.2 examples/sec; 0.331 sec/batch)
2016-12-08 10:18:17.646436: step 10370, loss = 0.85 (398.0 examples/sec; 0.322 sec/batch)
2016-12-08 10:18:20.846724: step 10380, loss = 0.98 (324.9 examples/sec; 0.394 sec/batch)
2016-12-08 10:18:24.174602: step 10390, loss = 1.07 (396.3 examples/sec; 0.323 sec/batch)
2016-12-08 10:18:27.443554: step 10400, loss = 0.91 (369.9 examples/sec; 0.346 sec/batch)
2016-12-08 10:18:31.039977: step 10410, loss = 0.84 (415.3 examples/sec; 0.308 sec/batch)
2016-12-08 10:18:34.334891: step 10420, loss = 0.91 (394.6 examples/sec; 0.324 sec/batch)
2016-12-08 10:18:37.569820: step 10430, loss = 0.89 (359.2 examples/sec; 0.356 sec/batch)
2016-12-08 10:18:40.718520: step 10440, loss = 1.20 (451.3 examples/sec; 0.284 sec/batch)
2016-12-08 10:18:43.909211: step 10450, loss = 1.04 (393.0 examples/sec; 0.326 sec/batch)
2016-12-08 10:18:47.112357: step 10460, loss = 0.93 (426.6 examples/sec; 0.300 sec/batch)
2016-12-08 10:18:50.399989: step 10470, loss = 0.91 (374.3 examples/sec; 0.342 sec/batch)
2016-12-08 10:18:53.627780: step 10480, loss = 1.01 (376.1 examples/sec; 0.340 sec/batch)
2016-12-08 10:18:57.107210: step 10490, loss = 0.90 (389.4 examples/sec; 0.329 sec/batch)
2016-12-08 10:19:00.499877: step 10500, loss = 0.79 (388.3 examples/sec; 0.330 sec/batch)
2016-12-08 10:19:04.189767: step 10510, loss = 0.95 (413.4 examples/sec; 0.310 sec/batch)
2016-12-08 10:19:07.279612: step 10520, loss = 0.93 (394.0 examples/sec; 0.325 sec/batch)
2016-12-08 10:19:10.500026: step 10530, loss = 0.77 (364.5 examples/sec; 0.351 sec/batch)
2016-12-08 10:19:13.749738: step 10540, loss = 0.85 (379.9 examples/sec; 0.337 sec/batch)
2016-12-08 10:19:16.905388: step 10550, loss = 0.83 (458.5 examples/sec; 0.279 sec/batch)
2016-12-08 10:19:20.049577: step 10560, loss = 0.84 (424.0 examples/sec; 0.302 sec/batch)
2016-12-08 10:19:23.297864: step 10570, loss = 0.83 (467.4 examples/sec; 0.274 sec/batch)
2016-12-08 10:19:26.588305: step 10580, loss = 1.02 (428.1 examples/sec; 0.299 sec/batch)
2016-12-08 10:19:29.839321: step 10590, loss = 0.90 (368.2 examples/sec; 0.348 sec/batch)
2016-12-08 10:19:33.146265: step 10600, loss = 0.91 (439.4 examples/sec; 0.291 sec/batch)
2016-12-08 10:19:36.711931: step 10610, loss = 0.79 (384.3 examples/sec; 0.333 sec/batch)
2016-12-08 10:19:39.867314: step 10620, loss = 0.80 (414.1 examples/sec; 0.309 sec/batch)
2016-12-08 10:19:42.978557: step 10630, loss = 0.94 (389.8 examples/sec; 0.328 sec/batch)
2016-12-08 10:19:46.330918: step 10640, loss = 0.88 (398.3 examples/sec; 0.321 sec/batch)
2016-12-08 10:19:49.798699: step 10650, loss = 0.89 (424.3 examples/sec; 0.302 sec/batch)
2016-12-08 10:19:52.957679: step 10660, loss = 0.79 (351.5 examples/sec; 0.364 sec/batch)
2016-12-08 10:19:56.277464: step 10670, loss = 1.02 (409.7 examples/sec; 0.312 sec/batch)
2016-12-08 10:19:59.513349: step 10680, loss = 0.97 (389.5 examples/sec; 0.329 sec/batch)
2016-12-08 10:20:02.720002: step 10690, loss = 0.72 (392.8 examples/sec; 0.326 sec/batch)
2016-12-08 10:20:06.007333: step 10700, loss = 1.03 (371.9 examples/sec; 0.344 sec/batch)
2016-12-08 10:20:09.556803: step 10710, loss = 0.89 (363.5 examples/sec; 0.352 sec/batch)
2016-12-08 10:20:12.696940: step 10720, loss = 0.85 (386.0 examples/sec; 0.332 sec/batch)
2016-12-08 10:20:15.834105: step 10730, loss = 0.87 (407.0 examples/sec; 0.314 sec/batch)
2016-12-08 10:20:19.114003: step 10740, loss = 1.03 (381.1 examples/sec; 0.336 sec/batch)
2016-12-08 10:20:22.268910: step 10750, loss = 0.78 (359.6 examples/sec; 0.356 sec/batch)
2016-12-08 10:20:25.699862: step 10760, loss = 0.79 (349.0 examples/sec; 0.367 sec/batch)
2016-12-08 10:20:28.871696: step 10770, loss = 0.78 (420.1 examples/sec; 0.305 sec/batch)
2016-12-08 10:20:32.038591: step 10780, loss = 0.85 (398.1 examples/sec; 0.322 sec/batch)
2016-12-08 10:20:35.349149: step 10790, loss = 0.83 (414.3 examples/sec; 0.309 sec/batch)
2016-12-08 10:20:38.663021: step 10800, loss = 1.07 (372.7 examples/sec; 0.343 sec/batch)
2016-12-08 10:20:42.015452: step 10810, loss = 0.79 (416.8 examples/sec; 0.307 sec/batch)
2016-12-08 10:20:45.188694: step 10820, loss = 0.83 (382.5 examples/sec; 0.335 sec/batch)
2016-12-08 10:20:48.263590: step 10830, loss = 0.78 (370.2 examples/sec; 0.346 sec/batch)
2016-12-08 10:20:51.569056: step 10840, loss = 1.06 (356.3 examples/sec; 0.359 sec/batch)
2016-12-08 10:20:54.702328: step 10850, loss = 0.84 (362.9 examples/sec; 0.353 sec/batch)
2016-12-08 10:20:57.913827: step 10860, loss = 0.73 (389.9 examples/sec; 0.328 sec/batch)
2016-12-08 10:21:01.283121: step 10870, loss = 0.80 (400.0 examples/sec; 0.320 sec/batch)
2016-12-08 10:21:04.666940: step 10880, loss = 0.89 (396.9 examples/sec; 0.323 sec/batch)
2016-12-08 10:21:07.811680: step 10890, loss = 0.80 (340.5 examples/sec; 0.376 sec/batch)
2016-12-08 10:21:11.228461: step 10900, loss = 0.85 (355.4 examples/sec; 0.360 sec/batch)
