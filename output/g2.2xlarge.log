I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.7.5 locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: 
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:03.0
Total memory: 4.00GiB
Free memory: 3.95GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3cb7180
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties: 
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:04.0
Total memory: 4.00GiB
Free memory: 3.95GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3f35fa0
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 2 with properties: 
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:05.0
Total memory: 4.00GiB
Free memory: 3.95GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x41b50c0
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 3 with properties: 
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:06.0
Total memory: 4.00GiB
Free memory: 3.95GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 0 to device ordinal 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 0 to device ordinal 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 0 to device ordinal 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 1 to device ordinal 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 1 to device ordinal 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 1 to device ordinal 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 2 to device ordinal 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 2 to device ordinal 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 2 to device ordinal 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 3 to device ordinal 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 3 to device ordinal 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 3 to device ordinal 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 1 2 3 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y N N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 1:   N Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 2:   N N Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 3:   N N N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GRID K520, pci bus id: 0000:00:04.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GRID K520, pci bus id: 0000:00:05.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GRID K520, pci bus id: 0000:00:06.0)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2016-11-29 03:09:25.213976: step 0, loss = 4.68 (5.8 examples/sec; 22.258 sec/batch)
2016-11-29 03:09:27.680759: step 10, loss = 4.63 (700.9 examples/sec; 0.183 sec/batch)
2016-11-29 03:09:29.541712: step 20, loss = 4.48 (684.9 examples/sec; 0.187 sec/batch)
2016-11-29 03:09:31.411493: step 30, loss = 4.42 (686.3 examples/sec; 0.187 sec/batch)
2016-11-29 03:09:33.258350: step 40, loss = 4.38 (684.5 examples/sec; 0.187 sec/batch)
2016-11-29 03:09:35.126076: step 50, loss = 4.26 (681.5 examples/sec; 0.188 sec/batch)
2016-11-29 03:09:36.992839: step 60, loss = 4.28 (740.6 examples/sec; 0.173 sec/batch)
2016-11-29 03:09:38.846357: step 70, loss = 4.21 (684.4 examples/sec; 0.187 sec/batch)
2016-11-29 03:09:40.718706: step 80, loss = 4.10 (626.3 examples/sec; 0.204 sec/batch)
2016-11-29 03:09:42.550534: step 90, loss = 4.02 (689.7 examples/sec; 0.186 sec/batch)
2016-11-29 03:09:44.408239: step 100, loss = 4.09 (701.1 examples/sec; 0.183 sec/batch)
2016-11-29 03:09:46.457672: step 110, loss = 4.19 (689.6 examples/sec; 0.186 sec/batch)
2016-11-29 03:09:48.320279: step 120, loss = 3.98 (688.5 examples/sec; 0.186 sec/batch)
2016-11-29 03:09:50.168660: step 130, loss = 4.06 (691.4 examples/sec; 0.185 sec/batch)
2016-11-29 03:09:52.042824: step 140, loss = 4.03 (685.5 examples/sec; 0.187 sec/batch)
2016-11-29 03:09:53.902365: step 150, loss = 3.97 (704.9 examples/sec; 0.182 sec/batch)
2016-11-29 03:09:55.763672: step 160, loss = 3.97 (699.9 examples/sec; 0.183 sec/batch)
2016-11-29 03:09:57.637359: step 170, loss = 4.03 (701.2 examples/sec; 0.183 sec/batch)
2016-11-29 03:09:59.496145: step 180, loss = 3.79 (695.8 examples/sec; 0.184 sec/batch)
2016-11-29 03:10:01.323137: step 190, loss = 4.23 (690.6 examples/sec; 0.185 sec/batch)
2016-11-29 03:10:03.194036: step 200, loss = 3.88 (717.0 examples/sec; 0.179 sec/batch)
2016-11-29 03:10:05.213090: step 210, loss = 3.77 (707.6 examples/sec; 0.181 sec/batch)
2016-11-29 03:10:07.041650: step 220, loss = 4.12 (703.6 examples/sec; 0.182 sec/batch)
2016-11-29 03:10:08.881332: step 230, loss = 3.81 (677.9 examples/sec; 0.189 sec/batch)
2016-11-29 03:10:10.716028: step 240, loss = 3.62 (686.5 examples/sec; 0.186 sec/batch)
2016-11-29 03:10:12.544137: step 250, loss = 3.69 (684.9 examples/sec; 0.187 sec/batch)
2016-11-29 03:10:14.379664: step 260, loss = 3.83 (691.5 examples/sec; 0.185 sec/batch)
2016-11-29 03:10:16.207460: step 270, loss = 3.66 (817.5 examples/sec; 0.157 sec/batch)
2016-11-29 03:10:18.037559: step 280, loss = 3.55 (685.4 examples/sec; 0.187 sec/batch)
2016-11-29 03:10:19.866986: step 290, loss = 3.66 (690.4 examples/sec; 0.185 sec/batch)
2016-11-29 03:10:21.701250: step 300, loss = 3.62 (698.6 examples/sec; 0.183 sec/batch)
2016-11-29 03:10:23.722276: step 310, loss = 3.61 (716.9 examples/sec; 0.179 sec/batch)
2016-11-29 03:10:25.561725: step 320, loss = 3.60 (692.0 examples/sec; 0.185 sec/batch)
2016-11-29 03:10:27.400375: step 330, loss = 3.48 (696.4 examples/sec; 0.184 sec/batch)
2016-11-29 03:10:29.223408: step 340, loss = 3.70 (675.3 examples/sec; 0.190 sec/batch)
2016-11-29 03:10:31.047537: step 350, loss = 3.59 (704.5 examples/sec; 0.182 sec/batch)
2016-11-29 03:10:32.929766: step 360, loss = 3.37 (664.2 examples/sec; 0.193 sec/batch)
2016-11-29 03:10:34.872670: step 370, loss = 3.54 (624.6 examples/sec; 0.205 sec/batch)
2016-11-29 03:10:36.698111: step 380, loss = 3.40 (682.4 examples/sec; 0.188 sec/batch)
2016-11-29 03:10:38.555936: step 390, loss = 3.41 (673.4 examples/sec; 0.190 sec/batch)
2016-11-29 03:10:40.429943: step 400, loss = 3.40 (663.9 examples/sec; 0.193 sec/batch)
2016-11-29 03:10:42.444769: step 410, loss = 3.17 (685.5 examples/sec; 0.187 sec/batch)
2016-11-29 03:10:44.274163: step 420, loss = 3.36 (695.9 examples/sec; 0.184 sec/batch)
2016-11-29 03:10:46.104061: step 430, loss = 3.31 (696.6 examples/sec; 0.184 sec/batch)
2016-11-29 03:10:47.934937: step 440, loss = 3.47 (721.0 examples/sec; 0.178 sec/batch)
2016-11-29 03:10:49.762295: step 450, loss = 3.08 (713.7 examples/sec; 0.179 sec/batch)
2016-11-29 03:10:51.582810: step 460, loss = 3.14 (700.5 examples/sec; 0.183 sec/batch)
2016-11-29 03:10:53.401811: step 470, loss = 3.25 (712.6 examples/sec; 0.180 sec/batch)
2016-11-29 03:10:55.252695: step 480, loss = 3.16 (624.2 examples/sec; 0.205 sec/batch)
2016-11-29 03:10:57.060848: step 490, loss = 3.03 (692.8 examples/sec; 0.185 sec/batch)
2016-11-29 03:10:58.870882: step 500, loss = 3.05 (705.2 examples/sec; 0.182 sec/batch)
2016-11-29 03:11:00.904175: step 510, loss = 3.09 (685.7 examples/sec; 0.187 sec/batch)
2016-11-29 03:11:02.751821: step 520, loss = 3.14 (702.0 examples/sec; 0.182 sec/batch)
2016-11-29 03:11:04.566980: step 530, loss = 3.12 (691.9 examples/sec; 0.185 sec/batch)
2016-11-29 03:11:06.390854: step 540, loss = 3.42 (691.3 examples/sec; 0.185 sec/batch)
2016-11-29 03:11:08.227219: step 550, loss = 3.14 (708.8 examples/sec; 0.181 sec/batch)
2016-11-29 03:11:10.045002: step 560, loss = 3.09 (736.3 examples/sec; 0.174 sec/batch)
2016-11-29 03:11:11.884284: step 570, loss = 3.18 (708.9 examples/sec; 0.181 sec/batch)
2016-11-29 03:11:13.713527: step 580, loss = 3.03 (708.0 examples/sec; 0.181 sec/batch)
2016-11-29 03:11:15.527034: step 590, loss = 3.13 (709.2 examples/sec; 0.180 sec/batch)
2016-11-29 03:11:17.330354: step 600, loss = 3.09 (709.0 examples/sec; 0.181 sec/batch)
2016-11-29 03:11:19.325421: step 610, loss = 3.24 (689.2 examples/sec; 0.186 sec/batch)
2016-11-29 03:11:21.148514: step 620, loss = 2.94 (682.5 examples/sec; 0.188 sec/batch)
2016-11-29 03:11:22.973208: step 630, loss = 2.97 (694.5 examples/sec; 0.184 sec/batch)
2016-11-29 03:11:24.806657: step 640, loss = 3.08 (803.0 examples/sec; 0.159 sec/batch)
2016-11-29 03:11:26.616158: step 650, loss = 2.79 (709.9 examples/sec; 0.180 sec/batch)
2016-11-29 03:11:28.449711: step 660, loss = 2.75 (639.9 examples/sec; 0.200 sec/batch)
2016-11-29 03:11:30.279330: step 670, loss = 2.81 (608.3 examples/sec; 0.210 sec/batch)
2016-11-29 03:11:32.081679: step 680, loss = 2.75 (712.2 examples/sec; 0.180 sec/batch)
2016-11-29 03:11:33.906051: step 690, loss = 3.07 (730.7 examples/sec; 0.175 sec/batch)
2016-11-29 03:11:35.721922: step 700, loss = 2.83 (699.9 examples/sec; 0.183 sec/batch)
2016-11-29 03:11:37.745565: step 710, loss = 2.66 (700.6 examples/sec; 0.183 sec/batch)
2016-11-29 03:11:39.568917: step 720, loss = 2.59 (722.4 examples/sec; 0.177 sec/batch)
2016-11-29 03:11:41.378969: step 730, loss = 2.82 (708.0 examples/sec; 0.181 sec/batch)
2016-11-29 03:11:43.203638: step 740, loss = 2.91 (695.5 examples/sec; 0.184 sec/batch)
2016-11-29 03:11:45.009971: step 750, loss = 3.01 (727.3 examples/sec; 0.176 sec/batch)
2016-11-29 03:11:46.815391: step 760, loss = 2.88 (698.8 examples/sec; 0.183 sec/batch)
2016-11-29 03:11:48.617157: step 770, loss = 2.66 (717.5 examples/sec; 0.178 sec/batch)
2016-11-29 03:11:50.429732: step 780, loss = 2.55 (718.8 examples/sec; 0.178 sec/batch)
2016-11-29 03:11:52.253639: step 790, loss = 2.69 (707.4 examples/sec; 0.181 sec/batch)
2016-11-29 03:11:54.069135: step 800, loss = 2.62 (706.3 examples/sec; 0.181 sec/batch)
2016-11-29 03:11:56.084918: step 810, loss = 2.62 (704.5 examples/sec; 0.182 sec/batch)
2016-11-29 03:11:57.899434: step 820, loss = 2.89 (730.5 examples/sec; 0.175 sec/batch)
2016-11-29 03:11:59.728270: step 830, loss = 2.40 (692.8 examples/sec; 0.185 sec/batch)
2016-11-29 03:12:01.581080: step 840, loss = 2.82 (678.8 examples/sec; 0.189 sec/batch)
2016-11-29 03:12:03.447494: step 850, loss = 2.67 (707.1 examples/sec; 0.181 sec/batch)
2016-11-29 03:12:05.346465: step 860, loss = 2.72 (661.8 examples/sec; 0.193 sec/batch)
2016-11-29 03:12:07.207431: step 870, loss = 2.60 (707.1 examples/sec; 0.181 sec/batch)
2016-11-29 03:12:09.079093: step 880, loss = 2.55 (743.0 examples/sec; 0.172 sec/batch)
2016-11-29 03:12:10.926865: step 890, loss = 2.47 (705.9 examples/sec; 0.181 sec/batch)
2016-11-29 03:12:12.772676: step 900, loss = 2.59 (739.3 examples/sec; 0.173 sec/batch)
2016-11-29 03:12:14.823309: step 910, loss = 2.52 (699.4 examples/sec; 0.183 sec/batch)
2016-11-29 03:12:16.647075: step 920, loss = 2.54 (710.0 examples/sec; 0.180 sec/batch)
2016-11-29 03:12:18.476722: step 930, loss = 2.56 (751.1 examples/sec; 0.170 sec/batch)
2016-11-29 03:12:20.286565: step 940, loss = 2.44 (745.4 examples/sec; 0.172 sec/batch)
2016-11-29 03:12:22.094894: step 950, loss = 2.51 (702.8 examples/sec; 0.182 sec/batch)
2016-11-29 03:12:23.897354: step 960, loss = 2.62 (701.9 examples/sec; 0.182 sec/batch)
2016-11-29 03:12:25.711232: step 970, loss = 2.48 (700.8 examples/sec; 0.183 sec/batch)
2016-11-29 03:12:27.548120: step 980, loss = 2.39 (705.5 examples/sec; 0.181 sec/batch)
2016-11-29 03:12:29.388963: step 990, loss = 2.45 (711.1 examples/sec; 0.180 sec/batch)
2016-11-29 03:12:31.218487: step 1000, loss = 2.41 (704.7 examples/sec; 0.182 sec/batch)
2016-11-29 03:12:33.775913: step 1010, loss = 2.44 (695.7 examples/sec; 0.184 sec/batch)
2016-11-29 03:12:35.593936: step 1020, loss = 2.35 (707.7 examples/sec; 0.181 sec/batch)
2016-11-29 03:12:37.417500: step 1030, loss = 2.57 (705.6 examples/sec; 0.181 sec/batch)
2016-11-29 03:12:39.252031: step 1040, loss = 2.37 (632.5 examples/sec; 0.202 sec/batch)
2016-11-29 03:12:41.023956: step 1050, loss = 2.37 (724.3 examples/sec; 0.177 sec/batch)
2016-11-29 03:12:42.821874: step 1060, loss = 2.45 (780.4 examples/sec; 0.164 sec/batch)
2016-11-29 03:12:44.627262: step 1070, loss = 2.38 (716.4 examples/sec; 0.179 sec/batch)
2016-11-29 03:12:46.429723: step 1080, loss = 2.29 (708.8 examples/sec; 0.181 sec/batch)
2016-11-29 03:12:48.223812: step 1090, loss = 2.33 (713.0 examples/sec; 0.180 sec/batch)
2016-11-29 03:12:50.038462: step 1100, loss = 2.39 (702.0 examples/sec; 0.182 sec/batch)
2016-11-29 03:12:52.033535: step 1110, loss = 2.28 (817.7 examples/sec; 0.157 sec/batch)
2016-11-29 03:12:53.842399: step 1120, loss = 2.23 (717.5 examples/sec; 0.178 sec/batch)
2016-11-29 03:12:55.661284: step 1130, loss = 2.36 (678.6 examples/sec; 0.189 sec/batch)
2016-11-29 03:12:57.477252: step 1140, loss = 2.28 (724.2 examples/sec; 0.177 sec/batch)
2016-11-29 03:12:59.278151: step 1150, loss = 2.27 (707.1 examples/sec; 0.181 sec/batch)
2016-11-29 03:13:01.110910: step 1160, loss = 2.23 (700.0 examples/sec; 0.183 sec/batch)
2016-11-29 03:13:02.945248: step 1170, loss = 2.30 (695.6 examples/sec; 0.184 sec/batch)
2016-11-29 03:13:04.766985: step 1180, loss = 2.32 (693.9 examples/sec; 0.184 sec/batch)
2016-11-29 03:13:06.557846: step 1190, loss = 2.13 (701.1 examples/sec; 0.183 sec/batch)
2016-11-29 03:13:08.356225: step 1200, loss = 2.23 (728.7 examples/sec; 0.176 sec/batch)
2016-11-29 03:13:10.361246: step 1210, loss = 2.26 (711.8 examples/sec; 0.180 sec/batch)
2016-11-29 03:13:12.186446: step 1220, loss = 2.11 (690.1 examples/sec; 0.185 sec/batch)
2016-11-29 03:13:13.983414: step 1230, loss = 2.13 (729.5 examples/sec; 0.175 sec/batch)
2016-11-29 03:13:15.813573: step 1240, loss = 2.14 (621.1 examples/sec; 0.206 sec/batch)
2016-11-29 03:13:17.610923: step 1250, loss = 2.01 (688.3 examples/sec; 0.186 sec/batch)
2016-11-29 03:13:19.424610: step 1260, loss = 2.07 (709.1 examples/sec; 0.181 sec/batch)
2016-11-29 03:13:21.222882: step 1270, loss = 2.68 (731.5 examples/sec; 0.175 sec/batch)
2016-11-29 03:13:23.043998: step 1280, loss = 2.13 (735.9 examples/sec; 0.174 sec/batch)
2016-11-29 03:13:24.864401: step 1290, loss = 2.11 (704.7 examples/sec; 0.182 sec/batch)
2016-11-29 03:13:26.674131: step 1300, loss = 2.20 (713.5 examples/sec; 0.179 sec/batch)
2016-11-29 03:13:28.661786: step 1310, loss = 2.14 (703.6 examples/sec; 0.182 sec/batch)
2016-11-29 03:13:30.470758: step 1320, loss = 2.10 (700.8 examples/sec; 0.183 sec/batch)
2016-11-29 03:13:32.277016: step 1330, loss = 2.29 (782.7 examples/sec; 0.164 sec/batch)
2016-11-29 03:13:34.075069: step 1340, loss = 1.84 (717.8 examples/sec; 0.178 sec/batch)
2016-11-29 03:13:35.859286: step 1350, loss = 2.00 (710.9 examples/sec; 0.180 sec/batch)
2016-11-29 03:13:37.669646: step 1360, loss = 2.16 (715.4 examples/sec; 0.179 sec/batch)
2016-11-29 03:13:39.484313: step 1370, loss = 2.06 (716.5 examples/sec; 0.179 sec/batch)
2016-11-29 03:13:41.278546: step 1380, loss = 1.94 (840.5 examples/sec; 0.152 sec/batch)
2016-11-29 03:13:43.102051: step 1390, loss = 2.10 (703.5 examples/sec; 0.182 sec/batch)
2016-11-29 03:13:44.936326: step 1400, loss = 2.05 (701.2 examples/sec; 0.183 sec/batch)
2016-11-29 03:13:46.941457: step 1410, loss = 1.93 (702.6 examples/sec; 0.182 sec/batch)
2016-11-29 03:13:48.763130: step 1420, loss = 2.00 (696.8 examples/sec; 0.184 sec/batch)
2016-11-29 03:13:50.574750: step 1430, loss = 2.04 (687.9 examples/sec; 0.186 sec/batch)
2016-11-29 03:13:52.408657: step 1440, loss = 1.92 (631.7 examples/sec; 0.203 sec/batch)
2016-11-29 03:13:54.215967: step 1450, loss = 2.21 (646.4 examples/sec; 0.198 sec/batch)
2016-11-29 03:13:55.996033: step 1460, loss = 1.83 (730.4 examples/sec; 0.175 sec/batch)
2016-11-29 03:13:57.817122: step 1470, loss = 2.17 (700.4 examples/sec; 0.183 sec/batch)
2016-11-29 03:13:59.622553: step 1480, loss = 1.95 (715.5 examples/sec; 0.179 sec/batch)
2016-11-29 03:14:01.441669: step 1490, loss = 1.99 (706.0 examples/sec; 0.181 sec/batch)
2016-11-29 03:14:03.275076: step 1500, loss = 1.98 (706.7 examples/sec; 0.181 sec/batch)
2016-11-29 03:14:05.268494: step 1510, loss = 2.26 (722.5 examples/sec; 0.177 sec/batch)
2016-11-29 03:14:07.082473: step 1520, loss = 1.89 (700.2 examples/sec; 0.183 sec/batch)
2016-11-29 03:14:08.891512: step 1530, loss = 2.08 (697.8 examples/sec; 0.183 sec/batch)
2016-11-29 03:14:10.701724: step 1540, loss = 1.98 (709.4 examples/sec; 0.180 sec/batch)
2016-11-29 03:14:12.524286: step 1550, loss = 2.07 (687.3 examples/sec; 0.186 sec/batch)
2016-11-29 03:14:14.334127: step 1560, loss = 1.90 (705.2 examples/sec; 0.182 sec/batch)
2016-11-29 03:14:16.134881: step 1570, loss = 1.94 (719.3 examples/sec; 0.178 sec/batch)
2016-11-29 03:14:17.943219: step 1580, loss = 1.76 (720.9 examples/sec; 0.178 sec/batch)
2016-11-29 03:14:19.774750: step 1590, loss = 1.92 (698.5 examples/sec; 0.183 sec/batch)
2016-11-29 03:14:21.592806: step 1600, loss = 2.02 (811.1 examples/sec; 0.158 sec/batch)
2016-11-29 03:14:23.591043: step 1610, loss = 1.89 (647.5 examples/sec; 0.198 sec/batch)
2016-11-29 03:14:25.366488: step 1620, loss = 2.07 (773.2 examples/sec; 0.166 sec/batch)
2016-11-29 03:14:27.193268: step 1630, loss = 2.27 (685.6 examples/sec; 0.187 sec/batch)
2016-11-29 03:14:28.988854: step 1640, loss = 1.73 (726.1 examples/sec; 0.176 sec/batch)
2016-11-29 03:14:30.805730: step 1650, loss = 1.70 (710.9 examples/sec; 0.180 sec/batch)
2016-11-29 03:14:32.610784: step 1660, loss = 2.15 (693.6 examples/sec; 0.185 sec/batch)
2016-11-29 03:14:34.425230: step 1670, loss = 1.76 (704.4 examples/sec; 0.182 sec/batch)
2016-11-29 03:14:36.259470: step 1680, loss = 1.88 (627.9 examples/sec; 0.204 sec/batch)
2016-11-29 03:14:38.025031: step 1690, loss = 1.67 (701.0 examples/sec; 0.183 sec/batch)
2016-11-29 03:14:39.831780: step 1700, loss = 1.78 (727.2 examples/sec; 0.176 sec/batch)
2016-11-29 03:14:41.827277: step 1710, loss = 1.82 (693.2 examples/sec; 0.185 sec/batch)
2016-11-29 03:14:43.634183: step 1720, loss = 1.90 (716.6 examples/sec; 0.179 sec/batch)
2016-11-29 03:14:45.437786: step 1730, loss = 1.63 (714.1 examples/sec; 0.179 sec/batch)
2016-11-29 03:14:47.234996: step 1740, loss = 1.93 (730.3 examples/sec; 0.175 sec/batch)
2016-11-29 03:14:49.047809: step 1750, loss = 1.83 (687.4 examples/sec; 0.186 sec/batch)
2016-11-29 03:14:50.852067: step 1760, loss = 1.78 (682.1 examples/sec; 0.188 sec/batch)
2016-11-29 03:14:52.650198: step 1770, loss = 1.73 (713.9 examples/sec; 0.179 sec/batch)
2016-11-29 03:14:54.461988: step 1780, loss = 1.71 (714.1 examples/sec; 0.179 sec/batch)
2016-11-29 03:14:56.269993: step 1790, loss = 1.69 (709.4 examples/sec; 0.180 sec/batch)
2016-11-29 03:14:58.068683: step 1800, loss = 1.72 (720.3 examples/sec; 0.178 sec/batch)
2016-11-29 03:15:00.055615: step 1810, loss = 1.67 (710.8 examples/sec; 0.180 sec/batch)
2016-11-29 03:15:01.904470: step 1820, loss = 1.79 (634.4 examples/sec; 0.202 sec/batch)
2016-11-29 03:15:03.731764: step 1830, loss = 1.96 (697.5 examples/sec; 0.184 sec/batch)
2016-11-29 03:15:05.531294: step 1840, loss = 1.70 (712.1 examples/sec; 0.180 sec/batch)
2016-11-29 03:15:07.341087: step 1850, loss = 1.63 (710.0 examples/sec; 0.180 sec/batch)
2016-11-29 03:15:09.155975: step 1860, loss = 1.97 (729.7 examples/sec; 0.175 sec/batch)
2016-11-29 03:15:10.977592: step 1870, loss = 1.63 (701.6 examples/sec; 0.182 sec/batch)
2016-11-29 03:15:12.785737: step 1880, loss = 1.61 (715.4 examples/sec; 0.179 sec/batch)
2016-11-29 03:15:14.598273: step 1890, loss = 1.82 (686.1 examples/sec; 0.187 sec/batch)
2016-11-29 03:15:16.383124: step 1900, loss = 1.58 (720.3 examples/sec; 0.178 sec/batch)
2016-11-29 03:15:18.361660: step 1910, loss = 1.75 (696.3 examples/sec; 0.184 sec/batch)
2016-11-29 03:15:20.172940: step 1920, loss = 1.72 (752.8 examples/sec; 0.170 sec/batch)
2016-11-29 03:15:21.981487: step 1930, loss = 1.78 (702.0 examples/sec; 0.182 sec/batch)
2016-11-29 03:15:23.790869: step 1940, loss = 1.65 (702.0 examples/sec; 0.182 sec/batch)
2016-11-29 03:15:25.626422: step 1950, loss = 1.55 (681.0 examples/sec; 0.188 sec/batch)
2016-11-29 03:15:27.436933: step 1960, loss = 1.71 (699.4 examples/sec; 0.183 sec/batch)
2016-11-29 03:15:29.248835: step 1970, loss = 1.64 (687.0 examples/sec; 0.186 sec/batch)
2016-11-29 03:15:31.046584: step 1980, loss = 1.65 (684.6 examples/sec; 0.187 sec/batch)
2016-11-29 03:15:32.838410: step 1990, loss = 1.69 (698.2 examples/sec; 0.183 sec/batch)
2016-11-29 03:15:34.636548: step 2000, loss = 1.55 (752.7 examples/sec; 0.170 sec/batch)
2016-11-29 03:15:37.096101: step 2010, loss = 1.76 (699.6 examples/sec; 0.183 sec/batch)
2016-11-29 03:15:38.920722: step 2020, loss = 1.67 (701.9 examples/sec; 0.182 sec/batch)
2016-11-29 03:15:40.733582: step 2030, loss = 1.55 (632.1 examples/sec; 0.203 sec/batch)
2016-11-29 03:15:42.515259: step 2040, loss = 1.56 (701.5 examples/sec; 0.182 sec/batch)
2016-11-29 03:15:44.282195: step 2050, loss = 1.72 (729.3 examples/sec; 0.176 sec/batch)
2016-11-29 03:15:46.067584: step 2060, loss = 1.56 (702.4 examples/sec; 0.182 sec/batch)
2016-11-29 03:15:47.875652: step 2070, loss = 1.54 (723.8 examples/sec; 0.177 sec/batch)
2016-11-29 03:15:49.666936: step 2080, loss = 1.55 (720.7 examples/sec; 0.178 sec/batch)
2016-11-29 03:15:51.456842: step 2090, loss = 1.88 (717.6 examples/sec; 0.178 sec/batch)
2016-11-29 03:15:53.250181: step 2100, loss = 1.72 (717.0 examples/sec; 0.179 sec/batch)
2016-11-29 03:15:55.217942: step 2110, loss = 1.79 (740.0 examples/sec; 0.173 sec/batch)
2016-11-29 03:15:57.028480: step 2120, loss = 1.50 (640.1 examples/sec; 0.200 sec/batch)
2016-11-29 03:15:58.806669: step 2130, loss = 1.60 (720.6 examples/sec; 0.178 sec/batch)
2016-11-29 03:16:00.608725: step 2140, loss = 1.44 (700.4 examples/sec; 0.183 sec/batch)
2016-11-29 03:16:02.413793: step 2150, loss = 1.49 (729.6 examples/sec; 0.175 sec/batch)
2016-11-29 03:16:04.215115: step 2160, loss = 1.66 (734.7 examples/sec; 0.174 sec/batch)
2016-11-29 03:16:06.004887: step 2170, loss = 1.43 (707.9 examples/sec; 0.181 sec/batch)
2016-11-29 03:16:07.812426: step 2180, loss = 1.42 (701.5 examples/sec; 0.182 sec/batch)
2016-11-29 03:16:09.630077: step 2190, loss = 1.54 (703.6 examples/sec; 0.182 sec/batch)
2016-11-29 03:16:11.438919: step 2200, loss = 1.47 (688.7 examples/sec; 0.186 sec/batch)
2016-11-29 03:16:13.424988: step 2210, loss = 1.54 (714.2 examples/sec; 0.179 sec/batch)
2016-11-29 03:16:15.220095: step 2220, loss = 1.60 (710.8 examples/sec; 0.180 sec/batch)
2016-11-29 03:16:17.018056: step 2230, loss = 1.53 (712.5 examples/sec; 0.180 sec/batch)
2016-11-29 03:16:18.815873: step 2240, loss = 1.49 (719.9 examples/sec; 0.178 sec/batch)
2016-11-29 03:16:20.600706: step 2250, loss = 1.75 (723.1 examples/sec; 0.177 sec/batch)
2016-11-29 03:16:22.425047: step 2260, loss = 1.43 (684.6 examples/sec; 0.187 sec/batch)
2016-11-29 03:16:24.243972: step 2270, loss = 1.50 (655.2 examples/sec; 0.195 sec/batch)
2016-11-29 03:16:26.026588: step 2280, loss = 1.56 (696.3 examples/sec; 0.184 sec/batch)
2016-11-29 03:16:27.824107: step 2290, loss = 1.57 (701.8 examples/sec; 0.182 sec/batch)
2016-11-29 03:16:29.631606: step 2300, loss = 1.46 (709.6 examples/sec; 0.180 sec/batch)
2016-11-29 03:16:31.620289: step 2310, loss = 1.64 (678.9 examples/sec; 0.189 sec/batch)
2016-11-29 03:16:33.421040: step 2320, loss = 1.54 (717.4 examples/sec; 0.178 sec/batch)
2016-11-29 03:16:35.233614: step 2330, loss = 1.43 (715.9 examples/sec; 0.179 sec/batch)
2016-11-29 03:16:37.042034: step 2340, loss = 1.63 (719.5 examples/sec; 0.178 sec/batch)
2016-11-29 03:16:38.845789: step 2350, loss = 1.50 (699.3 examples/sec; 0.183 sec/batch)
2016-11-29 03:16:40.644993: step 2360, loss = 1.55 (712.7 examples/sec; 0.180 sec/batch)
2016-11-29 03:16:42.456124: step 2370, loss = 1.40 (725.9 examples/sec; 0.176 sec/batch)
2016-11-29 03:16:44.259802: step 2380, loss = 1.73 (743.3 examples/sec; 0.172 sec/batch)
2016-11-29 03:16:46.064569: step 2390, loss = 1.56 (709.9 examples/sec; 0.180 sec/batch)
2016-11-29 03:16:47.878858: step 2400, loss = 1.47 (706.3 examples/sec; 0.181 sec/batch)
2016-11-29 03:16:49.881510: step 2410, loss = 1.50 (684.8 examples/sec; 0.187 sec/batch)
2016-11-29 03:16:51.689794: step 2420, loss = 1.37 (704.2 examples/sec; 0.182 sec/batch)
2016-11-29 03:16:53.490696: step 2430, loss = 1.46 (707.4 examples/sec; 0.181 sec/batch)
2016-11-29 03:16:55.311758: step 2440, loss = 1.42 (691.5 examples/sec; 0.185 sec/batch)
2016-11-29 03:16:57.128214: step 2450, loss = 1.31 (716.2 examples/sec; 0.179 sec/batch)
2016-11-29 03:16:58.934289: step 2460, loss = 1.47 (716.8 examples/sec; 0.179 sec/batch)
2016-11-29 03:17:00.744240: step 2470, loss = 1.44 (719.7 examples/sec; 0.178 sec/batch)
2016-11-29 03:17:02.561266: step 2480, loss = 1.37 (731.2 examples/sec; 0.175 sec/batch)
2016-11-29 03:17:04.380115: step 2490, loss = 1.38 (716.4 examples/sec; 0.179 sec/batch)
2016-11-29 03:17:06.168810: step 2500, loss = 1.36 (744.7 examples/sec; 0.172 sec/batch)
2016-11-29 03:17:08.156047: step 2510, loss = 1.44 (696.1 examples/sec; 0.184 sec/batch)
2016-11-29 03:17:09.967939: step 2520, loss = 1.46 (688.3 examples/sec; 0.186 sec/batch)
2016-11-29 03:17:11.764974: step 2530, loss = 1.41 (720.5 examples/sec; 0.178 sec/batch)
2016-11-29 03:17:13.587984: step 2540, loss = 1.32 (695.0 examples/sec; 0.184 sec/batch)
2016-11-29 03:17:15.408851: step 2550, loss = 1.43 (684.3 examples/sec; 0.187 sec/batch)
2016-11-29 03:17:17.214973: step 2560, loss = 1.33 (698.0 examples/sec; 0.183 sec/batch)
2016-11-29 03:17:19.030094: step 2570, loss = 1.68 (691.5 examples/sec; 0.185 sec/batch)
2016-11-29 03:17:20.835936: step 2580, loss = 1.31 (730.8 examples/sec; 0.175 sec/batch)
2016-11-29 03:17:22.632407: step 2590, loss = 1.34 (712.0 examples/sec; 0.180 sec/batch)
2016-11-29 03:17:24.438735: step 2600, loss = 1.47 (713.7 examples/sec; 0.179 sec/batch)
2016-11-29 03:17:26.444246: step 2610, loss = 1.51 (696.1 examples/sec; 0.184 sec/batch)
2016-11-29 03:17:28.254161: step 2620, loss = 1.32 (695.1 examples/sec; 0.184 sec/batch)
2016-11-29 03:17:30.070385: step 2630, loss = 1.41 (694.8 examples/sec; 0.184 sec/batch)
2016-11-29 03:17:31.879553: step 2640, loss = 1.63 (718.2 examples/sec; 0.178 sec/batch)
2016-11-29 03:17:33.691884: step 2650, loss = 1.27 (707.1 examples/sec; 0.181 sec/batch)
2016-11-29 03:17:35.491194: step 2660, loss = 1.36 (706.2 examples/sec; 0.181 sec/batch)
2016-11-29 03:17:37.280217: step 2670, loss = 1.38 (697.7 examples/sec; 0.183 sec/batch)
2016-11-29 03:17:39.089357: step 2680, loss = 1.32 (713.3 examples/sec; 0.179 sec/batch)
2016-11-29 03:17:40.899874: step 2690, loss = 1.37 (688.8 examples/sec; 0.186 sec/batch)
2016-11-29 03:17:42.701114: step 2700, loss = 1.38 (697.8 examples/sec; 0.183 sec/batch)
2016-11-29 03:17:44.693141: step 2710, loss = 1.50 (693.0 examples/sec; 0.185 sec/batch)
2016-11-29 03:17:46.505421: step 2720, loss = 1.39 (710.1 examples/sec; 0.180 sec/batch)
2016-11-29 03:17:48.310254: step 2730, loss = 1.52 (723.9 examples/sec; 0.177 sec/batch)
2016-11-29 03:17:50.129708: step 2740, loss = 1.49 (641.1 examples/sec; 0.200 sec/batch)
2016-11-29 03:17:51.906160: step 2750, loss = 1.43 (701.2 examples/sec; 0.183 sec/batch)
2016-11-29 03:17:53.709467: step 2760, loss = 1.59 (712.1 examples/sec; 0.180 sec/batch)
2016-11-29 03:17:55.514743: step 2770, loss = 1.37 (725.2 examples/sec; 0.177 sec/batch)
2016-11-29 03:17:57.320199: step 2780, loss = 1.33 (698.0 examples/sec; 0.183 sec/batch)
2016-11-29 03:17:59.132559: step 2790, loss = 1.36 (705.8 examples/sec; 0.181 sec/batch)
2016-11-29 03:18:00.946674: step 2800, loss = 1.36 (707.0 examples/sec; 0.181 sec/batch)
2016-11-29 03:18:02.959004: step 2810, loss = 1.14 (726.4 examples/sec; 0.176 sec/batch)
2016-11-29 03:18:04.758521: step 2820, loss = 1.36 (712.0 examples/sec; 0.180 sec/batch)
2016-11-29 03:18:06.568209: step 2830, loss = 1.28 (714.9 examples/sec; 0.179 sec/batch)
2016-11-29 03:18:08.380164: step 2840, loss = 1.33 (785.2 examples/sec; 0.163 sec/batch)
2016-11-29 03:18:10.210110: step 2850, loss = 1.25 (698.4 examples/sec; 0.183 sec/batch)
2016-11-29 03:18:12.027161: step 2860, loss = 1.37 (690.5 examples/sec; 0.185 sec/batch)
2016-11-29 03:18:13.840384: step 2870, loss = 1.28 (733.3 examples/sec; 0.175 sec/batch)
2016-11-29 03:18:15.664772: step 2880, loss = 1.35 (713.1 examples/sec; 0.180 sec/batch)
2016-11-29 03:18:17.466367: step 2890, loss = 1.29 (708.4 examples/sec; 0.181 sec/batch)
2016-11-29 03:18:19.287659: step 2900, loss = 1.58 (695.6 examples/sec; 0.184 sec/batch)
2016-11-29 03:18:21.292610: step 2910, loss = 1.29 (735.6 examples/sec; 0.174 sec/batch)
2016-11-29 03:18:23.125174: step 2920, loss = 1.35 (699.5 examples/sec; 0.183 sec/batch)
2016-11-29 03:18:24.955346: step 2930, loss = 1.59 (681.4 examples/sec; 0.188 sec/batch)
2016-11-29 03:18:26.791376: step 2940, loss = 1.48 (681.9 examples/sec; 0.188 sec/batch)
2016-11-29 03:18:28.707377: step 2950, loss = 1.31 (679.9 examples/sec; 0.188 sec/batch)
2016-11-29 03:18:30.586315: step 2960, loss = 1.22 (695.6 examples/sec; 0.184 sec/batch)
2016-11-29 03:18:32.475778: step 2970, loss = 1.38 (662.0 examples/sec; 0.193 sec/batch)
2016-11-29 03:18:34.310397: step 2980, loss = 1.28 (762.7 examples/sec; 0.168 sec/batch)
2016-11-29 03:18:36.120745: step 2990, loss = 1.16 (807.7 examples/sec; 0.158 sec/batch)
2016-11-29 03:18:37.928318: step 3000, loss = 1.15 (697.3 examples/sec; 0.184 sec/batch)
2016-11-29 03:18:40.452503: step 3010, loss = 1.29 (715.2 examples/sec; 0.179 sec/batch)
2016-11-29 03:18:42.259917: step 3020, loss = 1.22 (724.7 examples/sec; 0.177 sec/batch)
2016-11-29 03:18:44.064351: step 3030, loss = 1.38 (718.3 examples/sec; 0.178 sec/batch)
2016-11-29 03:18:45.871417: step 3040, loss = 1.10 (699.3 examples/sec; 0.183 sec/batch)
2016-11-29 03:18:47.676657: step 3050, loss = 1.17 (721.0 examples/sec; 0.178 sec/batch)
2016-11-29 03:18:49.491780: step 3060, loss = 1.13 (709.2 examples/sec; 0.180 sec/batch)
2016-11-29 03:18:51.309270: step 3070, loss = 1.24 (718.1 examples/sec; 0.178 sec/batch)
2016-11-29 03:18:53.118548: step 3080, loss = 1.21 (704.4 examples/sec; 0.182 sec/batch)
2016-11-29 03:18:54.930977: step 3090, loss = 1.23 (697.1 examples/sec; 0.184 sec/batch)
2016-11-29 03:18:56.734570: step 3100, loss = 1.25 (711.0 examples/sec; 0.180 sec/batch)
2016-11-29 03:18:58.735497: step 3110, loss = 1.29 (692.6 examples/sec; 0.185 sec/batch)
2016-11-29 03:19:00.552194: step 3120, loss = 1.23 (705.6 examples/sec; 0.181 sec/batch)
2016-11-29 03:19:02.395810: step 3130, loss = 1.31 (695.4 examples/sec; 0.184 sec/batch)
2016-11-29 03:19:04.202433: step 3140, loss = 1.25 (726.6 examples/sec; 0.176 sec/batch)
2016-11-29 03:19:06.013157: step 3150, loss = 1.18 (703.0 examples/sec; 0.182 sec/batch)
2016-11-29 03:19:07.848500: step 3160, loss = 1.16 (692.0 examples/sec; 0.185 sec/batch)
2016-11-29 03:19:09.656380: step 3170, loss = 1.31 (708.4 examples/sec; 0.181 sec/batch)
2016-11-29 03:19:11.461377: step 3180, loss = 1.25 (690.7 examples/sec; 0.185 sec/batch)
2016-11-29 03:19:13.270865: step 3190, loss = 1.33 (745.5 examples/sec; 0.172 sec/batch)
2016-11-29 03:19:15.079961: step 3200, loss = 1.19 (705.3 examples/sec; 0.181 sec/batch)
2016-11-29 03:19:17.072534: step 3210, loss = 1.19 (696.2 examples/sec; 0.184 sec/batch)
2016-11-29 03:19:18.885009: step 3220, loss = 1.18 (714.1 examples/sec; 0.179 sec/batch)
2016-11-29 03:19:20.697985: step 3230, loss = 1.32 (713.9 examples/sec; 0.179 sec/batch)
2016-11-29 03:19:22.490645: step 3240, loss = 1.47 (724.3 examples/sec; 0.177 sec/batch)
2016-11-29 03:19:24.302757: step 3250, loss = 1.19 (689.6 examples/sec; 0.186 sec/batch)
2016-11-29 03:19:26.110291: step 3260, loss = 1.31 (712.2 examples/sec; 0.180 sec/batch)
2016-11-29 03:19:27.935960: step 3270, loss = 1.27 (699.6 examples/sec; 0.183 sec/batch)
2016-11-29 03:19:29.747327: step 3280, loss = 1.14 (730.6 examples/sec; 0.175 sec/batch)
2016-11-29 03:19:31.572126: step 3290, loss = 1.18 (697.7 examples/sec; 0.183 sec/batch)
2016-11-29 03:19:33.392924: step 3300, loss = 1.34 (701.2 examples/sec; 0.183 sec/batch)
2016-11-29 03:19:35.396665: step 3310, loss = 1.24 (712.1 examples/sec; 0.180 sec/batch)
2016-11-29 03:19:37.211563: step 3320, loss = 1.13 (735.7 examples/sec; 0.174 sec/batch)
2016-11-29 03:19:39.023597: step 3330, loss = 1.22 (711.8 examples/sec; 0.180 sec/batch)
2016-11-29 03:19:40.834239: step 3340, loss = 1.30 (713.7 examples/sec; 0.179 sec/batch)
2016-11-29 03:19:42.658977: step 3350, loss = 1.26 (685.7 examples/sec; 0.187 sec/batch)
2016-11-29 03:19:44.465908: step 3360, loss = 1.07 (711.8 examples/sec; 0.180 sec/batch)
2016-11-29 03:19:46.283130: step 3370, loss = 1.20 (696.5 examples/sec; 0.184 sec/batch)
2016-11-29 03:19:48.090002: step 3380, loss = 1.34 (711.8 examples/sec; 0.180 sec/batch)
2016-11-29 03:19:49.903651: step 3390, loss = 1.37 (720.1 examples/sec; 0.178 sec/batch)
2016-11-29 03:19:51.701428: step 3400, loss = 1.23 (707.9 examples/sec; 0.181 sec/batch)
2016-11-29 03:19:53.699028: step 3410, loss = 1.16 (687.8 examples/sec; 0.186 sec/batch)
2016-11-29 03:19:55.522357: step 3420, loss = 1.28 (702.8 examples/sec; 0.182 sec/batch)
2016-11-29 03:19:57.330061: step 3430, loss = 1.20 (708.1 examples/sec; 0.181 sec/batch)
2016-11-29 03:19:59.170982: step 3440, loss = 1.32 (628.9 examples/sec; 0.204 sec/batch)
2016-11-29 03:20:00.960443: step 3450, loss = 1.19 (768.0 examples/sec; 0.167 sec/batch)
2016-11-29 03:20:02.840187: step 3460, loss = 1.33 (717.4 examples/sec; 0.178 sec/batch)
2016-11-29 03:20:04.644517: step 3470, loss = 1.11 (717.4 examples/sec; 0.178 sec/batch)
2016-11-29 03:20:06.462021: step 3480, loss = 1.21 (733.0 examples/sec; 0.175 sec/batch)
2016-11-29 03:20:08.268715: step 3490, loss = 1.15 (712.6 examples/sec; 0.180 sec/batch)
2016-11-29 03:20:10.071181: step 3500, loss = 1.23 (716.8 examples/sec; 0.179 sec/batch)
2016-11-29 03:20:12.055767: step 3510, loss = 1.11 (734.2 examples/sec; 0.174 sec/batch)
2016-11-29 03:20:13.876095: step 3520, loss = 1.26 (714.0 examples/sec; 0.179 sec/batch)
2016-11-29 03:20:15.683737: step 3530, loss = 1.17 (710.1 examples/sec; 0.180 sec/batch)
2016-11-29 03:20:17.510427: step 3540, loss = 1.24 (695.3 examples/sec; 0.184 sec/batch)
2016-11-29 03:20:19.323995: step 3550, loss = 1.06 (710.3 examples/sec; 0.180 sec/batch)
2016-11-29 03:20:21.123171: step 3560, loss = 1.35 (711.5 examples/sec; 0.180 sec/batch)
2016-11-29 03:20:22.932489: step 3570, loss = 1.16 (712.5 examples/sec; 0.180 sec/batch)
2016-11-29 03:20:24.742623: step 3580, loss = 1.23 (713.5 examples/sec; 0.179 sec/batch)
2016-11-29 03:20:26.560045: step 3590, loss = 1.17 (709.5 examples/sec; 0.180 sec/batch)
2016-11-29 03:20:28.360566: step 3600, loss = 1.15 (731.2 examples/sec; 0.175 sec/batch)
2016-11-29 03:20:30.344827: step 3610, loss = 1.11 (700.6 examples/sec; 0.183 sec/batch)
2016-11-29 03:20:32.162297: step 3620, loss = 1.13 (681.1 examples/sec; 0.188 sec/batch)
2016-11-29 03:20:33.977683: step 3630, loss = 1.15 (706.8 examples/sec; 0.181 sec/batch)
2016-11-29 03:20:35.796556: step 3640, loss = 1.40 (725.3 examples/sec; 0.176 sec/batch)
2016-11-29 03:20:37.628865: step 3650, loss = 1.21 (650.2 examples/sec; 0.197 sec/batch)
2016-11-29 03:20:39.433917: step 3660, loss = 1.04 (706.6 examples/sec; 0.181 sec/batch)
2016-11-29 03:20:41.259693: step 3670, loss = 1.18 (708.9 examples/sec; 0.181 sec/batch)
2016-11-29 03:20:43.068822: step 3680, loss = 1.17 (707.3 examples/sec; 0.181 sec/batch)
2016-11-29 03:20:44.872174: step 3690, loss = 1.01 (716.6 examples/sec; 0.179 sec/batch)
2016-11-29 03:20:46.675290: step 3700, loss = 1.23 (735.3 examples/sec; 0.174 sec/batch)
2016-11-29 03:20:48.658087: step 3710, loss = 1.26 (713.2 examples/sec; 0.179 sec/batch)
2016-11-29 03:20:50.483578: step 3720, loss = 1.16 (645.1 examples/sec; 0.198 sec/batch)
2016-11-29 03:20:52.284732: step 3730, loss = 1.37 (707.1 examples/sec; 0.181 sec/batch)
2016-11-29 03:20:54.076290: step 3740, loss = 1.31 (785.9 examples/sec; 0.163 sec/batch)
2016-11-29 03:20:55.897500: step 3750, loss = 1.16 (714.0 examples/sec; 0.179 sec/batch)
2016-11-29 03:20:57.716196: step 3760, loss = 1.04 (695.2 examples/sec; 0.184 sec/batch)
2016-11-29 03:20:59.530290: step 3770, loss = 1.15 (690.1 examples/sec; 0.185 sec/batch)
2016-11-29 03:21:01.347676: step 3780, loss = 0.98 (702.9 examples/sec; 0.182 sec/batch)
2016-11-29 03:21:03.179331: step 3790, loss = 1.14 (813.8 examples/sec; 0.157 sec/batch)
2016-11-29 03:21:04.980792: step 3800, loss = 1.37 (698.8 examples/sec; 0.183 sec/batch)
2016-11-29 03:21:06.962299: step 3810, loss = 1.12 (720.3 examples/sec; 0.178 sec/batch)
2016-11-29 03:21:08.757644: step 3820, loss = 1.30 (714.9 examples/sec; 0.179 sec/batch)
2016-11-29 03:21:10.577927: step 3830, loss = 1.04 (715.0 examples/sec; 0.179 sec/batch)
2016-11-29 03:21:12.427329: step 3840, loss = 1.03 (681.2 examples/sec; 0.188 sec/batch)
2016-11-29 03:21:14.228842: step 3850, loss = 1.01 (706.2 examples/sec; 0.181 sec/batch)
2016-11-29 03:21:16.040646: step 3860, loss = 1.05 (702.8 examples/sec; 0.182 sec/batch)
2016-11-29 03:21:17.866833: step 3870, loss = 1.15 (708.8 examples/sec; 0.181 sec/batch)
2016-11-29 03:21:19.692840: step 3880, loss = 1.03 (654.6 examples/sec; 0.196 sec/batch)
2016-11-29 03:21:21.483172: step 3890, loss = 1.22 (705.4 examples/sec; 0.181 sec/batch)
2016-11-29 03:21:23.291855: step 3900, loss = 1.08 (727.4 examples/sec; 0.176 sec/batch)
2016-11-29 03:21:25.276007: step 3910, loss = 1.15 (783.2 examples/sec; 0.163 sec/batch)
2016-11-29 03:21:27.093781: step 3920, loss = 1.12 (790.5 examples/sec; 0.162 sec/batch)
2016-11-29 03:21:28.898696: step 3930, loss = 1.16 (714.4 examples/sec; 0.179 sec/batch)
2016-11-29 03:21:30.700651: step 3940, loss = 1.09 (721.1 examples/sec; 0.178 sec/batch)
2016-11-29 03:21:32.499841: step 3950, loss = 1.04 (728.1 examples/sec; 0.176 sec/batch)
2016-11-29 03:21:34.313145: step 3960, loss = 0.95 (705.6 examples/sec; 0.181 sec/batch)
2016-11-29 03:21:36.133519: step 3970, loss = 1.04 (694.5 examples/sec; 0.184 sec/batch)
2016-11-29 03:21:37.932711: step 3980, loss = 1.33 (723.0 examples/sec; 0.177 sec/batch)
2016-11-29 03:21:39.743910: step 3990, loss = 1.25 (718.7 examples/sec; 0.178 sec/batch)
2016-11-29 03:21:41.548770: step 4000, loss = 1.17 (706.6 examples/sec; 0.181 sec/batch)
2016-11-29 03:21:44.024719: step 4010, loss = 1.12 (709.3 examples/sec; 0.180 sec/batch)
2016-11-29 03:21:45.833651: step 4020, loss = 0.94 (705.9 examples/sec; 0.181 sec/batch)
2016-11-29 03:21:47.645058: step 4030, loss = 1.19 (707.1 examples/sec; 0.181 sec/batch)
2016-11-29 03:21:49.465755: step 4040, loss = 1.18 (678.3 examples/sec; 0.189 sec/batch)
2016-11-29 03:21:51.276075: step 4050, loss = 1.03 (710.7 examples/sec; 0.180 sec/batch)
2016-11-29 03:21:53.078889: step 4060, loss = 1.15 (739.1 examples/sec; 0.173 sec/batch)
2016-11-29 03:21:54.881812: step 4070, loss = 1.20 (704.2 examples/sec; 0.182 sec/batch)
2016-11-29 03:21:56.696948: step 4080, loss = 1.08 (681.8 examples/sec; 0.188 sec/batch)
2016-11-29 03:21:58.496802: step 4090, loss = 1.06 (711.0 examples/sec; 0.180 sec/batch)
2016-11-29 03:22:00.305775: step 4100, loss = 1.03 (691.4 examples/sec; 0.185 sec/batch)
2016-11-29 03:22:02.325977: step 4110, loss = 1.02 (694.0 examples/sec; 0.184 sec/batch)
2016-11-29 03:22:04.153894: step 4120, loss = 1.14 (700.8 examples/sec; 0.183 sec/batch)
2016-11-29 03:22:05.964525: step 4130, loss = 1.02 (703.3 examples/sec; 0.182 sec/batch)
2016-11-29 03:22:07.789226: step 4140, loss = 1.14 (703.5 examples/sec; 0.182 sec/batch)
2016-11-29 03:22:09.595227: step 4150, loss = 1.00 (701.6 examples/sec; 0.182 sec/batch)
2016-11-29 03:22:11.393420: step 4160, loss = 1.15 (708.7 examples/sec; 0.181 sec/batch)
2016-11-29 03:22:13.220911: step 4170, loss = 1.12 (694.1 examples/sec; 0.184 sec/batch)
2016-11-29 03:22:15.165180: step 4180, loss = 0.91 (629.1 examples/sec; 0.203 sec/batch)
2016-11-29 03:22:17.071994: step 4190, loss = 0.86 (668.7 examples/sec; 0.191 sec/batch)
2016-11-29 03:22:18.915449: step 4200, loss = 0.91 (691.7 examples/sec; 0.185 sec/batch)
2016-11-29 03:22:20.912787: step 4210, loss = 1.10 (700.0 examples/sec; 0.183 sec/batch)
2016-11-29 03:22:22.724265: step 4220, loss = 1.06 (678.2 examples/sec; 0.189 sec/batch)
2016-11-29 03:22:24.558245: step 4230, loss = 1.02 (673.8 examples/sec; 0.190 sec/batch)
2016-11-29 03:22:26.419548: step 4240, loss = 1.11 (775.7 examples/sec; 0.165 sec/batch)
2016-11-29 03:22:28.217390: step 4250, loss = 1.09 (709.1 examples/sec; 0.181 sec/batch)
2016-11-29 03:22:30.022760: step 4260, loss = 1.32 (732.6 examples/sec; 0.175 sec/batch)
2016-11-29 03:22:31.847406: step 4270, loss = 1.01 (684.5 examples/sec; 0.187 sec/batch)
2016-11-29 03:22:33.666894: step 4280, loss = 0.98 (699.0 examples/sec; 0.183 sec/batch)
2016-11-29 03:22:35.496213: step 4290, loss = 1.14 (699.6 examples/sec; 0.183 sec/batch)
2016-11-29 03:22:37.311611: step 4300, loss = 1.09 (696.5 examples/sec; 0.184 sec/batch)
2016-11-29 03:22:39.330795: step 4310, loss = 1.02 (680.8 examples/sec; 0.188 sec/batch)
2016-11-29 03:22:41.160787: step 4320, loss = 1.03 (691.1 examples/sec; 0.185 sec/batch)
2016-11-29 03:22:42.980812: step 4330, loss = 1.06 (707.4 examples/sec; 0.181 sec/batch)
2016-11-29 03:22:44.804249: step 4340, loss = 0.93 (708.4 examples/sec; 0.181 sec/batch)
2016-11-29 03:22:46.632273: step 4350, loss = 0.93 (694.5 examples/sec; 0.184 sec/batch)
2016-11-29 03:22:48.443641: step 4360, loss = 0.89 (705.7 examples/sec; 0.181 sec/batch)
2016-11-29 03:22:50.248861: step 4370, loss = 1.29 (701.5 examples/sec; 0.182 sec/batch)
2016-11-29 03:22:52.064777: step 4380, loss = 1.01 (694.8 examples/sec; 0.184 sec/batch)
2016-11-29 03:22:53.866710: step 4390, loss = 0.93 (711.5 examples/sec; 0.180 sec/batch)
2016-11-29 03:22:55.664835: step 4400, loss = 1.26 (706.9 examples/sec; 0.181 sec/batch)
2016-11-29 03:22:57.648649: step 4410, loss = 1.08 (686.7 examples/sec; 0.186 sec/batch)
2016-11-29 03:22:59.460963: step 4420, loss = 0.99 (690.6 examples/sec; 0.185 sec/batch)
2016-11-29 03:23:01.277096: step 4430, loss = 0.99 (700.6 examples/sec; 0.183 sec/batch)
2016-11-29 03:23:03.097857: step 4440, loss = 1.09 (698.0 examples/sec; 0.183 sec/batch)
2016-11-29 03:23:04.899538: step 4450, loss = 1.19 (716.8 examples/sec; 0.179 sec/batch)
2016-11-29 03:23:06.709261: step 4460, loss = 0.98 (705.7 examples/sec; 0.181 sec/batch)
2016-11-29 03:23:08.498076: step 4470, loss = 1.02 (724.5 examples/sec; 0.177 sec/batch)
2016-11-29 03:23:10.308198: step 4480, loss = 1.09 (691.0 examples/sec; 0.185 sec/batch)
2016-11-29 03:23:12.129628: step 4490, loss = 1.10 (764.4 examples/sec; 0.167 sec/batch)
2016-11-29 03:23:13.939294: step 4500, loss = 0.97 (694.6 examples/sec; 0.184 sec/batch)
2016-11-29 03:23:15.931107: step 4510, loss = 1.10 (718.0 examples/sec; 0.178 sec/batch)
2016-11-29 03:23:17.735761: step 4520, loss = 1.04 (749.5 examples/sec; 0.171 sec/batch)
2016-11-29 03:23:19.557923: step 4530, loss = 0.88 (698.1 examples/sec; 0.183 sec/batch)
2016-11-29 03:23:21.365752: step 4540, loss = 1.08 (714.4 examples/sec; 0.179 sec/batch)
2016-11-29 03:23:23.186434: step 4550, loss = 0.97 (667.4 examples/sec; 0.192 sec/batch)
2016-11-29 03:23:25.000534: step 4560, loss = 1.02 (710.9 examples/sec; 0.180 sec/batch)
2016-11-29 03:23:26.804900: step 4570, loss = 1.02 (717.7 examples/sec; 0.178 sec/batch)
2016-11-29 03:23:28.617168: step 4580, loss = 1.07 (703.4 examples/sec; 0.182 sec/batch)
2016-11-29 03:23:30.443764: step 4590, loss = 1.08 (691.4 examples/sec; 0.185 sec/batch)
2016-11-29 03:23:32.250163: step 4600, loss = 1.01 (695.8 examples/sec; 0.184 sec/batch)
2016-11-29 03:23:34.260068: step 4610, loss = 0.91 (624.1 examples/sec; 0.205 sec/batch)
2016-11-29 03:23:36.032091: step 4620, loss = 0.99 (734.2 examples/sec; 0.174 sec/batch)
2016-11-29 03:23:37.855940: step 4630, loss = 1.07 (640.7 examples/sec; 0.200 sec/batch)
2016-11-29 03:23:39.634083: step 4640, loss = 1.09 (692.5 examples/sec; 0.185 sec/batch)
2016-11-29 03:23:41.441358: step 4650, loss = 1.07 (725.4 examples/sec; 0.176 sec/batch)
2016-11-29 03:23:43.244410: step 4660, loss = 0.96 (698.7 examples/sec; 0.183 sec/batch)
2016-11-29 03:23:45.041627: step 4670, loss = 1.03 (708.7 examples/sec; 0.181 sec/batch)
2016-11-29 03:23:46.840462: step 4680, loss = 1.09 (715.7 examples/sec; 0.179 sec/batch)
2016-11-29 03:23:48.631036: step 4690, loss = 1.02 (723.1 examples/sec; 0.177 sec/batch)
2016-11-29 03:23:50.430112: step 4700, loss = 1.08 (703.5 examples/sec; 0.182 sec/batch)
2016-11-29 03:23:52.417894: step 4710, loss = 1.16 (720.1 examples/sec; 0.178 sec/batch)
2016-11-29 03:23:54.226297: step 4720, loss = 1.28 (707.6 examples/sec; 0.181 sec/batch)
2016-11-29 03:23:56.031992: step 4730, loss = 1.05 (715.7 examples/sec; 0.179 sec/batch)
2016-11-29 03:23:57.849966: step 4740, loss = 0.98 (721.4 examples/sec; 0.177 sec/batch)
2016-11-29 03:23:59.659284: step 4750, loss = 1.02 (703.6 examples/sec; 0.182 sec/batch)
2016-11-29 03:24:01.457905: step 4760, loss = 1.14 (723.7 examples/sec; 0.177 sec/batch)
2016-11-29 03:24:03.272348: step 4770, loss = 0.96 (716.6 examples/sec; 0.179 sec/batch)
2016-11-29 03:24:05.088092: step 4780, loss = 1.05 (698.6 examples/sec; 0.183 sec/batch)
2016-11-29 03:24:06.892261: step 4790, loss = 0.97 (693.7 examples/sec; 0.185 sec/batch)
2016-11-29 03:24:08.715942: step 4800, loss = 0.95 (709.0 examples/sec; 0.181 sec/batch)
2016-11-29 03:24:10.705353: step 4810, loss = 1.00 (707.1 examples/sec; 0.181 sec/batch)
2016-11-29 03:24:12.500964: step 4820, loss = 0.94 (714.4 examples/sec; 0.179 sec/batch)
2016-11-29 03:24:14.307760: step 4830, loss = 0.84 (724.4 examples/sec; 0.177 sec/batch)
2016-11-29 03:24:16.104146: step 4840, loss = 1.06 (723.3 examples/sec; 0.177 sec/batch)
2016-11-29 03:24:17.928831: step 4850, loss = 0.88 (690.5 examples/sec; 0.185 sec/batch)
2016-11-29 03:24:19.739145: step 4860, loss = 1.09 (696.8 examples/sec; 0.184 sec/batch)
2016-11-29 03:24:21.526251: step 4870, loss = 0.96 (778.7 examples/sec; 0.164 sec/batch)
2016-11-29 03:24:23.340702: step 4880, loss = 1.16 (717.3 examples/sec; 0.178 sec/batch)
2016-11-29 03:24:25.143693: step 4890, loss = 1.04 (710.2 examples/sec; 0.180 sec/batch)
2016-11-29 03:24:26.953265: step 4900, loss = 0.99 (722.5 examples/sec; 0.177 sec/batch)
2016-11-29 03:24:28.936803: step 4910, loss = 0.74 (736.9 examples/sec; 0.174 sec/batch)
2016-11-29 03:24:30.731108: step 4920, loss = 0.98 (706.0 examples/sec; 0.181 sec/batch)
2016-11-29 03:24:32.540667: step 4930, loss = 0.93 (702.5 examples/sec; 0.182 sec/batch)
2016-11-29 03:24:34.351395: step 4940, loss = 1.17 (709.3 examples/sec; 0.180 sec/batch)
2016-11-29 03:24:36.159675: step 4950, loss = 0.94 (829.7 examples/sec; 0.154 sec/batch)
2016-11-29 03:24:37.984928: step 4960, loss = 1.04 (643.1 examples/sec; 0.199 sec/batch)
2016-11-29 03:24:39.763321: step 4970, loss = 0.83 (699.4 examples/sec; 0.183 sec/batch)
2016-11-29 03:24:41.573549: step 4980, loss = 1.02 (700.8 examples/sec; 0.183 sec/batch)
2016-11-29 03:24:43.376458: step 4990, loss = 1.04 (722.5 examples/sec; 0.177 sec/batch)
