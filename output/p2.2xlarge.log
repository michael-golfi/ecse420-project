ubuntu@ip-172-31-38-101:~/ecse420-project/cifar10$ time python cifar10_multi_gpu_train.py --num_gpus=1
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.7.5 locally
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties:
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:1e.0
Total memory: 11.25GiB
Free memory: 11.13GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)
2016-11-28 18:20:01.585815: step 0, loss = 4.67 (1.8 examples/sec; 70.150 sec/batch)
2016-11-28 18:20:03.480175: step 10, loss = 4.64 (821.9 examples/sec; 0.156 sec/batch)
2016-11-28 18:20:04.981457: step 20, loss = 4.41 (863.4 examples/sec; 0.148 sec/batch)
2016-11-28 18:20:06.523929: step 30, loss = 4.34 (857.4 examples/sec; 0.149 sec/batch)
2016-11-28 18:20:08.077421: step 40, loss = 4.63 (842.7 examples/sec; 0.152 sec/batch)
2016-11-28 18:20:09.653306: step 50, loss = 4.25 (816.4 examples/sec; 0.157 sec/batch)
2016-11-28 18:20:11.237509: step 60, loss = 4.24 (785.0 examples/sec; 0.163 sec/batch)
2016-11-28 18:20:12.816201: step 70, loss = 4.16 (797.4 examples/sec; 0.161 sec/batch)
2016-11-28 18:20:14.404978: step 80, loss = 4.17 (788.8 examples/sec; 0.162 sec/batch)
2016-11-28 18:20:16.023453: step 90, loss = 4.15 (826.4 examples/sec; 0.155 sec/batch)
2016-11-28 18:20:17.644319: step 100, loss = 4.11 (767.7 examples/sec; 0.167 sec/batch)
2016-11-28 18:20:19.452995: step 110, loss = 4.06 (841.8 examples/sec; 0.152 sec/batch)
2016-11-28 18:20:21.036347: step 120, loss = 4.29 (804.3 examples/sec; 0.159 sec/batch)
2016-11-28 18:20:22.617239: step 130, loss = 4.08 (791.1 examples/sec; 0.162 sec/batch)
2016-11-28 18:20:24.201144: step 140, loss = 3.98 (818.7 examples/sec; 0.156 sec/batch)
2016-11-28 18:20:25.782207: step 150, loss = 4.02 (794.9 examples/sec; 0.161 sec/batch)
2016-11-28 18:20:27.356558: step 160, loss = 3.81 (857.7 examples/sec; 0.149 sec/batch)
2016-11-28 18:20:28.944432: step 170, loss = 3.88 (778.9 examples/sec; 0.164 sec/batch)
2016-11-28 18:20:30.529587: step 180, loss = 4.06 (812.4 examples/sec; 0.158 sec/batch)
2016-11-28 18:20:32.118991: step 190, loss = 3.87 (808.3 examples/sec; 0.158 sec/batch)
2016-11-28 18:20:33.721581: step 200, loss = 3.82 (790.9 examples/sec; 0.162 sec/batch)
2016-11-28 18:20:35.495177: step 210, loss = 3.72 (824.4 examples/sec; 0.155 sec/batch)
2016-11-28 18:20:37.047799: step 220, loss = 3.70 (826.3 examples/sec; 0.155 sec/batch)
2016-11-28 18:20:38.605716: step 230, loss = 3.83 (799.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:20:40.158977: step 240, loss = 3.66 (823.8 examples/sec; 0.155 sec/batch)
2016-11-28 18:20:41.730562: step 250, loss = 3.63 (779.0 examples/sec; 0.164 sec/batch)
2016-11-28 18:20:43.282831: step 260, loss = 3.66 (824.6 examples/sec; 0.155 sec/batch)
2016-11-28 18:20:44.844798: step 270, loss = 3.78 (815.3 examples/sec; 0.157 sec/batch)
2016-11-28 18:20:46.405657: step 280, loss = 3.59 (839.8 examples/sec; 0.152 sec/batch)
2016-11-28 18:20:47.979639: step 290, loss = 3.56 (798.7 examples/sec; 0.160 sec/batch)
2016-11-28 18:20:49.537861: step 300, loss = 3.49 (810.2 examples/sec; 0.158 sec/batch)
2016-11-28 18:20:51.294310: step 310, loss = 3.62 (826.8 examples/sec; 0.155 sec/batch)
2016-11-28 18:20:52.854896: step 320, loss = 3.49 (847.5 examples/sec; 0.151 sec/batch)
2016-11-28 18:20:54.429490: step 330, loss = 3.57 (811.8 examples/sec; 0.158 sec/batch)
2016-11-28 18:20:55.988104: step 340, loss = 3.52 (803.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:20:57.551367: step 350, loss = 3.60 (796.1 examples/sec; 0.161 sec/batch)
2016-11-28 18:20:59.106802: step 360, loss = 3.50 (849.3 examples/sec; 0.151 sec/batch)
2016-11-28 18:21:00.663396: step 370, loss = 3.48 (836.7 examples/sec; 0.153 sec/batch)
2016-11-28 18:21:02.241416: step 380, loss = 3.41 (823.5 examples/sec; 0.155 sec/batch)
2016-11-28 18:21:03.831289: step 390, loss = 3.38 (823.5 examples/sec; 0.155 sec/batch)
2016-11-28 18:21:05.421339: step 400, loss = 3.47 (832.8 examples/sec; 0.154 sec/batch)
2016-11-28 18:21:07.223910: step 410, loss = 3.26 (792.0 examples/sec; 0.162 sec/batch)
2016-11-28 18:21:08.822700: step 420, loss = 3.43 (807.7 examples/sec; 0.158 sec/batch)
2016-11-28 18:21:10.459185: step 430, loss = 3.55 (755.2 examples/sec; 0.169 sec/batch)
2016-11-28 18:21:12.081876: step 440, loss = 3.37 (817.1 examples/sec; 0.157 sec/batch)
2016-11-28 18:21:13.708344: step 450, loss = 3.27 (772.6 examples/sec; 0.166 sec/batch)
2016-11-28 18:21:15.335741: step 460, loss = 3.35 (790.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:21:16.959618: step 470, loss = 3.12 (773.4 examples/sec; 0.166 sec/batch)
2016-11-28 18:21:18.589375: step 480, loss = 3.46 (815.2 examples/sec; 0.157 sec/batch)
2016-11-28 18:21:20.208528: step 490, loss = 3.45 (778.9 examples/sec; 0.164 sec/batch)
2016-11-28 18:21:21.830041: step 500, loss = 3.52 (815.3 examples/sec; 0.157 sec/batch)
2016-11-28 18:21:23.607094: step 510, loss = 3.11 (836.5 examples/sec; 0.153 sec/batch)
2016-11-28 18:21:25.180163: step 520, loss = 3.18 (837.5 examples/sec; 0.153 sec/batch)
2016-11-28 18:21:26.743742: step 530, loss = 3.11 (795.0 examples/sec; 0.161 sec/batch)
2016-11-28 18:21:28.306385: step 540, loss = 2.94 (830.3 examples/sec; 0.154 sec/batch)
2016-11-28 18:21:29.880895: step 550, loss = 3.08 (796.3 examples/sec; 0.161 sec/batch)
2016-11-28 18:21:31.452419: step 560, loss = 3.01 (823.9 examples/sec; 0.155 sec/batch)
2016-11-28 18:21:33.015589: step 570, loss = 3.15 (799.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:21:34.583346: step 580, loss = 3.03 (815.0 examples/sec; 0.157 sec/batch)
2016-11-28 18:21:36.139539: step 590, loss = 3.06 (831.8 examples/sec; 0.154 sec/batch)
2016-11-28 18:21:37.692737: step 600, loss = 3.01 (826.8 examples/sec; 0.155 sec/batch)
2016-11-28 18:21:39.452150: step 610, loss = 2.94 (823.7 examples/sec; 0.155 sec/batch)
2016-11-28 18:21:41.008751: step 620, loss = 3.01 (818.4 examples/sec; 0.156 sec/batch)
2016-11-28 18:21:42.573496: step 630, loss = 2.85 (829.5 examples/sec; 0.154 sec/batch)
2016-11-28 18:21:44.129423: step 640, loss = 3.10 (797.7 examples/sec; 0.160 sec/batch)
2016-11-28 18:21:45.675920: step 650, loss = 2.90 (792.5 examples/sec; 0.162 sec/batch)
2016-11-28 18:21:47.225915: step 660, loss = 2.78 (806.0 examples/sec; 0.159 sec/batch)
2016-11-28 18:21:48.783037: step 670, loss = 2.88 (828.4 examples/sec; 0.155 sec/batch)
2016-11-28 18:21:50.345380: step 680, loss = 2.83 (828.6 examples/sec; 0.154 sec/batch)
2016-11-28 18:21:51.907596: step 690, loss = 2.91 (811.6 examples/sec; 0.158 sec/batch)
2016-11-28 18:21:53.462209: step 700, loss = 2.86 (849.7 examples/sec; 0.151 sec/batch)
2016-11-28 18:21:55.238430: step 710, loss = 2.73 (818.6 examples/sec; 0.156 sec/batch)
2016-11-28 18:21:56.794371: step 720, loss = 3.27 (805.8 examples/sec; 0.159 sec/batch)
2016-11-28 18:21:58.392049: step 730, loss = 2.85 (841.8 examples/sec; 0.152 sec/batch)
2016-11-28 18:21:59.994659: step 740, loss = 2.86 (790.3 examples/sec; 0.162 sec/batch)
2016-11-28 18:22:01.595916: step 750, loss = 2.86 (766.3 examples/sec; 0.167 sec/batch)
2016-11-28 18:22:03.204702: step 760, loss = 2.79 (774.5 examples/sec; 0.165 sec/batch)
2016-11-28 18:22:04.825605: step 770, loss = 2.60 (778.2 examples/sec; 0.164 sec/batch)
2016-11-28 18:22:06.438847: step 780, loss = 2.77 (808.3 examples/sec; 0.158 sec/batch)
2016-11-28 18:22:08.064926: step 790, loss = 2.80 (786.1 examples/sec; 0.163 sec/batch)
2016-11-28 18:22:09.687062: step 800, loss = 2.99 (767.1 examples/sec; 0.167 sec/batch)
2016-11-28 18:22:11.519523: step 810, loss = 2.76 (784.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:22:13.139487: step 820, loss = 2.87 (821.5 examples/sec; 0.156 sec/batch)
2016-11-28 18:22:14.769116: step 830, loss = 2.80 (771.1 examples/sec; 0.166 sec/batch)
2016-11-28 18:22:16.403010: step 840, loss = 2.55 (756.4 examples/sec; 0.169 sec/batch)
2016-11-28 18:22:18.029023: step 850, loss = 2.81 (822.0 examples/sec; 0.156 sec/batch)
2016-11-28 18:22:19.653880: step 860, loss = 2.74 (785.5 examples/sec; 0.163 sec/batch)
2016-11-28 18:22:21.276394: step 870, loss = 2.71 (795.9 examples/sec; 0.161 sec/batch)
2016-11-28 18:22:22.895099: step 880, loss = 2.63 (808.6 examples/sec; 0.158 sec/batch)
2016-11-28 18:22:24.524752: step 890, loss = 2.48 (777.1 examples/sec; 0.165 sec/batch)
2016-11-28 18:22:26.131077: step 900, loss = 2.81 (808.7 examples/sec; 0.158 sec/batch)
2016-11-28 18:22:27.960755: step 910, loss = 2.64 (755.3 examples/sec; 0.169 sec/batch)
2016-11-28 18:22:29.580195: step 920, loss = 2.52 (799.3 examples/sec; 0.160 sec/batch)
2016-11-28 18:22:31.198951: step 930, loss = 2.66 (788.0 examples/sec; 0.162 sec/batch)
2016-11-28 18:22:32.809494: step 940, loss = 2.45 (783.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:22:34.426141: step 950, loss = 2.56 (843.0 examples/sec; 0.152 sec/batch)
2016-11-28 18:22:36.053581: step 960, loss = 2.44 (813.2 examples/sec; 0.157 sec/batch)
2016-11-28 18:22:37.669396: step 970, loss = 2.41 (788.0 examples/sec; 0.162 sec/batch)
2016-11-28 18:22:39.292854: step 980, loss = 2.71 (772.3 examples/sec; 0.166 sec/batch)
2016-11-28 18:22:40.907284: step 990, loss = 2.65 (794.9 examples/sec; 0.161 sec/batch)
2016-11-28 18:22:42.536619: step 1000, loss = 2.55 (765.1 examples/sec; 0.167 sec/batch)
2016-11-28 18:22:44.750701: step 1010, loss = 2.45 (820.9 examples/sec; 0.156 sec/batch)
2016-11-28 18:22:46.337982: step 1020, loss = 2.45 (794.9 examples/sec; 0.161 sec/batch)
2016-11-28 18:22:47.924202: step 1030, loss = 2.34 (830.8 examples/sec; 0.154 sec/batch)
2016-11-28 18:22:49.509333: step 1040, loss = 2.53 (789.5 examples/sec; 0.162 sec/batch)
2016-11-28 18:22:51.130413: step 1050, loss = 2.36 (798.6 examples/sec; 0.160 sec/batch)
2016-11-28 18:22:52.751462: step 1060, loss = 2.31 (797.5 examples/sec; 0.161 sec/batch)
2016-11-28 18:22:54.356304: step 1070, loss = 2.51 (777.0 examples/sec; 0.165 sec/batch)
2016-11-28 18:22:55.976854: step 1080, loss = 2.42 (807.2 examples/sec; 0.159 sec/batch)
2016-11-28 18:22:57.597104: step 1090, loss = 2.35 (821.3 examples/sec; 0.156 sec/batch)
2016-11-28 18:22:59.222678: step 1100, loss = 2.45 (806.1 examples/sec; 0.159 sec/batch)
2016-11-28 18:23:01.047757: step 1110, loss = 2.37 (796.1 examples/sec; 0.161 sec/batch)
2016-11-28 18:23:02.673547: step 1120, loss = 2.31 (777.2 examples/sec; 0.165 sec/batch)
2016-11-28 18:23:04.295348: step 1130, loss = 2.27 (768.5 examples/sec; 0.167 sec/batch)
2016-11-28 18:23:05.920735: step 1140, loss = 2.22 (767.5 examples/sec; 0.167 sec/batch)
2016-11-28 18:23:07.536212: step 1150, loss = 2.20 (802.1 examples/sec; 0.160 sec/batch)
2016-11-28 18:23:09.149873: step 1160, loss = 2.31 (776.3 examples/sec; 0.165 sec/batch)
2016-11-28 18:23:10.769890: step 1170, loss = 2.21 (799.2 examples/sec; 0.160 sec/batch)
2016-11-28 18:23:12.387920: step 1180, loss = 2.24 (806.5 examples/sec; 0.159 sec/batch)
2016-11-28 18:23:14.006942: step 1190, loss = 2.49 (799.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:23:15.633435: step 1200, loss = 2.26 (773.5 examples/sec; 0.165 sec/batch)
2016-11-28 18:23:17.468115: step 1210, loss = 2.19 (781.9 examples/sec; 0.164 sec/batch)
2016-11-28 18:23:19.092014: step 1220, loss = 2.35 (769.7 examples/sec; 0.166 sec/batch)
2016-11-28 18:23:20.704313: step 1230, loss = 2.62 (793.1 examples/sec; 0.161 sec/batch)
2016-11-28 18:23:22.319894: step 1240, loss = 2.37 (809.1 examples/sec; 0.158 sec/batch)
2016-11-28 18:23:23.931884: step 1250, loss = 2.03 (822.2 examples/sec; 0.156 sec/batch)
2016-11-28 18:23:25.557178: step 1260, loss = 2.05 (768.9 examples/sec; 0.166 sec/batch)
2016-11-28 18:23:27.178091: step 1270, loss = 2.12 (773.8 examples/sec; 0.165 sec/batch)
2016-11-28 18:23:28.792914: step 1280, loss = 2.30 (788.5 examples/sec; 0.162 sec/batch)
2016-11-28 18:23:30.422288: step 1290, loss = 1.98 (779.4 examples/sec; 0.164 sec/batch)
2016-11-28 18:23:32.053590: step 1300, loss = 2.15 (788.6 examples/sec; 0.162 sec/batch)
2016-11-28 18:23:33.831861: step 1310, loss = 2.01 (828.8 examples/sec; 0.154 sec/batch)
2016-11-28 18:23:35.404961: step 1320, loss = 2.14 (808.1 examples/sec; 0.158 sec/batch)
2016-11-28 18:23:36.978447: step 1330, loss = 2.10 (781.5 examples/sec; 0.164 sec/batch)
2016-11-28 18:23:38.570029: step 1340, loss = 2.21 (784.0 examples/sec; 0.163 sec/batch)
2016-11-28 18:23:40.177958: step 1350, loss = 1.98 (774.1 examples/sec; 0.165 sec/batch)
2016-11-28 18:23:41.817691: step 1360, loss = 2.06 (787.3 examples/sec; 0.163 sec/batch)
2016-11-28 18:23:43.436424: step 1370, loss = 2.05 (803.3 examples/sec; 0.159 sec/batch)
2016-11-28 18:23:45.057489: step 1380, loss = 2.16 (797.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:23:46.685067: step 1390, loss = 1.96 (754.2 examples/sec; 0.170 sec/batch)
2016-11-28 18:23:48.296212: step 1400, loss = 2.02 (793.0 examples/sec; 0.161 sec/batch)
2016-11-28 18:23:50.123383: step 1410, loss = 2.00 (776.0 examples/sec; 0.165 sec/batch)
2016-11-28 18:23:51.752119: step 1420, loss = 1.88 (785.6 examples/sec; 0.163 sec/batch)
2016-11-28 18:23:53.363325: step 1430, loss = 2.02 (803.5 examples/sec; 0.159 sec/batch)
2016-11-28 18:23:54.987122: step 1440, loss = 2.02 (779.1 examples/sec; 0.164 sec/batch)
2016-11-28 18:23:56.605321: step 1450, loss = 2.04 (787.9 examples/sec; 0.162 sec/batch)
2016-11-28 18:23:58.239091: step 1460, loss = 2.28 (776.5 examples/sec; 0.165 sec/batch)
2016-11-28 18:23:59.852533: step 1470, loss = 2.00 (804.9 examples/sec; 0.159 sec/batch)
2016-11-28 18:24:01.480153: step 1480, loss = 1.99 (754.5 examples/sec; 0.170 sec/batch)
2016-11-28 18:24:03.090376: step 1490, loss = 1.95 (805.2 examples/sec; 0.159 sec/batch)
2016-11-28 18:24:04.718587: step 1500, loss = 1.86 (813.7 examples/sec; 0.157 sec/batch)
2016-11-28 18:24:06.554057: step 1510, loss = 2.24 (811.1 examples/sec; 0.158 sec/batch)
2016-11-28 18:24:08.175869: step 1520, loss = 2.43 (783.8 examples/sec; 0.163 sec/batch)
2016-11-28 18:24:09.790662: step 1530, loss = 1.86 (801.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:24:11.419330: step 1540, loss = 1.78 (776.3 examples/sec; 0.165 sec/batch)
2016-11-28 18:24:13.039344: step 1550, loss = 1.92 (760.8 examples/sec; 0.168 sec/batch)
2016-11-28 18:24:14.653654: step 1560, loss = 1.98 (805.5 examples/sec; 0.159 sec/batch)
2016-11-28 18:24:16.276268: step 1570, loss = 2.02 (808.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:24:17.893153: step 1580, loss = 1.78 (779.0 examples/sec; 0.164 sec/batch)
2016-11-28 18:24:19.497013: step 1590, loss = 1.93 (818.2 examples/sec; 0.156 sec/batch)
2016-11-28 18:24:21.114469: step 1600, loss = 2.20 (781.5 examples/sec; 0.164 sec/batch)
2016-11-28 18:24:22.939437: step 1610, loss = 1.87 (790.1 examples/sec; 0.162 sec/batch)
2016-11-28 18:24:24.553510: step 1620, loss = 1.95 (795.0 examples/sec; 0.161 sec/batch)
2016-11-28 18:24:26.182448: step 1630, loss = 1.84 (766.2 examples/sec; 0.167 sec/batch)
2016-11-28 18:24:27.797648: step 1640, loss = 1.78 (780.1 examples/sec; 0.164 sec/batch)
2016-11-28 18:24:29.419710: step 1650, loss = 1.83 (802.2 examples/sec; 0.160 sec/batch)
2016-11-28 18:24:31.042801: step 1660, loss = 1.88 (755.2 examples/sec; 0.170 sec/batch)
2016-11-28 18:24:32.664268: step 1670, loss = 1.74 (775.7 examples/sec; 0.165 sec/batch)
2016-11-28 18:24:34.292960: step 1680, loss = 1.85 (737.6 examples/sec; 0.174 sec/batch)
2016-11-28 18:24:35.902565: step 1690, loss = 1.69 (779.2 examples/sec; 0.164 sec/batch)
2016-11-28 18:24:37.522470: step 1700, loss = 1.96 (754.9 examples/sec; 0.170 sec/batch)
2016-11-28 18:24:39.332730: step 1710, loss = 1.95 (799.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:24:40.952664: step 1720, loss = 1.84 (780.3 examples/sec; 0.164 sec/batch)
2016-11-28 18:24:42.569298: step 1730, loss = 1.75 (806.4 examples/sec; 0.159 sec/batch)
2016-11-28 18:24:44.183292: step 1740, loss = 1.81 (788.0 examples/sec; 0.162 sec/batch)
2016-11-28 18:24:45.807492: step 1750, loss = 1.81 (794.8 examples/sec; 0.161 sec/batch)
2016-11-28 18:24:47.421933: step 1760, loss = 1.76 (782.4 examples/sec; 0.164 sec/batch)
2016-11-28 18:24:49.042958: step 1770, loss = 1.79 (797.2 examples/sec; 0.161 sec/batch)
2016-11-28 18:24:50.671726: step 1780, loss = 1.73 (811.0 examples/sec; 0.158 sec/batch)
2016-11-28 18:24:52.286122: step 1790, loss = 1.78 (805.9 examples/sec; 0.159 sec/batch)
2016-11-28 18:24:53.899399: step 1800, loss = 1.71 (765.7 examples/sec; 0.167 sec/batch)
2016-11-28 18:24:55.722809: step 1810, loss = 1.92 (807.0 examples/sec; 0.159 sec/batch)
2016-11-28 18:24:57.339923: step 1820, loss = 1.69 (810.8 examples/sec; 0.158 sec/batch)
2016-11-28 18:24:58.960783: step 1830, loss = 1.91 (808.8 examples/sec; 0.158 sec/batch)
2016-11-28 18:25:00.576131: step 1840, loss = 1.71 (794.6 examples/sec; 0.161 sec/batch)
2016-11-28 18:25:02.196350: step 1850, loss = 1.89 (788.2 examples/sec; 0.162 sec/batch)
2016-11-28 18:25:03.821651: step 1860, loss = 1.78 (768.0 examples/sec; 0.167 sec/batch)
2016-11-28 18:25:05.447357: step 1870, loss = 1.62 (757.0 examples/sec; 0.169 sec/batch)
2016-11-28 18:25:07.075440: step 1880, loss = 1.79 (780.2 examples/sec; 0.164 sec/batch)
2016-11-28 18:25:08.701327: step 1890, loss = 1.70 (760.5 examples/sec; 0.168 sec/batch)
2016-11-28 18:25:10.324589: step 1900, loss = 1.92 (761.6 examples/sec; 0.168 sec/batch)
2016-11-28 18:25:12.114601: step 1910, loss = 1.63 (800.5 examples/sec; 0.160 sec/batch)
2016-11-28 18:25:13.689112: step 1920, loss = 1.72 (842.6 examples/sec; 0.152 sec/batch)
2016-11-28 18:25:15.270608: step 1930, loss = 1.74 (832.6 examples/sec; 0.154 sec/batch)
2016-11-28 18:25:16.899337: step 1940, loss = 1.86 (758.7 examples/sec; 0.169 sec/batch)
2016-11-28 18:25:18.509554: step 1950, loss = 1.66 (779.9 examples/sec; 0.164 sec/batch)
2016-11-28 18:25:20.137683: step 1960, loss = 1.60 (793.6 examples/sec; 0.161 sec/batch)
2016-11-28 18:25:21.755280: step 1970, loss = 1.55 (771.7 examples/sec; 0.166 sec/batch)
2016-11-28 18:25:23.367712: step 1980, loss = 2.01 (809.3 examples/sec; 0.158 sec/batch)
2016-11-28 18:25:24.984235: step 1990, loss = 1.67 (810.4 examples/sec; 0.158 sec/batch)
2016-11-28 18:25:26.603826: step 2000, loss = 1.58 (801.7 examples/sec; 0.160 sec/batch)
2016-11-28 18:25:28.784162: step 2010, loss = 1.66 (836.2 examples/sec; 0.153 sec/batch)
2016-11-28 18:25:30.351325: step 2020, loss = 1.79 (829.0 examples/sec; 0.154 sec/batch)
2016-11-28 18:25:31.928670: step 2030, loss = 1.55 (795.7 examples/sec; 0.161 sec/batch)
2016-11-28 18:25:33.492903: step 2040, loss = 1.61 (841.0 examples/sec; 0.152 sec/batch)
2016-11-28 18:25:35.062334: step 2050, loss = 1.61 (789.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:25:36.624471: step 2060, loss = 1.56 (799.0 examples/sec; 0.160 sec/batch)
2016-11-28 18:25:38.203897: step 2070, loss = 1.62 (855.9 examples/sec; 0.150 sec/batch)
2016-11-28 18:25:39.778907: step 2080, loss = 1.56 (811.9 examples/sec; 0.158 sec/batch)
2016-11-28 18:25:41.345301: step 2090, loss = 1.81 (823.0 examples/sec; 0.156 sec/batch)
2016-11-28 18:25:42.909954: step 2100, loss = 1.92 (824.5 examples/sec; 0.155 sec/batch)
2016-11-28 18:25:44.681235: step 2110, loss = 1.75 (802.1 examples/sec; 0.160 sec/batch)
2016-11-28 18:25:46.243406: step 2120, loss = 1.65 (839.4 examples/sec; 0.152 sec/batch)
2016-11-28 18:25:47.815304: step 2130, loss = 1.51 (799.7 examples/sec; 0.160 sec/batch)
2016-11-28 18:25:49.385959: step 2140, loss = 1.46 (825.9 examples/sec; 0.155 sec/batch)
2016-11-28 18:25:50.951317: step 2150, loss = 1.56 (795.5 examples/sec; 0.161 sec/batch)
2016-11-28 18:25:52.507865: step 2160, loss = 1.69 (837.9 examples/sec; 0.153 sec/batch)
2016-11-28 18:25:54.067851: step 2170, loss = 1.57 (812.2 examples/sec; 0.158 sec/batch)
2016-11-28 18:25:55.628573: step 2180, loss = 1.62 (845.3 examples/sec; 0.151 sec/batch)
2016-11-28 18:25:57.199379: step 2190, loss = 1.70 (834.9 examples/sec; 0.153 sec/batch)
2016-11-28 18:25:58.771013: step 2200, loss = 1.62 (811.4 examples/sec; 0.158 sec/batch)
2016-11-28 18:26:00.536656: step 2210, loss = 1.62 (804.4 examples/sec; 0.159 sec/batch)
2016-11-28 18:26:02.113051: step 2220, loss = 1.55 (788.8 examples/sec; 0.162 sec/batch)
2016-11-28 18:26:03.694439: step 2230, loss = 1.58 (781.4 examples/sec; 0.164 sec/batch)
2016-11-28 18:26:05.318124: step 2240, loss = 1.50 (802.5 examples/sec; 0.159 sec/batch)
2016-11-28 18:26:06.924474: step 2250, loss = 1.63 (788.2 examples/sec; 0.162 sec/batch)
2016-11-28 18:26:08.540737: step 2260, loss = 1.57 (811.9 examples/sec; 0.158 sec/batch)
2016-11-28 18:26:10.165220: step 2270, loss = 1.44 (790.2 examples/sec; 0.162 sec/batch)
2016-11-28 18:26:11.786948: step 2280, loss = 1.62 (796.7 examples/sec; 0.161 sec/batch)
2016-11-28 18:26:13.403639: step 2290, loss = 1.58 (825.0 examples/sec; 0.155 sec/batch)
2016-11-28 18:26:15.015845: step 2300, loss = 1.56 (791.3 examples/sec; 0.162 sec/batch)
2016-11-28 18:26:16.804018: step 2310, loss = 1.74 (802.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:26:18.374304: step 2320, loss = 1.53 (795.5 examples/sec; 0.161 sec/batch)
2016-11-28 18:26:19.947533: step 2330, loss = 1.68 (776.1 examples/sec; 0.165 sec/batch)
2016-11-28 18:26:21.522209: step 2340, loss = 1.50 (828.5 examples/sec; 0.155 sec/batch)
2016-11-28 18:26:23.104614: step 2350, loss = 1.56 (811.7 examples/sec; 0.158 sec/batch)
2016-11-28 18:26:24.686268: step 2360, loss = 1.51 (843.8 examples/sec; 0.152 sec/batch)
2016-11-28 18:26:26.259716: step 2370, loss = 1.44 (801.6 examples/sec; 0.160 sec/batch)
2016-11-28 18:26:27.837560: step 2380, loss = 1.51 (765.2 examples/sec; 0.167 sec/batch)
2016-11-28 18:26:29.426472: step 2390, loss = 1.55 (790.3 examples/sec; 0.162 sec/batch)
2016-11-28 18:26:31.016216: step 2400, loss = 1.30 (794.7 examples/sec; 0.161 sec/batch)
2016-11-28 18:26:32.810742: step 2410, loss = 1.54 (786.2 examples/sec; 0.163 sec/batch)
2016-11-28 18:26:34.385813: step 2420, loss = 1.48 (787.2 examples/sec; 0.163 sec/batch)
2016-11-28 18:26:35.957575: step 2430, loss = 1.41 (830.8 examples/sec; 0.154 sec/batch)
2016-11-28 18:26:37.541878: step 2440, loss = 1.42 (819.3 examples/sec; 0.156 sec/batch)
2016-11-28 18:26:39.120371: step 2450, loss = 1.52 (782.5 examples/sec; 0.164 sec/batch)
2016-11-28 18:26:40.692555: step 2460, loss = 1.44 (813.7 examples/sec; 0.157 sec/batch)
2016-11-28 18:26:42.269899: step 2470, loss = 1.73 (826.3 examples/sec; 0.155 sec/batch)
2016-11-28 18:26:43.858655: step 2480, loss = 1.40 (820.8 examples/sec; 0.156 sec/batch)
2016-11-28 18:26:45.441051: step 2490, loss = 1.39 (855.3 examples/sec; 0.150 sec/batch)
2016-11-28 18:26:47.032225: step 2500, loss = 1.58 (771.3 examples/sec; 0.166 sec/batch)
2016-11-28 18:26:48.808276: step 2510, loss = 1.36 (828.8 examples/sec; 0.154 sec/batch)
2016-11-28 18:26:50.390454: step 2520, loss = 1.47 (819.3 examples/sec; 0.156 sec/batch)
2016-11-28 18:26:51.979918: step 2530, loss = 1.41 (816.7 examples/sec; 0.157 sec/batch)
2016-11-28 18:26:53.601851: step 2540, loss = 1.56 (808.8 examples/sec; 0.158 sec/batch)
2016-11-28 18:26:55.226235: step 2550, loss = 1.24 (795.3 examples/sec; 0.161 sec/batch)
2016-11-28 18:26:56.869459: step 2560, loss = 1.55 (770.6 examples/sec; 0.166 sec/batch)
2016-11-28 18:26:58.495423: step 2570, loss = 1.48 (786.0 examples/sec; 0.163 sec/batch)
2016-11-28 18:27:00.107500: step 2580, loss = 1.30 (841.7 examples/sec; 0.152 sec/batch)
2016-11-28 18:27:01.730510: step 2590, loss = 1.57 (753.1 examples/sec; 0.170 sec/batch)
2016-11-28 18:27:03.352562: step 2600, loss = 1.38 (793.8 examples/sec; 0.161 sec/batch)
2016-11-28 18:27:05.175701: step 2610, loss = 1.46 (769.9 examples/sec; 0.166 sec/batch)
2016-11-28 18:27:06.792069: step 2620, loss = 1.34 (799.3 examples/sec; 0.160 sec/batch)
2016-11-28 18:27:08.416246: step 2630, loss = 1.35 (796.3 examples/sec; 0.161 sec/batch)
2016-11-28 18:27:10.030511: step 2640, loss = 1.39 (772.1 examples/sec; 0.166 sec/batch)
2016-11-28 18:27:11.640697: step 2650, loss = 1.46 (802.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:27:13.258594: step 2660, loss = 1.27 (774.4 examples/sec; 0.165 sec/batch)
2016-11-28 18:27:14.869085: step 2670, loss = 1.33 (804.8 examples/sec; 0.159 sec/batch)
2016-11-28 18:27:16.474100: step 2680, loss = 1.29 (786.5 examples/sec; 0.163 sec/batch)
2016-11-28 18:27:18.079906: step 2690, loss = 1.50 (808.1 examples/sec; 0.158 sec/batch)
2016-11-28 18:27:19.698952: step 2700, loss = 1.26 (792.6 examples/sec; 0.161 sec/batch)
2016-11-28 18:27:21.515757: step 2710, loss = 1.35 (776.7 examples/sec; 0.165 sec/batch)
2016-11-28 18:27:23.129914: step 2720, loss = 1.31 (786.3 examples/sec; 0.163 sec/batch)
2016-11-28 18:27:24.738988: step 2730, loss = 1.37 (815.0 examples/sec; 0.157 sec/batch)
2016-11-28 18:27:26.353324: step 2740, loss = 1.39 (806.7 examples/sec; 0.159 sec/batch)
2016-11-28 18:27:27.971076: step 2750, loss = 1.44 (760.6 examples/sec; 0.168 sec/batch)
2016-11-28 18:27:29.587158: step 2760, loss = 1.17 (783.7 examples/sec; 0.163 sec/batch)
2016-11-28 18:27:31.216997: step 2770, loss = 1.38 (763.8 examples/sec; 0.168 sec/batch)
2016-11-28 18:27:32.824215: step 2780, loss = 1.40 (813.3 examples/sec; 0.157 sec/batch)
2016-11-28 18:27:34.439973: step 2790, loss = 1.36 (822.7 examples/sec; 0.156 sec/batch)
2016-11-28 18:27:36.053273: step 2800, loss = 1.46 (813.5 examples/sec; 0.157 sec/batch)
2016-11-28 18:27:37.837821: step 2810, loss = 1.07 (825.3 examples/sec; 0.155 sec/batch)
2016-11-28 18:27:39.417084: step 2820, loss = 1.35 (816.5 examples/sec; 0.157 sec/batch)
2016-11-28 18:27:41.003519: step 2830, loss = 1.30 (818.4 examples/sec; 0.156 sec/batch)
2016-11-28 18:27:42.617409: step 2840, loss = 1.43 (798.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:27:44.236660: step 2850, loss = 1.27 (786.5 examples/sec; 0.163 sec/batch)
2016-11-28 18:27:45.844578: step 2860, loss = 1.46 (790.9 examples/sec; 0.162 sec/batch)
2016-11-28 18:27:47.474875: step 2870, loss = 1.31 (789.0 examples/sec; 0.162 sec/batch)
2016-11-28 18:27:49.089583: step 2880, loss = 1.33 (798.5 examples/sec; 0.160 sec/batch)
2016-11-28 18:27:50.701424: step 2890, loss = 1.39 (785.1 examples/sec; 0.163 sec/batch)
2016-11-28 18:27:52.317174: step 2900, loss = 1.28 (796.2 examples/sec; 0.161 sec/batch)
2016-11-28 18:27:54.135693: step 2910, loss = 1.32 (776.3 examples/sec; 0.165 sec/batch)
2016-11-28 18:27:55.726379: step 2920, loss = 1.33 (828.2 examples/sec; 0.155 sec/batch)
2016-11-28 18:27:57.319266: step 2930, loss = 1.38 (791.8 examples/sec; 0.162 sec/batch)
2016-11-28 18:27:58.905808: step 2940, loss = 1.35 (812.3 examples/sec; 0.158 sec/batch)
2016-11-28 18:28:00.492304: step 2950, loss = 1.25 (813.7 examples/sec; 0.157 sec/batch)
2016-11-28 18:28:02.090228: step 2960, loss = 1.22 (767.6 examples/sec; 0.167 sec/batch)
2016-11-28 18:28:03.671762: step 2970, loss = 1.40 (801.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:28:05.260575: step 2980, loss = 1.47 (804.7 examples/sec; 0.159 sec/batch)
2016-11-28 18:28:06.849146: step 2990, loss = 1.38 (819.1 examples/sec; 0.156 sec/batch)
2016-11-28 18:28:08.435404: step 3000, loss = 1.31 (793.5 examples/sec; 0.161 sec/batch)
2016-11-28 18:28:10.590724: step 3010, loss = 1.40 (820.9 examples/sec; 0.156 sec/batch)
2016-11-28 18:28:12.132574: step 3020, loss = 1.42 (863.4 examples/sec; 0.148 sec/batch)
2016-11-28 18:28:13.674976: step 3030, loss = 1.31 (853.2 examples/sec; 0.150 sec/batch)
2016-11-28 18:28:15.225500: step 3040, loss = 1.23 (839.1 examples/sec; 0.153 sec/batch)
2016-11-28 18:28:16.770977: step 3050, loss = 1.21 (813.8 examples/sec; 0.157 sec/batch)
2016-11-28 18:28:18.311219: step 3060, loss = 1.32 (894.7 examples/sec; 0.143 sec/batch)
2016-11-28 18:28:19.870151: step 3070, loss = 1.39 (831.5 examples/sec; 0.154 sec/batch)
2016-11-28 18:28:21.418275: step 3080, loss = 1.35 (811.0 examples/sec; 0.158 sec/batch)
2016-11-28 18:28:22.965359: step 3090, loss = 1.32 (830.0 examples/sec; 0.154 sec/batch)
2016-11-28 18:28:24.521882: step 3100, loss = 1.22 (851.1 examples/sec; 0.150 sec/batch)
2016-11-28 18:28:26.260468: step 3110, loss = 1.07 (864.9 examples/sec; 0.148 sec/batch)
2016-11-28 18:28:27.802254: step 3120, loss = 1.17 (827.0 examples/sec; 0.155 sec/batch)
2016-11-28 18:28:29.389115: step 3130, loss = 1.23 (783.7 examples/sec; 0.163 sec/batch)
2016-11-28 18:28:30.976218: step 3140, loss = 1.41 (808.3 examples/sec; 0.158 sec/batch)
2016-11-28 18:28:32.571248: step 3150, loss = 1.27 (822.2 examples/sec; 0.156 sec/batch)
2016-11-28 18:28:34.157952: step 3160, loss = 1.18 (820.4 examples/sec; 0.156 sec/batch)
2016-11-28 18:28:35.763316: step 3170, loss = 1.44 (786.6 examples/sec; 0.163 sec/batch)
2016-11-28 18:28:37.356716: step 3180, loss = 1.20 (790.2 examples/sec; 0.162 sec/batch)
2016-11-28 18:28:38.972750: step 3190, loss = 1.16 (780.5 examples/sec; 0.164 sec/batch)
2016-11-28 18:28:40.586670: step 3200, loss = 1.38 (788.2 examples/sec; 0.162 sec/batch)
2016-11-28 18:28:42.399096: step 3210, loss = 1.38 (799.1 examples/sec; 0.160 sec/batch)
2016-11-28 18:28:44.013064: step 3220, loss = 1.14 (781.2 examples/sec; 0.164 sec/batch)
2016-11-28 18:28:45.611742: step 3230, loss = 1.15 (799.1 examples/sec; 0.160 sec/batch)
2016-11-28 18:28:47.226213: step 3240, loss = 1.46 (807.3 examples/sec; 0.159 sec/batch)
2016-11-28 18:28:48.842930: step 3250, loss = 1.29 (772.2 examples/sec; 0.166 sec/batch)
2016-11-28 18:28:50.452732: step 3260, loss = 1.24 (750.4 examples/sec; 0.171 sec/batch)
2016-11-28 18:28:52.068472: step 3270, loss = 1.22 (797.0 examples/sec; 0.161 sec/batch)
2016-11-28 18:28:53.684551: step 3280, loss = 1.13 (796.2 examples/sec; 0.161 sec/batch)
2016-11-28 18:28:55.307551: step 3290, loss = 1.20 (759.9 examples/sec; 0.168 sec/batch)
2016-11-28 18:28:56.909912: step 3300, loss = 1.34 (799.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:28:58.734591: step 3310, loss = 1.30 (825.0 examples/sec; 0.155 sec/batch)
2016-11-28 18:29:00.357785: step 3320, loss = 1.15 (794.4 examples/sec; 0.161 sec/batch)
2016-11-28 18:29:01.976025: step 3330, loss = 1.15 (788.7 examples/sec; 0.162 sec/batch)
2016-11-28 18:29:03.589058: step 3340, loss = 1.25 (771.7 examples/sec; 0.166 sec/batch)
2016-11-28 18:29:05.206210: step 3350, loss = 1.25 (779.1 examples/sec; 0.164 sec/batch)
2016-11-28 18:29:06.837210: step 3360, loss = 1.23 (802.0 examples/sec; 0.160 sec/batch)
2016-11-28 18:29:08.454285: step 3370, loss = 1.25 (785.5 examples/sec; 0.163 sec/batch)
2016-11-28 18:29:10.064647: step 3380, loss = 1.12 (793.8 examples/sec; 0.161 sec/batch)
2016-11-28 18:29:11.685726: step 3390, loss = 1.29 (779.0 examples/sec; 0.164 sec/batch)
2016-11-28 18:29:13.314583: step 3400, loss = 1.11 (800.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:29:15.135447: step 3410, loss = 1.40 (783.3 examples/sec; 0.163 sec/batch)
2016-11-28 18:29:16.750081: step 3420, loss = 1.38 (790.7 examples/sec; 0.162 sec/batch)
2016-11-28 18:29:18.380737: step 3430, loss = 1.09 (764.7 examples/sec; 0.167 sec/batch)
2016-11-28 18:29:20.000019: step 3440, loss = 1.31 (795.6 examples/sec; 0.161 sec/batch)
2016-11-28 18:29:21.630792: step 3450, loss = 1.10 (788.9 examples/sec; 0.162 sec/batch)
2016-11-28 18:29:23.249940: step 3460, loss = 1.25 (838.9 examples/sec; 0.153 sec/batch)
2016-11-28 18:29:24.871128: step 3470, loss = 1.20 (782.3 examples/sec; 0.164 sec/batch)
2016-11-28 18:29:26.495849: step 3480, loss = 1.34 (780.9 examples/sec; 0.164 sec/batch)
2016-11-28 18:29:28.111239: step 3490, loss = 1.25 (773.7 examples/sec; 0.165 sec/batch)
2016-11-28 18:29:29.728875: step 3500, loss = 1.20 (769.7 examples/sec; 0.166 sec/batch)
2016-11-28 18:29:31.483521: step 3510, loss = 1.22 (797.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:29:33.022411: step 3520, loss = 1.14 (870.8 examples/sec; 0.147 sec/batch)
2016-11-28 18:29:34.596843: step 3530, loss = 1.22 (815.6 examples/sec; 0.157 sec/batch)
2016-11-28 18:29:36.209483: step 3540, loss = 1.04 (794.0 examples/sec; 0.161 sec/batch)
2016-11-28 18:29:37.826551: step 3550, loss = 1.11 (825.0 examples/sec; 0.155 sec/batch)
2016-11-28 18:29:39.444975: step 3560, loss = 1.01 (788.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:29:41.065824: step 3570, loss = 1.16 (777.0 examples/sec; 0.165 sec/batch)
2016-11-28 18:29:42.678794: step 3580, loss = 1.19 (788.6 examples/sec; 0.162 sec/batch)
2016-11-28 18:29:44.287418: step 3590, loss = 1.10 (806.9 examples/sec; 0.159 sec/batch)
2016-11-28 18:29:45.907091: step 3600, loss = 1.47 (807.9 examples/sec; 0.158 sec/batch)
2016-11-28 18:29:47.733829: step 3610, loss = 1.18 (767.3 examples/sec; 0.167 sec/batch)
2016-11-28 18:29:49.351441: step 3620, loss = 1.18 (793.6 examples/sec; 0.161 sec/batch)
2016-11-28 18:29:50.976475: step 3630, loss = 1.51 (781.3 examples/sec; 0.164 sec/batch)
2016-11-28 18:29:52.592376: step 3640, loss = 1.09 (810.0 examples/sec; 0.158 sec/batch)
2016-11-28 18:29:54.225238: step 3650, loss = 1.09 (798.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:29:55.843440: step 3660, loss = 1.20 (811.0 examples/sec; 0.158 sec/batch)
2016-11-28 18:29:57.463336: step 3670, loss = 1.07 (802.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:29:59.079780: step 3680, loss = 1.15 (775.0 examples/sec; 0.165 sec/batch)
2016-11-28 18:30:00.694587: step 3690, loss = 0.95 (804.2 examples/sec; 0.159 sec/batch)
2016-11-28 18:30:02.317490: step 3700, loss = 1.07 (803.0 examples/sec; 0.159 sec/batch)
2016-11-28 18:30:04.122143: step 3710, loss = 1.54 (814.6 examples/sec; 0.157 sec/batch)
2016-11-28 18:30:05.720382: step 3720, loss = 1.06 (818.4 examples/sec; 0.156 sec/batch)
2016-11-28 18:30:07.319175: step 3730, loss = 1.20 (808.6 examples/sec; 0.158 sec/batch)
2016-11-28 18:30:08.924123: step 3740, loss = 1.29 (794.8 examples/sec; 0.161 sec/batch)
2016-11-28 18:30:10.517605: step 3750, loss = 1.11 (793.1 examples/sec; 0.161 sec/batch)
2016-11-28 18:30:12.113361: step 3760, loss = 1.22 (791.0 examples/sec; 0.162 sec/batch)
2016-11-28 18:30:13.703406: step 3770, loss = 1.00 (792.5 examples/sec; 0.162 sec/batch)
2016-11-28 18:30:15.287771: step 3780, loss = 1.33 (801.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:30:16.868895: step 3790, loss = 1.15 (817.6 examples/sec; 0.157 sec/batch)
2016-11-28 18:30:18.462677: step 3800, loss = 1.11 (798.0 examples/sec; 0.160 sec/batch)
2016-11-28 18:30:20.258456: step 3810, loss = 1.30 (791.5 examples/sec; 0.162 sec/batch)
2016-11-28 18:30:21.856520: step 3820, loss = 0.97 (802.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:30:23.460003: step 3830, loss = 1.34 (773.0 examples/sec; 0.166 sec/batch)
2016-11-28 18:30:25.065636: step 3840, loss = 1.06 (822.7 examples/sec; 0.156 sec/batch)
2016-11-28 18:30:26.687409: step 3850, loss = 1.00 (746.9 examples/sec; 0.171 sec/batch)
2016-11-28 18:30:28.299921: step 3860, loss = 1.10 (782.3 examples/sec; 0.164 sec/batch)
2016-11-28 18:30:29.902429: step 3870, loss = 0.94 (807.7 examples/sec; 0.158 sec/batch)
2016-11-28 18:30:31.511023: step 3880, loss = 1.11 (806.4 examples/sec; 0.159 sec/batch)
2016-11-28 18:30:33.130131: step 3890, loss = 1.16 (786.6 examples/sec; 0.163 sec/batch)
2016-11-28 18:30:34.747976: step 3900, loss = 0.98 (786.9 examples/sec; 0.163 sec/batch)
2016-11-28 18:30:36.501861: step 3910, loss = 0.97 (825.3 examples/sec; 0.155 sec/batch)
2016-11-28 18:30:38.057052: step 3920, loss = 1.09 (871.1 examples/sec; 0.147 sec/batch)
2016-11-28 18:30:39.606101: step 3930, loss = 1.06 (828.1 examples/sec; 0.155 sec/batch)
2016-11-28 18:30:41.164096: step 3940, loss = 1.11 (800.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:30:42.736684: step 3950, loss = 1.23 (778.6 examples/sec; 0.164 sec/batch)
2016-11-28 18:30:44.288089: step 3960, loss = 1.10 (793.8 examples/sec; 0.161 sec/batch)
2016-11-28 18:30:45.839023: step 3970, loss = 1.13 (822.6 examples/sec; 0.156 sec/batch)
2016-11-28 18:30:47.389829: step 3980, loss = 1.10 (834.6 examples/sec; 0.153 sec/batch)
2016-11-28 18:30:48.938476: step 3990, loss = 0.98 (847.6 examples/sec; 0.151 sec/batch)
2016-11-28 18:30:50.499188: step 4000, loss = 1.10 (816.6 examples/sec; 0.157 sec/batch)
2016-11-28 18:30:52.617896: step 4010, loss = 1.03 (853.8 examples/sec; 0.150 sec/batch)
2016-11-28 18:30:54.164778: step 4020, loss = 0.86 (844.7 examples/sec; 0.152 sec/batch)
2016-11-28 18:30:55.703898: step 4030, loss = 1.01 (838.4 examples/sec; 0.153 sec/batch)
2016-11-28 18:30:57.245054: step 4040, loss = 1.04 (817.0 examples/sec; 0.157 sec/batch)
2016-11-28 18:30:58.780820: step 4050, loss = 0.98 (847.1 examples/sec; 0.151 sec/batch)
2016-11-28 18:31:00.331416: step 4060, loss = 1.18 (789.6 examples/sec; 0.162 sec/batch)
2016-11-28 18:31:01.873121: step 4070, loss = 1.19 (824.8 examples/sec; 0.155 sec/batch)
2016-11-28 18:31:03.414108: step 4080, loss = 1.01 (808.8 examples/sec; 0.158 sec/batch)
2016-11-28 18:31:04.961252: step 4090, loss = 1.10 (822.7 examples/sec; 0.156 sec/batch)
2016-11-28 18:31:06.509499: step 4100, loss = 1.08 (827.4 examples/sec; 0.155 sec/batch)
2016-11-28 18:31:08.224870: step 4110, loss = 1.02 (863.2 examples/sec; 0.148 sec/batch)
2016-11-28 18:31:09.767080: step 4120, loss = 0.96 (817.9 examples/sec; 0.156 sec/batch)
2016-11-28 18:31:11.317560: step 4130, loss = 1.12 (812.3 examples/sec; 0.158 sec/batch)
2016-11-28 18:31:12.904058: step 4140, loss = 1.16 (803.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:31:14.481028: step 4150, loss = 1.11 (817.2 examples/sec; 0.157 sec/batch)
2016-11-28 18:31:16.062043: step 4160, loss = 1.06 (831.9 examples/sec; 0.154 sec/batch)
2016-11-28 18:31:17.649587: step 4170, loss = 1.23 (843.7 examples/sec; 0.152 sec/batch)
2016-11-28 18:31:19.259666: step 4180, loss = 1.22 (795.3 examples/sec; 0.161 sec/batch)
2016-11-28 18:31:20.876314: step 4190, loss = 1.13 (780.9 examples/sec; 0.164 sec/batch)
2016-11-28 18:31:22.488165: step 4200, loss = 1.15 (807.9 examples/sec; 0.158 sec/batch)
2016-11-28 18:31:24.313928: step 4210, loss = 1.00 (767.9 examples/sec; 0.167 sec/batch)
2016-11-28 18:31:25.929662: step 4220, loss = 0.98 (830.7 examples/sec; 0.154 sec/batch)
2016-11-28 18:31:27.550252: step 4230, loss = 1.15 (806.9 examples/sec; 0.159 sec/batch)
2016-11-28 18:31:29.173429: step 4240, loss = 1.16 (809.1 examples/sec; 0.158 sec/batch)
2016-11-28 18:31:30.787331: step 4250, loss = 1.13 (799.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:31:32.401739: step 4260, loss = 1.12 (781.7 examples/sec; 0.164 sec/batch)
2016-11-28 18:31:34.021442: step 4270, loss = 1.75 (806.5 examples/sec; 0.159 sec/batch)
2016-11-28 18:31:35.634079: step 4280, loss = 1.08 (777.5 examples/sec; 0.165 sec/batch)
2016-11-28 18:31:37.243514: step 4290, loss = 1.12 (805.8 examples/sec; 0.159 sec/batch)
2016-11-28 18:31:38.866651: step 4300, loss = 1.00 (788.7 examples/sec; 0.162 sec/batch)
2016-11-28 18:31:40.682416: step 4310, loss = 1.03 (809.4 examples/sec; 0.158 sec/batch)
2016-11-28 18:31:42.284537: step 4320, loss = 1.14 (809.3 examples/sec; 0.158 sec/batch)
2016-11-28 18:31:43.904747: step 4330, loss = 0.94 (778.0 examples/sec; 0.165 sec/batch)
2016-11-28 18:31:45.520624: step 4340, loss = 1.14 (791.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:31:47.144058: step 4350, loss = 1.12 (798.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:31:48.779426: step 4360, loss = 1.05 (772.1 examples/sec; 0.166 sec/batch)
2016-11-28 18:31:50.382483: step 4370, loss = 1.04 (799.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:31:52.012571: step 4380, loss = 1.06 (769.3 examples/sec; 0.166 sec/batch)
2016-11-28 18:31:53.632760: step 4390, loss = 1.23 (774.8 examples/sec; 0.165 sec/batch)
2016-11-28 18:31:55.247500: step 4400, loss = 1.02 (794.6 examples/sec; 0.161 sec/batch)
2016-11-28 18:31:57.070402: step 4410, loss = 1.50 (804.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:31:58.686528: step 4420, loss = 1.21 (806.2 examples/sec; 0.159 sec/batch)
2016-11-28 18:32:00.298642: step 4430, loss = 1.13 (797.7 examples/sec; 0.160 sec/batch)
2016-11-28 18:32:01.910714: step 4440, loss = 1.18 (794.7 examples/sec; 0.161 sec/batch)
2016-11-28 18:32:03.520108: step 4450, loss = 1.26 (789.5 examples/sec; 0.162 sec/batch)
2016-11-28 18:32:05.141730: step 4460, loss = 1.05 (815.7 examples/sec; 0.157 sec/batch)
2016-11-28 18:32:06.771588: step 4470, loss = 0.97 (812.8 examples/sec; 0.157 sec/batch)
2016-11-28 18:32:08.390985: step 4480, loss = 1.01 (765.5 examples/sec; 0.167 sec/batch)
2016-11-28 18:32:10.008549: step 4490, loss = 1.24 (772.4 examples/sec; 0.166 sec/batch)
2016-11-28 18:32:11.627060: step 4500, loss = 0.92 (805.5 examples/sec; 0.159 sec/batch)
2016-11-28 18:32:13.452462: step 4510, loss = 0.98 (760.9 examples/sec; 0.168 sec/batch)
2016-11-28 18:32:15.073988: step 4520, loss = 0.94 (793.1 examples/sec; 0.161 sec/batch)
2016-11-28 18:32:16.690853: step 4530, loss = 1.24 (806.3 examples/sec; 0.159 sec/batch)
2016-11-28 18:32:18.290574: step 4540, loss = 0.98 (801.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:32:19.900628: step 4550, loss = 1.07 (829.7 examples/sec; 0.154 sec/batch)
2016-11-28 18:32:21.512506: step 4560, loss = 1.08 (786.5 examples/sec; 0.163 sec/batch)
2016-11-28 18:32:23.133943: step 4570, loss = 1.10 (809.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:32:24.749164: step 4580, loss = 1.10 (771.9 examples/sec; 0.166 sec/batch)
2016-11-28 18:32:26.365914: step 4590, loss = 1.07 (749.2 examples/sec; 0.171 sec/batch)
2016-11-28 18:32:27.966292: step 4600, loss = 1.04 (804.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:32:29.785125: step 4610, loss = 1.02 (819.0 examples/sec; 0.156 sec/batch)
2016-11-28 18:32:31.397041: step 4620, loss = 0.97 (786.6 examples/sec; 0.163 sec/batch)
2016-11-28 18:32:33.010548: step 4630, loss = 0.83 (783.0 examples/sec; 0.163 sec/batch)
2016-11-28 18:32:34.629544: step 4640, loss = 1.00 (769.3 examples/sec; 0.166 sec/batch)
2016-11-28 18:32:36.236414: step 4650, loss = 1.30 (787.8 examples/sec; 0.162 sec/batch)
2016-11-28 18:32:37.856350: step 4660, loss = 1.06 (802.1 examples/sec; 0.160 sec/batch)
2016-11-28 18:32:39.467408: step 4670, loss = 1.07 (794.1 examples/sec; 0.161 sec/batch)
2016-11-28 18:32:41.083036: step 4680, loss = 1.08 (814.1 examples/sec; 0.157 sec/batch)
2016-11-28 18:32:42.698467: step 4690, loss = 0.96 (789.1 examples/sec; 0.162 sec/batch)
2016-11-28 18:32:44.308363: step 4700, loss = 0.92 (804.9 examples/sec; 0.159 sec/batch)
2016-11-28 18:32:46.114317: step 4710, loss = 0.95 (810.0 examples/sec; 0.158 sec/batch)
2016-11-28 18:32:47.699838: step 4720, loss = 1.08 (794.2 examples/sec; 0.161 sec/batch)
2016-11-28 18:32:49.279837: step 4730, loss = 0.93 (829.3 examples/sec; 0.154 sec/batch)
2016-11-28 18:32:50.873899: step 4740, loss = 0.88 (801.2 examples/sec; 0.160 sec/batch)
2016-11-28 18:32:52.460640: step 4750, loss = 0.89 (808.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:32:54.032943: step 4760, loss = 0.96 (806.1 examples/sec; 0.159 sec/batch)
2016-11-28 18:32:55.609256: step 4770, loss = 0.98 (810.3 examples/sec; 0.158 sec/batch)
2016-11-28 18:32:57.185435: step 4780, loss = 1.07 (804.7 examples/sec; 0.159 sec/batch)
2016-11-28 18:32:58.761652: step 4790, loss = 0.99 (788.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:33:00.346416: step 4800, loss = 0.97 (819.4 examples/sec; 0.156 sec/batch)
2016-11-28 18:33:02.080607: step 4810, loss = 1.06 (835.0 examples/sec; 0.153 sec/batch)
2016-11-28 18:33:03.625956: step 4820, loss = 1.04 (803.3 examples/sec; 0.159 sec/batch)
2016-11-28 18:33:05.221578: step 4830, loss = 1.05 (800.5 examples/sec; 0.160 sec/batch)
2016-11-28 18:33:06.803850: step 4840, loss = 1.02 (805.1 examples/sec; 0.159 sec/batch)
2016-11-28 18:33:08.397149: step 4850, loss = 1.08 (821.1 examples/sec; 0.156 sec/batch)
2016-11-28 18:33:09.990603: step 4860, loss = 1.05 (818.2 examples/sec; 0.156 sec/batch)
2016-11-28 18:33:11.581632: step 4870, loss = 1.03 (802.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:33:13.205385: step 4880, loss = 0.91 (761.3 examples/sec; 0.168 sec/batch)
2016-11-28 18:33:14.821122: step 4890, loss = 1.04 (784.9 examples/sec; 0.163 sec/batch)
2016-11-28 18:33:16.445661: step 4900, loss = 1.02 (797.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:33:18.267300: step 4910, loss = 1.02 (783.5 examples/sec; 0.163 sec/batch)
2016-11-28 18:33:19.885210: step 4920, loss = 1.06 (798.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:33:21.492805: step 4930, loss = 0.96 (795.9 examples/sec; 0.161 sec/batch)
2016-11-28 18:33:23.113976: step 4940, loss = 0.94 (801.1 examples/sec; 0.160 sec/batch)
2016-11-28 18:33:24.723473: step 4950, loss = 1.09 (805.1 examples/sec; 0.159 sec/batch)
2016-11-28 18:33:26.340279: step 4960, loss = 1.00 (787.8 examples/sec; 0.162 sec/batch)
2016-11-28 18:33:27.954339: step 4970, loss = 1.05 (764.9 examples/sec; 0.167 sec/batch)
2016-11-28 18:33:29.589020: step 4980, loss = 0.98 (788.9 examples/sec; 0.162 sec/batch)
2016-11-28 18:33:31.215860: step 4990, loss = 0.98 (776.7 examples/sec; 0.165 sec/batch)
2016-11-28 18:33:32.839186: step 5000, loss = 0.95 (791.6 examples/sec; 0.162 sec/batch)
2016-11-28 18:33:35.028576: step 5010, loss = 1.06 (836.7 examples/sec; 0.153 sec/batch)
2016-11-28 18:33:36.592259: step 5020, loss = 0.93 (815.3 examples/sec; 0.157 sec/batch)
2016-11-28 18:33:38.155502: step 5030, loss = 0.99 (837.1 examples/sec; 0.153 sec/batch)
2016-11-28 18:33:39.719620: step 5040, loss = 0.94 (832.4 examples/sec; 0.154 sec/batch)
2016-11-28 18:33:41.288444: step 5050, loss = 1.00 (836.2 examples/sec; 0.153 sec/batch)
2016-11-28 18:33:42.865214: step 5060, loss = 1.02 (838.0 examples/sec; 0.153 sec/batch)
2016-11-28 18:33:44.428635: step 5070, loss = 1.14 (803.3 examples/sec; 0.159 sec/batch)
2016-11-28 18:33:45.999351: step 5080, loss = 1.03 (831.6 examples/sec; 0.154 sec/batch)
2016-11-28 18:33:47.562383: step 5090, loss = 1.17 (818.3 examples/sec; 0.156 sec/batch)
2016-11-28 18:33:49.132836: step 5100, loss = 0.95 (788.9 examples/sec; 0.162 sec/batch)
2016-11-28 18:33:50.892421: step 5110, loss = 1.06 (762.4 examples/sec; 0.168 sec/batch)
2016-11-28 18:33:52.455304: step 5120, loss = 1.00 (828.8 examples/sec; 0.154 sec/batch)
2016-11-28 18:33:54.020623: step 5130, loss = 1.02 (807.2 examples/sec; 0.159 sec/batch)
2016-11-28 18:33:55.576001: step 5140, loss = 0.86 (842.9 examples/sec; 0.152 sec/batch)
2016-11-28 18:33:57.138392: step 5150, loss = 0.97 (865.2 examples/sec; 0.148 sec/batch)
2016-11-28 18:33:58.693438: step 5160, loss = 0.94 (812.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:34:00.264202: step 5170, loss = 0.86 (798.1 examples/sec; 0.160 sec/batch)
2016-11-28 18:34:01.853457: step 5180, loss = 0.94 (806.4 examples/sec; 0.159 sec/batch)
2016-11-28 18:34:03.453624: step 5190, loss = 0.99 (779.3 examples/sec; 0.164 sec/batch)
2016-11-28 18:34:05.061748: step 5200, loss = 0.84 (795.9 examples/sec; 0.161 sec/batch)
2016-11-28 18:34:06.900587: step 5210, loss = 0.96 (761.3 examples/sec; 0.168 sec/batch)
2016-11-28 18:34:08.517184: step 5220, loss = 0.89 (775.7 examples/sec; 0.165 sec/batch)
2016-11-28 18:34:10.131806: step 5230, loss = 0.77 (810.2 examples/sec; 0.158 sec/batch)
2016-11-28 18:34:11.744163: step 5240, loss = 1.02 (830.3 examples/sec; 0.154 sec/batch)
2016-11-28 18:34:13.359578: step 5250, loss = 1.03 (796.7 examples/sec; 0.161 sec/batch)
2016-11-28 18:34:14.975613: step 5260, loss = 0.98 (788.5 examples/sec; 0.162 sec/batch)
2016-11-28 18:34:16.587104: step 5270, loss = 0.99 (783.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:34:18.202017: step 5280, loss = 0.92 (771.6 examples/sec; 0.166 sec/batch)
2016-11-28 18:34:19.823476: step 5290, loss = 0.92 (783.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:34:21.429782: step 5300, loss = 0.81 (825.7 examples/sec; 0.155 sec/batch)
2016-11-28 18:34:23.238046: step 5310, loss = 1.03 (786.7 examples/sec; 0.163 sec/batch)
2016-11-28 18:34:24.840119: step 5320, loss = 0.91 (816.1 examples/sec; 0.157 sec/batch)
2016-11-28 18:34:26.442646: step 5330, loss = 1.08 (810.4 examples/sec; 0.158 sec/batch)
2016-11-28 18:34:28.038454: step 5340, loss = 1.13 (806.4 examples/sec; 0.159 sec/batch)
2016-11-28 18:34:29.652284: step 5350, loss = 0.90 (816.6 examples/sec; 0.157 sec/batch)
2016-11-28 18:34:31.267152: step 5360, loss = 1.03 (771.5 examples/sec; 0.166 sec/batch)
2016-11-28 18:34:32.880401: step 5370, loss = 1.10 (776.2 examples/sec; 0.165 sec/batch)
2016-11-28 18:34:34.480692: step 5380, loss = 0.93 (838.8 examples/sec; 0.153 sec/batch)
2016-11-28 18:34:36.090982: step 5390, loss = 1.33 (787.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:34:37.691667: step 5400, loss = 1.18 (780.9 examples/sec; 0.164 sec/batch)
2016-11-28 18:34:39.496829: step 5410, loss = 1.14 (784.5 examples/sec; 0.163 sec/batch)
2016-11-28 18:34:41.090966: step 5420, loss = 0.88 (816.0 examples/sec; 0.157 sec/batch)
2016-11-28 18:34:42.693727: step 5430, loss = 1.05 (801.7 examples/sec; 0.160 sec/batch)
2016-11-28 18:34:44.298157: step 5440, loss = 1.01 (791.3 examples/sec; 0.162 sec/batch)
2016-11-28 18:34:45.905544: step 5450, loss = 0.86 (822.8 examples/sec; 0.156 sec/batch)
2016-11-28 18:34:47.514577: step 5460, loss = 1.22 (806.1 examples/sec; 0.159 sec/batch)
2016-11-28 18:34:49.117989: step 5470, loss = 0.97 (786.9 examples/sec; 0.163 sec/batch)
2016-11-28 18:34:50.708114: step 5480, loss = 0.99 (824.1 examples/sec; 0.155 sec/batch)
2016-11-28 18:34:52.301414: step 5490, loss = 0.90 (817.2 examples/sec; 0.157 sec/batch)
2016-11-28 18:34:53.905278: step 5500, loss = 0.91 (811.8 examples/sec; 0.158 sec/batch)
2016-11-28 18:34:55.706124: step 5510, loss = 0.95 (784.3 examples/sec; 0.163 sec/batch)
2016-11-28 18:34:57.303090: step 5520, loss = 1.01 (805.1 examples/sec; 0.159 sec/batch)
2016-11-28 18:34:58.931643: step 5530, loss = 0.97 (771.4 examples/sec; 0.166 sec/batch)
2016-11-28 18:35:00.540006: step 5540, loss = 0.83 (798.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:35:02.154390: step 5550, loss = 1.03 (802.0 examples/sec; 0.160 sec/batch)
2016-11-28 18:35:03.772018: step 5560, loss = 1.06 (838.6 examples/sec; 0.153 sec/batch)
2016-11-28 18:35:05.384479: step 5570, loss = 1.02 (787.0 examples/sec; 0.163 sec/batch)
2016-11-28 18:35:06.996239: step 5580, loss = 0.88 (791.7 examples/sec; 0.162 sec/batch)
2016-11-28 18:35:08.608899: step 5590, loss = 1.15 (766.3 examples/sec; 0.167 sec/batch)
2016-11-28 18:35:10.215885: step 5600, loss = 0.96 (790.6 examples/sec; 0.162 sec/batch)
2016-11-28 18:35:11.971697: step 5610, loss = 1.11 (809.8 examples/sec; 0.158 sec/batch)
2016-11-28 18:35:13.525435: step 5620, loss = 1.01 (814.4 examples/sec; 0.157 sec/batch)
2016-11-28 18:35:15.073833: step 5630, loss = 1.06 (833.3 examples/sec; 0.154 sec/batch)
2016-11-28 18:35:16.628084: step 5640, loss = 1.08 (822.5 examples/sec; 0.156 sec/batch)
2016-11-28 18:35:18.187897: step 5650, loss = 0.90 (846.5 examples/sec; 0.151 sec/batch)
2016-11-28 18:35:19.750998: step 5660, loss = 0.88 (789.7 examples/sec; 0.162 sec/batch)
2016-11-28 18:35:21.308257: step 5670, loss = 1.06 (796.3 examples/sec; 0.161 sec/batch)
2016-11-28 18:35:22.871641: step 5680, loss = 1.00 (812.3 examples/sec; 0.158 sec/batch)
2016-11-28 18:35:24.425111: step 5690, loss = 0.92 (858.3 examples/sec; 0.149 sec/batch)
2016-11-28 18:35:25.992438: step 5700, loss = 1.27 (807.2 examples/sec; 0.159 sec/batch)
2016-11-28 18:35:27.742005: step 5710, loss = 0.90 (844.9 examples/sec; 0.151 sec/batch)
2016-11-28 18:35:29.302556: step 5720, loss = 1.03 (831.2 examples/sec; 0.154 sec/batch)
2016-11-28 18:35:30.861366: step 5730, loss = 1.02 (809.8 examples/sec; 0.158 sec/batch)
2016-11-28 18:35:32.421773: step 5740, loss = 0.91 (827.9 examples/sec; 0.155 sec/batch)
2016-11-28 18:35:33.976075: step 5750, loss = 0.99 (832.3 examples/sec; 0.154 sec/batch)
2016-11-28 18:35:35.527308: step 5760, loss = 0.83 (840.3 examples/sec; 0.152 sec/batch)
2016-11-28 18:35:37.075448: step 5770, loss = 1.05 (824.8 examples/sec; 0.155 sec/batch)
2016-11-28 18:35:38.628156: step 5780, loss = 0.78 (857.3 examples/sec; 0.149 sec/batch)
2016-11-28 18:35:40.193255: step 5790, loss = 1.09 (826.9 examples/sec; 0.155 sec/batch)
2016-11-28 18:35:41.744423: step 5800, loss = 1.02 (844.9 examples/sec; 0.151 sec/batch)
2016-11-28 18:35:43.480440: step 5810, loss = 1.01 (803.7 examples/sec; 0.159 sec/batch)
2016-11-28 18:35:45.041422: step 5820, loss = 1.15 (787.7 examples/sec; 0.162 sec/batch)
2016-11-28 18:35:46.653572: step 5830, loss = 0.83 (776.6 examples/sec; 0.165 sec/batch)
2016-11-28 18:35:48.256674: step 5840, loss = 0.93 (801.7 examples/sec; 0.160 sec/batch)
2016-11-28 18:35:49.867370: step 5850, loss = 1.10 (770.8 examples/sec; 0.166 sec/batch)
2016-11-28 18:35:51.484544: step 5860, loss = 0.93 (780.8 examples/sec; 0.164 sec/batch)
2016-11-28 18:35:53.099494: step 5870, loss = 1.00 (796.1 examples/sec; 0.161 sec/batch)
2016-11-28 18:35:54.720773: step 5880, loss = 1.01 (800.5 examples/sec; 0.160 sec/batch)
2016-11-28 18:35:56.341508: step 5890, loss = 0.97 (803.3 examples/sec; 0.159 sec/batch)
2016-11-28 18:35:57.958118: step 5900, loss = 0.90 (776.9 examples/sec; 0.165 sec/batch)
2016-11-28 18:35:59.788482: step 5910, loss = 1.15 (779.3 examples/sec; 0.164 sec/batch)
2016-11-28 18:36:01.404120: step 5920, loss = 0.96 (791.8 examples/sec; 0.162 sec/batch)
2016-11-28 18:36:03.034374: step 5930, loss = 0.91 (789.3 examples/sec; 0.162 sec/batch)
2016-11-28 18:36:04.648006: step 5940, loss = 0.83 (791.7 examples/sec; 0.162 sec/batch)
2016-11-28 18:36:06.260864: step 5950, loss = 1.08 (799.2 examples/sec; 0.160 sec/batch)
2016-11-28 18:36:07.879338: step 5960, loss = 1.07 (794.5 examples/sec; 0.161 sec/batch)
2016-11-28 18:36:09.489229: step 5970, loss = 1.22 (802.5 examples/sec; 0.160 sec/batch)
2016-11-28 18:36:11.098849: step 5980, loss = 0.79 (789.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:36:12.713945: step 5990, loss = 0.99 (776.4 examples/sec; 0.165 sec/batch)
2016-11-28 18:36:14.332037: step 6000, loss = 1.00 (811.0 examples/sec; 0.158 sec/batch)
2016-11-28 18:36:16.566635: step 6010, loss = 0.99 (814.9 examples/sec; 0.157 sec/batch)
2016-11-28 18:36:18.164784: step 6020, loss = 1.05 (816.3 examples/sec; 0.157 sec/batch)
2016-11-28 18:36:19.762837: step 6030, loss = 1.12 (791.0 examples/sec; 0.162 sec/batch)
2016-11-28 18:36:21.352607: step 6040, loss = 0.95 (805.8 examples/sec; 0.159 sec/batch)
2016-11-28 18:36:22.955967: step 6050, loss = 1.09 (782.9 examples/sec; 0.163 sec/batch)
2016-11-28 18:36:24.554156: step 6060, loss = 0.95 (796.7 examples/sec; 0.161 sec/batch)
2016-11-28 18:36:26.140611: step 6070, loss = 0.93 (854.3 examples/sec; 0.150 sec/batch)
2016-11-28 18:36:27.744447: step 6080, loss = 1.00 (792.2 examples/sec; 0.162 sec/batch)
2016-11-28 18:36:29.329888: step 6090, loss = 0.93 (800.0 examples/sec; 0.160 sec/batch)
2016-11-28 18:36:30.934533: step 6100, loss = 0.92 (809.0 examples/sec; 0.158 sec/batch)
2016-11-28 18:36:32.737076: step 6110, loss = 0.87 (820.6 examples/sec; 0.156 sec/batch)
2016-11-28 18:36:34.319692: step 6120, loss = 0.82 (822.5 examples/sec; 0.156 sec/batch)
2016-11-28 18:36:35.908706: step 6130, loss = 0.90 (785.6 examples/sec; 0.163 sec/batch)
2016-11-28 18:36:37.503880: step 6140, loss = 0.83 (806.7 examples/sec; 0.159 sec/batch)
2016-11-28 18:36:39.128799: step 6150, loss = 0.95 (818.0 examples/sec; 0.156 sec/batch)
2016-11-28 18:36:40.756967: step 6160, loss = 0.92 (776.7 examples/sec; 0.165 sec/batch)
2016-11-28 18:36:42.366813: step 6170, loss = 0.94 (775.4 examples/sec; 0.165 sec/batch)
2016-11-28 18:36:44.001922: step 6180, loss = 0.96 (786.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:36:45.625390: step 6190, loss = 1.07 (826.5 examples/sec; 0.155 sec/batch)
2016-11-28 18:36:47.231924: step 6200, loss = 0.81 (813.5 examples/sec; 0.157 sec/batch)
2016-11-28 18:36:49.068826: step 6210, loss = 1.02 (812.9 examples/sec; 0.157 sec/batch)
2016-11-28 18:36:50.686667: step 6220, loss = 0.94 (781.1 examples/sec; 0.164 sec/batch)
2016-11-28 18:36:52.311710: step 6230, loss = 1.04 (792.6 examples/sec; 0.161 sec/batch)
2016-11-28 18:36:53.931917: step 6240, loss = 1.03 (802.7 examples/sec; 0.159 sec/batch)
2016-11-28 18:36:55.548221: step 6250, loss = 1.04 (796.4 examples/sec; 0.161 sec/batch)
2016-11-28 18:36:57.176588: step 6260, loss = 1.00 (744.3 examples/sec; 0.172 sec/batch)
2016-11-28 18:36:58.777160: step 6270, loss = 1.13 (810.0 examples/sec; 0.158 sec/batch)
2016-11-28 18:37:00.404348: step 6280, loss = 0.86 (776.1 examples/sec; 0.165 sec/batch)
2016-11-28 18:37:02.023192: step 6290, loss = 0.94 (774.0 examples/sec; 0.165 sec/batch)
2016-11-28 18:37:03.634678: step 6300, loss = 0.88 (781.9 examples/sec; 0.164 sec/batch)
2016-11-28 18:37:05.452051: step 6310, loss = 1.09 (802.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:37:07.064024: step 6320, loss = 1.10 (810.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:37:08.682601: step 6330, loss = 1.11 (793.2 examples/sec; 0.161 sec/batch)
2016-11-28 18:37:10.301385: step 6340, loss = 0.88 (807.1 examples/sec; 0.159 sec/batch)
2016-11-28 18:37:11.927855: step 6350, loss = 0.97 (765.7 examples/sec; 0.167 sec/batch)
2016-11-28 18:37:13.536881: step 6360, loss = 1.00 (779.1 examples/sec; 0.164 sec/batch)
2016-11-28 18:37:15.151294: step 6370, loss = 0.76 (800.1 examples/sec; 0.160 sec/batch)
2016-11-28 18:37:16.764187: step 6380, loss = 0.81 (784.1 examples/sec; 0.163 sec/batch)
2016-11-28 18:37:18.380262: step 6390, loss = 1.03 (803.3 examples/sec; 0.159 sec/batch)
2016-11-28 18:37:20.009653: step 6400, loss = 0.85 (773.9 examples/sec; 0.165 sec/batch)
2016-11-28 18:37:21.807477: step 6410, loss = 1.11 (811.8 examples/sec; 0.158 sec/batch)
2016-11-28 18:37:23.432519: step 6420, loss = 0.97 (774.5 examples/sec; 0.165 sec/batch)
2016-11-28 18:37:25.033088: step 6430, loss = 0.95 (775.0 examples/sec; 0.165 sec/batch)
2016-11-28 18:37:26.641213: step 6440, loss = 0.93 (769.1 examples/sec; 0.166 sec/batch)
2016-11-28 18:37:28.249873: step 6450, loss = 0.91 (814.9 examples/sec; 0.157 sec/batch)
2016-11-28 18:37:29.869153: step 6460, loss = 0.86 (783.9 examples/sec; 0.163 sec/batch)
2016-11-28 18:37:31.490232: step 6470, loss = 0.98 (807.5 examples/sec; 0.159 sec/batch)
2016-11-28 18:37:33.121772: step 6480, loss = 0.88 (803.0 examples/sec; 0.159 sec/batch)
2016-11-28 18:37:34.738574: step 6490, loss = 0.84 (787.3 examples/sec; 0.163 sec/batch)
2016-11-28 18:37:36.351779: step 6500, loss = 1.04 (791.6 examples/sec; 0.162 sec/batch)
2016-11-28 18:37:38.176979: step 6510, loss = 0.93 (783.0 examples/sec; 0.163 sec/batch)
2016-11-28 18:37:39.781962: step 6520, loss = 1.01 (824.8 examples/sec; 0.155 sec/batch)
2016-11-28 18:37:41.412614: step 6530, loss = 0.93 (775.6 examples/sec; 0.165 sec/batch)
2016-11-28 18:37:43.028697: step 6540, loss = 0.83 (805.5 examples/sec; 0.159 sec/batch)
2016-11-28 18:37:44.636510: step 6550, loss = 0.83 (811.4 examples/sec; 0.158 sec/batch)
2016-11-28 18:37:46.256985: step 6560, loss = 1.06 (791.2 examples/sec; 0.162 sec/batch)
2016-11-28 18:37:47.888088: step 6570, loss = 0.94 (768.3 examples/sec; 0.167 sec/batch)
2016-11-28 18:37:49.496412: step 6580, loss = 1.16 (769.5 examples/sec; 0.166 sec/batch)
2016-11-28 18:37:51.108208: step 6590, loss = 0.74 (777.9 examples/sec; 0.165 sec/batch)
2016-11-28 18:37:52.714339: step 6600, loss = 0.91 (821.9 examples/sec; 0.156 sec/batch)
2016-11-28 18:37:54.544936: step 6610, loss = 1.06 (813.9 examples/sec; 0.157 sec/batch)
2016-11-28 18:37:56.162062: step 6620, loss = 0.99 (811.8 examples/sec; 0.158 sec/batch)
2016-11-28 18:37:57.782878: step 6630, loss = 0.86 (798.6 examples/sec; 0.160 sec/batch)
2016-11-28 18:37:59.405593: step 6640, loss = 0.87 (777.4 examples/sec; 0.165 sec/batch)
2016-11-28 18:38:01.027357: step 6650, loss = 0.85 (767.4 examples/sec; 0.167 sec/batch)
2016-11-28 18:38:02.649629: step 6660, loss = 0.86 (764.3 examples/sec; 0.167 sec/batch)
2016-11-28 18:38:04.266346: step 6670, loss = 1.10 (829.0 examples/sec; 0.154 sec/batch)
2016-11-28 18:38:05.876272: step 6680, loss = 0.90 (814.7 examples/sec; 0.157 sec/batch)
2016-11-28 18:38:07.488766: step 6690, loss = 0.90 (798.5 examples/sec; 0.160 sec/batch)
2016-11-28 18:38:09.105520: step 6700, loss = 1.08 (789.1 examples/sec; 0.162 sec/batch)
2016-11-28 18:38:10.917344: step 6710, loss = 0.99 (819.1 examples/sec; 0.156 sec/batch)
2016-11-28 18:38:12.534123: step 6720, loss = 0.92 (806.5 examples/sec; 0.159 sec/batch)
2016-11-28 18:38:14.138783: step 6730, loss = 0.93 (828.9 examples/sec; 0.154 sec/batch)
2016-11-28 18:38:15.757769: step 6740, loss = 1.12 (802.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:38:17.378173: step 6750, loss = 0.89 (806.1 examples/sec; 0.159 sec/batch)
2016-11-28 18:38:18.990625: step 6760, loss = 1.01 (797.2 examples/sec; 0.161 sec/batch)
2016-11-28 18:38:20.595178: step 6770, loss = 0.85 (795.4 examples/sec; 0.161 sec/batch)
2016-11-28 18:38:22.205149: step 6780, loss = 0.82 (831.3 examples/sec; 0.154 sec/batch)
2016-11-28 18:38:23.831828: step 6790, loss = 1.05 (791.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:38:25.458378: step 6800, loss = 0.84 (775.1 examples/sec; 0.165 sec/batch)
2016-11-28 18:38:27.263794: step 6810, loss = 0.89 (825.2 examples/sec; 0.155 sec/batch)
2016-11-28 18:38:28.859012: step 6820, loss = 0.97 (784.3 examples/sec; 0.163 sec/batch)
2016-11-28 18:38:30.471528: step 6830, loss = 0.72 (790.8 examples/sec; 0.162 sec/batch)
2016-11-28 18:38:32.085989: step 6840, loss = 1.02 (798.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:38:33.699411: step 6850, loss = 0.88 (758.0 examples/sec; 0.169 sec/batch)
2016-11-28 18:38:35.321694: step 6860, loss = 1.03 (767.1 examples/sec; 0.167 sec/batch)
2016-11-28 18:38:36.933160: step 6870, loss = 0.87 (818.1 examples/sec; 0.156 sec/batch)
2016-11-28 18:38:38.547366: step 6880, loss = 0.96 (773.9 examples/sec; 0.165 sec/batch)
2016-11-28 18:38:40.156352: step 6890, loss = 1.01 (816.7 examples/sec; 0.157 sec/batch)
2016-11-28 18:38:41.773227: step 6900, loss = 0.82 (806.2 examples/sec; 0.159 sec/batch)
2016-11-28 18:38:43.597826: step 6910, loss = 0.85 (774.1 examples/sec; 0.165 sec/batch)
2016-11-28 18:38:45.214798: step 6920, loss = 1.02 (775.1 examples/sec; 0.165 sec/batch)
2016-11-28 18:38:46.835554: step 6930, loss = 0.96 (770.1 examples/sec; 0.166 sec/batch)
2016-11-28 18:38:48.455622: step 6940, loss = 0.86 (776.6 examples/sec; 0.165 sec/batch)
2016-11-28 18:38:50.080176: step 6950, loss = 0.99 (813.2 examples/sec; 0.157 sec/batch)
2016-11-28 18:38:51.688930: step 6960, loss = 1.14 (810.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:38:53.291758: step 6970, loss = 0.90 (838.3 examples/sec; 0.153 sec/batch)
2016-11-28 18:38:54.900298: step 6980, loss = 0.88 (803.7 examples/sec; 0.159 sec/batch)
2016-11-28 18:38:56.512896: step 6990, loss = 0.89 (816.1 examples/sec; 0.157 sec/batch)
2016-11-28 18:38:58.129233: step 7000, loss = 0.88 (808.9 examples/sec; 0.158 sec/batch)
2016-11-28 18:39:00.294865: step 7010, loss = 1.11 (820.9 examples/sec; 0.156 sec/batch)
2016-11-28 18:39:01.864743: step 7020, loss = 1.00 (859.0 examples/sec; 0.149 sec/batch)
2016-11-28 18:39:03.417258: step 7030, loss = 0.92 (835.2 examples/sec; 0.153 sec/batch)
2016-11-28 18:39:04.976046: step 7040, loss = 0.87 (823.4 examples/sec; 0.155 sec/batch)
2016-11-28 18:39:06.538507: step 7050, loss = 0.97 (799.6 examples/sec; 0.160 sec/batch)
2016-11-28 18:39:08.097647: step 7060, loss = 0.84 (855.4 examples/sec; 0.150 sec/batch)
2016-11-28 18:39:09.651348: step 7070, loss = 0.72 (863.9 examples/sec; 0.148 sec/batch)
2016-11-28 18:39:11.233359: step 7080, loss = 1.05 (846.6 examples/sec; 0.151 sec/batch)
2016-11-28 18:39:12.797153: step 7090, loss = 0.95 (806.1 examples/sec; 0.159 sec/batch)
2016-11-28 18:39:14.395611: step 7100, loss = 0.90 (786.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:39:16.209724: step 7110, loss = 1.15 (783.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:39:17.821070: step 7120, loss = 0.85 (787.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:39:19.422848: step 7130, loss = 0.83 (805.3 examples/sec; 0.159 sec/batch)
2016-11-28 18:39:21.027539: step 7140, loss = 0.98 (786.1 examples/sec; 0.163 sec/batch)
2016-11-28 18:39:22.650180: step 7150, loss = 0.77 (757.8 examples/sec; 0.169 sec/batch)
2016-11-28 18:39:24.264305: step 7160, loss = 0.81 (807.8 examples/sec; 0.158 sec/batch)
2016-11-28 18:39:25.874625: step 7170, loss = 1.14 (770.8 examples/sec; 0.166 sec/batch)
2016-11-28 18:39:27.480060: step 7180, loss = 0.91 (810.9 examples/sec; 0.158 sec/batch)
2016-11-28 18:39:29.092923: step 7190, loss = 1.09 (783.7 examples/sec; 0.163 sec/batch)
2016-11-28 18:39:30.705959: step 7200, loss = 0.87 (791.2 examples/sec; 0.162 sec/batch)
2016-11-28 18:39:32.524488: step 7210, loss = 1.11 (803.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:39:34.135671: step 7220, loss = 0.97 (789.3 examples/sec; 0.162 sec/batch)
2016-11-28 18:39:35.752564: step 7230, loss = 1.05 (784.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:39:37.359875: step 7240, loss = 0.83 (830.8 examples/sec; 0.154 sec/batch)
2016-11-28 18:39:38.976491: step 7250, loss = 0.77 (789.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:39:40.582803: step 7260, loss = 1.04 (797.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:39:42.186528: step 7270, loss = 0.89 (808.9 examples/sec; 0.158 sec/batch)
2016-11-28 18:39:43.804675: step 7280, loss = 1.06 (767.8 examples/sec; 0.167 sec/batch)
2016-11-28 18:39:45.411651: step 7290, loss = 0.91 (773.1 examples/sec; 0.166 sec/batch)
2016-11-28 18:39:47.025302: step 7300, loss = 0.94 (801.3 examples/sec; 0.160 sec/batch)
2016-11-28 18:39:48.815680: step 7310, loss = 0.92 (821.4 examples/sec; 0.156 sec/batch)
2016-11-28 18:39:50.403224: step 7320, loss = 1.02 (808.2 examples/sec; 0.158 sec/batch)
2016-11-28 18:39:51.981113: step 7330, loss = 1.15 (813.5 examples/sec; 0.157 sec/batch)
2016-11-28 18:39:53.559020: step 7340, loss = 0.89 (832.8 examples/sec; 0.154 sec/batch)
2016-11-28 18:39:55.138792: step 7350, loss = 1.08 (824.9 examples/sec; 0.155 sec/batch)
2016-11-28 18:39:56.708882: step 7360, loss = 0.86 (823.9 examples/sec; 0.155 sec/batch)
2016-11-28 18:39:58.289864: step 7370, loss = 0.98 (802.2 examples/sec; 0.160 sec/batch)
2016-11-28 18:39:59.881154: step 7380, loss = 0.99 (804.2 examples/sec; 0.159 sec/batch)
2016-11-28 18:40:01.459461: step 7390, loss = 0.91 (794.9 examples/sec; 0.161 sec/batch)
2016-11-28 18:40:03.042537: step 7400, loss = 0.95 (844.2 examples/sec; 0.152 sec/batch)
2016-11-28 18:40:04.781506: step 7410, loss = 0.81 (839.0 examples/sec; 0.153 sec/batch)
2016-11-28 18:40:06.336697: step 7420, loss = 0.95 (810.9 examples/sec; 0.158 sec/batch)
2016-11-28 18:40:07.929150: step 7430, loss = 0.97 (797.5 examples/sec; 0.161 sec/batch)
2016-11-28 18:40:09.521639: step 7440, loss = 0.78 (806.2 examples/sec; 0.159 sec/batch)
2016-11-28 18:40:11.131003: step 7450, loss = 0.99 (806.9 examples/sec; 0.159 sec/batch)
2016-11-28 18:40:12.748429: step 7460, loss = 0.81 (793.8 examples/sec; 0.161 sec/batch)
2016-11-28 18:40:14.350584: step 7470, loss = 0.84 (782.9 examples/sec; 0.163 sec/batch)
2016-11-28 18:40:15.960424: step 7480, loss = 0.96 (780.1 examples/sec; 0.164 sec/batch)
2016-11-28 18:40:17.574308: step 7490, loss = 0.97 (804.7 examples/sec; 0.159 sec/batch)
2016-11-28 18:40:19.191037: step 7500, loss = 1.00 (819.0 examples/sec; 0.156 sec/batch)
2016-11-28 18:40:20.995263: step 7510, loss = 0.90 (763.3 examples/sec; 0.168 sec/batch)
2016-11-28 18:40:22.599227: step 7520, loss = 0.98 (847.2 examples/sec; 0.151 sec/batch)
2016-11-28 18:40:24.205799: step 7530, loss = 1.12 (804.5 examples/sec; 0.159 sec/batch)
2016-11-28 18:40:25.819183: step 7540, loss = 0.91 (778.6 examples/sec; 0.164 sec/batch)
2016-11-28 18:40:27.432575: step 7550, loss = 0.85 (784.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:40:29.046385: step 7560, loss = 0.86 (815.9 examples/sec; 0.157 sec/batch)
2016-11-28 18:40:30.658277: step 7570, loss = 0.96 (802.8 examples/sec; 0.159 sec/batch)
2016-11-28 18:40:32.259679: step 7580, loss = 0.97 (827.3 examples/sec; 0.155 sec/batch)
2016-11-28 18:40:33.874896: step 7590, loss = 0.84 (797.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:40:35.492617: step 7600, loss = 0.98 (796.2 examples/sec; 0.161 sec/batch)
2016-11-28 18:40:37.263356: step 7610, loss = 1.00 (831.3 examples/sec; 0.154 sec/batch)
2016-11-28 18:40:38.832909: step 7620, loss = 0.95 (793.4 examples/sec; 0.161 sec/batch)
2016-11-28 18:40:40.398073: step 7630, loss = 0.97 (814.4 examples/sec; 0.157 sec/batch)
2016-11-28 18:40:41.948328: step 7640, loss = 0.84 (794.9 examples/sec; 0.161 sec/batch)
2016-11-28 18:40:43.506463: step 7650, loss = 0.98 (805.7 examples/sec; 0.159 sec/batch)
2016-11-28 18:40:45.065712: step 7660, loss = 0.81 (847.4 examples/sec; 0.151 sec/batch)
2016-11-28 18:40:46.622147: step 7670, loss = 0.82 (849.1 examples/sec; 0.151 sec/batch)
2016-11-28 18:40:48.187387: step 7680, loss = 0.94 (815.0 examples/sec; 0.157 sec/batch)
2016-11-28 18:40:49.741520: step 7690, loss = 1.00 (805.5 examples/sec; 0.159 sec/batch)
2016-11-28 18:40:51.299545: step 7700, loss = 0.85 (825.4 examples/sec; 0.155 sec/batch)
2016-11-28 18:40:53.065293: step 7710, loss = 1.05 (808.2 examples/sec; 0.158 sec/batch)
2016-11-28 18:40:54.627596: step 7720, loss = 0.93 (809.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:40:56.184458: step 7730, loss = 1.05 (818.2 examples/sec; 0.156 sec/batch)
2016-11-28 18:40:57.751804: step 7740, loss = 0.93 (806.0 examples/sec; 0.159 sec/batch)
2016-11-28 18:40:59.345971: step 7750, loss = 1.00 (828.8 examples/sec; 0.154 sec/batch)
2016-11-28 18:41:00.950485: step 7760, loss = 0.84 (818.9 examples/sec; 0.156 sec/batch)
2016-11-28 18:41:02.564835: step 7770, loss = 0.85 (799.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:41:04.180111: step 7780, loss = 0.90 (770.6 examples/sec; 0.166 sec/batch)
2016-11-28 18:41:05.802005: step 7790, loss = 0.91 (759.5 examples/sec; 0.169 sec/batch)
2016-11-28 18:41:07.420145: step 7800, loss = 0.92 (766.8 examples/sec; 0.167 sec/batch)
2016-11-28 18:41:09.208035: step 7810, loss = 1.02 (781.1 examples/sec; 0.164 sec/batch)
2016-11-28 18:41:10.783180: step 7820, loss = 0.88 (847.2 examples/sec; 0.151 sec/batch)
2016-11-28 18:41:12.353837: step 7830, loss = 1.06 (834.4 examples/sec; 0.153 sec/batch)
2016-11-28 18:41:13.922768: step 7840, loss = 0.96 (823.0 examples/sec; 0.156 sec/batch)
2016-11-28 18:41:15.486486: step 7850, loss = 0.97 (796.6 examples/sec; 0.161 sec/batch)
2016-11-28 18:41:17.059664: step 7860, loss = 1.03 (785.0 examples/sec; 0.163 sec/batch)
2016-11-28 18:41:18.627836: step 7870, loss = 1.02 (813.1 examples/sec; 0.157 sec/batch)
2016-11-28 18:41:20.190309: step 7880, loss = 0.95 (815.0 examples/sec; 0.157 sec/batch)
2016-11-28 18:41:21.760385: step 7890, loss = 1.03 (811.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:41:23.331070: step 7900, loss = 0.87 (874.2 examples/sec; 0.146 sec/batch)
2016-11-28 18:41:25.109723: step 7910, loss = 1.16 (852.7 examples/sec; 0.150 sec/batch)
2016-11-28 18:41:26.695349: step 7920, loss = 0.97 (797.2 examples/sec; 0.161 sec/batch)
2016-11-28 18:41:28.259028: step 7930, loss = 0.85 (811.1 examples/sec; 0.158 sec/batch)
2016-11-28 18:41:29.824856: step 7940, loss = 0.81 (825.8 examples/sec; 0.155 sec/batch)
2016-11-28 18:41:31.391007: step 7950, loss = 0.87 (817.1 examples/sec; 0.157 sec/batch)
2016-11-28 18:41:32.960782: step 7960, loss = 0.98 (779.0 examples/sec; 0.164 sec/batch)
2016-11-28 18:41:34.540820: step 7970, loss = 0.89 (828.2 examples/sec; 0.155 sec/batch)
2016-11-28 18:41:36.121670: step 7980, loss = 0.90 (832.0 examples/sec; 0.154 sec/batch)
2016-11-28 18:41:37.691733: step 7990, loss = 0.85 (834.3 examples/sec; 0.153 sec/batch)
2016-11-28 18:41:39.262854: step 8000, loss = 1.05 (812.9 examples/sec; 0.157 sec/batch)
2016-11-28 18:41:41.381301: step 8010, loss = 0.94 (848.2 examples/sec; 0.151 sec/batch)
2016-11-28 18:41:42.933541: step 8020, loss = 0.92 (780.9 examples/sec; 0.164 sec/batch)
2016-11-28 18:41:44.457260: step 8030, loss = 0.89 (844.2 examples/sec; 0.152 sec/batch)
2016-11-28 18:41:45.995414: step 8040, loss = 0.91 (796.4 examples/sec; 0.161 sec/batch)
2016-11-28 18:41:47.559980: step 8050, loss = 0.77 (769.6 examples/sec; 0.166 sec/batch)
2016-11-28 18:41:49.141411: step 8060, loss = 0.89 (778.0 examples/sec; 0.165 sec/batch)
2016-11-28 18:41:50.726725: step 8070, loss = 0.89 (789.3 examples/sec; 0.162 sec/batch)
2016-11-28 18:41:52.363738: step 8080, loss = 0.82 (794.1 examples/sec; 0.161 sec/batch)
2016-11-28 18:41:53.980174: step 8090, loss = 0.85 (800.2 examples/sec; 0.160 sec/batch)
2016-11-28 18:41:55.593740: step 8100, loss = 0.97 (793.6 examples/sec; 0.161 sec/batch)
2016-11-28 18:41:57.426711: step 8110, loss = 0.71 (741.3 examples/sec; 0.173 sec/batch)
2016-11-28 18:41:59.045884: step 8120, loss = 0.92 (795.0 examples/sec; 0.161 sec/batch)
2016-11-28 18:42:00.677411: step 8130, loss = 0.88 (783.5 examples/sec; 0.163 sec/batch)
2016-11-28 18:42:02.295299: step 8140, loss = 0.91 (769.7 examples/sec; 0.166 sec/batch)
2016-11-28 18:42:03.911473: step 8150, loss = 0.91 (786.6 examples/sec; 0.163 sec/batch)
2016-11-28 18:42:05.540961: step 8160, loss = 1.04 (812.4 examples/sec; 0.158 sec/batch)
2016-11-28 18:42:07.155710: step 8170, loss = 0.94 (781.7 examples/sec; 0.164 sec/batch)
2016-11-28 18:42:08.769309: step 8180, loss = 0.91 (783.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:42:10.379708: step 8190, loss = 1.14 (803.7 examples/sec; 0.159 sec/batch)
2016-11-28 18:42:12.003050: step 8200, loss = 1.18 (787.2 examples/sec; 0.163 sec/batch)
2016-11-28 18:42:13.837727: step 8210, loss = 1.02 (769.6 examples/sec; 0.166 sec/batch)
2016-11-28 18:42:15.448116: step 8220, loss = 0.95 (776.8 examples/sec; 0.165 sec/batch)
2016-11-28 18:42:17.064173: step 8230, loss = 0.77 (792.9 examples/sec; 0.161 sec/batch)
2016-11-28 18:42:18.681291: step 8240, loss = 0.85 (780.0 examples/sec; 0.164 sec/batch)
2016-11-28 18:42:20.300175: step 8250, loss = 0.98 (784.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:42:21.926001: step 8260, loss = 0.87 (825.4 examples/sec; 0.155 sec/batch)
2016-11-28 18:42:23.542209: step 8270, loss = 0.82 (804.4 examples/sec; 0.159 sec/batch)
2016-11-28 18:42:25.148299: step 8280, loss = 1.02 (793.9 examples/sec; 0.161 sec/batch)
2016-11-28 18:42:26.763661: step 8290, loss = 0.91 (751.2 examples/sec; 0.170 sec/batch)
2016-11-28 18:42:28.363588: step 8300, loss = 0.96 (782.6 examples/sec; 0.164 sec/batch)
2016-11-28 18:42:30.192680: step 8310, loss = 0.92 (800.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:42:31.814915: step 8320, loss = 0.77 (748.5 examples/sec; 0.171 sec/batch)
2016-11-28 18:42:33.421431: step 8330, loss = 1.02 (798.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:42:35.026888: step 8340, loss = 1.03 (765.4 examples/sec; 0.167 sec/batch)
2016-11-28 18:42:36.633160: step 8350, loss = 1.03 (794.7 examples/sec; 0.161 sec/batch)
2016-11-28 18:42:38.248093: step 8360, loss = 0.99 (818.0 examples/sec; 0.156 sec/batch)
2016-11-28 18:42:39.865122: step 8370, loss = 1.00 (806.0 examples/sec; 0.159 sec/batch)
2016-11-28 18:42:41.470969: step 8380, loss = 0.92 (788.7 examples/sec; 0.162 sec/batch)
2016-11-28 18:42:43.075912: step 8390, loss = 0.89 (806.5 examples/sec; 0.159 sec/batch)
2016-11-28 18:42:44.687926: step 8400, loss = 0.93 (784.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:42:46.476949: step 8410, loss = 0.87 (808.7 examples/sec; 0.158 sec/batch)
2016-11-28 18:42:48.045516: step 8420, loss = 1.03 (782.7 examples/sec; 0.164 sec/batch)
2016-11-28 18:42:49.611615: step 8430, loss = 0.90 (875.4 examples/sec; 0.146 sec/batch)
2016-11-28 18:42:51.199096: step 8440, loss = 1.05 (805.5 examples/sec; 0.159 sec/batch)
2016-11-28 18:42:52.811776: step 8450, loss = 0.89 (792.9 examples/sec; 0.161 sec/batch)
2016-11-28 18:42:54.428313: step 8460, loss = 0.82 (806.7 examples/sec; 0.159 sec/batch)
2016-11-28 18:42:56.042620: step 8470, loss = 0.83 (806.9 examples/sec; 0.159 sec/batch)
2016-11-28 18:42:57.660470: step 8480, loss = 1.01 (791.3 examples/sec; 0.162 sec/batch)
2016-11-28 18:42:59.265006: step 8490, loss = 0.78 (797.2 examples/sec; 0.161 sec/batch)
2016-11-28 18:43:00.880673: step 8500, loss = 0.94 (769.7 examples/sec; 0.166 sec/batch)
2016-11-28 18:43:02.664702: step 8510, loss = 0.89 (802.8 examples/sec; 0.159 sec/batch)
2016-11-28 18:43:04.243120: step 8520, loss = 1.04 (790.3 examples/sec; 0.162 sec/batch)
2016-11-28 18:43:05.827802: step 8530, loss = 1.02 (807.9 examples/sec; 0.158 sec/batch)
2016-11-28 18:43:07.410807: step 8540, loss = 0.74 (820.6 examples/sec; 0.156 sec/batch)
2016-11-28 18:43:08.997058: step 8550, loss = 0.85 (776.0 examples/sec; 0.165 sec/batch)
2016-11-28 18:43:10.582241: step 8560, loss = 0.80 (803.3 examples/sec; 0.159 sec/batch)
2016-11-28 18:43:12.161994: step 8570, loss = 0.79 (806.4 examples/sec; 0.159 sec/batch)
2016-11-28 18:43:13.746080: step 8580, loss = 0.97 (781.7 examples/sec; 0.164 sec/batch)
2016-11-28 18:43:15.315029: step 8590, loss = 1.02 (818.5 examples/sec; 0.156 sec/batch)
2016-11-28 18:43:16.902530: step 8600, loss = 0.95 (836.0 examples/sec; 0.153 sec/batch)
2016-11-28 18:43:18.680925: step 8610, loss = 0.99 (789.0 examples/sec; 0.162 sec/batch)
2016-11-28 18:43:20.262461: step 8620, loss = 0.98 (837.8 examples/sec; 0.153 sec/batch)
2016-11-28 18:43:21.850399: step 8630, loss = 0.78 (851.4 examples/sec; 0.150 sec/batch)
2016-11-28 18:43:23.441549: step 8640, loss = 0.91 (829.9 examples/sec; 0.154 sec/batch)
2016-11-28 18:43:25.017883: step 8650, loss = 0.86 (813.9 examples/sec; 0.157 sec/batch)
2016-11-28 18:43:26.598803: step 8660, loss = 0.92 (816.4 examples/sec; 0.157 sec/batch)
2016-11-28 18:43:28.181825: step 8670, loss = 0.82 (807.1 examples/sec; 0.159 sec/batch)
2016-11-28 18:43:29.774572: step 8680, loss = 0.74 (785.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:43:31.365397: step 8690, loss = 0.98 (759.5 examples/sec; 0.169 sec/batch)
2016-11-28 18:43:32.943176: step 8700, loss = 1.04 (797.6 examples/sec; 0.160 sec/batch)
2016-11-28 18:43:34.720966: step 8710, loss = 1.06 (813.3 examples/sec; 0.157 sec/batch)
2016-11-28 18:43:36.307180: step 8720, loss = 0.88 (798.7 examples/sec; 0.160 sec/batch)
2016-11-28 18:43:37.893507: step 8730, loss = 1.02 (770.2 examples/sec; 0.166 sec/batch)
2016-11-28 18:43:39.468080: step 8740, loss = 0.92 (802.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:43:41.043874: step 8750, loss = 0.96 (799.0 examples/sec; 0.160 sec/batch)
2016-11-28 18:43:42.622115: step 8760, loss = 0.97 (833.5 examples/sec; 0.154 sec/batch)
2016-11-28 18:43:44.230363: step 8770, loss = 0.86 (766.3 examples/sec; 0.167 sec/batch)
2016-11-28 18:43:45.841337: step 8780, loss = 0.94 (769.3 examples/sec; 0.166 sec/batch)
2016-11-28 18:43:47.441502: step 8790, loss = 0.84 (798.7 examples/sec; 0.160 sec/batch)
2016-11-28 18:43:49.061519: step 8800, loss = 0.96 (769.5 examples/sec; 0.166 sec/batch)
2016-11-28 18:43:50.875392: step 8810, loss = 0.85 (785.7 examples/sec; 0.163 sec/batch)
2016-11-28 18:43:52.487478: step 8820, loss = 0.94 (809.1 examples/sec; 0.158 sec/batch)
2016-11-28 18:43:54.101286: step 8830, loss = 1.07 (810.0 examples/sec; 0.158 sec/batch)
2016-11-28 18:43:55.722941: step 8840, loss = 0.92 (793.1 examples/sec; 0.161 sec/batch)
2016-11-28 18:43:57.338314: step 8850, loss = 1.04 (809.1 examples/sec; 0.158 sec/batch)
2016-11-28 18:43:58.954988: step 8860, loss = 0.79 (791.1 examples/sec; 0.162 sec/batch)
2016-11-28 18:44:00.571018: step 8870, loss = 0.98 (770.4 examples/sec; 0.166 sec/batch)
2016-11-28 18:44:02.193649: step 8880, loss = 1.01 (787.7 examples/sec; 0.162 sec/batch)
2016-11-28 18:44:03.809263: step 8890, loss = 0.96 (799.1 examples/sec; 0.160 sec/batch)
2016-11-28 18:44:05.416063: step 8900, loss = 0.86 (778.9 examples/sec; 0.164 sec/batch)
2016-11-28 18:44:07.165259: step 8910, loss = 0.90 (817.2 examples/sec; 0.157 sec/batch)
2016-11-28 18:44:08.732135: step 8920, loss = 0.71 (822.5 examples/sec; 0.156 sec/batch)
2016-11-28 18:44:10.311513: step 8930, loss = 0.80 (847.7 examples/sec; 0.151 sec/batch)
2016-11-28 18:44:11.887950: step 8940, loss = 0.84 (839.2 examples/sec; 0.153 sec/batch)
2016-11-28 18:44:13.451243: step 8950, loss = 0.87 (794.3 examples/sec; 0.161 sec/batch)
2016-11-28 18:44:15.003069: step 8960, loss = 0.85 (824.7 examples/sec; 0.155 sec/batch)
2016-11-28 18:44:16.566357: step 8970, loss = 0.94 (806.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:44:18.126036: step 8980, loss = 0.89 (811.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:44:19.681626: step 8990, loss = 0.96 (818.6 examples/sec; 0.156 sec/batch)
2016-11-28 18:44:21.240607: step 9000, loss = 0.87 (831.2 examples/sec; 0.154 sec/batch)
2016-11-28 18:44:23.403189: step 9010, loss = 0.93 (811.6 examples/sec; 0.158 sec/batch)
2016-11-28 18:44:24.967925: step 9020, loss = 0.98 (823.5 examples/sec; 0.155 sec/batch)
2016-11-28 18:44:26.529684: step 9030, loss = 0.90 (823.6 examples/sec; 0.155 sec/batch)
2016-11-28 18:44:28.093324: step 9040, loss = 0.91 (816.6 examples/sec; 0.157 sec/batch)
2016-11-28 18:44:29.689456: step 9050, loss = 0.88 (791.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:44:31.296017: step 9060, loss = 0.81 (815.9 examples/sec; 0.157 sec/batch)
2016-11-28 18:44:32.899372: step 9070, loss = 0.96 (823.3 examples/sec; 0.155 sec/batch)
2016-11-28 18:44:34.507742: step 9080, loss = 0.94 (812.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:44:36.112958: step 9090, loss = 0.95 (797.4 examples/sec; 0.161 sec/batch)
2016-11-28 18:44:37.721367: step 9100, loss = 1.14 (781.3 examples/sec; 0.164 sec/batch)
2016-11-28 18:44:39.533253: step 9110, loss = 0.98 (807.1 examples/sec; 0.159 sec/batch)
2016-11-28 18:44:41.151042: step 9120, loss = 0.82 (801.6 examples/sec; 0.160 sec/batch)
2016-11-28 18:44:42.763575: step 9130, loss = 0.80 (778.9 examples/sec; 0.164 sec/batch)
2016-11-28 18:44:44.378864: step 9140, loss = 1.00 (792.1 examples/sec; 0.162 sec/batch)
2016-11-28 18:44:45.991015: step 9150, loss = 0.98 (793.9 examples/sec; 0.161 sec/batch)
2016-11-28 18:44:47.618798: step 9160, loss = 0.84 (824.3 examples/sec; 0.155 sec/batch)
2016-11-28 18:44:49.234513: step 9170, loss = 0.93 (787.6 examples/sec; 0.163 sec/batch)
2016-11-28 18:44:50.840778: step 9180, loss = 0.95 (807.6 examples/sec; 0.158 sec/batch)
2016-11-28 18:44:52.453846: step 9190, loss = 0.93 (787.7 examples/sec; 0.163 sec/batch)
2016-11-28 18:44:54.070691: step 9200, loss = 0.82 (787.9 examples/sec; 0.162 sec/batch)
2016-11-28 18:44:55.882982: step 9210, loss = 0.88 (810.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:44:57.488033: step 9220, loss = 0.79 (825.0 examples/sec; 0.155 sec/batch)
2016-11-28 18:44:59.101872: step 9230, loss = 0.82 (782.7 examples/sec; 0.164 sec/batch)
2016-11-28 18:45:00.707017: step 9240, loss = 0.80 (791.6 examples/sec; 0.162 sec/batch)
2016-11-28 18:45:02.329623: step 9250, loss = 0.84 (808.6 examples/sec; 0.158 sec/batch)
2016-11-28 18:45:03.957391: step 9260, loss = 0.94 (769.0 examples/sec; 0.166 sec/batch)
2016-11-28 18:45:05.567165: step 9270, loss = 0.86 (787.5 examples/sec; 0.163 sec/batch)
2016-11-28 18:45:07.172905: step 9280, loss = 0.82 (798.1 examples/sec; 0.160 sec/batch)
2016-11-28 18:45:08.784535: step 9290, loss = 0.85 (805.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:45:10.384144: step 9300, loss = 0.83 (845.0 examples/sec; 0.151 sec/batch)
2016-11-28 18:45:12.198105: step 9310, loss = 0.89 (813.8 examples/sec; 0.157 sec/batch)
2016-11-28 18:45:13.812219: step 9320, loss = 0.89 (812.6 examples/sec; 0.158 sec/batch)
2016-11-28 18:45:15.423336: step 9330, loss = 0.86 (782.3 examples/sec; 0.164 sec/batch)
2016-11-28 18:45:17.039140: step 9340, loss = 0.81 (771.4 examples/sec; 0.166 sec/batch)
2016-11-28 18:45:18.640982: step 9350, loss = 0.79 (789.0 examples/sec; 0.162 sec/batch)
2016-11-28 18:45:20.249220: step 9360, loss = 0.79 (797.2 examples/sec; 0.161 sec/batch)
2016-11-28 18:45:21.859970: step 9370, loss = 0.95 (807.0 examples/sec; 0.159 sec/batch)
2016-11-28 18:45:23.475991: step 9380, loss = 0.88 (787.7 examples/sec; 0.162 sec/batch)
2016-11-28 18:45:25.085566: step 9390, loss = 1.01 (786.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:45:26.689433: step 9400, loss = 0.89 (852.3 examples/sec; 0.150 sec/batch)
2016-11-28 18:45:28.514934: step 9410, loss = 0.80 (782.4 examples/sec; 0.164 sec/batch)
2016-11-28 18:45:30.145416: step 9420, loss = 1.03 (789.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:45:31.765284: step 9430, loss = 0.95 (828.1 examples/sec; 0.155 sec/batch)
2016-11-28 18:45:33.381831: step 9440, loss = 0.93 (806.5 examples/sec; 0.159 sec/batch)
2016-11-28 18:45:34.992649: step 9450, loss = 0.96 (798.3 examples/sec; 0.160 sec/batch)
2016-11-28 18:45:36.615179: step 9460, loss = 0.84 (773.0 examples/sec; 0.166 sec/batch)
2016-11-28 18:45:38.233766: step 9470, loss = 0.98 (786.1 examples/sec; 0.163 sec/batch)
2016-11-28 18:45:39.842630: step 9480, loss = 0.98 (775.7 examples/sec; 0.165 sec/batch)
2016-11-28 18:45:41.443662: step 9490, loss = 0.75 (799.3 examples/sec; 0.160 sec/batch)
2016-11-28 18:45:43.057724: step 9500, loss = 0.84 (776.9 examples/sec; 0.165 sec/batch)
2016-11-28 18:45:44.863034: step 9510, loss = 0.92 (811.4 examples/sec; 0.158 sec/batch)
2016-11-28 18:45:46.466259: step 9520, loss = 0.85 (832.7 examples/sec; 0.154 sec/batch)
2016-11-28 18:45:48.085462: step 9530, loss = 0.88 (779.8 examples/sec; 0.164 sec/batch)
2016-11-28 18:45:49.700589: step 9540, loss = 0.81 (782.8 examples/sec; 0.164 sec/batch)
2016-11-28 18:45:51.315784: step 9550, loss = 0.92 (773.0 examples/sec; 0.166 sec/batch)
2016-11-28 18:45:52.923807: step 9560, loss = 0.87 (791.1 examples/sec; 0.162 sec/batch)
2016-11-28 18:45:54.534524: step 9570, loss = 0.81 (793.1 examples/sec; 0.161 sec/batch)
2016-11-28 18:45:56.140824: step 9580, loss = 0.80 (792.1 examples/sec; 0.162 sec/batch)
2016-11-28 18:45:57.765035: step 9590, loss = 0.99 (802.3 examples/sec; 0.160 sec/batch)
2016-11-28 18:45:59.394386: step 9600, loss = 0.74 (759.4 examples/sec; 0.169 sec/batch)
2016-11-28 18:46:01.150481: step 9610, loss = 0.89 (801.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:46:02.707985: step 9620, loss = 0.83 (810.4 examples/sec; 0.158 sec/batch)
2016-11-28 18:46:04.305899: step 9630, loss = 0.79 (776.5 examples/sec; 0.165 sec/batch)
2016-11-28 18:46:05.909471: step 9640, loss = 0.87 (789.5 examples/sec; 0.162 sec/batch)
2016-11-28 18:46:07.513553: step 9650, loss = 0.78 (800.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:46:09.120419: step 9660, loss = 0.85 (800.6 examples/sec; 0.160 sec/batch)
2016-11-28 18:46:10.725995: step 9670, loss = 0.82 (794.8 examples/sec; 0.161 sec/batch)
2016-11-28 18:46:12.348682: step 9680, loss = 0.66 (794.6 examples/sec; 0.161 sec/batch)
2016-11-28 18:46:13.961327: step 9690, loss = 0.84 (736.5 examples/sec; 0.174 sec/batch)
2016-11-28 18:46:15.564292: step 9700, loss = 0.88 (833.0 examples/sec; 0.154 sec/batch)
2016-11-28 18:46:17.385501: step 9710, loss = 0.82 (801.3 examples/sec; 0.160 sec/batch)
2016-11-28 18:46:18.997388: step 9720, loss = 0.88 (785.8 examples/sec; 0.163 sec/batch)
2016-11-28 18:46:20.605372: step 9730, loss = 0.71 (811.3 examples/sec; 0.158 sec/batch)
2016-11-28 18:46:22.229649: step 9740, loss = 0.93 (779.2 examples/sec; 0.164 sec/batch)
2016-11-28 18:46:23.837232: step 9750, loss = 0.87 (789.6 examples/sec; 0.162 sec/batch)
2016-11-28 18:46:25.444874: step 9760, loss = 0.99 (814.0 examples/sec; 0.157 sec/batch)
2016-11-28 18:46:27.055850: step 9770, loss = 0.99 (785.6 examples/sec; 0.163 sec/batch)
2016-11-28 18:46:28.666169: step 9780, loss = 0.93 (826.4 examples/sec; 0.155 sec/batch)
2016-11-28 18:46:30.293652: step 9790, loss = 0.97 (783.0 examples/sec; 0.163 sec/batch)
2016-11-28 18:46:31.911257: step 9800, loss = 0.89 (794.8 examples/sec; 0.161 sec/batch)
2016-11-28 18:46:33.715448: step 9810, loss = 0.82 (783.7 examples/sec; 0.163 sec/batch)
2016-11-28 18:46:35.329815: step 9820, loss = 0.89 (787.9 examples/sec; 0.162 sec/batch)
2016-11-28 18:46:36.936338: step 9830, loss = 0.94 (794.1 examples/sec; 0.161 sec/batch)
2016-11-28 18:46:38.555467: step 9840, loss = 0.95 (803.3 examples/sec; 0.159 sec/batch)
2016-11-28 18:46:40.159890: step 9850, loss = 0.74 (837.8 examples/sec; 0.153 sec/batch)
2016-11-28 18:46:41.767206: step 9860, loss = 0.89 (787.5 examples/sec; 0.163 sec/batch)
2016-11-28 18:46:43.376398: step 9870, loss = 0.91 (792.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:46:44.995871: step 9880, loss = 0.88 (810.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:46:46.605323: step 9890, loss = 0.84 (789.2 examples/sec; 0.162 sec/batch)
2016-11-28 18:46:48.220572: step 9900, loss = 0.86 (825.0 examples/sec; 0.155 sec/batch)
2016-11-28 18:46:49.988491: step 9910, loss = 0.92 (832.2 examples/sec; 0.154 sec/batch)
2016-11-28 18:46:51.553514: step 9920, loss = 0.84 (784.9 examples/sec; 0.163 sec/batch)
2016-11-28 18:46:53.115321: step 9930, loss = 0.90 (800.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:46:54.676448: step 9940, loss = 1.04 (800.5 examples/sec; 0.160 sec/batch)
2016-11-28 18:46:56.234970: step 9950, loss = 0.99 (822.0 examples/sec; 0.156 sec/batch)
2016-11-28 18:46:57.789156: step 9960, loss = 0.86 (823.2 examples/sec; 0.155 sec/batch)
2016-11-28 18:46:59.352915: step 9970, loss = 0.85 (790.6 examples/sec; 0.162 sec/batch)
2016-11-28 18:47:00.917887: step 9980, loss = 0.88 (798.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:47:02.478822: step 9990, loss = 0.83 (806.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:47:04.083860: step 10000, loss = 0.95 (802.2 examples/sec; 0.160 sec/batch)
2016-11-28 18:47:06.279246: step 10010, loss = 0.64 (779.1 examples/sec; 0.164 sec/batch)
2016-11-28 18:47:07.869652: step 10020, loss = 0.93 (764.8 examples/sec; 0.167 sec/batch)
2016-11-28 18:47:09.455514: step 10030, loss = 0.93 (837.5 examples/sec; 0.153 sec/batch)
2016-11-28 18:47:11.052629: step 10040, loss = 0.88 (798.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:47:12.648777: step 10050, loss = 1.02 (814.2 examples/sec; 0.157 sec/batch)
2016-11-28 18:47:14.241553: step 10060, loss = 1.02 (763.4 examples/sec; 0.168 sec/batch)
2016-11-28 18:47:15.826696: step 10070, loss = 0.83 (808.3 examples/sec; 0.158 sec/batch)
2016-11-28 18:47:17.426449: step 10080, loss = 0.80 (806.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:47:19.007768: step 10090, loss = 0.72 (821.5 examples/sec; 0.156 sec/batch)
2016-11-28 18:47:20.601249: step 10100, loss = 0.80 (796.1 examples/sec; 0.161 sec/batch)
2016-11-28 18:47:22.389956: step 10110, loss = 0.75 (798.5 examples/sec; 0.160 sec/batch)
2016-11-28 18:47:23.976924: step 10120, loss = 0.94 (815.2 examples/sec; 0.157 sec/batch)
2016-11-28 18:47:25.564101: step 10130, loss = 0.91 (846.1 examples/sec; 0.151 sec/batch)
2016-11-28 18:47:27.147014: step 10140, loss = 0.79 (831.6 examples/sec; 0.154 sec/batch)
2016-11-28 18:47:28.742537: step 10150, loss = 0.87 (812.2 examples/sec; 0.158 sec/batch)
2016-11-28 18:47:30.331017: step 10160, loss = 0.89 (801.6 examples/sec; 0.160 sec/batch)
2016-11-28 18:47:31.933730: step 10170, loss = 0.96 (784.8 examples/sec; 0.163 sec/batch)
2016-11-28 18:47:33.522435: step 10180, loss = 0.86 (808.9 examples/sec; 0.158 sec/batch)
2016-11-28 18:47:35.125518: step 10190, loss = 0.79 (789.1 examples/sec; 0.162 sec/batch)
2016-11-28 18:47:36.725123: step 10200, loss = 0.79 (786.3 examples/sec; 0.163 sec/batch)
2016-11-28 18:47:38.516298: step 10210, loss = 0.69 (802.0 examples/sec; 0.160 sec/batch)
2016-11-28 18:47:40.092059: step 10220, loss = 0.76 (802.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:47:41.680669: step 10230, loss = 1.11 (826.5 examples/sec; 0.155 sec/batch)
2016-11-28 18:47:43.277796: step 10240, loss = 0.75 (801.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:47:44.865063: step 10250, loss = 0.87 (818.6 examples/sec; 0.156 sec/batch)
2016-11-28 18:47:46.446411: step 10260, loss = 0.79 (841.1 examples/sec; 0.152 sec/batch)
2016-11-28 18:47:48.033332: step 10270, loss = 0.93 (785.7 examples/sec; 0.163 sec/batch)
2016-11-28 18:47:49.621779: step 10280, loss = 1.07 (787.1 examples/sec; 0.163 sec/batch)
2016-11-28 18:47:51.202140: step 10290, loss = 0.86 (831.5 examples/sec; 0.154 sec/batch)
2016-11-28 18:47:52.789586: step 10300, loss = 0.84 (806.1 examples/sec; 0.159 sec/batch)
2016-11-28 18:47:54.570135: step 10310, loss = 0.80 (805.3 examples/sec; 0.159 sec/batch)
2016-11-28 18:47:56.162461: step 10320, loss = 1.01 (786.8 examples/sec; 0.163 sec/batch)
2016-11-28 18:47:57.742065: step 10330, loss = 0.86 (782.8 examples/sec; 0.164 sec/batch)
2016-11-28 18:47:59.329002: step 10340, loss = 0.84 (816.1 examples/sec; 0.157 sec/batch)
2016-11-28 18:48:00.910728: step 10350, loss = 0.81 (809.0 examples/sec; 0.158 sec/batch)
2016-11-28 18:48:02.514134: step 10360, loss = 0.91 (797.6 examples/sec; 0.160 sec/batch)
2016-11-28 18:48:04.119009: step 10370, loss = 0.81 (804.8 examples/sec; 0.159 sec/batch)
2016-11-28 18:48:05.728205: step 10380, loss = 0.71 (808.8 examples/sec; 0.158 sec/batch)
2016-11-28 18:48:07.344764: step 10390, loss = 0.79 (814.4 examples/sec; 0.157 sec/batch)
2016-11-28 18:48:08.956790: step 10400, loss = 0.91 (801.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:48:10.735076: step 10410, loss = 1.04 (796.1 examples/sec; 0.161 sec/batch)
2016-11-28 18:48:12.323576: step 10420, loss = 0.87 (807.6 examples/sec; 0.158 sec/batch)
2016-11-28 18:48:13.902778: step 10430, loss = 0.90 (809.2 examples/sec; 0.158 sec/batch)
2016-11-28 18:48:15.484663: step 10440, loss = 0.71 (779.4 examples/sec; 0.164 sec/batch)
2016-11-28 18:48:17.053515: step 10450, loss = 0.75 (814.9 examples/sec; 0.157 sec/batch)
2016-11-28 18:48:18.637158: step 10460, loss = 0.80 (813.7 examples/sec; 0.157 sec/batch)
2016-11-28 18:48:20.214973: step 10470, loss = 0.96 (802.2 examples/sec; 0.160 sec/batch)
2016-11-28 18:48:21.800587: step 10480, loss = 0.72 (797.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:48:23.387220: step 10490, loss = 0.91 (787.0 examples/sec; 0.163 sec/batch)
2016-11-28 18:48:24.964442: step 10500, loss = 1.13 (794.4 examples/sec; 0.161 sec/batch)
2016-11-28 18:48:26.730669: step 10510, loss = 0.91 (771.3 examples/sec; 0.166 sec/batch)
2016-11-28 18:48:28.286950: step 10520, loss = 0.89 (813.8 examples/sec; 0.157 sec/batch)
2016-11-28 18:48:29.855154: step 10530, loss = 0.95 (801.7 examples/sec; 0.160 sec/batch)
2016-11-28 18:48:31.420715: step 10540, loss = 0.83 (808.4 examples/sec; 0.158 sec/batch)
2016-11-28 18:48:32.984316: step 10550, loss = 0.93 (830.7 examples/sec; 0.154 sec/batch)
2016-11-28 18:48:34.552185: step 10560, loss = 0.99 (809.1 examples/sec; 0.158 sec/batch)
2016-11-28 18:48:36.127209: step 10570, loss = 0.62 (810.8 examples/sec; 0.158 sec/batch)
2016-11-28 18:48:37.698060: step 10580, loss = 0.95 (811.2 examples/sec; 0.158 sec/batch)
2016-11-28 18:48:39.268044: step 10590, loss = 0.99 (825.8 examples/sec; 0.155 sec/batch)
2016-11-28 18:48:40.840237: step 10600, loss = 0.86 (792.9 examples/sec; 0.161 sec/batch)
2016-11-28 18:48:42.607650: step 10610, loss = 0.81 (798.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:48:44.174855: step 10620, loss = 0.82 (827.5 examples/sec; 0.155 sec/batch)
2016-11-28 18:48:45.740347: step 10630, loss = 0.99 (827.0 examples/sec; 0.155 sec/batch)
2016-11-28 18:48:47.304418: step 10640, loss = 0.77 (833.5 examples/sec; 0.154 sec/batch)
2016-11-28 18:48:48.875568: step 10650, loss = 0.82 (841.2 examples/sec; 0.152 sec/batch)
2016-11-28 18:48:50.453593: step 10660, loss = 0.73 (828.6 examples/sec; 0.154 sec/batch)
2016-11-28 18:48:52.024715: step 10670, loss = 0.96 (815.9 examples/sec; 0.157 sec/batch)
2016-11-28 18:48:53.592412: step 10680, loss = 0.86 (835.2 examples/sec; 0.153 sec/batch)
2016-11-28 18:48:55.155510: step 10690, loss = 0.76 (798.1 examples/sec; 0.160 sec/batch)
2016-11-28 18:48:56.712852: step 10700, loss = 0.77 (828.7 examples/sec; 0.154 sec/batch)
2016-11-28 18:48:58.484180: step 10710, loss = 0.80 (814.2 examples/sec; 0.157 sec/batch)
2016-11-28 18:49:00.052699: step 10720, loss = 0.96 (859.3 examples/sec; 0.149 sec/batch)
2016-11-28 18:49:01.629295: step 10730, loss = 0.84 (801.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:49:03.200425: step 10740, loss = 1.00 (808.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:49:04.787869: step 10750, loss = 0.79 (800.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:49:06.390749: step 10760, loss = 0.86 (805.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:49:07.997600: step 10770, loss = 0.91 (811.1 examples/sec; 0.158 sec/batch)
2016-11-28 18:49:09.608108: step 10780, loss = 0.94 (785.0 examples/sec; 0.163 sec/batch)
2016-11-28 18:49:11.221648: step 10790, loss = 0.95 (740.4 examples/sec; 0.173 sec/batch)
2016-11-28 18:49:12.821826: step 10800, loss = 0.89 (773.2 examples/sec; 0.166 sec/batch)
2016-11-28 18:49:14.633005: step 10810, loss = 1.00 (812.2 examples/sec; 0.158 sec/batch)
2016-11-28 18:49:16.249676: step 10820, loss = 0.82 (798.3 examples/sec; 0.160 sec/batch)
2016-11-28 18:49:17.872747: step 10830, loss = 0.76 (763.1 examples/sec; 0.168 sec/batch)
2016-11-28 18:49:19.478152: step 10840, loss = 0.95 (761.5 examples/sec; 0.168 sec/batch)
2016-11-28 18:49:21.082727: step 10850, loss = 0.90 (784.7 examples/sec; 0.163 sec/batch)
2016-11-28 18:49:22.676252: step 10860, loss = 0.82 (804.0 examples/sec; 0.159 sec/batch)
2016-11-28 18:49:24.291960: step 10870, loss = 0.76 (810.4 examples/sec; 0.158 sec/batch)
2016-11-28 18:49:25.902702: step 10880, loss = 0.94 (809.3 examples/sec; 0.158 sec/batch)
2016-11-28 18:49:27.517344: step 10890, loss = 0.87 (777.1 examples/sec; 0.165 sec/batch)
2016-11-28 18:49:29.126015: step 10900, loss = 0.90 (814.0 examples/sec; 0.157 sec/batch)
2016-11-28 18:49:30.926769: step 10910, loss = 1.02 (814.9 examples/sec; 0.157 sec/batch)
2016-11-28 18:49:32.544918: step 10920, loss = 0.71 (778.6 examples/sec; 0.164 sec/batch)
2016-11-28 18:49:34.140664: step 10930, loss = 1.00 (806.6 examples/sec; 0.159 sec/batch)
2016-11-28 18:49:35.758215: step 10940, loss = 0.87 (777.8 examples/sec; 0.165 sec/batch)
2016-11-28 18:49:37.366355: step 10950, loss = 0.86 (788.9 examples/sec; 0.162 sec/batch)
2016-11-28 18:49:38.978287: step 10960, loss = 0.83 (792.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:49:40.582298: step 10970, loss = 0.91 (809.7 examples/sec; 0.158 sec/batch)
2016-11-28 18:49:42.186173: step 10980, loss = 1.00 (814.9 examples/sec; 0.157 sec/batch)
2016-11-28 18:49:43.786403: step 10990, loss = 0.81 (807.2 examples/sec; 0.159 sec/batch)
2016-11-28 18:49:45.402363: step 11000, loss = 0.93 (793.5 examples/sec; 0.161 sec/batch)
2016-11-28 18:49:47.581857: step 11010, loss = 0.87 (793.4 examples/sec; 0.161 sec/batch)
2016-11-28 18:49:49.197382: step 11020, loss = 0.75 (777.1 examples/sec; 0.165 sec/batch)
2016-11-28 18:49:50.797574: step 11030, loss = 0.87 (811.2 examples/sec; 0.158 sec/batch)
2016-11-28 18:49:52.414475: step 11040, loss = 0.91 (788.2 examples/sec; 0.162 sec/batch)
2016-11-28 18:49:54.026730: step 11050, loss = 0.89 (836.4 examples/sec; 0.153 sec/batch)
2016-11-28 18:49:55.633577: step 11060, loss = 0.88 (804.9 examples/sec; 0.159 sec/batch)
2016-11-28 18:49:57.250310: step 11070, loss = 0.90 (811.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:49:58.865359: step 11080, loss = 1.02 (785.6 examples/sec; 0.163 sec/batch)
2016-11-28 18:50:00.472923: step 11090, loss = 1.05 (807.9 examples/sec; 0.158 sec/batch)
2016-11-28 18:50:02.081996: step 11100, loss = 1.06 (763.5 examples/sec; 0.168 sec/batch)
2016-11-28 18:50:03.881552: step 11110, loss = 0.80 (824.3 examples/sec; 0.155 sec/batch)
2016-11-28 18:50:05.473523: step 11120, loss = 0.98 (810.6 examples/sec; 0.158 sec/batch)
2016-11-28 18:50:07.059728: step 11130, loss = 0.84 (804.5 examples/sec; 0.159 sec/batch)
2016-11-28 18:50:08.637653: step 11140, loss = 0.79 (811.8 examples/sec; 0.158 sec/batch)
2016-11-28 18:50:10.234209: step 11150, loss = 0.98 (789.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:50:11.820673: step 11160, loss = 0.97 (802.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:50:13.406558: step 11170, loss = 0.84 (819.5 examples/sec; 0.156 sec/batch)
2016-11-28 18:50:15.000215: step 11180, loss = 0.83 (826.8 examples/sec; 0.155 sec/batch)
2016-11-28 18:50:16.592867: step 11190, loss = 0.93 (788.0 examples/sec; 0.162 sec/batch)
2016-11-28 18:50:18.181106: step 11200, loss = 0.81 (813.0 examples/sec; 0.157 sec/batch)
2016-11-28 18:50:19.968941: step 11210, loss = 0.87 (831.2 examples/sec; 0.154 sec/batch)
2016-11-28 18:50:21.572853: step 11220, loss = 0.87 (827.8 examples/sec; 0.155 sec/batch)
2016-11-28 18:50:23.190449: step 11230, loss = 0.96 (797.0 examples/sec; 0.161 sec/batch)
2016-11-28 18:50:24.809268: step 11240, loss = 0.80 (830.3 examples/sec; 0.154 sec/batch)
2016-11-28 18:50:26.426994: step 11250, loss = 0.90 (821.3 examples/sec; 0.156 sec/batch)
2016-11-28 18:50:28.046410: step 11260, loss = 0.78 (803.4 examples/sec; 0.159 sec/batch)
2016-11-28 18:50:29.668188: step 11270, loss = 1.17 (779.8 examples/sec; 0.164 sec/batch)
2016-11-28 18:50:31.283637: step 11280, loss = 0.68 (812.8 examples/sec; 0.157 sec/batch)
2016-11-28 18:50:32.887425: step 11290, loss = 0.90 (792.9 examples/sec; 0.161 sec/batch)
2016-11-28 18:50:34.494578: step 11300, loss = 0.85 (781.9 examples/sec; 0.164 sec/batch)
2016-11-28 18:50:36.310958: step 11310, loss = 0.83 (803.9 examples/sec; 0.159 sec/batch)
2016-11-28 18:50:37.910395: step 11320, loss = 1.00 (821.8 examples/sec; 0.156 sec/batch)
2016-11-28 18:50:39.532118: step 11330, loss = 0.74 (783.3 examples/sec; 0.163 sec/batch)
2016-11-28 18:50:41.145406: step 11340, loss = 0.82 (779.8 examples/sec; 0.164 sec/batch)
2016-11-28 18:50:42.748243: step 11350, loss = 0.84 (792.6 examples/sec; 0.161 sec/batch)
2016-11-28 18:50:44.355501: step 11360, loss = 0.94 (794.1 examples/sec; 0.161 sec/batch)
2016-11-28 18:50:45.968970: step 11370, loss = 0.92 (773.7 examples/sec; 0.165 sec/batch)
2016-11-28 18:50:47.587273: step 11380, loss = 0.97 (773.4 examples/sec; 0.165 sec/batch)
2016-11-28 18:50:49.201121: step 11390, loss = 0.72 (792.6 examples/sec; 0.161 sec/batch)
2016-11-28 18:50:50.815232: step 11400, loss = 0.93 (787.8 examples/sec; 0.162 sec/batch)
2016-11-28 18:50:52.619632: step 11410, loss = 0.88 (793.2 examples/sec; 0.161 sec/batch)
2016-11-28 18:50:54.221634: step 11420, loss = 0.78 (769.3 examples/sec; 0.166 sec/batch)
2016-11-28 18:50:55.832623: step 11430, loss = 0.90 (821.5 examples/sec; 0.156 sec/batch)
2016-11-28 18:50:57.454460: step 11440, loss = 0.87 (816.6 examples/sec; 0.157 sec/batch)
2016-11-28 18:50:59.069317: step 11450, loss = 0.80 (760.3 examples/sec; 0.168 sec/batch)
2016-11-28 18:51:00.682900: step 11460, loss = 0.81 (771.3 examples/sec; 0.166 sec/batch)
2016-11-28 18:51:02.301034: step 11470, loss = 0.74 (793.8 examples/sec; 0.161 sec/batch)
2016-11-28 18:51:03.914372: step 11480, loss = 0.81 (830.7 examples/sec; 0.154 sec/batch)
2016-11-28 18:51:05.535132: step 11490, loss = 0.96 (807.9 examples/sec; 0.158 sec/batch)
2016-11-28 18:51:07.150789: step 11500, loss = 1.06 (790.1 examples/sec; 0.162 sec/batch)
2016-11-28 18:51:08.953840: step 11510, loss = 0.91 (781.8 examples/sec; 0.164 sec/batch)
2016-11-28 18:51:10.558447: step 11520, loss = 0.81 (780.6 examples/sec; 0.164 sec/batch)
2016-11-28 18:51:12.170443: step 11530, loss = 0.77 (789.5 examples/sec; 0.162 sec/batch)
2016-11-28 18:51:13.794073: step 11540, loss = 0.89 (780.4 examples/sec; 0.164 sec/batch)
2016-11-28 18:51:15.421211: step 11550, loss = 0.78 (771.9 examples/sec; 0.166 sec/batch)
2016-11-28 18:51:17.027059: step 11560, loss = 0.97 (788.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:51:18.640451: step 11570, loss = 0.81 (773.7 examples/sec; 0.165 sec/batch)
2016-11-28 18:51:20.249283: step 11580, loss = 0.83 (810.0 examples/sec; 0.158 sec/batch)
2016-11-28 18:51:21.855903: step 11590, loss = 0.78 (798.2 examples/sec; 0.160 sec/batch)
2016-11-28 18:51:23.472918: step 11600, loss = 0.84 (814.1 examples/sec; 0.157 sec/batch)
2016-11-28 18:51:25.245975: step 11610, loss = 0.98 (832.1 examples/sec; 0.154 sec/batch)
2016-11-28 18:51:26.806327: step 11620, loss = 0.74 (830.1 examples/sec; 0.154 sec/batch)
2016-11-28 18:51:28.379675: step 11630, loss = 0.98 (819.9 examples/sec; 0.156 sec/batch)
2016-11-28 18:51:29.944682: step 11640, loss = 0.83 (831.8 examples/sec; 0.154 sec/batch)
2016-11-28 18:51:31.513068: step 11650, loss = 0.92 (836.9 examples/sec; 0.153 sec/batch)
2016-11-28 18:51:33.084549: step 11660, loss = 0.93 (814.8 examples/sec; 0.157 sec/batch)
2016-11-28 18:51:34.651492: step 11670, loss = 0.81 (807.4 examples/sec; 0.159 sec/batch)
2016-11-28 18:51:36.212463: step 11680, loss = 0.88 (830.4 examples/sec; 0.154 sec/batch)
2016-11-28 18:51:37.769265: step 11690, loss = 0.97 (858.9 examples/sec; 0.149 sec/batch)
2016-11-28 18:51:39.331966: step 11700, loss = 0.80 (830.5 examples/sec; 0.154 sec/batch)
2016-11-28 18:51:41.087096: step 11710, loss = 0.91 (835.4 examples/sec; 0.153 sec/batch)
2016-11-28 18:51:42.663495: step 11720, loss = 0.85 (812.4 examples/sec; 0.158 sec/batch)
2016-11-28 18:51:44.222364: step 11730, loss = 0.84 (824.1 examples/sec; 0.155 sec/batch)
2016-11-28 18:51:45.795390: step 11740, loss = 0.95 (837.5 examples/sec; 0.153 sec/batch)
2016-11-28 18:51:47.407302: step 11750, loss = 0.77 (823.0 examples/sec; 0.156 sec/batch)
2016-11-28 18:51:49.016986: step 11760, loss = 0.71 (757.5 examples/sec; 0.169 sec/batch)
2016-11-28 18:51:50.621032: step 11770, loss = 0.97 (775.4 examples/sec; 0.165 sec/batch)
2016-11-28 18:51:52.231371: step 11780, loss = 0.91 (790.9 examples/sec; 0.162 sec/batch)
2016-11-28 18:51:53.834024: step 11790, loss = 0.91 (834.4 examples/sec; 0.153 sec/batch)
2016-11-28 18:51:55.441870: step 11800, loss = 0.93 (802.7 examples/sec; 0.159 sec/batch)
2016-11-28 18:51:57.233486: step 11810, loss = 0.87 (803.7 examples/sec; 0.159 sec/batch)
2016-11-28 18:51:58.837912: step 11820, loss = 1.05 (756.9 examples/sec; 0.169 sec/batch)
2016-11-28 18:52:00.443982: step 11830, loss = 0.82 (758.9 examples/sec; 0.169 sec/batch)
2016-11-28 18:52:02.040334: step 11840, loss = 0.71 (828.6 examples/sec; 0.154 sec/batch)
2016-11-28 18:52:03.636965: step 11850, loss = 0.77 (782.6 examples/sec; 0.164 sec/batch)
2016-11-28 18:52:05.247583: step 11860, loss = 0.73 (789.6 examples/sec; 0.162 sec/batch)
2016-11-28 18:52:06.855826: step 11870, loss = 0.84 (800.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:52:08.462511: step 11880, loss = 0.91 (803.1 examples/sec; 0.159 sec/batch)
2016-11-28 18:52:10.060404: step 11890, loss = 0.97 (793.6 examples/sec; 0.161 sec/batch)
2016-11-28 18:52:11.662763: step 11900, loss = 0.99 (810.2 examples/sec; 0.158 sec/batch)
2016-11-28 18:52:13.457655: step 11910, loss = 0.92 (815.0 examples/sec; 0.157 sec/batch)
2016-11-28 18:52:15.050872: step 11920, loss = 0.84 (785.2 examples/sec; 0.163 sec/batch)
2016-11-28 18:52:16.650221: step 11930, loss = 0.98 (795.4 examples/sec; 0.161 sec/batch)
2016-11-28 18:52:18.239842: step 11940, loss = 0.69 (825.2 examples/sec; 0.155 sec/batch)
2016-11-28 18:52:19.828579: step 11950, loss = 0.95 (829.8 examples/sec; 0.154 sec/batch)
2016-11-28 18:52:21.419402: step 11960, loss = 0.96 (812.1 examples/sec; 0.158 sec/batch)
2016-11-28 18:52:23.020489: step 11970, loss = 1.07 (796.5 examples/sec; 0.161 sec/batch)
2016-11-28 18:52:24.608329: step 11980, loss = 0.84 (805.3 examples/sec; 0.159 sec/batch)
2016-11-28 18:52:26.214805: step 11990, loss = 0.97 (804.1 examples/sec; 0.159 sec/batch)
2016-11-28 18:52:27.809748: step 12000, loss = 0.95 (778.5 examples/sec; 0.164 sec/batch)
2016-11-28 18:52:29.929471: step 12010, loss = 0.86 (839.6 examples/sec; 0.152 sec/batch)
2016-11-28 18:52:31.466722: step 12020, loss = 0.81 (832.4 examples/sec; 0.154 sec/batch)
2016-11-28 18:52:33.038249: step 12030, loss = 0.96 (829.1 examples/sec; 0.154 sec/batch)
2016-11-28 18:52:34.617777: step 12040, loss = 0.77 (835.9 examples/sec; 0.153 sec/batch)
2016-11-28 18:52:36.195744: step 12050, loss = 1.07 (810.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:52:37.775882: step 12060, loss = 0.82 (786.1 examples/sec; 0.163 sec/batch)
2016-11-28 18:52:39.347096: step 12070, loss = 0.81 (789.2 examples/sec; 0.162 sec/batch)
2016-11-28 18:52:40.930353: step 12080, loss = 0.89 (799.7 examples/sec; 0.160 sec/batch)
2016-11-28 18:52:42.500598: step 12090, loss = 1.17 (844.9 examples/sec; 0.152 sec/batch)
2016-11-28 18:52:44.076684: step 12100, loss = 0.96 (824.3 examples/sec; 0.155 sec/batch)
2016-11-28 18:52:45.825425: step 12110, loss = 0.84 (830.8 examples/sec; 0.154 sec/batch)
2016-11-28 18:52:47.378802: step 12120, loss = 0.93 (825.4 examples/sec; 0.155 sec/batch)
2016-11-28 18:52:48.926356: step 12130, loss = 0.76 (844.1 examples/sec; 0.152 sec/batch)
2016-11-28 18:52:50.476438: step 12140, loss = 0.84 (818.4 examples/sec; 0.156 sec/batch)
2016-11-28 18:52:52.015752: step 12150, loss = 0.85 (843.8 examples/sec; 0.152 sec/batch)
2016-11-28 18:52:53.554418: step 12160, loss = 0.94 (844.2 examples/sec; 0.152 sec/batch)
2016-11-28 18:52:55.111141: step 12170, loss = 0.73 (821.7 examples/sec; 0.156 sec/batch)
2016-11-28 18:52:56.655886: step 12180, loss = 0.76 (829.1 examples/sec; 0.154 sec/batch)
2016-11-28 18:52:58.192667: step 12190, loss = 0.78 (887.3 examples/sec; 0.144 sec/batch)
2016-11-28 18:52:59.746193: step 12200, loss = 0.85 (820.7 examples/sec; 0.156 sec/batch)
2016-11-28 18:53:01.480779: step 12210, loss = 0.69 (819.3 examples/sec; 0.156 sec/batch)
2016-11-28 18:53:03.035986: step 12220, loss = 0.81 (813.7 examples/sec; 0.157 sec/batch)
2016-11-28 18:53:04.580155: step 12230, loss = 0.77 (816.1 examples/sec; 0.157 sec/batch)
2016-11-28 18:53:06.136990: step 12240, loss = 0.99 (828.1 examples/sec; 0.155 sec/batch)
2016-11-28 18:53:07.674592: step 12250, loss = 1.03 (838.6 examples/sec; 0.153 sec/batch)
2016-11-28 18:53:09.214213: step 12260, loss = 0.85 (842.7 examples/sec; 0.152 sec/batch)
2016-11-28 18:53:10.768334: step 12270, loss = 0.83 (835.3 examples/sec; 0.153 sec/batch)
2016-11-28 18:53:12.314084: step 12280, loss = 0.84 (834.4 examples/sec; 0.153 sec/batch)
2016-11-28 18:53:13.867615: step 12290, loss = 0.87 (880.5 examples/sec; 0.145 sec/batch)
2016-11-28 18:53:15.422289: step 12300, loss = 0.88 (839.4 examples/sec; 0.152 sec/batch)
2016-11-28 18:53:17.187170: step 12310, loss = 0.87 (865.6 examples/sec; 0.148 sec/batch)
2016-11-28 18:53:18.740702: step 12320, loss = 0.85 (820.8 examples/sec; 0.156 sec/batch)
2016-11-28 18:53:20.306993: step 12330, loss = 0.82 (779.4 examples/sec; 0.164 sec/batch)
2016-11-28 18:53:21.894977: step 12340, loss = 0.84 (801.5 examples/sec; 0.160 sec/batch)
2016-11-28 18:53:23.484037: step 12350, loss = 0.77 (772.2 examples/sec; 0.166 sec/batch)
2016-11-28 18:53:25.066360: step 12360, loss = 0.96 (795.9 examples/sec; 0.161 sec/batch)
2016-11-28 18:53:26.645227: step 12370, loss = 0.82 (834.6 examples/sec; 0.153 sec/batch)
2016-11-28 18:53:28.233129: step 12380, loss = 0.83 (813.9 examples/sec; 0.157 sec/batch)
2016-11-28 18:53:29.820975: step 12390, loss = 0.79 (795.5 examples/sec; 0.161 sec/batch)
2016-11-28 18:53:31.396661: step 12400, loss = 0.84 (801.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:53:33.196915: step 12410, loss = 0.87 (802.7 examples/sec; 0.159 sec/batch)
2016-11-28 18:53:34.788924: step 12420, loss = 0.75 (790.1 examples/sec; 0.162 sec/batch)
2016-11-28 18:53:36.375960: step 12430, loss = 0.74 (793.0 examples/sec; 0.161 sec/batch)
2016-11-28 18:53:37.957398: step 12440, loss = 0.84 (815.8 examples/sec; 0.157 sec/batch)
2016-11-28 18:53:39.546203: step 12450, loss = 0.87 (801.3 examples/sec; 0.160 sec/batch)
2016-11-28 18:53:41.125545: step 12460, loss = 0.96 (800.2 examples/sec; 0.160 sec/batch)
2016-11-28 18:53:42.705189: step 12470, loss = 0.97 (817.6 examples/sec; 0.157 sec/batch)
2016-11-28 18:53:44.301831: step 12480, loss = 0.90 (815.1 examples/sec; 0.157 sec/batch)
2016-11-28 18:53:45.882254: step 12490, loss = 1.02 (803.4 examples/sec; 0.159 sec/batch)
2016-11-28 18:53:47.465596: step 12500, loss = 0.88 (804.3 examples/sec; 0.159 sec/batch)
2016-11-28 18:53:49.254883: step 12510, loss = 0.88 (759.2 examples/sec; 0.169 sec/batch)
2016-11-28 18:53:50.834052: step 12520, loss = 0.75 (796.5 examples/sec; 0.161 sec/batch)
2016-11-28 18:53:52.426096: step 12530, loss = 0.91 (797.4 examples/sec; 0.161 sec/batch)
2016-11-28 18:53:54.002736: step 12540, loss = 0.94 (855.3 examples/sec; 0.150 sec/batch)
2016-11-28 18:53:55.598318: step 12550, loss = 0.98 (803.7 examples/sec; 0.159 sec/batch)
2016-11-28 18:53:57.179962: step 12560, loss = 0.88 (806.0 examples/sec; 0.159 sec/batch)
2016-11-28 18:53:58.768160: step 12570, loss = 0.95 (796.8 examples/sec; 0.161 sec/batch)
2016-11-28 18:54:00.360311: step 12580, loss = 0.77 (792.6 examples/sec; 0.161 sec/batch)
2016-11-28 18:54:01.954242: step 12590, loss = 0.76 (796.5 examples/sec; 0.161 sec/batch)
2016-11-28 18:54:03.547628: step 12600, loss = 0.91 (821.3 examples/sec; 0.156 sec/batch)
2016-11-28 18:54:05.358910: step 12610, loss = 0.91 (784.6 examples/sec; 0.163 sec/batch)
2016-11-28 18:54:06.972329: step 12620, loss = 0.78 (801.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:54:08.588161: step 12630, loss = 0.81 (830.5 examples/sec; 0.154 sec/batch)
2016-11-28 18:54:10.213314: step 12640, loss = 0.94 (767.2 examples/sec; 0.167 sec/batch)
2016-11-28 18:54:11.821637: step 12650, loss = 0.81 (816.5 examples/sec; 0.157 sec/batch)
2016-11-28 18:54:13.436260: step 12660, loss = 0.86 (806.1 examples/sec; 0.159 sec/batch)
2016-11-28 18:54:15.044201: step 12670, loss = 1.11 (798.2 examples/sec; 0.160 sec/batch)
2016-11-28 18:54:16.643511: step 12680, loss = 0.86 (796.5 examples/sec; 0.161 sec/batch)
2016-11-28 18:54:18.252272: step 12690, loss = 0.93 (817.3 examples/sec; 0.157 sec/batch)
2016-11-28 18:54:19.866792: step 12700, loss = 0.81 (792.7 examples/sec; 0.161 sec/batch)
2016-11-28 18:54:21.669625: step 12710, loss = 0.88 (809.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:54:23.265619: step 12720, loss = 1.14 (812.8 examples/sec; 0.157 sec/batch)
2016-11-28 18:54:24.856239: step 12730, loss = 0.89 (804.2 examples/sec; 0.159 sec/batch)
2016-11-28 18:54:26.449796: step 12740, loss = 0.99 (807.8 examples/sec; 0.158 sec/batch)
2016-11-28 18:54:28.048566: step 12750, loss = 0.79 (812.6 examples/sec; 0.158 sec/batch)
2016-11-28 18:54:29.643397: step 12760, loss = 0.72 (816.6 examples/sec; 0.157 sec/batch)
2016-11-28 18:54:31.247977: step 12770, loss = 0.94 (750.9 examples/sec; 0.170 sec/batch)
2016-11-28 18:54:32.844830: step 12780, loss = 0.81 (809.5 examples/sec; 0.158 sec/batch)
2016-11-28 18:54:34.447575: step 12790, loss = 0.87 (796.0 examples/sec; 0.161 sec/batch)
2016-11-28 18:54:36.051219: step 12800, loss = 0.92 (835.1 examples/sec; 0.153 sec/batch)
2016-11-28 18:54:37.814757: step 12810, loss = 0.84 (845.8 examples/sec; 0.151 sec/batch)
2016-11-28 18:54:39.375335: step 12820, loss = 1.00 (836.7 examples/sec; 0.153 sec/batch)
2016-11-28 18:54:40.950050: step 12830, loss = 0.65 (810.6 examples/sec; 0.158 sec/batch)
2016-11-28 18:54:42.496534: step 12840, loss = 0.85 (858.0 examples/sec; 0.149 sec/batch)
2016-11-28 18:54:44.053892: step 12850, loss = 0.71 (801.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:54:45.604452: step 12860, loss = 0.99 (826.1 examples/sec; 0.155 sec/batch)
2016-11-28 18:54:47.156722: step 12870, loss = 0.77 (822.8 examples/sec; 0.156 sec/batch)
2016-11-28 18:54:48.714843: step 12880, loss = 0.80 (815.8 examples/sec; 0.157 sec/batch)
2016-11-28 18:54:50.270961: step 12890, loss = 0.78 (806.4 examples/sec; 0.159 sec/batch)
2016-11-28 18:54:51.823019: step 12900, loss = 0.85 (832.0 examples/sec; 0.154 sec/batch)
2016-11-28 18:54:53.619820: step 12910, loss = 0.94 (794.5 examples/sec; 0.161 sec/batch)
2016-11-28 18:54:55.240883: step 12920, loss = 1.11 (790.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:54:56.845195: step 12930, loss = 0.78 (771.3 examples/sec; 0.166 sec/batch)
2016-11-28 18:54:58.453116: step 12940, loss = 0.78 (811.3 examples/sec; 0.158 sec/batch)
2016-11-28 18:55:00.048528: step 12950, loss = 0.94 (845.8 examples/sec; 0.151 sec/batch)
2016-11-28 18:55:01.678401: step 12960, loss = 0.80 (790.8 examples/sec; 0.162 sec/batch)
2016-11-28 18:55:03.282270: step 12970, loss = 0.72 (796.6 examples/sec; 0.161 sec/batch)
2016-11-28 18:55:04.894706: step 12980, loss = 1.06 (777.3 examples/sec; 0.165 sec/batch)
2016-11-28 18:55:06.495659: step 12990, loss = 0.92 (817.0 examples/sec; 0.157 sec/batch)
2016-11-28 18:55:08.109479: step 13000, loss = 0.95 (782.6 examples/sec; 0.164 sec/batch)
2016-11-28 18:55:10.269564: step 13010, loss = 0.76 (828.7 examples/sec; 0.154 sec/batch)
2016-11-28 18:55:11.842567: step 13020, loss = 0.87 (799.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:55:13.413756: step 13030, loss = 0.80 (840.7 examples/sec; 0.152 sec/batch)
2016-11-28 18:55:14.994092: step 13040, loss = 0.93 (787.9 examples/sec; 0.162 sec/batch)
2016-11-28 18:55:16.583466: step 13050, loss = 0.95 (812.7 examples/sec; 0.158 sec/batch)
2016-11-28 18:55:18.169298: step 13060, loss = 0.82 (774.2 examples/sec; 0.165 sec/batch)
2016-11-28 18:55:19.762370: step 13070, loss = 0.88 (793.1 examples/sec; 0.161 sec/batch)
2016-11-28 18:55:21.343010: step 13080, loss = 0.67 (850.3 examples/sec; 0.151 sec/batch)
2016-11-28 18:55:22.924341: step 13090, loss = 0.76 (823.2 examples/sec; 0.155 sec/batch)
2016-11-28 18:55:24.498079: step 13100, loss = 0.85 (784.7 examples/sec; 0.163 sec/batch)
2016-11-28 18:55:26.279423: step 13110, loss = 0.76 (820.0 examples/sec; 0.156 sec/batch)
2016-11-28 18:55:27.833998: step 13120, loss = 0.86 (810.0 examples/sec; 0.158 sec/batch)
2016-11-28 18:55:29.384925: step 13130, loss = 1.03 (821.3 examples/sec; 0.156 sec/batch)
2016-11-28 18:55:30.927052: step 13140, loss = 0.84 (834.3 examples/sec; 0.153 sec/batch)
2016-11-28 18:55:32.470789: step 13150, loss = 0.86 (823.8 examples/sec; 0.155 sec/batch)
2016-11-28 18:55:34.011848: step 13160, loss = 0.85 (844.7 examples/sec; 0.152 sec/batch)
2016-11-28 18:55:35.568181: step 13170, loss = 0.74 (842.3 examples/sec; 0.152 sec/batch)
2016-11-28 18:55:37.114052: step 13180, loss = 0.73 (847.2 examples/sec; 0.151 sec/batch)
2016-11-28 18:55:38.663422: step 13190, loss = 0.79 (818.5 examples/sec; 0.156 sec/batch)
2016-11-28 18:55:40.211019: step 13200, loss = 0.74 (802.9 examples/sec; 0.159 sec/batch)
2016-11-28 18:55:41.989868: step 13210, loss = 0.91 (796.2 examples/sec; 0.161 sec/batch)
2016-11-28 18:55:43.556027: step 13220, loss = 1.04 (814.9 examples/sec; 0.157 sec/batch)
2016-11-28 18:55:45.103331: step 13230, loss = 1.01 (821.0 examples/sec; 0.156 sec/batch)
2016-11-28 18:55:46.672140: step 13240, loss = 0.86 (802.2 examples/sec; 0.160 sec/batch)
2016-11-28 18:55:48.227263: step 13250, loss = 0.77 (816.6 examples/sec; 0.157 sec/batch)
2016-11-28 18:55:49.776981: step 13260, loss = 0.98 (820.6 examples/sec; 0.156 sec/batch)
2016-11-28 18:55:51.333087: step 13270, loss = 0.96 (798.3 examples/sec; 0.160 sec/batch)
2016-11-28 18:55:52.886792: step 13280, loss = 0.91 (817.0 examples/sec; 0.157 sec/batch)
2016-11-28 18:55:54.445039: step 13290, loss = 0.86 (826.1 examples/sec; 0.155 sec/batch)
2016-11-28 18:55:55.999235: step 13300, loss = 0.71 (798.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:55:57.754792: step 13310, loss = 0.87 (824.8 examples/sec; 0.155 sec/batch)
2016-11-28 18:55:59.319902: step 13320, loss = 0.82 (798.0 examples/sec; 0.160 sec/batch)
2016-11-28 18:56:00.872101: step 13330, loss = 0.72 (813.1 examples/sec; 0.157 sec/batch)
2016-11-28 18:56:02.423840: step 13340, loss = 0.74 (827.5 examples/sec; 0.155 sec/batch)
2016-11-28 18:56:03.976273: step 13350, loss = 0.84 (842.1 examples/sec; 0.152 sec/batch)
2016-11-28 18:56:05.546294: step 13360, loss = 0.84 (828.7 examples/sec; 0.154 sec/batch)
2016-11-28 18:56:07.111357: step 13370, loss = 0.76 (824.8 examples/sec; 0.155 sec/batch)
2016-11-28 18:56:08.660299: step 13380, loss = 0.87 (856.2 examples/sec; 0.149 sec/batch)
2016-11-28 18:56:10.225303: step 13390, loss = 0.91 (784.6 examples/sec; 0.163 sec/batch)
2016-11-28 18:56:11.780125: step 13400, loss = 0.83 (827.4 examples/sec; 0.155 sec/batch)
2016-11-28 18:56:13.542789: step 13410, loss = 0.79 (834.4 examples/sec; 0.153 sec/batch)
2016-11-28 18:56:15.102338: step 13420, loss = 0.88 (846.0 examples/sec; 0.151 sec/batch)
2016-11-28 18:56:16.654993: step 13430, loss = 0.91 (830.2 examples/sec; 0.154 sec/batch)
2016-11-28 18:56:18.212425: step 13440, loss = 0.81 (833.9 examples/sec; 0.153 sec/batch)
2016-11-28 18:56:19.772419: step 13450, loss = 0.85 (841.0 examples/sec; 0.152 sec/batch)
2016-11-28 18:56:21.320690: step 13460, loss = 0.85 (852.0 examples/sec; 0.150 sec/batch)
2016-11-28 18:56:22.880728: step 13470, loss = 0.83 (832.8 examples/sec; 0.154 sec/batch)
2016-11-28 18:56:24.445593: step 13480, loss = 0.92 (799.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:56:25.996928: step 13490, loss = 1.01 (789.5 examples/sec; 0.162 sec/batch)
2016-11-28 18:56:27.555580: step 13500, loss = 0.71 (825.2 examples/sec; 0.155 sec/batch)
2016-11-28 18:56:29.319058: step 13510, loss = 0.95 (806.4 examples/sec; 0.159 sec/batch)
2016-11-28 18:56:30.879330: step 13520, loss = 0.77 (792.2 examples/sec; 0.162 sec/batch)
2016-11-28 18:56:32.480591: step 13530, loss = 0.78 (818.1 examples/sec; 0.156 sec/batch)
2016-11-28 18:56:34.063512: step 13540, loss = 0.84 (815.3 examples/sec; 0.157 sec/batch)
2016-11-28 18:56:35.660955: step 13550, loss = 0.87 (801.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:56:37.255286: step 13560, loss = 0.72 (802.1 examples/sec; 0.160 sec/batch)
2016-11-28 18:56:38.857783: step 13570, loss = 1.13 (804.2 examples/sec; 0.159 sec/batch)
2016-11-28 18:56:40.444139: step 13580, loss = 0.89 (816.3 examples/sec; 0.157 sec/batch)
2016-11-28 18:56:42.044142: step 13590, loss = 0.82 (812.1 examples/sec; 0.158 sec/batch)
2016-11-28 18:56:43.643730: step 13600, loss = 0.83 (784.9 examples/sec; 0.163 sec/batch)
2016-11-28 18:56:45.464997: step 13610, loss = 0.74 (759.9 examples/sec; 0.168 sec/batch)
2016-11-28 18:56:47.062636: step 13620, loss = 0.89 (799.0 examples/sec; 0.160 sec/batch)
2016-11-28 18:56:48.667387: step 13630, loss = 0.71 (810.8 examples/sec; 0.158 sec/batch)
2016-11-28 18:56:50.283543: step 13640, loss = 0.97 (814.8 examples/sec; 0.157 sec/batch)
2016-11-28 18:56:51.887727: step 13650, loss = 0.76 (790.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:56:53.503362: step 13660, loss = 0.84 (768.7 examples/sec; 0.167 sec/batch)
2016-11-28 18:56:55.108405: step 13670, loss = 1.00 (798.1 examples/sec; 0.160 sec/batch)
2016-11-28 18:56:56.715092: step 13680, loss = 0.87 (772.8 examples/sec; 0.166 sec/batch)
2016-11-28 18:56:58.335730: step 13690, loss = 0.85 (806.8 examples/sec; 0.159 sec/batch)
2016-11-28 18:56:59.947184: step 13700, loss = 0.89 (777.3 examples/sec; 0.165 sec/batch)
2016-11-28 18:57:01.747275: step 13710, loss = 0.85 (799.6 examples/sec; 0.160 sec/batch)
2016-11-28 18:57:03.315148: step 13720, loss = 0.88 (804.3 examples/sec; 0.159 sec/batch)
2016-11-28 18:57:04.888124: step 13730, loss = 0.83 (817.6 examples/sec; 0.157 sec/batch)
2016-11-28 18:57:06.474507: step 13740, loss = 0.92 (797.7 examples/sec; 0.160 sec/batch)
2016-11-28 18:57:08.066603: step 13750, loss = 0.94 (787.1 examples/sec; 0.163 sec/batch)
2016-11-28 18:57:09.650189: step 13760, loss = 0.85 (777.3 examples/sec; 0.165 sec/batch)
2016-11-28 18:57:11.234319: step 13770, loss = 0.89 (824.2 examples/sec; 0.155 sec/batch)
2016-11-28 18:57:12.823925: step 13780, loss = 0.96 (788.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:57:14.405707: step 13790, loss = 1.01 (794.5 examples/sec; 0.161 sec/batch)
2016-11-28 18:57:15.981363: step 13800, loss = 0.94 (823.9 examples/sec; 0.155 sec/batch)
2016-11-28 18:57:17.729827: step 13810, loss = 0.98 (825.3 examples/sec; 0.155 sec/batch)
2016-11-28 18:57:19.282973: step 13820, loss = 0.85 (838.0 examples/sec; 0.153 sec/batch)
2016-11-28 18:57:20.850921: step 13830, loss = 0.79 (801.5 examples/sec; 0.160 sec/batch)
2016-11-28 18:57:22.400646: step 13840, loss = 1.02 (800.7 examples/sec; 0.160 sec/batch)
2016-11-28 18:57:23.956514: step 13850, loss = 0.88 (841.7 examples/sec; 0.152 sec/batch)
2016-11-28 18:57:25.500668: step 13860, loss = 0.70 (835.8 examples/sec; 0.153 sec/batch)
2016-11-28 18:57:27.059830: step 13870, loss = 0.92 (795.4 examples/sec; 0.161 sec/batch)
2016-11-28 18:57:28.617496: step 13880, loss = 0.94 (800.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:57:30.177421: step 13890, loss = 0.84 (808.9 examples/sec; 0.158 sec/batch)
2016-11-28 18:57:31.767249: step 13900, loss = 0.92 (824.1 examples/sec; 0.155 sec/batch)
2016-11-28 18:57:33.553388: step 13910, loss = 0.88 (799.5 examples/sec; 0.160 sec/batch)
2016-11-28 18:57:35.138541: step 13920, loss = 0.86 (832.1 examples/sec; 0.154 sec/batch)
2016-11-28 18:57:36.726000: step 13930, loss = 0.72 (801.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:57:38.318894: step 13940, loss = 0.75 (780.3 examples/sec; 0.164 sec/batch)
2016-11-28 18:57:39.896903: step 13950, loss = 0.79 (823.9 examples/sec; 0.155 sec/batch)
2016-11-28 18:57:41.483376: step 13960, loss = 0.89 (793.5 examples/sec; 0.161 sec/batch)
2016-11-28 18:57:43.079371: step 13970, loss = 0.79 (789.9 examples/sec; 0.162 sec/batch)
2016-11-28 18:57:44.663680: step 13980, loss = 0.77 (772.0 examples/sec; 0.166 sec/batch)
2016-11-28 18:57:46.246911: step 13990, loss = 0.88 (779.1 examples/sec; 0.164 sec/batch)
2016-11-28 18:57:47.830369: step 14000, loss = 0.78 (831.6 examples/sec; 0.154 sec/batch)
2016-11-28 18:57:49.993410: step 14010, loss = 0.88 (829.3 examples/sec; 0.154 sec/batch)
2016-11-28 18:57:51.550925: step 14020, loss = 0.92 (821.6 examples/sec; 0.156 sec/batch)
2016-11-28 18:57:53.116261: step 14030, loss = 0.75 (811.7 examples/sec; 0.158 sec/batch)
2016-11-28 18:57:54.677623: step 14040, loss = 0.94 (806.0 examples/sec; 0.159 sec/batch)
2016-11-28 18:57:56.238933: step 14050, loss = 0.83 (853.7 examples/sec; 0.150 sec/batch)
2016-11-28 18:57:57.811107: step 14060, loss = 0.90 (827.6 examples/sec; 0.155 sec/batch)
2016-11-28 18:57:59.382588: step 14070, loss = 0.92 (812.2 examples/sec; 0.158 sec/batch)
2016-11-28 18:58:00.947807: step 14080, loss = 0.81 (822.4 examples/sec; 0.156 sec/batch)
2016-11-28 18:58:02.492563: step 14090, loss = 0.99 (823.6 examples/sec; 0.155 sec/batch)
2016-11-28 18:58:04.046260: step 14100, loss = 0.99 (814.5 examples/sec; 0.157 sec/batch)
2016-11-28 18:58:05.798193: step 14110, loss = 0.81 (814.6 examples/sec; 0.157 sec/batch)
2016-11-28 18:58:07.355404: step 14120, loss = 0.93 (823.0 examples/sec; 0.156 sec/batch)
2016-11-28 18:58:08.909251: step 14130, loss = 0.92 (796.5 examples/sec; 0.161 sec/batch)
2016-11-28 18:58:10.456070: step 14140, loss = 0.75 (840.4 examples/sec; 0.152 sec/batch)
2016-11-28 18:58:12.008429: step 14150, loss = 0.83 (837.3 examples/sec; 0.153 sec/batch)
2016-11-28 18:58:13.567100: step 14160, loss = 0.85 (840.4 examples/sec; 0.152 sec/batch)
2016-11-28 18:58:15.136587: step 14170, loss = 0.77 (800.0 examples/sec; 0.160 sec/batch)
2016-11-28 18:58:16.697017: step 14180, loss = 0.89 (834.4 examples/sec; 0.153 sec/batch)
2016-11-28 18:58:18.260110: step 14190, loss = 0.96 (872.0 examples/sec; 0.147 sec/batch)
2016-11-28 18:58:19.821630: step 14200, loss = 0.92 (805.4 examples/sec; 0.159 sec/batch)
2016-11-28 18:58:21.583932: step 14210, loss = 0.67 (808.6 examples/sec; 0.158 sec/batch)
2016-11-28 18:58:23.145385: step 14220, loss = 0.96 (826.4 examples/sec; 0.155 sec/batch)
2016-11-28 18:58:24.708363: step 14230, loss = 0.94 (812.4 examples/sec; 0.158 sec/batch)
2016-11-28 18:58:26.255970: step 14240, loss = 0.88 (835.8 examples/sec; 0.153 sec/batch)
2016-11-28 18:58:27.824073: step 14250, loss = 0.73 (800.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:58:29.378752: step 14260, loss = 0.89 (826.5 examples/sec; 0.155 sec/batch)
2016-11-28 18:58:30.949388: step 14270, loss = 0.82 (847.4 examples/sec; 0.151 sec/batch)
2016-11-28 18:58:32.507193: step 14280, loss = 0.81 (832.7 examples/sec; 0.154 sec/batch)
2016-11-28 18:58:34.064067: step 14290, loss = 0.78 (833.2 examples/sec; 0.154 sec/batch)
2016-11-28 18:58:35.620528: step 14300, loss = 1.09 (827.5 examples/sec; 0.155 sec/batch)
2016-11-28 18:58:37.390031: step 14310, loss = 1.10 (797.7 examples/sec; 0.160 sec/batch)
2016-11-28 18:58:38.984599: step 14320, loss = 0.94 (799.0 examples/sec; 0.160 sec/batch)
2016-11-28 18:58:40.580628: step 14330, loss = 1.02 (818.3 examples/sec; 0.156 sec/batch)
2016-11-28 18:58:42.176440: step 14340, loss = 0.85 (820.6 examples/sec; 0.156 sec/batch)
2016-11-28 18:58:43.772029: step 14350, loss = 0.88 (801.3 examples/sec; 0.160 sec/batch)
2016-11-28 18:58:45.373738: step 14360, loss = 0.85 (787.4 examples/sec; 0.163 sec/batch)
2016-11-28 18:58:46.975198: step 14370, loss = 0.89 (813.9 examples/sec; 0.157 sec/batch)
2016-11-28 18:58:48.582753: step 14380, loss = 0.78 (774.5 examples/sec; 0.165 sec/batch)
2016-11-28 18:58:50.191419: step 14390, loss = 0.82 (822.8 examples/sec; 0.156 sec/batch)
2016-11-28 18:58:51.805595: step 14400, loss = 0.89 (781.0 examples/sec; 0.164 sec/batch)
2016-11-28 18:58:53.635841: step 14410, loss = 0.72 (788.4 examples/sec; 0.162 sec/batch)
2016-11-28 18:58:55.242995: step 14420, loss = 0.84 (818.0 examples/sec; 0.156 sec/batch)
2016-11-28 18:58:56.856675: step 14430, loss = 0.71 (792.8 examples/sec; 0.161 sec/batch)
2016-11-28 18:58:58.450720: step 14440, loss = 0.87 (808.7 examples/sec; 0.158 sec/batch)
2016-11-28 18:59:00.065983: step 14450, loss = 0.86 (790.8 examples/sec; 0.162 sec/batch)
2016-11-28 18:59:01.680697: step 14460, loss = 0.80 (827.9 examples/sec; 0.155 sec/batch)
2016-11-28 18:59:03.282259: step 14470, loss = 0.74 (802.5 examples/sec; 0.159 sec/batch)
2016-11-28 18:59:04.901270: step 14480, loss = 0.89 (796.9 examples/sec; 0.161 sec/batch)
2016-11-28 18:59:06.515337: step 14490, loss = 0.86 (797.4 examples/sec; 0.161 sec/batch)
2016-11-28 18:59:08.119121: step 14500, loss = 0.84 (774.9 examples/sec; 0.165 sec/batch)
2016-11-28 18:59:09.916570: step 14510, loss = 0.91 (800.4 examples/sec; 0.160 sec/batch)
2016-11-28 18:59:11.497007: step 14520, loss = 0.99 (793.6 examples/sec; 0.161 sec/batch)
2016-11-28 18:59:13.082617: step 14530, loss = 0.89 (829.4 examples/sec; 0.154 sec/batch)
2016-11-28 18:59:14.667863: step 14540, loss = 1.02 (813.8 examples/sec; 0.157 sec/batch)
2016-11-28 18:59:16.256041: step 14550, loss = 0.86 (774.4 examples/sec; 0.165 sec/batch)
2016-11-28 18:59:17.841030: step 14560, loss = 0.86 (799.2 examples/sec; 0.160 sec/batch)
2016-11-28 18:59:19.449693: step 14570, loss = 0.91 (795.3 examples/sec; 0.161 sec/batch)
2016-11-28 18:59:21.032965: step 14580, loss = 0.87 (831.5 examples/sec; 0.154 sec/batch)
2016-11-28 18:59:22.622098: step 14590, loss = 0.89 (808.4 examples/sec; 0.158 sec/batch)
2016-11-28 18:59:24.195456: step 14600, loss = 0.90 (823.1 examples/sec; 0.156 sec/batch)
2016-11-28 18:59:25.964960: step 14610, loss = 0.94 (812.8 examples/sec; 0.157 sec/batch)
2016-11-28 18:59:27.519844: step 14620, loss = 0.74 (801.8 examples/sec; 0.160 sec/batch)
2016-11-28 18:59:29.076046: step 14630, loss = 0.94 (799.9 examples/sec; 0.160 sec/batch)
2016-11-28 18:59:30.629691: step 14640, loss = 1.06 (843.5 examples/sec; 0.152 sec/batch)
2016-11-28 18:59:32.183859: step 14650, loss = 0.86 (807.0 examples/sec; 0.159 sec/batch)
2016-11-28 18:59:33.727679: step 14660, loss = 0.89 (834.1 examples/sec; 0.153 sec/batch)
2016-11-28 18:59:35.278083: step 14670, loss = 0.82 (819.3 examples/sec; 0.156 sec/batch)
2016-11-28 18:59:36.832679: step 14680, loss = 0.87 (791.9 examples/sec; 0.162 sec/batch)
2016-11-28 18:59:38.390715: step 14690, loss = 0.99 (779.3 examples/sec; 0.164 sec/batch)
2016-11-28 18:59:39.937844: step 14700, loss = 0.90 (800.7 examples/sec; 0.160 sec/batch)
2016-11-28 18:59:41.702958: step 14710, loss = 0.95 (807.3 examples/sec; 0.159 sec/batch)
2016-11-28 18:59:43.278768: step 14720, loss = 0.81 (797.2 examples/sec; 0.161 sec/batch)
2016-11-28 18:59:44.868291: step 14730, loss = 0.85 (810.7 examples/sec; 0.158 sec/batch)
2016-11-28 18:59:46.470098: step 14740, loss = 0.83 (809.1 examples/sec; 0.158 sec/batch)
2016-11-28 18:59:48.074142: step 14750, loss = 0.87 (802.7 examples/sec; 0.159 sec/batch)
2016-11-28 18:59:49.670901: step 14760, loss = 0.77 (807.2 examples/sec; 0.159 sec/batch)
2016-11-28 18:59:51.268831: step 14770, loss = 0.77 (784.1 examples/sec; 0.163 sec/batch)
2016-11-28 18:59:52.868417: step 14780, loss = 0.97 (798.2 examples/sec; 0.160 sec/batch)
2016-11-28 18:59:54.468228: step 14790, loss = 0.91 (826.4 examples/sec; 0.155 sec/batch)
2016-11-28 18:59:56.090531: step 14800, loss = 0.80 (779.6 examples/sec; 0.164 sec/batch)
2016-11-28 18:59:57.894076: step 14810, loss = 0.73 (766.5 examples/sec; 0.167 sec/batch)
2016-11-28 18:59:59.481677: step 14820, loss = 0.66 (844.0 examples/sec; 0.152 sec/batch)
2016-11-28 19:00:01.080645: step 14830, loss = 0.79 (801.2 examples/sec; 0.160 sec/batch)
2016-11-28 19:00:02.671415: step 14840, loss = 0.83 (785.8 examples/sec; 0.163 sec/batch)
2016-11-28 19:00:04.267496: step 14850, loss = 0.85 (804.9 examples/sec; 0.159 sec/batch)
2016-11-28 19:00:05.852174: step 14860, loss = 1.23 (784.9 examples/sec; 0.163 sec/batch)
2016-11-28 19:00:07.446413: step 14870, loss = 0.76 (836.0 examples/sec; 0.153 sec/batch)
2016-11-28 19:00:09.042346: step 14880, loss = 0.78 (808.7 examples/sec; 0.158 sec/batch)
2016-11-28 19:00:10.637989: step 14890, loss = 0.75 (811.5 examples/sec; 0.158 sec/batch)
2016-11-28 19:00:12.229342: step 14900, loss = 0.67 (813.8 examples/sec; 0.157 sec/batch)
2016-11-28 19:00:14.033640: step 14910, loss = 0.83 (784.3 examples/sec; 0.163 sec/batch)
2016-11-28 19:00:15.625970: step 14920, loss = 0.76 (790.1 examples/sec; 0.162 sec/batch)
2016-11-28 19:00:17.231349: step 14930, loss = 0.92 (771.0 examples/sec; 0.166 sec/batch)
2016-11-28 19:00:18.836748: step 14940, loss = 0.78 (779.3 examples/sec; 0.164 sec/batch)
2016-11-28 19:00:20.438800: step 14950, loss = 0.73 (814.9 examples/sec; 0.157 sec/batch)
2016-11-28 19:00:22.056405: step 14960, loss = 0.71 (802.7 examples/sec; 0.159 sec/batch)
2016-11-28 19:00:23.667106: step 14970, loss = 0.94 (813.1 examples/sec; 0.157 sec/batch)
2016-11-28 19:00:25.265330: step 14980, loss = 0.61 (795.5 examples/sec; 0.161 sec/batch)
2016-11-28 19:00:26.878265: step 14990, loss = 0.85 (786.6 examples/sec; 0.163 sec/batch)
2016-11-28 19:00:28.486415: step 15000, loss = 0.72 (808.2 examples/sec; 0.158 sec/batch)
2016-11-28 19:00:30.637515: step 15010, loss = 0.73 (806.1 examples/sec; 0.159 sec/batch)
2016-11-28 19:00:32.192779: step 15020, loss = 0.74 (812.6 examples/sec; 0.158 sec/batch)
2016-11-28 19:00:33.741676: step 15030, loss = 0.82 (847.8 examples/sec; 0.151 sec/batch)
2016-11-28 19:00:35.297433: step 15040, loss = 0.96 (862.0 examples/sec; 0.148 sec/batch)
2016-11-28 19:00:36.859256: step 15050, loss = 0.86 (810.1 examples/sec; 0.158 sec/batch)
2016-11-28 19:00:38.407327: step 15060, loss = 0.85 (869.9 examples/sec; 0.147 sec/batch)
2016-11-28 19:00:39.959402: step 15070, loss = 0.86 (830.6 examples/sec; 0.154 sec/batch)
2016-11-28 19:00:41.535624: step 15080, loss = 0.78 (814.4 examples/sec; 0.157 sec/batch)
2016-11-28 19:00:43.099623: step 15090, loss = 0.93 (824.4 examples/sec; 0.155 sec/batch)
2016-11-28 19:00:44.655085: step 15100, loss = 0.83 (829.8 examples/sec; 0.154 sec/batch)
2016-11-28 19:00:46.390299: step 15110, loss = 0.91 (841.4 examples/sec; 0.152 sec/batch)
2016-11-28 19:00:47.954039: step 15120, loss = 0.71 (821.9 examples/sec; 0.156 sec/batch)
2016-11-28 19:00:49.518300: step 15130, loss = 0.72 (799.7 examples/sec; 0.160 sec/batch)
2016-11-28 19:00:51.069129: step 15140, loss = 0.87 (849.8 examples/sec; 0.151 sec/batch)
2016-11-28 19:00:52.629518: step 15150, loss = 0.87 (802.2 examples/sec; 0.160 sec/batch)
2016-11-28 19:00:54.194887: step 15160, loss = 0.81 (814.8 examples/sec; 0.157 sec/batch)
2016-11-28 19:00:55.748345: step 15170, loss = 0.95 (803.4 examples/sec; 0.159 sec/batch)
2016-11-28 19:00:57.308220: step 15180, loss = 0.73 (799.6 examples/sec; 0.160 sec/batch)
2016-11-28 19:00:58.862640: step 15190, loss = 0.72 (854.3 examples/sec; 0.150 sec/batch)
2016-11-28 19:01:00.422094: step 15200, loss = 0.71 (825.3 examples/sec; 0.155 sec/batch)
2016-11-28 19:01:02.164872: step 15210, loss = 0.91 (820.8 examples/sec; 0.156 sec/batch)
2016-11-28 19:01:03.721273: step 15220, loss = 0.97 (803.3 examples/sec; 0.159 sec/batch)
2016-11-28 19:01:05.331266: step 15230, loss = 0.89 (821.4 examples/sec; 0.156 sec/batch)
2016-11-28 19:01:06.922778: step 15240, loss = 0.66 (789.5 examples/sec; 0.162 sec/batch)
2016-11-28 19:01:08.512429: step 15250, loss = 0.89 (813.4 examples/sec; 0.157 sec/batch)
2016-11-28 19:01:10.101388: step 15260, loss = 0.88 (789.5 examples/sec; 0.162 sec/batch)
2016-11-28 19:01:11.690024: step 15270, loss = 0.83 (787.6 examples/sec; 0.163 sec/batch)
2016-11-28 19:01:13.292614: step 15280, loss = 0.83 (796.8 examples/sec; 0.161 sec/batch)
2016-11-28 19:01:14.896462: step 15290, loss = 0.80 (778.5 examples/sec; 0.164 sec/batch)
2016-11-28 19:01:16.499304: step 15300, loss = 1.02 (782.2 examples/sec; 0.164 sec/batch)
2016-11-28 19:01:18.300394: step 15310, loss = 0.68 (802.8 examples/sec; 0.159 sec/batch)
2016-11-28 19:01:19.886379: step 15320, loss = 0.84 (806.4 examples/sec; 0.159 sec/batch)
2016-11-28 19:01:21.479689: step 15330, loss = 0.84 (783.4 examples/sec; 0.163 sec/batch)
2016-11-28 19:01:23.090479: step 15340, loss = 0.70 (806.2 examples/sec; 0.159 sec/batch)
2016-11-28 19:01:24.678134: step 15350, loss = 0.60 (816.6 examples/sec; 0.157 sec/batch)
2016-11-28 19:01:26.271251: step 15360, loss = 0.69 (820.5 examples/sec; 0.156 sec/batch)
2016-11-28 19:01:27.853245: step 15370, loss = 0.85 (814.6 examples/sec; 0.157 sec/batch)
2016-11-28 19:01:29.442915: step 15380, loss = 0.81 (804.4 examples/sec; 0.159 sec/batch)
2016-11-28 19:01:31.032698: step 15390, loss = 0.79 (818.1 examples/sec; 0.156 sec/batch)
2016-11-28 19:01:32.633061: step 15400, loss = 0.90 (821.2 examples/sec; 0.156 sec/batch)
2016-11-28 19:01:34.422911: step 15410, loss = 0.86 (791.9 examples/sec; 0.162 sec/batch)
2016-11-28 19:01:35.998397: step 15420, loss = 0.87 (832.6 examples/sec; 0.154 sec/batch)
2016-11-28 19:01:37.581425: step 15430, loss = 0.86 (812.2 examples/sec; 0.158 sec/batch)
2016-11-28 19:01:39.182129: step 15440, loss = 0.85 (773.6 examples/sec; 0.165 sec/batch)
2016-11-28 19:01:40.773105: step 15450, loss = 0.82 (791.1 examples/sec; 0.162 sec/batch)
2016-11-28 19:01:42.367796: step 15460, loss = 0.86 (775.3 examples/sec; 0.165 sec/batch)
2016-11-28 19:01:43.949615: step 15470, loss = 0.92 (795.5 examples/sec; 0.161 sec/batch)
2016-11-28 19:01:45.536327: step 15480, loss = 0.80 (786.0 examples/sec; 0.163 sec/batch)
2016-11-28 19:01:47.115659: step 15490, loss = 0.78 (812.3 examples/sec; 0.158 sec/batch)
2016-11-28 19:01:48.697705: step 15500, loss = 0.85 (807.7 examples/sec; 0.158 sec/batch)
2016-11-28 19:01:50.457573: step 15510, loss = 0.82 (810.0 examples/sec; 0.158 sec/batch)
2016-11-28 19:01:52.054760: step 15520, loss = 0.85 (806.3 examples/sec; 0.159 sec/batch)
2016-11-28 19:01:53.639169: step 15530, loss = 0.96 (791.5 examples/sec; 0.162 sec/batch)
2016-11-28 19:01:55.228831: step 15540, loss = 1.11 (807.3 examples/sec; 0.159 sec/batch)
2016-11-28 19:01:56.823234: step 15550, loss = 0.76 (795.7 examples/sec; 0.161 sec/batch)
2016-11-28 19:01:58.412000: step 15560, loss = 0.80 (768.5 examples/sec; 0.167 sec/batch)
2016-11-28 19:02:00.008473: step 15570, loss = 1.01 (814.4 examples/sec; 0.157 sec/batch)
2016-11-28 19:02:01.619878: step 15580, loss = 0.81 (815.7 examples/sec; 0.157 sec/batch)
2016-11-28 19:02:03.243216: step 15590, loss = 0.86 (794.4 examples/sec; 0.161 sec/batch)
2016-11-28 19:02:04.847214: step 15600, loss = 0.78 (822.6 examples/sec; 0.156 sec/batch)
2016-11-28 19:02:06.675922: step 15610, loss = 0.87 (804.5 examples/sec; 0.159 sec/batch)
2016-11-28 19:02:08.292885: step 15620, loss = 0.78 (815.4 examples/sec; 0.157 sec/batch)
2016-11-28 19:02:09.900645: step 15630, loss = 0.75 (789.7 examples/sec; 0.162 sec/batch)
2016-11-28 19:02:11.509646: step 15640, loss = 0.85 (791.7 examples/sec; 0.162 sec/batch)
2016-11-28 19:02:13.123132: step 15650, loss = 0.86 (790.0 examples/sec; 0.162 sec/batch)
2016-11-28 19:02:14.729630: step 15660, loss = 1.00 (785.9 examples/sec; 0.163 sec/batch)
2016-11-28 19:02:16.341619: step 15670, loss = 0.71 (779.4 examples/sec; 0.164 sec/batch)
2016-11-28 19:02:17.950092: step 15680, loss = 0.79 (780.5 examples/sec; 0.164 sec/batch)
2016-11-28 19:02:19.562658: step 15690, loss = 0.95 (808.0 examples/sec; 0.158 sec/batch)
2016-11-28 19:02:21.172325: step 15700, loss = 0.88 (786.9 examples/sec; 0.163 sec/batch)
2016-11-28 19:02:22.994611: step 15710, loss = 0.95 (784.9 examples/sec; 0.163 sec/batch)
2016-11-28 19:02:24.607990: step 15720, loss = 0.69 (781.7 examples/sec; 0.164 sec/batch)
2016-11-28 19:02:26.219968: step 15730, loss = 0.92 (796.7 examples/sec; 0.161 sec/batch)
2016-11-28 19:02:27.834305: step 15740, loss = 0.73 (796.6 examples/sec; 0.161 sec/batch)
2016-11-28 19:02:29.438957: step 15750, loss = 0.75 (777.1 examples/sec; 0.165 sec/batch)
2016-11-28 19:02:31.054662: step 15760, loss = 0.89 (802.4 examples/sec; 0.160 sec/batch)
2016-11-28 19:02:32.674468: step 15770, loss = 1.05 (737.2 examples/sec; 0.174 sec/batch)
2016-11-28 19:02:34.275566: step 15780, loss = 0.97 (801.9 examples/sec; 0.160 sec/batch)
2016-11-28 19:02:35.886898: step 15790, loss = 0.79 (802.8 examples/sec; 0.159 sec/batch)
2016-11-28 19:02:37.496514: step 15800, loss = 0.90 (794.3 examples/sec; 0.161 sec/batch)
2016-11-28 19:02:39.295471: step 15810, loss = 0.77 (827.4 examples/sec; 0.155 sec/batch)
2016-11-28 19:02:40.911398: step 15820, loss = 0.97 (802.3 examples/sec; 0.160 sec/batch)
2016-11-28 19:02:42.525233: step 15830, loss = 1.12 (835.1 examples/sec; 0.153 sec/batch)
2016-11-28 19:02:44.125292: step 15840, loss = 0.89 (803.9 examples/sec; 0.159 sec/batch)
2016-11-28 19:02:45.729483: step 15850, loss = 0.84 (796.0 examples/sec; 0.161 sec/batch)
2016-11-28 19:02:47.332507: step 15860, loss = 0.90 (802.7 examples/sec; 0.159 sec/batch)
2016-11-28 19:02:48.953128: step 15870, loss = 0.79 (811.8 examples/sec; 0.158 sec/batch)
2016-11-28 19:02:50.568811: step 15880, loss = 0.63 (791.6 examples/sec; 0.162 sec/batch)
2016-11-28 19:02:52.177769: step 15890, loss = 0.88 (802.5 examples/sec; 0.159 sec/batch)
2016-11-28 19:02:53.788066: step 15900, loss = 0.96 (782.2 examples/sec; 0.164 sec/batch)
2016-11-28 19:02:55.600451: step 15910, loss = 0.75 (800.4 examples/sec; 0.160 sec/batch)
2016-11-28 19:02:57.201173: step 15920, loss = 0.84 (807.3 examples/sec; 0.159 sec/batch)
2016-11-28 19:02:58.809535: step 15930, loss = 0.87 (824.4 examples/sec; 0.155 sec/batch)
2016-11-28 19:03:00.419372: step 15940, loss = 0.77 (786.3 examples/sec; 0.163 sec/batch)
2016-11-28 19:03:02.031905: step 15950, loss = 1.00 (829.8 examples/sec; 0.154 sec/batch)
2016-11-28 19:03:03.647113: step 15960, loss = 0.95 (809.7 examples/sec; 0.158 sec/batch)
2016-11-28 19:03:05.268069: step 15970, loss = 0.96 (779.1 examples/sec; 0.164 sec/batch)
2016-11-28 19:03:06.889377: step 15980, loss = 0.89 (772.9 examples/sec; 0.166 sec/batch)
2016-11-28 19:03:08.497197: step 15990, loss = 0.84 (786.8 examples/sec; 0.163 sec/batch)
2016-11-28 19:03:10.108863: step 16000, loss = 0.84 (827.5 examples/sec; 0.155 sec/batch)
2016-11-28 19:03:12.276276: step 16010, loss = 0.85 (840.0 examples/sec; 0.152 sec/batch)
2016-11-28 19:03:13.833420: step 16020, loss = 0.95 (873.5 examples/sec; 0.147 sec/batch)
2016-11-28 19:03:15.395874: step 16030, loss = 0.69 (834.0 examples/sec; 0.153 sec/batch)
2016-11-28 19:03:16.952862: step 16040, loss = 0.93 (872.6 examples/sec; 0.147 sec/batch)
2016-11-28 19:03:18.513738: step 16050, loss = 0.85 (788.8 examples/sec; 0.162 sec/batch)
2016-11-28 19:03:20.074923: step 16060, loss = 0.82 (821.0 examples/sec; 0.156 sec/batch)
2016-11-28 19:03:21.625623: step 16070, loss = 0.72 (835.1 examples/sec; 0.153 sec/batch)
2016-11-28 19:03:23.198600: step 16080, loss = 0.79 (799.5 examples/sec; 0.160 sec/batch)
2016-11-28 19:03:24.812602: step 16090, loss = 0.88 (756.2 examples/sec; 0.169 sec/batch)
2016-11-28 19:03:26.398780: step 16100, loss = 0.96 (778.4 examples/sec; 0.164 sec/batch)
2016-11-28 19:03:28.151349: step 16110, loss = 0.66 (842.9 examples/sec; 0.152 sec/batch)
2016-11-28 19:03:29.690422: step 16120, loss = 0.91 (831.2 examples/sec; 0.154 sec/batch)
2016-11-28 19:03:31.243352: step 16130, loss = 0.88 (814.3 examples/sec; 0.157 sec/batch)
2016-11-28 19:03:32.803534: step 16140, loss = 0.82 (840.1 examples/sec; 0.152 sec/batch)
2016-11-28 19:03:34.364442: step 16150, loss = 0.82 (822.8 examples/sec; 0.156 sec/batch)
2016-11-28 19:03:35.921129: step 16160, loss = 0.76 (833.8 examples/sec; 0.154 sec/batch)
2016-11-28 19:03:37.477486: step 16170, loss = 1.01 (834.2 examples/sec; 0.153 sec/batch)
2016-11-28 19:03:39.033323: step 16180, loss = 0.85 (846.8 examples/sec; 0.151 sec/batch)
2016-11-28 19:03:40.601058: step 16190, loss = 0.83 (811.0 examples/sec; 0.158 sec/batch)
2016-11-28 19:03:42.157157: step 16200, loss = 0.80 (803.2 examples/sec; 0.159 sec/batch)
2016-11-28 19:03:43.914993: step 16210, loss = 0.77 (837.9 examples/sec; 0.153 sec/batch)
2016-11-28 19:03:45.475941: step 16220, loss = 0.89 (791.1 examples/sec; 0.162 sec/batch)
2016-11-28 19:03:47.031386: step 16230, loss = 0.85 (806.9 examples/sec; 0.159 sec/batch)
2016-11-28 19:03:48.592018: step 16240, loss = 0.80 (817.5 examples/sec; 0.157 sec/batch)
2016-11-28 19:03:50.145454: step 16250, loss = 0.89 (824.3 examples/sec; 0.155 sec/batch)
2016-11-28 19:03:51.703583: step 16260, loss = 0.74 (813.3 examples/sec; 0.157 sec/batch)
2016-11-28 19:03:53.250165: step 16270, loss = 0.95 (840.0 examples/sec; 0.152 sec/batch)
2016-11-28 19:03:54.806541: step 16280, loss = 0.90 (813.2 examples/sec; 0.157 sec/batch)
2016-11-28 19:03:56.365767: step 16290, loss = 0.80 (801.5 examples/sec; 0.160 sec/batch)
2016-11-28 19:03:57.929880: step 16300, loss = 0.97 (846.0 examples/sec; 0.151 sec/batch)
2016-11-28 19:03:59.688872: step 16310, loss = 0.86 (811.0 examples/sec; 0.158 sec/batch)
2016-11-28 19:04:01.251816: step 16320, loss = 0.88 (820.8 examples/sec; 0.156 sec/batch)
2016-11-28 19:04:02.815892: step 16330, loss = 0.91 (811.0 examples/sec; 0.158 sec/batch)
2016-11-28 19:04:04.370214: step 16340, loss = 0.70 (844.8 examples/sec; 0.152 sec/batch)
2016-11-28 19:04:05.925371: step 16350, loss = 0.71 (829.6 examples/sec; 0.154 sec/batch)
2016-11-28 19:04:07.488285: step 16360, loss = 0.83 (820.7 examples/sec; 0.156 sec/batch)
2016-11-28 19:04:09.049987: step 16370, loss = 0.74 (788.5 examples/sec; 0.162 sec/batch)
2016-11-28 19:04:10.639102: step 16380, loss = 0.86 (836.2 examples/sec; 0.153 sec/batch)
2016-11-28 19:04:12.223250: step 16390, loss = 0.75 (852.3 examples/sec; 0.150 sec/batch)
2016-11-28 19:04:13.825900: step 16400, loss = 0.92 (797.0 examples/sec; 0.161 sec/batch)
2016-11-28 19:04:15.631481: step 16410, loss = 0.76 (803.2 examples/sec; 0.159 sec/batch)
2016-11-28 19:04:17.232528: step 16420, loss = 0.89 (791.7 examples/sec; 0.162 sec/batch)
2016-11-28 19:04:18.814316: step 16430, loss = 0.81 (809.4 examples/sec; 0.158 sec/batch)
2016-11-28 19:04:20.411696: step 16440, loss = 0.90 (801.4 examples/sec; 0.160 sec/batch)
2016-11-28 19:04:22.016147: step 16450, loss = 0.87 (795.9 examples/sec; 0.161 sec/batch)
2016-11-28 19:04:23.618049: step 16460, loss = 0.83 (782.4 examples/sec; 0.164 sec/batch)
2016-11-28 19:04:25.206811: step 16470, loss = 0.81 (794.5 examples/sec; 0.161 sec/batch)
2016-11-28 19:04:26.807485: step 16480, loss = 0.88 (790.7 examples/sec; 0.162 sec/batch)
2016-11-28 19:04:28.414428: step 16490, loss = 0.92 (801.0 examples/sec; 0.160 sec/batch)
2016-11-28 19:04:30.018314: step 16500, loss = 0.88 (781.8 examples/sec; 0.164 sec/batch)
2016-11-28 19:04:31.813658: step 16510, loss = 0.89 (761.6 examples/sec; 0.168 sec/batch)
2016-11-28 19:04:33.415016: step 16520, loss = 0.89 (789.9 examples/sec; 0.162 sec/batch)
2016-11-28 19:04:35.023581: step 16530, loss = 0.84 (810.2 examples/sec; 0.158 sec/batch)
2016-11-28 19:04:36.636467: step 16540, loss = 0.75 (771.5 examples/sec; 0.166 sec/batch)
2016-11-28 19:04:38.238718: step 16550, loss = 0.75 (800.8 examples/sec; 0.160 sec/batch)
2016-11-28 19:04:39.836462: step 16560, loss = 0.75 (784.9 examples/sec; 0.163 sec/batch)
2016-11-28 19:04:41.448712: step 16570, loss = 0.92 (757.9 examples/sec; 0.169 sec/batch)
2016-11-28 19:04:43.054884: step 16580, loss = 0.91 (758.8 examples/sec; 0.169 sec/batch)
2016-11-28 19:04:44.663073: step 16590, loss = 0.86 (793.9 examples/sec; 0.161 sec/batch)
2016-11-28 19:04:46.267838: step 16600, loss = 0.96 (826.5 examples/sec; 0.155 sec/batch)
2016-11-28 19:04:48.062432: step 16610, loss = 0.84 (804.4 examples/sec; 0.159 sec/batch)
2016-11-28 19:04:49.648784: step 16620, loss = 0.72 (821.7 examples/sec; 0.156 sec/batch)
2016-11-28 19:04:51.217145: step 16630, loss = 0.66 (816.4 examples/sec; 0.157 sec/batch)
2016-11-28 19:04:52.807355: step 16640, loss = 0.75 (798.2 examples/sec; 0.160 sec/batch)
2016-11-28 19:04:54.377583: step 16650, loss = 0.78 (828.9 examples/sec; 0.154 sec/batch)
2016-11-28 19:04:55.961595: step 16660, loss = 0.91 (796.9 examples/sec; 0.161 sec/batch)
2016-11-28 19:04:57.535371: step 16670, loss = 0.87 (819.9 examples/sec; 0.156 sec/batch)
2016-11-28 19:04:59.113591: step 16680, loss = 0.80 (802.4 examples/sec; 0.160 sec/batch)
2016-11-28 19:05:00.693799: step 16690, loss = 0.88 (818.7 examples/sec; 0.156 sec/batch)
2016-11-28 19:05:02.269447: step 16700, loss = 0.70 (835.7 examples/sec; 0.153 sec/batch)
2016-11-28 19:05:04.038690: step 16710, loss = 0.77 (824.6 examples/sec; 0.155 sec/batch)
2016-11-28 19:05:05.625005: step 16720, loss = 0.70 (834.2 examples/sec; 0.153 sec/batch)
2016-11-28 19:05:07.203162: step 16730, loss = 0.81 (811.7 examples/sec; 0.158 sec/batch)
2016-11-28 19:05:08.771350: step 16740, loss = 0.88 (780.8 examples/sec; 0.164 sec/batch)
2016-11-28 19:05:10.346869: step 16750, loss = 0.83 (815.8 examples/sec; 0.157 sec/batch)
2016-11-28 19:05:11.935195: step 16760, loss = 0.70 (812.9 examples/sec; 0.157 sec/batch)
2016-11-28 19:05:13.510542: step 16770, loss = 0.75 (839.8 examples/sec; 0.152 sec/batch)
2016-11-28 19:05:15.096724: step 16780, loss = 0.79 (817.3 examples/sec; 0.157 sec/batch)
2016-11-28 19:05:16.677488: step 16790, loss = 0.84 (814.3 examples/sec; 0.157 sec/batch)
2016-11-28 19:05:18.257352: step 16800, loss = 0.65 (821.3 examples/sec; 0.156 sec/batch)
2016-11-28 19:05:20.038979: step 16810, loss = 0.89 (806.9 examples/sec; 0.159 sec/batch)
2016-11-28 19:05:21.610994: step 16820, loss = 0.86 (835.4 examples/sec; 0.153 sec/batch)
2016-11-28 19:05:23.186815: step 16830, loss = 0.76 (841.1 examples/sec; 0.152 sec/batch)
2016-11-28 19:05:24.762609: step 16840, loss = 0.81 (819.3 examples/sec; 0.156 sec/batch)
2016-11-28 19:05:26.337309: step 16850, loss = 0.72 (826.2 examples/sec; 0.155 sec/batch)
2016-11-28 19:05:27.917548: step 16860, loss = 0.83 (802.0 examples/sec; 0.160 sec/batch)
2016-11-28 19:05:29.496715: step 16870, loss = 0.82 (817.1 examples/sec; 0.157 sec/batch)
2016-11-28 19:05:31.072106: step 16880, loss = 0.93 (790.2 examples/sec; 0.162 sec/batch)
2016-11-28 19:05:32.645767: step 16890, loss = 0.76 (814.5 examples/sec; 0.157 sec/batch)
2016-11-28 19:05:34.230709: step 16900, loss = 0.83 (818.5 examples/sec; 0.156 sec/batch)
2016-11-28 19:05:36.016488: step 16910, loss = 0.89 (834.5 examples/sec; 0.153 sec/batch)
2016-11-28 19:05:37.574616: step 16920, loss = 0.81 (815.9 examples/sec; 0.157 sec/batch)
2016-11-28 19:05:39.138530: step 16930, loss = 0.73 (827.3 examples/sec; 0.155 sec/batch)
2016-11-28 19:05:40.737178: step 16940, loss = 0.92 (842.8 examples/sec; 0.152 sec/batch)
2016-11-28 19:05:42.343643: step 16950, loss = 0.84 (800.8 examples/sec; 0.160 sec/batch)
2016-11-28 19:05:43.945012: step 16960, loss = 0.88 (829.8 examples/sec; 0.154 sec/batch)
2016-11-28 19:05:45.548996: step 16970, loss = 0.84 (798.3 examples/sec; 0.160 sec/batch)
2016-11-28 19:05:47.158764: step 16980, loss = 1.03 (783.2 examples/sec; 0.163 sec/batch)
2016-11-28 19:05:48.764915: step 16990, loss = 0.76 (795.7 examples/sec; 0.161 sec/batch)
2016-11-28 19:05:50.380923: step 17000, loss = 0.78 (784.3 examples/sec; 0.163 sec/batch)
2016-11-28 19:05:52.562620: step 17010, loss = 0.79 (807.5 examples/sec; 0.159 sec/batch)
2016-11-28 19:05:54.125350: step 17020, loss = 0.94 (826.7 examples/sec; 0.155 sec/batch)
2016-11-28 19:05:55.677998: step 17030, loss = 0.71 (843.5 examples/sec; 0.152 sec/batch)
2016-11-28 19:05:57.235134: step 17040, loss = 0.80 (835.0 examples/sec; 0.153 sec/batch)
2016-11-28 19:05:58.794459: step 17050, loss = 0.94 (847.1 examples/sec; 0.151 sec/batch)
2016-11-28 19:06:00.343003: step 17060, loss = 0.84 (875.0 examples/sec; 0.146 sec/batch)
2016-11-28 19:06:01.905253: step 17070, loss = 0.79 (819.4 examples/sec; 0.156 sec/batch)
2016-11-28 19:06:03.469421: step 17080, loss = 0.76 (826.7 examples/sec; 0.155 sec/batch)
2016-11-28 19:06:05.063564: step 17090, loss = 0.97 (778.9 examples/sec; 0.164 sec/batch)
2016-11-28 19:06:06.626921: step 17100, loss = 0.87 (802.3 examples/sec; 0.160 sec/batch)
2016-11-28 19:06:08.372654: step 17110, loss = 0.74 (861.8 examples/sec; 0.149 sec/batch)
2016-11-28 19:06:09.938710: step 17120, loss = 0.80 (808.1 examples/sec; 0.158 sec/batch)
2016-11-28 19:06:11.496998: step 17130, loss = 0.99 (819.2 examples/sec; 0.156 sec/batch)
2016-11-28 19:06:13.052118: step 17140, loss = 0.88 (818.1 examples/sec; 0.156 sec/batch)
2016-11-28 19:06:14.600854: step 17150, loss = 0.87 (829.0 examples/sec; 0.154 sec/batch)
2016-11-28 19:06:16.163907: step 17160, loss = 0.77 (826.8 examples/sec; 0.155 sec/batch)
2016-11-28 19:06:17.713163: step 17170, loss = 0.68 (834.7 examples/sec; 0.153 sec/batch)
2016-11-28 19:06:19.264150: step 17180, loss = 0.75 (819.2 examples/sec; 0.156 sec/batch)
2016-11-28 19:06:20.813240: step 17190, loss = 0.98 (845.8 examples/sec; 0.151 sec/batch)
2016-11-28 19:06:22.362796: step 17200, loss = 0.88 (828.9 examples/sec; 0.154 sec/batch)
2016-11-28 19:06:24.106852: step 17210, loss = 0.91 (827.6 examples/sec; 0.155 sec/batch)
2016-11-28 19:06:25.669327: step 17220, loss = 0.89 (826.3 examples/sec; 0.155 sec/batch)
2016-11-28 19:06:27.245615: step 17230, loss = 0.69 (800.4 examples/sec; 0.160 sec/batch)
2016-11-28 19:06:28.839306: step 17240, loss = 0.76 (798.9 examples/sec; 0.160 sec/batch)
2016-11-28 19:06:30.433591: step 17250, loss = 0.88 (783.3 examples/sec; 0.163 sec/batch)
2016-11-28 19:06:32.030684: step 17260, loss = 0.81 (784.6 examples/sec; 0.163 sec/batch)
2016-11-28 19:06:33.636727: step 17270, loss = 0.77 (803.5 examples/sec; 0.159 sec/batch)
2016-11-28 19:06:35.255757: step 17280, loss = 0.83 (805.8 examples/sec; 0.159 sec/batch)
2016-11-28 19:06:36.875738: step 17290, loss = 0.85 (805.6 examples/sec; 0.159 sec/batch)
2016-11-28 19:06:38.491801: step 17300, loss = 0.64 (774.7 examples/sec; 0.165 sec/batch)
2016-11-28 19:06:40.293084: step 17310, loss = 0.84 (803.9 examples/sec; 0.159 sec/batch)
2016-11-28 19:06:41.904547: step 17320, loss = 0.91 (788.1 examples/sec; 0.162 sec/batch)
2016-11-28 19:06:43.517994: step 17330, loss = 0.87 (823.9 examples/sec; 0.155 sec/batch)
2016-11-28 19:06:45.135798: step 17340, loss = 0.80 (790.7 examples/sec; 0.162 sec/batch)
2016-11-28 19:06:46.748993: step 17350, loss = 0.80 (785.9 examples/sec; 0.163 sec/batch)
2016-11-28 19:06:48.361545: step 17360, loss = 0.75 (796.9 examples/sec; 0.161 sec/batch)
2016-11-28 19:06:49.979944: step 17370, loss = 0.86 (780.4 examples/sec; 0.164 sec/batch)
2016-11-28 19:06:51.583332: step 17380, loss = 0.90 (791.3 examples/sec; 0.162 sec/batch)
2016-11-28 19:06:53.195068: step 17390, loss = 0.91 (755.6 examples/sec; 0.169 sec/batch)
2016-11-28 19:06:54.795482: step 17400, loss = 0.92 (803.0 examples/sec; 0.159 sec/batch)
2016-11-28 19:06:56.612530: step 17410, loss = 0.85 (778.5 examples/sec; 0.164 sec/batch)
2016-11-28 19:06:58.227058: step 17420, loss = 0.71 (791.5 examples/sec; 0.162 sec/batch)
2016-11-28 19:06:59.824571: step 17430, loss = 0.75 (821.3 examples/sec; 0.156 sec/batch)
2016-11-28 19:07:01.432578: step 17440, loss = 0.83 (792.9 examples/sec; 0.161 sec/batch)
2016-11-28 19:07:03.054286: step 17450, loss = 0.68 (789.9 examples/sec; 0.162 sec/batch)
2016-11-28 19:07:04.668726: step 17460, loss = 0.81 (781.6 examples/sec; 0.164 sec/batch)
2016-11-28 19:07:06.275274: step 17470, loss = 0.85 (790.4 examples/sec; 0.162 sec/batch)
2016-11-28 19:07:07.886467: step 17480, loss = 0.84 (809.6 examples/sec; 0.158 sec/batch)
2016-11-28 19:07:09.507441: step 17490, loss = 0.92 (799.0 examples/sec; 0.160 sec/batch)
2016-11-28 19:07:11.113100: step 17500, loss = 0.98 (805.5 examples/sec; 0.159 sec/batch)
2016-11-28 19:07:12.934449: step 17510, loss = 0.77 (735.1 examples/sec; 0.174 sec/batch)
2016-11-28 19:07:14.525618: step 17520, loss = 0.68 (812.0 examples/sec; 0.158 sec/batch)
2016-11-28 19:07:16.143610: step 17530, loss = 0.71 (785.5 examples/sec; 0.163 sec/batch)
2016-11-28 19:07:17.759827: step 17540, loss = 0.83 (834.0 examples/sec; 0.153 sec/batch)
2016-11-28 19:07:19.370796: step 17550, loss = 0.96 (775.8 examples/sec; 0.165 sec/batch)
2016-11-28 19:07:20.971519: step 17560, loss = 0.80 (840.4 examples/sec; 0.152 sec/batch)
2016-11-28 19:07:22.582788: step 17570, loss = 0.89 (781.7 examples/sec; 0.164 sec/batch)
2016-11-28 19:07:24.191935: step 17580, loss = 0.76 (817.5 examples/sec; 0.157 sec/batch)
2016-11-28 19:07:25.799278: step 17590, loss = 0.90 (780.0 examples/sec; 0.164 sec/batch)
2016-11-28 19:07:27.409551: step 17600, loss = 0.81 (790.4 examples/sec; 0.162 sec/batch)
2016-11-28 19:07:29.220031: step 17610, loss = 0.69 (817.2 examples/sec; 0.157 sec/batch)
2016-11-28 19:07:30.830234: step 17620, loss = 1.02 (780.4 examples/sec; 0.164 sec/batch)
2016-11-28 19:07:32.432312: step 17630, loss = 0.67 (800.9 examples/sec; 0.160 sec/batch)
2016-11-28 19:07:34.035944: step 17640, loss = 0.91 (800.6 examples/sec; 0.160 sec/batch)
2016-11-28 19:07:35.643774: step 17650, loss = 0.84 (792.8 examples/sec; 0.161 sec/batch)
2016-11-28 19:07:37.256890: step 17660, loss = 0.78 (774.4 examples/sec; 0.165 sec/batch)
2016-11-28 19:07:38.871940: step 17670, loss = 0.71 (783.8 examples/sec; 0.163 sec/batch)
2016-11-28 19:07:40.486143: step 17680, loss = 0.71 (787.1 examples/sec; 0.163 sec/batch)
2016-11-28 19:07:42.090343: step 17690, loss = 0.82 (817.2 examples/sec; 0.157 sec/batch)
2016-11-28 19:07:43.704246: step 17700, loss = 0.86 (773.0 examples/sec; 0.166 sec/batch)
2016-11-28 19:07:45.544329: step 17710, loss = 0.81 (800.4 examples/sec; 0.160 sec/batch)
2016-11-28 19:07:47.154614: step 17720, loss = 0.82 (775.1 examples/sec; 0.165 sec/batch)
2016-11-28 19:07:48.768432: step 17730, loss = 0.72 (804.2 examples/sec; 0.159 sec/batch)
2016-11-28 19:07:50.379700: step 17740, loss = 0.83 (806.2 examples/sec; 0.159 sec/batch)
2016-11-28 19:07:51.982894: step 17750, loss = 0.87 (826.1 examples/sec; 0.155 sec/batch)
2016-11-28 19:07:53.585367: step 17760, loss = 0.79 (803.4 examples/sec; 0.159 sec/batch)
2016-11-28 19:07:55.204049: step 17770, loss = 0.94 (831.0 examples/sec; 0.154 sec/batch)
2016-11-28 19:07:56.819549: step 17780, loss = 0.79 (791.4 examples/sec; 0.162 sec/batch)
2016-11-28 19:07:58.438888: step 17790, loss = 0.83 (757.0 examples/sec; 0.169 sec/batch)
2016-11-28 19:08:00.038892: step 17800, loss = 1.02 (777.2 examples/sec; 0.165 sec/batch)
2016-11-28 19:08:01.849900: step 17810, loss = 0.79 (791.8 examples/sec; 0.162 sec/batch)
2016-11-28 19:08:03.453704: step 17820, loss = 0.74 (821.6 examples/sec; 0.156 sec/batch)
2016-11-28 19:08:05.064508: step 17830, loss = 0.72 (793.6 examples/sec; 0.161 sec/batch)
2016-11-28 19:08:06.659483: step 17840, loss = 0.73 (827.4 examples/sec; 0.155 sec/batch)
2016-11-28 19:08:08.261615: step 17850, loss = 1.00 (795.0 examples/sec; 0.161 sec/batch)
2016-11-28 19:08:09.868645: step 17860, loss = 0.78 (818.9 examples/sec; 0.156 sec/batch)
2016-11-28 19:08:11.475352: step 17870, loss = 0.80 (795.1 examples/sec; 0.161 sec/batch)
2016-11-28 19:08:13.077898: step 17880, loss = 0.68 (797.4 examples/sec; 0.161 sec/batch)
2016-11-28 19:08:14.675335: step 17890, loss = 0.72 (828.4 examples/sec; 0.155 sec/batch)
2016-11-28 19:08:16.273888: step 17900, loss = 0.89 (801.0 examples/sec; 0.160 sec/batch)
2016-11-28 19:08:18.084534: step 17910, loss = 0.85 (813.3 examples/sec; 0.157 sec/batch)
2016-11-28 19:08:19.688060: step 17920, loss = 0.67 (823.9 examples/sec; 0.155 sec/batch)
2016-11-28 19:08:21.304318: step 17930, loss = 0.89 (823.1 examples/sec; 0.156 sec/batch)
2016-11-28 19:08:22.919837: step 17940, loss = 0.97 (793.1 examples/sec; 0.161 sec/batch)
2016-11-28 19:08:24.538098: step 17950, loss = 0.95 (774.9 examples/sec; 0.165 sec/batch)
2016-11-28 19:08:26.136338: step 17960, loss = 0.92 (818.6 examples/sec; 0.156 sec/batch)
2016-11-28 19:08:27.738644: step 17970, loss = 0.82 (818.2 examples/sec; 0.156 sec/batch)
2016-11-28 19:08:29.355405: step 17980, loss = 0.88 (788.5 examples/sec; 0.162 sec/batch)
2016-11-28 19:08:30.977134: step 17990, loss = 0.71 (771.1 examples/sec; 0.166 sec/batch)
2016-11-28 19:08:32.578327: step 18000, loss = 0.88 (821.6 examples/sec; 0.156 sec/batch)
2016-11-28 19:08:34.785738: step 18010, loss = 0.91 (796.6 examples/sec; 0.161 sec/batch)
2016-11-28 19:08:36.362859: step 18020, loss = 0.81 (809.3 examples/sec; 0.158 sec/batch)
2016-11-28 19:08:37.951231: step 18030, loss = 0.97 (786.2 examples/sec; 0.163 sec/batch)
2016-11-28 19:08:39.535840: step 18040, loss = 0.81 (787.1 examples/sec; 0.163 sec/batch)
2016-11-28 19:08:41.135628: step 18050, loss = 0.95 (868.1 examples/sec; 0.147 sec/batch)
2016-11-28 19:08:42.730576: step 18060, loss = 0.83 (788.9 examples/sec; 0.162 sec/batch)
2016-11-28 19:08:44.317206: step 18070, loss = 0.84 (807.0 examples/sec; 0.159 sec/batch)
2016-11-28 19:08:45.907069: step 18080, loss = 0.87 (786.7 examples/sec; 0.163 sec/batch)
2016-11-28 19:08:47.496615: step 18090, loss = 0.80 (827.1 examples/sec; 0.155 sec/batch)
2016-11-28 19:08:49.069670: step 18100, loss = 0.90 (804.3 examples/sec; 0.159 sec/batch)
2016-11-28 19:08:50.849939: step 18110, loss = 0.83 (787.0 examples/sec; 0.163 sec/batch)
2016-11-28 19:08:52.464246: step 18120, loss = 0.79 (774.4 examples/sec; 0.165 sec/batch)
2016-11-28 19:08:54.075158: step 18130, loss = 0.84 (803.8 examples/sec; 0.159 sec/batch)
2016-11-28 19:08:55.679177: step 18140, loss = 0.71 (833.7 examples/sec; 0.154 sec/batch)
2016-11-28 19:08:57.288391: step 18150, loss = 0.89 (785.6 examples/sec; 0.163 sec/batch)
2016-11-28 19:08:58.903261: step 18160, loss = 0.73 (796.6 examples/sec; 0.161 sec/batch)
2016-11-28 19:09:00.508972: step 18170, loss = 0.84 (833.6 examples/sec; 0.154 sec/batch)
2016-11-28 19:09:02.122757: step 18180, loss = 0.77 (793.7 examples/sec; 0.161 sec/batch)
2016-11-28 19:09:03.724274: step 18190, loss = 1.00 (796.1 examples/sec; 0.161 sec/batch)
2016-11-28 19:09:05.335112: step 18200, loss = 0.81 (783.6 examples/sec; 0.163 sec/batch)
2016-11-28 19:09:07.116198: step 18210, loss = 0.84 (775.1 examples/sec; 0.165 sec/batch)
2016-11-28 19:09:08.698585: step 18220, loss = 0.73 (817.6 examples/sec; 0.157 sec/batch)
2016-11-28 19:09:10.282397: step 18230, loss = 0.93 (830.8 examples/sec; 0.154 sec/batch)
2016-11-28 19:09:11.875339: step 18240, loss = 1.04 (781.2 examples/sec; 0.164 sec/batch)
2016-11-28 19:09:13.463191: step 18250, loss = 0.77 (834.4 examples/sec; 0.153 sec/batch)
2016-11-28 19:09:15.050595: step 18260, loss = 0.69 (790.4 examples/sec; 0.162 sec/batch)
2016-11-28 19:09:16.624651: step 18270, loss = 0.81 (814.6 examples/sec; 0.157 sec/batch)
2016-11-28 19:09:18.225824: step 18280, loss = 0.71 (788.8 examples/sec; 0.162 sec/batch)
2016-11-28 19:09:19.814405: step 18290, loss = 0.87 (796.5 examples/sec; 0.161 sec/batch)
2016-11-28 19:09:21.390975: step 18300, loss = 0.80 (827.2 examples/sec; 0.155 sec/batch)
2016-11-28 19:09:23.146787: step 18310, loss = 1.07 (823.8 examples/sec; 0.155 sec/batch)
2016-11-28 19:09:24.707547: step 18320, loss = 1.03 (812.2 examples/sec; 0.158 sec/batch)
2016-11-28 19:09:26.253912: step 18330, loss = 0.93 (827.5 examples/sec; 0.155 sec/batch)
2016-11-28 19:09:27.818176: step 18340, loss = 0.98 (875.5 examples/sec; 0.146 sec/batch)
2016-11-28 19:09:29.378172: step 18350, loss = 0.78 (818.5 examples/sec; 0.156 sec/batch)
2016-11-28 19:09:30.939611: step 18360, loss = 0.77 (783.4 examples/sec; 0.163 sec/batch)
2016-11-28 19:09:32.486384: step 18370, loss = 0.80 (812.2 examples/sec; 0.158 sec/batch)
2016-11-28 19:09:34.045905: step 18380, loss = 0.81 (786.1 examples/sec; 0.163 sec/batch)
2016-11-28 19:09:35.612891: step 18390, loss = 0.84 (827.3 examples/sec; 0.155 sec/batch)
2016-11-28 19:09:37.170855: step 18400, loss = 0.94 (817.7 examples/sec; 0.157 sec/batch)
2016-11-28 19:09:38.925207: step 18410, loss = 0.82 (799.7 examples/sec; 0.160 sec/batch)
2016-11-28 19:09:40.486702: step 18420, loss = 1.00 (772.9 examples/sec; 0.166 sec/batch)
2016-11-28 19:09:42.040206: step 18430, loss = 0.94 (818.7 examples/sec; 0.156 sec/batch)
2016-11-28 19:09:43.610208: step 18440, loss = 0.86 (785.6 examples/sec; 0.163 sec/batch)
2016-11-28 19:09:45.204839: step 18450, loss = 0.90 (805.5 examples/sec; 0.159 sec/batch)
2016-11-28 19:09:46.794411: step 18460, loss = 0.81 (793.7 examples/sec; 0.161 sec/batch)
2016-11-28 19:09:48.396617: step 18470, loss = 0.77 (810.3 examples/sec; 0.158 sec/batch)
2016-11-28 19:09:49.995311: step 18480, loss = 0.99 (802.9 examples/sec; 0.159 sec/batch)
2016-11-28 19:09:51.603583: step 18490, loss = 0.90 (783.2 examples/sec; 0.163 sec/batch)
2016-11-28 19:09:53.206150: step 18500, loss = 0.92 (787.1 examples/sec; 0.163 sec/batch)
2016-11-28 19:09:55.017008: step 18510, loss = 0.77 (763.4 examples/sec; 0.168 sec/batch)
2016-11-28 19:09:56.624207: step 18520, loss = 0.72 (767.5 examples/sec; 0.167 sec/batch)
2016-11-28 19:09:58.237077: step 18530, loss = 0.82 (764.3 examples/sec; 0.167 sec/batch)
2016-11-28 19:09:59.832320: step 18540, loss = 0.91 (804.5 examples/sec; 0.159 sec/batch)
2016-11-28 19:10:01.448519: step 18550, loss = 1.01 (785.0 examples/sec; 0.163 sec/batch)
2016-11-28 19:10:03.047231: step 18560, loss = 0.84 (812.2 examples/sec; 0.158 sec/batch)
2016-11-28 19:10:04.659721: step 18570, loss = 0.80 (763.0 examples/sec; 0.168 sec/batch)
2016-11-28 19:10:06.262111: step 18580, loss = 0.72 (798.4 examples/sec; 0.160 sec/batch)
2016-11-28 19:10:07.856521: step 18590, loss = 0.69 (834.3 examples/sec; 0.153 sec/batch)
2016-11-28 19:10:09.475911: step 18600, loss = 0.72 (778.2 examples/sec; 0.164 sec/batch)
2016-11-28 19:10:11.306687: step 18610, loss = 0.88 (776.7 examples/sec; 0.165 sec/batch)
2016-11-28 19:10:12.923376: step 18620, loss = 0.97 (755.5 examples/sec; 0.169 sec/batch)
2016-11-28 19:10:14.525746: step 18630, loss = 0.98 (818.8 examples/sec; 0.156 sec/batch)
2016-11-28 19:10:16.126752: step 18640, loss = 0.83 (804.8 examples/sec; 0.159 sec/batch)
2016-11-28 19:10:17.733683: step 18650, loss = 0.70 (787.5 examples/sec; 0.163 sec/batch)
2016-11-28 19:10:19.347017: step 18660, loss = 1.01 (783.8 examples/sec; 0.163 sec/batch)
2016-11-28 19:10:20.957948: step 18670, loss = 0.87 (802.9 examples/sec; 0.159 sec/batch)
2016-11-28 19:10:22.555174: step 18680, loss = 0.91 (797.1 examples/sec; 0.161 sec/batch)
2016-11-28 19:10:24.162257: step 18690, loss = 0.76 (807.1 examples/sec; 0.159 sec/batch)
2016-11-28 19:10:25.771334: step 18700, loss = 0.99 (811.8 examples/sec; 0.158 sec/batch)
2016-11-28 19:10:27.556793: step 18710, loss = 0.74 (832.8 examples/sec; 0.154 sec/batch)
2016-11-28 19:10:29.127797: step 18720, loss = 0.79 (814.5 examples/sec; 0.157 sec/batch)
2016-11-28 19:10:30.710490: step 18730, loss = 0.71 (845.0 examples/sec; 0.151 sec/batch)
2016-11-28 19:10:32.291178: step 18740, loss = 0.86 (789.5 examples/sec; 0.162 sec/batch)
2016-11-28 19:10:33.859795: step 18750, loss = 0.91 (827.5 examples/sec; 0.155 sec/batch)
2016-11-28 19:10:35.435483: step 18760, loss = 0.80 (811.7 examples/sec; 0.158 sec/batch)
2016-11-28 19:10:37.026399: step 18770, loss = 0.78 (793.8 examples/sec; 0.161 sec/batch)
2016-11-28 19:10:38.604558: step 18780, loss = 0.67 (850.2 examples/sec; 0.151 sec/batch)
2016-11-28 19:10:40.179119: step 18790, loss = 0.94 (797.3 examples/sec; 0.161 sec/batch)
2016-11-28 19:10:41.765267: step 18800, loss = 0.91 (772.2 examples/sec; 0.166 sec/batch)
2016-11-28 19:10:43.532134: step 18810, loss = 0.83 (777.0 examples/sec; 0.165 sec/batch)
2016-11-28 19:10:45.144148: step 18820, loss = 0.75 (805.3 examples/sec; 0.159 sec/batch)
2016-11-28 19:10:46.754091: step 18830, loss = 0.81 (808.7 examples/sec; 0.158 sec/batch)
2016-11-28 19:10:48.352167: step 18840, loss = 0.86 (806.6 examples/sec; 0.159 sec/batch)
2016-11-28 19:10:49.957512: step 18850, loss = 0.86 (810.3 examples/sec; 0.158 sec/batch)
2016-11-28 19:10:51.579577: step 18860, loss = 0.92 (751.8 examples/sec; 0.170 sec/batch)
2016-11-28 19:10:53.188831: step 18870, loss = 0.88 (755.6 examples/sec; 0.169 sec/batch)
2016-11-28 19:10:54.785024: step 18880, loss = 0.91 (790.9 examples/sec; 0.162 sec/batch)
2016-11-28 19:10:56.391478: step 18890, loss = 0.93 (790.6 examples/sec; 0.162 sec/batch)
2016-11-28 19:10:57.997795: step 18900, loss = 0.85 (821.6 examples/sec; 0.156 sec/batch)
2016-11-28 19:10:59.822747: step 18910, loss = 0.79 (789.8 examples/sec; 0.162 sec/batch)
2016-11-28 19:11:01.417785: step 18920, loss = 0.64 (771.1 examples/sec; 0.166 sec/batch)
2016-11-28 19:11:03.032190: step 18930, loss = 0.63 (762.7 examples/sec; 0.168 sec/batch)
2016-11-28 19:11:04.636353: step 18940, loss = 0.88 (807.0 examples/sec; 0.159 sec/batch)
2016-11-28 19:11:06.252643: step 18950, loss = 0.65 (790.3 examples/sec; 0.162 sec/batch)
2016-11-28 19:11:07.855912: step 18960, loss = 0.96 (801.6 examples/sec; 0.160 sec/batch)
2016-11-28 19:11:09.473124: step 18970, loss = 0.80 (769.0 examples/sec; 0.166 sec/batch)
2016-11-28 19:11:11.075425: step 18980, loss = 0.77 (818.6 examples/sec; 0.156 sec/batch)
2016-11-28 19:11:12.694584: step 18990, loss = 0.87 (796.3 examples/sec; 0.161 sec/batch)
2016-11-28 19:11:14.300007: step 19000, loss = 0.90 (805.6 examples/sec; 0.159 sec/batch)
2016-11-28 19:11:16.457338: step 19010, loss = 0.70 (821.8 examples/sec; 0.156 sec/batch)
2016-11-28 19:11:18.012547: step 19020, loss = 0.81 (798.3 examples/sec; 0.160 sec/batch)
2016-11-28 19:11:19.567834: step 19030, loss = 0.83 (821.9 examples/sec; 0.156 sec/batch)
2016-11-28 19:11:21.128707: step 19040, loss = 0.90 (788.1 examples/sec; 0.162 sec/batch)
2016-11-28 19:11:22.678636: step 19050, loss = 0.83 (785.0 examples/sec; 0.163 sec/batch)
2016-11-28 19:11:24.233314: step 19060, loss = 0.79 (815.5 examples/sec; 0.157 sec/batch)
2016-11-28 19:11:25.801285: step 19070, loss = 0.79 (793.7 examples/sec; 0.161 sec/batch)
2016-11-28 19:11:27.360205: step 19080, loss = 0.73 (785.7 examples/sec; 0.163 sec/batch)
2016-11-28 19:11:28.907825: step 19090, loss = 0.87 (831.5 examples/sec; 0.154 sec/batch)
2016-11-28 19:11:30.462425: step 19100, loss = 0.81 (843.0 examples/sec; 0.152 sec/batch)
2016-11-28 19:11:32.214128: step 19110, loss = 0.78 (815.0 examples/sec; 0.157 sec/batch)
2016-11-28 19:11:33.764976: step 19120, loss = 0.90 (798.0 examples/sec; 0.160 sec/batch)
2016-11-28 19:11:35.367799: step 19130, loss = 0.82 (794.8 examples/sec; 0.161 sec/batch)
2016-11-28 19:11:36.947902: step 19140, loss = 0.78 (810.5 examples/sec; 0.158 sec/batch)
2016-11-28 19:11:38.533421: step 19150, loss = 0.72 (843.5 examples/sec; 0.152 sec/batch)
2016-11-28 19:11:40.116330: step 19160, loss = 0.97 (827.7 examples/sec; 0.155 sec/batch)
2016-11-28 19:11:41.702919: step 19170, loss = 0.65 (799.8 examples/sec; 0.160 sec/batch)
2016-11-28 19:11:43.287483: step 19180, loss = 0.74 (779.7 examples/sec; 0.164 sec/batch)
2016-11-28 19:11:44.875030: step 19190, loss = 0.86 (823.0 examples/sec; 0.156 sec/batch)
2016-11-28 19:11:46.460571: step 19200, loss = 0.91 (825.3 examples/sec; 0.155 sec/batch)
2016-11-28 19:11:48.230516: step 19210, loss = 0.79 (834.7 examples/sec; 0.153 sec/batch)
2016-11-28 19:11:49.817485: step 19220, loss = 0.74 (835.7 examples/sec; 0.153 sec/batch)
2016-11-28 19:11:51.399355: step 19230, loss = 0.86 (817.7 examples/sec; 0.157 sec/batch)
2016-11-28 19:11:52.989841: step 19240, loss = 1.02 (789.9 examples/sec; 0.162 sec/batch)
2016-11-28 19:11:54.572479: step 19250, loss = 0.99 (835.6 examples/sec; 0.153 sec/batch)
2016-11-28 19:11:56.157052: step 19260, loss = 0.78 (798.6 examples/sec; 0.160 sec/batch)
2016-11-28 19:11:57.733746: step 19270, loss = 0.78 (808.2 examples/sec; 0.158 sec/batch)
2016-11-28 19:11:59.322892: step 19280, loss = 0.80 (775.1 examples/sec; 0.165 sec/batch)
2016-11-28 19:12:00.904320: step 19290, loss = 0.75 (825.0 examples/sec; 0.155 sec/batch)
2016-11-28 19:12:02.490043: step 19300, loss = 0.75 (793.5 examples/sec; 0.161 sec/batch)
2016-11-28 19:12:04.304078: step 19310, loss = 0.78 (798.1 examples/sec; 0.160 sec/batch)
2016-11-28 19:12:05.908522: step 19320, loss = 0.78 (796.8 examples/sec; 0.161 sec/batch)
2016-11-28 19:12:07.507130: step 19330, loss = 0.89 (798.6 examples/sec; 0.160 sec/batch)
2016-11-28 19:12:09.121500: step 19340, loss = 0.80 (774.5 examples/sec; 0.165 sec/batch)
2016-11-28 19:12:10.725882: step 19350, loss = 0.93 (796.4 examples/sec; 0.161 sec/batch)
2016-11-28 19:12:12.335217: step 19360, loss = 0.99 (789.9 examples/sec; 0.162 sec/batch)
2016-11-28 19:12:13.940156: step 19370, loss = 0.74 (806.8 examples/sec; 0.159 sec/batch)
2016-11-28 19:12:15.547105: step 19380, loss = 0.74 (803.2 examples/sec; 0.159 sec/batch)
2016-11-28 19:12:17.162720: step 19390, loss = 0.90 (775.0 examples/sec; 0.165 sec/batch)
2016-11-28 19:12:18.761074: step 19400, loss = 1.00 (802.2 examples/sec; 0.160 sec/batch)
2016-11-28 19:12:20.539893: step 19410, loss = 1.01 (848.0 examples/sec; 0.151 sec/batch)
2016-11-28 19:12:22.125974: step 19420, loss = 0.87 (782.1 examples/sec; 0.164 sec/batch)
2016-11-28 19:12:23.692480: step 19430, loss = 0.77 (823.5 examples/sec; 0.155 sec/batch)
2016-11-28 19:12:25.260094: step 19440, loss = 0.71 (829.8 examples/sec; 0.154 sec/batch)
2016-11-28 19:12:26.840671: step 19450, loss = 0.77 (824.8 examples/sec; 0.155 sec/batch)
2016-11-28 19:12:28.412272: step 19460, loss = 0.66 (801.6 examples/sec; 0.160 sec/batch)
2016-11-28 19:12:29.993619: step 19470, loss = 0.87 (805.3 examples/sec; 0.159 sec/batch)
2016-11-28 19:12:31.569812: step 19480, loss = 0.74 (846.9 examples/sec; 0.151 sec/batch)
2016-11-28 19:12:33.153342: step 19490, loss = 0.72 (812.1 examples/sec; 0.158 sec/batch)
2016-11-28 19:12:34.728509: step 19500, loss = 0.74 (797.3 examples/sec; 0.161 sec/batch)
2016-11-28 19:12:36.518222: step 19510, loss = 0.79 (821.6 examples/sec; 0.156 sec/batch)
2016-11-28 19:12:38.095663: step 19520, loss = 0.91 (819.1 examples/sec; 0.156 sec/batch)
2016-11-28 19:12:39.676093: step 19530, loss = 1.00 (818.4 examples/sec; 0.156 sec/batch)
2016-11-28 19:12:41.253799: step 19540, loss = 0.77 (806.7 examples/sec; 0.159 sec/batch)
2016-11-28 19:12:42.831313: step 19550, loss = 0.82 (790.5 examples/sec; 0.162 sec/batch)
2016-11-28 19:12:44.398904: step 19560, loss = 0.71 (804.2 examples/sec; 0.159 sec/batch)
2016-11-28 19:12:45.975503: step 19570, loss = 0.80 (784.6 examples/sec; 0.163 sec/batch)
2016-11-28 19:12:47.550400: step 19580, loss = 0.90 (835.3 examples/sec; 0.153 sec/batch)
2016-11-28 19:12:49.129889: step 19590, loss = 0.70 (783.6 examples/sec; 0.163 sec/batch)
2016-11-28 19:12:50.709086: step 19600, loss = 0.82 (834.0 examples/sec; 0.153 sec/batch)
2016-11-28 19:12:52.479335: step 19610, loss = 0.92 (839.8 examples/sec; 0.152 sec/batch)
2016-11-28 19:12:54.066745: step 19620, loss = 0.85 (793.0 examples/sec; 0.161 sec/batch)
2016-11-28 19:12:55.636173: step 19630, loss = 0.92 (827.5 examples/sec; 0.155 sec/batch)
2016-11-28 19:12:57.207136: step 19640, loss = 0.91 (808.9 examples/sec; 0.158 sec/batch)
2016-11-28 19:12:58.790672: step 19650, loss = 0.73 (794.4 examples/sec; 0.161 sec/batch)
2016-11-28 19:13:00.370499: step 19660, loss = 0.95 (797.5 examples/sec; 0.161 sec/batch)
2016-11-28 19:13:01.938206: step 19670, loss = 0.92 (825.7 examples/sec; 0.155 sec/batch)
2016-11-28 19:13:03.510873: step 19680, loss = 0.94 (815.4 examples/sec; 0.157 sec/batch)
2016-11-28 19:13:05.093905: step 19690, loss = 0.92 (823.4 examples/sec; 0.155 sec/batch)
2016-11-28 19:13:06.687706: step 19700, loss = 0.86 (782.6 examples/sec; 0.164 sec/batch)
2016-11-28 19:13:08.484184: step 19710, loss = 0.75 (783.8 examples/sec; 0.163 sec/batch)
2016-11-28 19:13:10.087385: step 19720, loss = 0.80 (808.5 examples/sec; 0.158 sec/batch)
2016-11-28 19:13:11.686769: step 19730, loss = 0.72 (828.6 examples/sec; 0.154 sec/batch)
2016-11-28 19:13:13.294113: step 19740, loss = 0.80 (794.2 examples/sec; 0.161 sec/batch)
2016-11-28 19:13:14.900713: step 19750, loss = 0.84 (793.1 examples/sec; 0.161 sec/batch)
2016-11-28 19:13:16.503761: step 19760, loss = 0.81 (791.8 examples/sec; 0.162 sec/batch)
2016-11-28 19:13:18.105622: step 19770, loss = 0.74 (797.8 examples/sec; 0.160 sec/batch)
2016-11-28 19:13:19.714842: step 19780, loss = 0.78 (829.8 examples/sec; 0.154 sec/batch)
2016-11-28 19:13:21.325743: step 19790, loss = 0.71 (775.6 examples/sec; 0.165 sec/batch)
2016-11-28 19:13:22.924783: step 19800, loss = 0.82 (823.6 examples/sec; 0.155 sec/batch)
2016-11-28 19:13:24.713232: step 19810, loss = 0.80 (807.0 examples/sec; 0.159 sec/batch)
2016-11-28 19:13:26.294086: step 19820, loss = 0.76 (806.0 examples/sec; 0.159 sec/batch)
2016-11-28 19:13:27.872672: step 19830, loss = 0.96 (832.1 examples/sec; 0.154 sec/batch)
2016-11-28 19:13:29.454967: step 19840, loss = 0.68 (815.3 examples/sec; 0.157 sec/batch)
2016-11-28 19:13:31.046004: step 19850, loss = 0.80 (792.2 examples/sec; 0.162 sec/batch)
2016-11-28 19:13:32.639133: step 19860, loss = 0.72 (804.2 examples/sec; 0.159 sec/batch)
2016-11-28 19:13:34.232441: step 19870, loss = 0.76 (805.5 examples/sec; 0.159 sec/batch)
2016-11-28 19:13:35.821957: step 19880, loss = 0.78 (794.5 examples/sec; 0.161 sec/batch)
2016-11-28 19:13:37.405643: step 19890, loss = 0.76 (849.0 examples/sec; 0.151 sec/batch)
2016-11-28 19:13:39.001536: step 19900, loss = 0.88 (856.6 examples/sec; 0.149 sec/batch)
2016-11-28 19:13:40.794257: step 19910, loss = 0.84 (805.0 examples/sec; 0.159 sec/batch)
2016-11-28 19:13:42.379676: step 19920, loss = 0.88 (818.6 examples/sec; 0.156 sec/batch)
2016-11-28 19:13:43.966270: step 19930, loss = 0.79 (812.2 examples/sec; 0.158 sec/batch)
2016-11-28 19:13:45.558032: step 19940, loss = 0.75 (796.4 examples/sec; 0.161 sec/batch)
2016-11-28 19:13:47.147962: step 19950, loss = 1.10 (807.9 examples/sec; 0.158 sec/batch)
2016-11-28 19:13:48.732754: step 19960, loss = 0.83 (811.4 examples/sec; 0.158 sec/batch)
2016-11-28 19:13:50.323479: step 19970, loss = 0.69 (844.3 examples/sec; 0.152 sec/batch)
2016-11-28 19:13:51.912304: step 19980, loss = 0.97 (794.5 examples/sec; 0.161 sec/batch)
2016-11-28 19:13:53.493230: step 19990, loss = 0.73 (828.6 examples/sec; 0.154 sec/batch)
2016-11-28 19:13:55.094943: step 20000, loss = 0.91 (773.9 examples/sec; 0.165 sec/batch)
2016-11-28 19:13:57.246873: step 20010, loss = 0.88 (815.2 examples/sec; 0.157 sec/batch)
2016-11-28 19:13:58.806027: step 20020, loss = 0.71 (809.3 examples/sec; 0.158 sec/batch)
2016-11-28 19:14:00.360763: step 20030, loss = 0.85 (840.7 examples/sec; 0.152 sec/batch)
2016-11-28 19:14:01.915485: step 20040, loss = 0.84 (809.1 examples/sec; 0.158 sec/batch)
2016-11-28 19:14:03.476215: step 20050, loss = 1.00 (826.4 examples/sec; 0.155 sec/batch)
2016-11-28 19:14:05.031465: step 20060, loss = 0.93 (813.4 examples/sec; 0.157 sec/batch)
2016-11-28 19:14:06.593530: step 20070, loss = 0.78 (798.5 examples/sec; 0.160 sec/batch)
2016-11-28 19:14:08.150882: step 20080, loss = 0.82 (856.3 examples/sec; 0.149 sec/batch)
2016-11-28 19:14:09.719776: step 20090, loss = 0.83 (795.9 examples/sec; 0.161 sec/batch)
2016-11-28 19:14:11.265424: step 20100, loss = 0.93 (888.2 examples/sec; 0.144 sec/batch)
2016-11-28 19:14:13.021469: step 20110, loss = 0.91 (820.4 examples/sec; 0.156 sec/batch)
2016-11-28 19:14:14.584642: step 20120, loss = 0.73 (834.3 examples/sec; 0.153 sec/batch)
2016-11-28 19:14:16.140802: step 20130, loss = 0.61 (807.0 examples/sec; 0.159 sec/batch)
2016-11-28 19:14:17.703242: step 20140, loss = 0.87 (792.7 examples/sec; 0.161 sec/batch)
2016-11-28 19:14:19.260758: step 20150, loss = 0.78 (819.6 examples/sec; 0.156 sec/batch)
2016-11-28 19:14:20.809291: step 20160, loss = 0.64 (863.2 examples/sec; 0.148 sec/batch)
2016-11-28 19:14:22.384171: step 20170, loss = 0.78 (797.1 examples/sec; 0.161 sec/batch)
2016-11-28 19:14:23.953185: step 20180, loss = 0.86 (821.4 examples/sec; 0.156 sec/batch)
2016-11-28 19:14:25.519235: step 20190, loss = 0.88 (798.5 examples/sec; 0.160 sec/batch)
2016-11-28 19:14:27.077650: step 20200, loss = 0.78 (839.7 examples/sec; 0.152 sec/batch)
2016-11-28 19:14:28.836479: step 20210, loss = 0.76 (789.7 examples/sec; 0.162 sec/batch)
2016-11-28 19:14:30.399296: step 20220, loss = 0.87 (817.6 examples/sec; 0.157 sec/batch)
2016-11-28 19:14:31.953240: step 20230, loss = 0.75 (839.3 examples/sec; 0.153 sec/batch)
2016-11-28 19:14:33.514883: step 20240, loss = 1.02 (862.5 examples/sec; 0.148 sec/batch)
2016-11-28 19:14:35.072902: step 20250, loss = 0.88 (799.2 examples/sec; 0.160 sec/batch)
2016-11-28 19:14:36.628395: step 20260, loss = 0.67 (847.8 examples/sec; 0.151 sec/batch)
2016-11-28 19:14:38.195600: step 20270, loss = 0.79 (831.0 examples/sec; 0.154 sec/batch)
2016-11-28 19:14:39.762746: step 20280, loss = 0.72 (800.7 examples/sec; 0.160 sec/batch)
2016-11-28 19:14:41.348871: step 20290, loss = 0.78 (782.5 examples/sec; 0.164 sec/batch)
2016-11-28 19:14:42.959316: step 20300, loss = 0.89 (795.3 examples/sec; 0.161 sec/batch)
2016-11-28 19:14:44.783662: step 20310, loss = 0.84 (820.7 examples/sec; 0.156 sec/batch)
2016-11-28 19:14:46.397315: step 20320, loss = 0.91 (785.2 examples/sec; 0.163 sec/batch)
2016-11-28 19:14:48.010522: step 20330, loss = 0.92 (814.4 examples/sec; 0.157 sec/batch)
2016-11-28 19:14:49.633997: step 20340, loss = 0.69 (763.5 examples/sec; 0.168 sec/batch)
2016-11-28 19:14:51.243740: step 20350, loss = 0.85 (774.1 examples/sec; 0.165 sec/batch)
2016-11-28 19:14:52.848412: step 20360, loss = 0.94 (798.4 examples/sec; 0.160 sec/batch)
2016-11-28 19:14:54.460469: step 20370, loss = 0.84 (809.1 examples/sec; 0.158 sec/batch)
2016-11-28 19:14:56.066134: step 20380, loss = 0.86 (831.7 examples/sec; 0.154 sec/batch)
2016-11-28 19:14:57.689152: step 20390, loss = 0.85 (788.4 examples/sec; 0.162 sec/batch)
2016-11-28 19:14:59.288922: step 20400, loss = 0.78 (781.7 examples/sec; 0.164 sec/batch)
2016-11-28 19:15:01.095158: step 20410, loss = 0.73 (802.4 examples/sec; 0.160 sec/batch)
2016-11-28 19:15:02.717295: step 20420, loss = 0.73 (794.1 examples/sec; 0.161 sec/batch)
2016-11-28 19:15:04.328108: step 20430, loss = 0.75 (817.2 examples/sec; 0.157 sec/batch)
2016-11-28 19:15:05.930109: step 20440, loss = 0.93 (800.0 examples/sec; 0.160 sec/batch)
2016-11-28 19:15:07.535135: step 20450, loss = 0.84 (795.5 examples/sec; 0.161 sec/batch)
2016-11-28 19:15:09.149224: step 20460, loss = 0.85 (783.7 examples/sec; 0.163 sec/batch)
2016-11-28 19:15:10.755237: step 20470, loss = 0.75 (794.3 examples/sec; 0.161 sec/batch)
2016-11-28 19:15:12.360942: step 20480, loss = 1.01 (819.5 examples/sec; 0.156 sec/batch)
2016-11-28 19:15:13.976506: step 20490, loss = 0.95 (813.7 examples/sec; 0.157 sec/batch)
2016-11-28 19:15:15.591746: step 20500, loss = 0.86 (805.1 examples/sec; 0.159 sec/batch)
2016-11-28 19:15:17.365982: step 20510, loss = 0.76 (827.8 examples/sec; 0.155 sec/batch)
2016-11-28 19:15:18.934046: step 20520, loss = 0.74 (800.2 examples/sec; 0.160 sec/batch)
2016-11-28 19:15:20.502656: step 20530, loss = 0.85 (788.6 examples/sec; 0.162 sec/batch)
2016-11-28 19:15:22.074841: step 20540, loss = 0.77 (803.0 examples/sec; 0.159 sec/batch)
2016-11-28 19:15:23.646605: step 20550, loss = 0.84 (806.9 examples/sec; 0.159 sec/batch)
2016-11-28 19:15:25.210040: step 20560, loss = 0.97 (810.5 examples/sec; 0.158 sec/batch)
2016-11-28 19:15:26.775705: step 20570, loss = 0.96 (816.3 examples/sec; 0.157 sec/batch)
2016-11-28 19:15:28.343993: step 20580, loss = 0.81 (802.6 examples/sec; 0.159 sec/batch)
2016-11-28 19:15:29.922849: step 20590, loss = 0.77 (806.2 examples/sec; 0.159 sec/batch)
2016-11-28 19:15:31.512524: step 20600, loss = 0.88 (762.5 examples/sec; 0.168 sec/batch)
2016-11-28 19:15:33.312047: step 20610, loss = 0.89 (808.2 examples/sec; 0.158 sec/batch)
2016-11-28 19:15:34.926688: step 20620, loss = 0.90 (824.2 examples/sec; 0.155 sec/batch)
2016-11-28 19:15:36.550385: step 20630, loss = 0.86 (785.4 examples/sec; 0.163 sec/batch)
2016-11-28 19:15:38.154451: step 20640, loss = 0.85 (829.4 examples/sec; 0.154 sec/batch)
2016-11-28 19:15:39.766796: step 20650, loss = 0.77 (805.7 examples/sec; 0.159 sec/batch)
2016-11-28 19:15:41.380144: step 20660, loss = 0.88 (793.8 examples/sec; 0.161 sec/batch)
2016-11-28 19:15:42.992420: step 20670, loss = 0.91 (773.5 examples/sec; 0.165 sec/batch)
2016-11-28 19:15:44.596381: step 20680, loss = 0.87 (813.0 examples/sec; 0.157 sec/batch)
2016-11-28 19:15:46.217048: step 20690, loss = 0.73 (799.1 examples/sec; 0.160 sec/batch)
2016-11-28 19:15:47.829712: step 20700, loss = 0.91 (807.6 examples/sec; 0.158 sec/batch)
2016-11-28 19:15:49.621987: step 20710, loss = 0.85 (804.9 examples/sec; 0.159 sec/batch)
2016-11-28 19:15:51.196490: step 20720, loss = 0.86 (822.3 examples/sec; 0.156 sec/batch)
2016-11-28 19:15:52.784300: step 20730, loss = 0.83 (825.5 examples/sec; 0.155 sec/batch)
2016-11-28 19:15:54.352863: step 20740, loss = 0.73 (818.3 examples/sec; 0.156 sec/batch)
2016-11-28 19:15:55.932058: step 20750, loss = 0.78 (852.4 examples/sec; 0.150 sec/batch)
2016-11-28 19:15:57.506515: step 20760, loss = 0.82 (817.4 examples/sec; 0.157 sec/batch)
2016-11-28 19:15:59.076139: step 20770, loss = 0.84 (831.8 examples/sec; 0.154 sec/batch)
2016-11-28 19:16:00.656874: step 20780, loss = 0.80 (818.3 examples/sec; 0.156 sec/batch)
2016-11-28 19:16:02.241365: step 20790, loss = 0.82 (777.9 examples/sec; 0.165 sec/batch)
2016-11-28 19:16:03.835305: step 20800, loss = 0.78 (820.3 examples/sec; 0.156 sec/batch)
2016-11-28 19:16:05.616107: step 20810, loss = 0.85 (796.9 examples/sec; 0.161 sec/batch)
2016-11-28 19:16:07.211209: step 20820, loss = 0.80 (826.9 examples/sec; 0.155 sec/batch)
2016-11-28 19:16:08.795831: step 20830, loss = 0.75 (784.7 examples/sec; 0.163 sec/batch)
2016-11-28 19:16:10.371469: step 20840, loss = 0.74 (814.8 examples/sec; 0.157 sec/batch)
2016-11-28 19:16:11.954556: step 20850, loss = 0.86 (785.4 examples/sec; 0.163 sec/batch)
2016-11-28 19:16:13.530385: step 20860, loss = 0.80 (799.9 examples/sec; 0.160 sec/batch)
2016-11-28 19:16:15.114344: step 20870, loss = 0.84 (811.9 examples/sec; 0.158 sec/batch)
2016-11-28 19:16:16.697071: step 20880, loss = 0.79 (814.1 examples/sec; 0.157 sec/batch)
2016-11-28 19:16:18.281020: step 20890, loss = 0.67 (765.8 examples/sec; 0.167 sec/batch)
2016-11-28 19:16:19.844748: step 20900, loss = 0.71 (790.3 examples/sec; 0.162 sec/batch)
2016-11-28 19:16:21.654208: step 20910, loss = 0.83 (795.3 examples/sec; 0.161 sec/batch)
2016-11-28 19:16:23.257542: step 20920, loss = 0.71 (814.2 examples/sec; 0.157 sec/batch)
2016-11-28 19:16:24.870466: step 20930, loss = 0.81 (792.5 examples/sec; 0.162 sec/batch)
2016-11-28 19:16:26.469768: step 20940, loss = 0.68 (809.7 examples/sec; 0.158 sec/batch)
2016-11-28 19:16:28.086232: step 20950, loss = 0.98 (769.4 examples/sec; 0.166 sec/batch)
2016-11-28 19:16:29.692675: step 20960, loss = 0.94 (778.3 examples/sec; 0.164 sec/batch)
2016-11-28 19:16:31.299490: step 20970, loss = 0.86 (800.1 examples/sec; 0.160 sec/batch)
2016-11-28 19:16:32.913726: step 20980, loss = 0.79 (773.3 examples/sec; 0.166 sec/batch)
2016-11-28 19:16:34.521513: step 20990, loss = 0.70 (783.9 examples/sec; 0.163 sec/batch)
2016-11-28 19:16:36.127987: step 21000, loss = 0.86 (780.6 examples/sec; 0.164 sec/batch)
2016-11-28 19:16:38.285439: step 21010, loss = 0.74 (782.1 examples/sec; 0.164 sec/batch)
2016-11-28 19:16:39.848522: step 21020, loss = 0.74 (809.2 examples/sec; 0.158 sec/batch)
2016-11-28 19:16:41.421234: step 21030, loss = 0.77 (808.7 examples/sec; 0.158 sec/batch)
2016-11-28 19:16:42.982110: step 21040, loss = 0.79 (789.3 examples/sec; 0.162 sec/batch)
2016-11-28 19:16:44.534824: step 21050, loss = 0.74 (813.3 examples/sec; 0.157 sec/batch)
2016-11-28 19:16:46.110414: step 21060, loss = 0.89 (800.8 examples/sec; 0.160 sec/batch)
2016-11-28 19:16:47.674452: step 21070, loss = 0.73 (796.4 examples/sec; 0.161 sec/batch)
2016-11-28 19:16:49.246168: step 21080, loss = 0.86 (808.0 examples/sec; 0.158 sec/batch)
2016-11-28 19:16:50.818529: step 21090, loss = 0.78 (817.1 examples/sec; 0.157 sec/batch)
2016-11-28 19:16:52.381374: step 21100, loss = 0.89 (841.6 examples/sec; 0.152 sec/batch)
2016-11-28 19:16:54.134133: step 21110, loss = 0.85 (805.9 examples/sec; 0.159 sec/batch)
2016-11-28 19:16:55.680862: step 21120, loss = 0.77 (806.6 examples/sec; 0.159 sec/batch)
2016-11-28 19:16:57.235737: step 21130, loss = 0.73 (844.8 examples/sec; 0.152 sec/batch)
2016-11-28 19:16:58.786096: step 21140, loss = 0.78 (809.2 examples/sec; 0.158 sec/batch)
2016-11-28 19:17:00.345662: step 21150, loss = 0.81 (794.6 examples/sec; 0.161 sec/batch)
2016-11-28 19:17:01.893526: step 21160, loss = 0.87 (802.8 examples/sec; 0.159 sec/batch)
2016-11-28 19:17:03.437636: step 21170, loss = 0.86 (833.3 examples/sec; 0.154 sec/batch)
2016-11-28 19:17:04.986986: step 21180, loss = 0.89 (801.0 examples/sec; 0.160 sec/batch)
2016-11-28 19:17:06.539988: step 21190, loss = 0.87 (841.7 examples/sec; 0.152 sec/batch)
2016-11-28 19:17:08.098608: step 21200, loss = 0.72 (792.9 examples/sec; 0.161 sec/batch)
2016-11-28 19:17:09.867003: step 21210, loss = 0.75 (837.2 examples/sec; 0.153 sec/batch)
2016-11-28 19:17:11.420533: step 21220, loss = 0.61 (831.1 examples/sec; 0.154 sec/batch)
2016-11-28 19:17:13.040591: step 21230, loss = 0.75 (778.1 examples/sec; 0.165 sec/batch)
2016-11-28 19:17:14.664705: step 21240, loss = 0.96 (777.4 examples/sec; 0.165 sec/batch)
2016-11-28 19:17:16.270884: step 21250, loss = 0.80 (790.6 examples/sec; 0.162 sec/batch)
2016-11-28 19:17:17.890341: step 21260, loss = 0.92 (770.5 examples/sec; 0.166 sec/batch)
2016-11-28 19:17:19.507370: step 21270, loss = 0.77 (775.5 examples/sec; 0.165 sec/batch)
2016-11-28 19:17:21.118442: step 21280, loss = 0.88 (838.0 examples/sec; 0.153 sec/batch)
2016-11-28 19:17:22.741509: step 21290, loss = 0.77 (796.0 examples/sec; 0.161 sec/batch)
2016-11-28 19:17:24.364244: step 21300, loss = 0.80 (806.6 examples/sec; 0.159 sec/batch)
2016-11-28 19:17:26.193830: step 21310, loss = 0.77 (828.5 examples/sec; 0.155 sec/batch)
2016-11-28 19:17:27.829072: step 21320, loss = 0.71 (788.3 examples/sec; 0.162 sec/batch)
2016-11-28 19:17:29.462397: step 21330, loss = 0.82 (761.8 examples/sec; 0.168 sec/batch)
2016-11-28 19:17:31.073182: step 21340, loss = 0.71 (771.0 examples/sec; 0.166 sec/batch)
2016-11-28 19:17:32.696924: step 21350, loss = 0.79 (799.7 examples/sec; 0.160 sec/batch)
2016-11-28 19:17:34.311882: step 21360, loss = 0.87 (799.9 examples/sec; 0.160 sec/batch)
2016-11-28 19:17:35.924522: step 21370, loss = 0.85 (774.5 examples/sec; 0.165 sec/batch)
2016-11-28 19:17:37.548635: step 21380, loss = 0.84 (784.9 examples/sec; 0.163 sec/batch)
2016-11-28 19:17:39.171942: step 21390, loss = 0.88 (783.3 examples/sec; 0.163 sec/batch)
2016-11-28 19:17:40.778291: step 21400, loss = 0.81 (809.8 examples/sec; 0.158 sec/batch)
2016-11-28 19:17:42.596988: step 21410, loss = 0.73 (799.8 examples/sec; 0.160 sec/batch)
2016-11-28 19:17:44.216691: step 21420, loss = 0.94 (786.5 examples/sec; 0.163 sec/batch)
2016-11-28 19:17:45.840073: step 21430, loss = 0.95 (766.3 examples/sec; 0.167 sec/batch)
2016-11-28 19:17:47.465760: step 21440, loss = 0.88 (793.5 examples/sec; 0.161 sec/batch)
2016-11-28 19:17:49.081091: step 21450, loss = 0.65 (806.6 examples/sec; 0.159 sec/batch)
2016-11-28 19:17:50.697602: step 21460, loss = 0.80 (791.8 examples/sec; 0.162 sec/batch)
2016-11-28 19:17:52.312590: step 21470, loss = 0.83 (789.6 examples/sec; 0.162 sec/batch)
2016-11-28 19:17:53.941144: step 21480, loss = 0.82 (770.8 examples/sec; 0.166 sec/batch)
2016-11-28 19:17:55.560810: step 21490, loss = 0.84 (802.3 examples/sec; 0.160 sec/batch)
2016-11-28 19:17:57.177663: step 21500, loss = 0.87 (803.0 examples/sec; 0.159 sec/batch)
2016-11-28 19:17:59.014760: step 21510, loss = 0.80 (811.1 examples/sec; 0.158 sec/batch)
2016-11-28 19:18:00.639476: step 21520, loss = 0.77 (799.9 examples/sec; 0.160 sec/batch)
2016-11-28 19:18:02.257426: step 21530, loss = 0.90 (763.8 examples/sec; 0.168 sec/batch)
2016-11-28 19:18:03.875023: step 21540, loss = 1.06 (819.5 examples/sec; 0.156 sec/batch)
2016-11-28 19:18:05.488204: step 21550, loss = 0.97 (818.7 examples/sec; 0.156 sec/batch)
2016-11-28 19:18:07.109760: step 21560, loss = 0.80 (780.1 examples/sec; 0.164 sec/batch)
2016-11-28 19:18:08.716113: step 21570, loss = 0.77 (811.8 examples/sec; 0.158 sec/batch)
2016-11-28 19:18:10.340194: step 21580, loss = 0.96 (776.7 examples/sec; 0.165 sec/batch)
2016-11-28 19:18:11.955694: step 21590, loss = 0.84 (811.0 examples/sec; 0.158 sec/batch)
2016-11-28 19:18:13.586290: step 21600, loss = 0.82 (784.5 examples/sec; 0.163 sec/batch)
2016-11-28 19:18:15.375913: step 21610, loss = 0.80 (797.2 examples/sec; 0.161 sec/batch)
2016-11-28 19:18:16.971158: step 21620, loss = 0.79 (803.4 examples/sec; 0.159 sec/batch)
2016-11-28 19:18:18.550495: step 21630, loss = 0.89 (814.3 examples/sec; 0.157 sec/batch)
2016-11-28 19:18:20.164219: step 21640, loss = 0.72 (798.6 examples/sec; 0.160 sec/batch)
2016-11-28 19:18:21.786502: step 21650, loss = 1.05 (768.1 examples/sec; 0.167 sec/batch)
2016-11-28 19:18:23.406561: step 21660, loss = 0.79 (769.8 examples/sec; 0.166 sec/batch)
2016-11-28 19:18:25.030207: step 21670, loss = 0.82 (786.0 examples/sec; 0.163 sec/batch)
2016-11-28 19:18:26.642004: step 21680, loss = 0.88 (802.2 examples/sec; 0.160 sec/batch)
2016-11-28 19:18:28.264700: step 21690, loss = 0.63 (779.6 examples/sec; 0.164 sec/batch)
2016-11-28 19:18:29.896444: step 21700, loss = 0.88 (793.8 examples/sec; 0.161 sec/batch)
2016-11-28 19:18:31.737841: step 21710, loss = 0.82 (744.7 examples/sec; 0.172 sec/batch)
2016-11-28 19:18:33.360868: step 21720, loss = 0.77 (768.7 examples/sec; 0.167 sec/batch)
2016-11-28 19:18:35.000421: step 21730, loss = 0.67 (784.7 examples/sec; 0.163 sec/batch)
2016-11-28 19:18:36.624025: step 21740, loss = 0.69 (804.3 examples/sec; 0.159 sec/batch)
2016-11-28 19:18:38.247069: step 21750, loss = 0.79 (815.4 examples/sec; 0.157 sec/batch)
2016-11-28 19:18:39.875339: step 21760, loss = 0.94 (780.8 examples/sec; 0.164 sec/batch)
2016-11-28 19:18:41.495069: step 21770, loss = 0.78 (804.2 examples/sec; 0.159 sec/batch)
2016-11-28 19:18:43.123165: step 21780, loss = 0.85 (790.7 examples/sec; 0.162 sec/batch)
2016-11-28 19:18:44.746834: step 21790, loss = 0.74 (798.1 examples/sec; 0.160 sec/batch)
2016-11-28 19:18:46.371550: step 21800, loss = 0.77 (803.1 examples/sec; 0.159 sec/batch)
2016-11-28 19:18:48.171308: step 21810, loss = 0.77 (800.7 examples/sec; 0.160 sec/batch)
2016-11-28 19:18:49.777461: step 21820, loss = 0.80 (790.3 examples/sec; 0.162 sec/batch)
2016-11-28 19:18:51.382599: step 21830, loss = 0.81 (815.9 examples/sec; 0.157 sec/batch)
2016-11-28 19:18:52.981847: step 21840, loss = 0.71 (779.2 examples/sec; 0.164 sec/batch)
2016-11-28 19:18:54.580377: step 21850, loss = 0.80 (804.5 examples/sec; 0.159 sec/batch)
2016-11-28 19:18:56.191696: step 21860, loss = 0.74 (791.1 examples/sec; 0.162 sec/batch)
2016-11-28 19:18:57.788814: step 21870, loss = 0.80 (770.6 examples/sec; 0.166 sec/batch)
2016-11-28 19:18:59.376718: step 21880, loss = 0.90 (805.6 examples/sec; 0.159 sec/batch)
2016-11-28 19:19:00.980961: step 21890, loss = 0.58 (828.5 examples/sec; 0.154 sec/batch)
2016-11-28 19:19:02.579234: step 21900, loss = 0.87 (804.3 examples/sec; 0.159 sec/batch)
2016-11-28 19:19:04.336624: step 21910, loss = 0.80 (837.5 examples/sec; 0.153 sec/batch)
2016-11-28 19:19:05.902100: step 21920, loss = 0.95 (807.4 examples/sec; 0.159 sec/batch)
2016-11-28 19:19:07.471464: step 21930, loss = 0.91 (785.5 examples/sec; 0.163 sec/batch)
2016-11-28 19:19:09.068337: step 21940, loss = 0.76 (822.6 examples/sec; 0.156 sec/batch)
2016-11-28 19:19:10.668326: step 21950, loss = 0.78 (777.5 examples/sec; 0.165 sec/batch)
2016-11-28 19:19:12.257295: step 21960, loss = 0.72 (803.7 examples/sec; 0.159 sec/batch)
2016-11-28 19:19:13.859782: step 21970, loss = 0.77 (768.6 examples/sec; 0.167 sec/batch)
2016-11-28 19:19:15.449991: step 21980, loss = 0.85 (801.6 examples/sec; 0.160 sec/batch)
2016-11-28 19:19:17.052148: step 21990, loss = 0.80 (796.4 examples/sec; 0.161 sec/batch)
2016-11-28 19:19:18.654761: step 22000, loss = 0.69 (803.4 examples/sec; 0.159 sec/batch)
2016-11-28 19:19:20.864379: step 22010, loss = 0.85 (792.5 examples/sec; 0.162 sec/batch)
2016-11-28 19:19:22.440266: step 22020, loss = 0.85 (802.0 examples/sec; 0.160 sec/batch)
2016-11-28 19:19:24.002835: step 22030, loss = 0.74 (818.5 examples/sec; 0.156 sec/batch)
2016-11-28 19:19:25.562110: step 22040, loss = 0.85 (814.2 examples/sec; 0.157 sec/batch)
2016-11-28 19:19:27.126748: step 22050, loss = 0.80 (837.0 examples/sec; 0.153 sec/batch)
2016-11-28 19:19:28.698629: step 22060, loss = 0.81 (823.3 examples/sec; 0.155 sec/batch)
2016-11-28 19:19:30.271098: step 22070, loss = 1.05 (802.5 examples/sec; 0.160 sec/batch)
2016-11-28 19:19:31.836916: step 22080, loss = 0.75 (802.9 examples/sec; 0.159 sec/batch)
2016-11-28 19:19:33.402633: step 22090, loss = 0.78 (823.1 examples/sec; 0.156 sec/batch)
2016-11-28 19:19:34.948974: step 22100, loss = 0.84 (825.3 examples/sec; 0.155 sec/batch)
2016-11-28 19:19:36.698170: step 22110, loss = 0.79 (859.4 examples/sec; 0.149 sec/batch)
2016-11-28 19:19:38.258414: step 22120, loss = 0.75 (793.3 examples/sec; 0.161 sec/batch)
2016-11-28 19:19:39.812651: step 22130, loss = 0.72 (838.9 examples/sec; 0.153 sec/batch)
2016-11-28 19:19:41.360024: step 22140, loss = 0.78 (838.5 examples/sec; 0.153 sec/batch)
2016-11-28 19:19:42.916066: step 22150, loss = 0.81 (828.9 examples/sec; 0.154 sec/batch)
2016-11-28 19:19:44.478494: step 22160, loss = 0.81 (785.7 examples/sec; 0.163 sec/batch)
2016-11-28 19:19:46.028233: step 22170, loss = 0.71 (805.9 examples/sec; 0.159 sec/batch)
2016-11-28 19:19:47.570361: step 22180, loss = 0.87 (849.3 examples/sec; 0.151 sec/batch)
2016-11-28 19:19:49.139367: step 22190, loss = 0.82 (765.8 examples/sec; 0.167 sec/batch)
2016-11-28 19:19:50.689287: step 22200, loss = 0.77 (800.0 examples/sec; 0.160 sec/batch)
2016-11-28 19:19:52.441875: step 22210, loss = 0.79 (827.4 examples/sec; 0.155 sec/batch)
2016-11-28 19:19:54.012596: step 22220, loss = 0.82 (828.5 examples/sec; 0.154 sec/batch)
2016-11-28 19:19:55.564905: step 22230, loss = 0.83 (827.8 examples/sec; 0.155 sec/batch)
2016-11-28 19:19:57.127439: step 22240, loss = 1.09 (808.9 examples/sec; 0.158 sec/batch)
2016-11-28 19:19:58.695852: step 22250, loss = 0.83 (812.9 examples/sec; 0.157 sec/batch)
2016-11-28 19:20:00.244134: step 22260, loss = 0.85 (850.2 examples/sec; 0.151 sec/batch)
2016-11-28 19:20:01.812102: step 22270, loss = 0.64 (794.7 examples/sec; 0.161 sec/batch)
2016-11-28 19:20:03.374661: step 22280, loss = 0.86 (774.0 examples/sec; 0.165 sec/batch)
2016-11-28 19:20:04.957681: step 22290, loss = 0.79 (845.0 examples/sec; 0.151 sec/batch)
2016-11-28 19:20:06.570390: step 22300, loss = 0.75 (779.3 examples/sec; 0.164 sec/batch)
2016-11-28 19:20:08.337158: step 22310, loss = 0.79 (864.7 examples/sec; 0.148 sec/batch)
2016-11-28 19:20:09.910496: step 22320, loss = 0.89 (788.7 examples/sec; 0.162 sec/batch)
^CTraceback (most recent call last):
  File "cifar10_multi_gpu_train.py", line 271, in <module>
    tf.app.run()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File "cifar10_multi_gpu_train.py", line 267, in main
    train()
  File "cifar10_multi_gpu_train.py", line 237, in train
    _, loss_value = sess.run([train_op, loss])
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 717, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 915, in _run
    feed_dict_string, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 965, in _do_run
    target_list, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 972, in _do_call
    return fn(*args)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 954, in _run_fn
    status, run_metadata)
KeyboardInterrupt

real    61m38.976s
user    133m45.504s
sys     27m47.595s
ubuntu@ip-172-31-38-101:~/ecse420-project/cifar10$ python cifar10_multi_gpu_train.py --num_gpus=1