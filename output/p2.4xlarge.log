I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.7.5 locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties:
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:0f.0
Total memory: 11.25GiB
Free memory: 11.13GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x2db48f0
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties:
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:11.0
Total memory: 11.25GiB
Free memory: 11.13GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x31c4500
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 2 with properties:
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:12.0
Total memory: 11.25GiB
Free memory: 11.13GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x35d39f0
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 3 with properties:
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:13.0
Total memory: 11.25GiB
Free memory: 11.13GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 1 2 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y Y Y Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 1:   Y Y Y Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 2:   Y Y Y Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 3:   Y Y Y Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:0f.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:00:11.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K80, pci bus id: 0000:00:12.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K80, pci bus id: 0000:00:13.0)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2016-11-29 01:59:05.968231: step 0, loss = 4.68 (3.8 examples/sec; 33.803 sec/batch)
2016-11-29 01:59:08.760898: step 10, loss = 4.66 (540.1 examples/sec; 0.237 sec/batch)
2016-11-29 01:59:11.203018: step 20, loss = 4.55 (513.8 examples/sec; 0.249 sec/batch)
2016-11-29 01:59:13.677032: step 30, loss = 4.48 (522.1 examples/sec; 0.245 sec/batch)
2016-11-29 01:59:16.136998: step 40, loss = 4.33 (521.0 examples/sec; 0.246 sec/batch)
2016-11-29 01:59:18.495096: step 50, loss = 4.24 (512.1 examples/sec; 0.250 sec/batch)
2016-11-29 01:59:20.958053: step 60, loss = 4.32 (521.7 examples/sec; 0.245 sec/batch)
2016-11-29 01:59:23.370318: step 70, loss = 4.26 (590.3 examples/sec; 0.217 sec/batch)
2016-11-29 01:59:25.808959: step 80, loss = 4.31 (535.7 examples/sec; 0.239 sec/batch)
2016-11-29 01:59:28.222859: step 90, loss = 4.26 (522.5 examples/sec; 0.245 sec/batch)
2016-11-29 01:59:30.600084: step 100, loss = 4.12 (515.9 examples/sec; 0.248 sec/batch)
2016-11-29 01:59:33.239154: step 110, loss = 4.29 (520.7 examples/sec; 0.246 sec/batch)
2016-11-29 01:59:35.618866: step 120, loss = 4.03 (540.0 examples/sec; 0.237 sec/batch)
2016-11-29 01:59:37.997173: step 130, loss = 4.00 (464.2 examples/sec; 0.276 sec/batch)
2016-11-29 01:59:40.376111: step 140, loss = 4.11 (592.4 examples/sec; 0.216 sec/batch)
2016-11-29 01:59:42.745294: step 150, loss = 3.97 (536.2 examples/sec; 0.239 sec/batch)
2016-11-29 01:59:45.089852: step 160, loss = 3.93 (582.3 examples/sec; 0.220 sec/batch)
2016-11-29 01:59:47.512220: step 170, loss = 3.83 (519.5 examples/sec; 0.246 sec/batch)
2016-11-29 01:59:49.938594: step 180, loss = 3.95 (515.6 examples/sec; 0.248 sec/batch)
2016-11-29 01:59:52.373326: step 190, loss = 3.76 (512.8 examples/sec; 0.250 sec/batch)
2016-11-29 01:59:54.783222: step 200, loss = 3.75 (536.4 examples/sec; 0.239 sec/batch)
2016-11-29 01:59:57.378193: step 210, loss = 3.85 (504.0 examples/sec; 0.254 sec/batch)
2016-11-29 01:59:59.786407: step 220, loss = 3.74 (566.3 examples/sec; 0.226 sec/batch)
2016-11-29 02:00:02.147106: step 230, loss = 3.77 (549.5 examples/sec; 0.233 sec/batch)
2016-11-29 02:00:04.555231: step 240, loss = 3.94 (538.9 examples/sec; 0.237 sec/batch)
2016-11-29 02:00:06.970891: step 250, loss = 3.75 (527.0 examples/sec; 0.243 sec/batch)
2016-11-29 02:00:09.354480: step 260, loss = 3.61 (532.9 examples/sec; 0.240 sec/batch)
2016-11-29 02:00:11.728506: step 270, loss = 3.62 (531.9 examples/sec; 0.241 sec/batch)
2016-11-29 02:00:14.123203: step 280, loss = 3.80 (544.5 examples/sec; 0.235 sec/batch)
2016-11-29 02:00:16.450158: step 290, loss = 3.50 (524.2 examples/sec; 0.244 sec/batch)
2016-11-29 02:00:18.767895: step 300, loss = 3.49 (547.5 examples/sec; 0.234 sec/batch)
2016-11-29 02:00:21.432170: step 310, loss = 3.51 (504.4 examples/sec; 0.254 sec/batch)
2016-11-29 02:00:23.797124: step 320, loss = 3.71 (589.6 examples/sec; 0.217 sec/batch)
2016-11-29 02:00:26.246260: step 330, loss = 3.51 (476.8 examples/sec; 0.268 sec/batch)
2016-11-29 02:00:28.651240: step 340, loss = 3.54 (488.8 examples/sec; 0.262 sec/batch)
2016-11-29 02:00:31.036567: step 350, loss = 3.42 (518.5 examples/sec; 0.247 sec/batch)
2016-11-29 02:00:33.416977: step 360, loss = 3.44 (522.4 examples/sec; 0.245 sec/batch)
2016-11-29 02:00:35.777247: step 370, loss = 3.29 (539.5 examples/sec; 0.237 sec/batch)
2016-11-29 02:00:38.131859: step 380, loss = 3.49 (499.0 examples/sec; 0.257 sec/batch)
2016-11-29 02:00:40.503507: step 390, loss = 3.42 (545.6 examples/sec; 0.235 sec/batch)
2016-11-29 02:00:42.903815: step 400, loss = 3.54 (519.8 examples/sec; 0.246 sec/batch)
2016-11-29 02:00:45.469015: step 410, loss = 3.61 (547.7 examples/sec; 0.234 sec/batch)
2016-11-29 02:00:47.849393: step 420, loss = 3.24 (536.1 examples/sec; 0.239 sec/batch)
2016-11-29 02:00:50.222400: step 430, loss = 3.22 (544.9 examples/sec; 0.235 sec/batch)
2016-11-29 02:00:52.619756: step 440, loss = 3.35 (521.7 examples/sec; 0.245 sec/batch)
2016-11-29 02:00:54.878294: step 450, loss = 3.41 (646.5 examples/sec; 0.198 sec/batch)
2016-11-29 02:00:57.247268: step 460, loss = 3.25 (543.7 examples/sec; 0.235 sec/batch)
2016-11-29 02:00:59.698678: step 470, loss = 3.35 (477.5 examples/sec; 0.268 sec/batch)
2016-11-29 02:01:02.037357: step 480, loss = 3.29 (550.8 examples/sec; 0.232 sec/batch)
2016-11-29 02:01:04.342540: step 490, loss = 3.30 (531.5 examples/sec; 0.241 sec/batch)
2016-11-29 02:01:06.721440: step 500, loss = 3.21 (498.8 examples/sec; 0.257 sec/batch)
2016-11-29 02:01:09.315912: step 510, loss = 3.08 (544.6 examples/sec; 0.235 sec/batch)
2016-11-29 02:01:11.642568: step 520, loss = 3.00 (555.6 examples/sec; 0.230 sec/batch)
2016-11-29 02:01:14.038375: step 530, loss = 3.04 (513.0 examples/sec; 0.250 sec/batch)
2016-11-29 02:01:16.445727: step 540, loss = 3.20 (526.4 examples/sec; 0.243 sec/batch)
2016-11-29 02:01:18.832494: step 550, loss = 3.13 (539.7 examples/sec; 0.237 sec/batch)
2016-11-29 02:01:21.236668: step 560, loss = 2.99 (522.7 examples/sec; 0.245 sec/batch)
2016-11-29 02:01:23.614281: step 570, loss = 3.14 (565.0 examples/sec; 0.227 sec/batch)
2016-11-29 02:01:26.001965: step 580, loss = 3.01 (534.9 examples/sec; 0.239 sec/batch)
2016-11-29 02:01:28.374109: step 590, loss = 3.41 (538.3 examples/sec; 0.238 sec/batch)
2016-11-29 02:01:30.811680: step 600, loss = 2.88 (577.8 examples/sec; 0.222 sec/batch)
2016-11-29 02:01:33.378911: step 610, loss = 3.08 (519.1 examples/sec; 0.247 sec/batch)
2016-11-29 02:01:35.747746: step 620, loss = 2.91 (523.7 examples/sec; 0.244 sec/batch)
2016-11-29 02:01:38.047016: step 630, loss = 2.85 (537.5 examples/sec; 0.238 sec/batch)
2016-11-29 02:01:40.435011: step 640, loss = 2.94 (534.7 examples/sec; 0.239 sec/batch)
2016-11-29 02:01:42.782077: step 650, loss = 2.75 (540.5 examples/sec; 0.237 sec/batch)
2016-11-29 02:01:45.139138: step 660, loss = 2.82 (531.5 examples/sec; 0.241 sec/batch)
2016-11-29 02:01:47.514063: step 670, loss = 3.16 (546.0 examples/sec; 0.234 sec/batch)
2016-11-29 02:01:49.913199: step 680, loss = 2.70 (452.2 examples/sec; 0.283 sec/batch)
2016-11-29 02:01:52.236492: step 690, loss = 2.85 (518.7 examples/sec; 0.247 sec/batch)
2016-11-29 02:01:54.640117: step 700, loss = 3.03 (539.1 examples/sec; 0.237 sec/batch)
2016-11-29 02:01:57.311695: step 710, loss = 2.68 (456.9 examples/sec; 0.280 sec/batch)
2016-11-29 02:01:59.567993: step 720, loss = 2.85 (536.5 examples/sec; 0.239 sec/batch)
2016-11-29 02:02:01.969725: step 730, loss = 2.89 (587.4 examples/sec; 0.218 sec/batch)
2016-11-29 02:02:04.306807: step 740, loss = 2.67 (535.3 examples/sec; 0.239 sec/batch)
2016-11-29 02:02:06.636938: step 750, loss = 2.63 (540.2 examples/sec; 0.237 sec/batch)
2016-11-29 02:02:09.002647: step 760, loss = 2.64 (474.5 examples/sec; 0.270 sec/batch)
2016-11-29 02:02:11.319867: step 770, loss = 2.89 (544.3 examples/sec; 0.235 sec/batch)
2016-11-29 02:02:13.708635: step 780, loss = 2.71 (594.8 examples/sec; 0.215 sec/batch)
2016-11-29 02:02:15.996724: step 790, loss = 2.54 (523.2 examples/sec; 0.245 sec/batch)
2016-11-29 02:02:18.332120: step 800, loss = 2.56 (539.8 examples/sec; 0.237 sec/batch)
2016-11-29 02:02:20.925586: step 810, loss = 2.67 (534.2 examples/sec; 0.240 sec/batch)
2016-11-29 02:02:23.198560: step 820, loss = 2.75 (566.1 examples/sec; 0.226 sec/batch)
2016-11-29 02:02:25.604896: step 830, loss = 2.78 (441.6 examples/sec; 0.290 sec/batch)
2016-11-29 02:02:27.867395: step 840, loss = 2.96 (537.2 examples/sec; 0.238 sec/batch)
2016-11-29 02:02:30.286625: step 850, loss = 2.69 (559.7 examples/sec; 0.229 sec/batch)
2016-11-29 02:02:32.678298: step 860, loss = 2.84 (526.1 examples/sec; 0.243 sec/batch)
2016-11-29 02:02:35.065486: step 870, loss = 2.92 (546.6 examples/sec; 0.234 sec/batch)
2016-11-29 02:02:37.423247: step 880, loss = 2.72 (530.9 examples/sec; 0.241 sec/batch)
2016-11-29 02:02:39.802198: step 890, loss = 2.58 (552.2 examples/sec; 0.232 sec/batch)
2016-11-29 02:02:42.142448: step 900, loss = 2.42 (568.4 examples/sec; 0.225 sec/batch)
2016-11-29 02:02:44.760660: step 910, loss = 2.60 (555.4 examples/sec; 0.230 sec/batch)
2016-11-29 02:02:47.012884: step 920, loss = 2.55 (538.4 examples/sec; 0.238 sec/batch)
2016-11-29 02:02:49.368663: step 930, loss = 2.56 (534.2 examples/sec; 0.240 sec/batch)
2016-11-29 02:02:51.775814: step 940, loss = 2.39 (631.8 examples/sec; 0.203 sec/batch)
2016-11-29 02:02:54.069048: step 950, loss = 2.51 (528.7 examples/sec; 0.242 sec/batch)
2016-11-29 02:02:56.473983: step 960, loss = 2.48 (543.6 examples/sec; 0.235 sec/batch)
2016-11-29 02:02:58.864465: step 970, loss = 2.48 (538.4 examples/sec; 0.238 sec/batch)
2016-11-29 02:03:01.253553: step 980, loss = 2.51 (537.4 examples/sec; 0.238 sec/batch)
2016-11-29 02:03:03.597920: step 990, loss = 2.24 (535.0 examples/sec; 0.239 sec/batch)
2016-11-29 02:03:05.987626: step 1000, loss = 2.52 (551.5 examples/sec; 0.232 sec/batch)
2016-11-29 02:03:08.873901: step 1010, loss = 2.31 (549.7 examples/sec; 0.233 sec/batch)
2016-11-29 02:03:11.259766: step 1020, loss = 2.52 (521.7 examples/sec; 0.245 sec/batch)
2016-11-29 02:03:13.673211: step 1030, loss = 2.35 (506.6 examples/sec; 0.253 sec/batch)
2016-11-29 02:03:16.060497: step 1040, loss = 2.31 (524.9 examples/sec; 0.244 sec/batch)
2016-11-29 02:03:18.440577: step 1050, loss = 2.22 (531.5 examples/sec; 0.241 sec/batch)
2016-11-29 02:03:20.806263: step 1060, loss = 2.29 (577.6 examples/sec; 0.222 sec/batch)
2016-11-29 02:03:23.123043: step 1070, loss = 2.30 (725.4 examples/sec; 0.176 sec/batch)
2016-11-29 02:03:25.474368: step 1080, loss = 2.57 (516.6 examples/sec; 0.248 sec/batch)
2016-11-29 02:03:27.899122: step 1090, loss = 2.24 (560.5 examples/sec; 0.228 sec/batch)
2016-11-29 02:03:30.200946: step 1100, loss = 2.35 (557.3 examples/sec; 0.230 sec/batch)
2016-11-29 02:03:32.779069: step 1110, loss = 2.51 (543.5 examples/sec; 0.235 sec/batch)
2016-11-29 02:03:35.196372: step 1120, loss = 2.15 (546.7 examples/sec; 0.234 sec/batch)
2016-11-29 02:03:37.605584: step 1130, loss = 2.34 (532.3 examples/sec; 0.240 sec/batch)
2016-11-29 02:03:39.945076: step 1140, loss = 2.36 (532.6 examples/sec; 0.240 sec/batch)
2016-11-29 02:03:42.351371: step 1150, loss = 2.29 (528.1 examples/sec; 0.242 sec/batch)
2016-11-29 02:03:44.675762: step 1160, loss = 2.29 (576.3 examples/sec; 0.222 sec/batch)
2016-11-29 02:03:47.053703: step 1170, loss = 2.26 (538.8 examples/sec; 0.238 sec/batch)
2016-11-29 02:03:49.377122: step 1180, loss = 2.27 (594.3 examples/sec; 0.215 sec/batch)
2016-11-29 02:03:51.726533: step 1190, loss = 2.26 (647.5 examples/sec; 0.198 sec/batch)
2016-11-29 02:03:54.139124: step 1200, loss = 2.28 (520.7 examples/sec; 0.246 sec/batch)
2016-11-29 02:03:56.704818: step 1210, loss = 2.19 (527.0 examples/sec; 0.243 sec/batch)
2016-11-29 02:03:59.050961: step 1220, loss = 2.08 (569.6 examples/sec; 0.225 sec/batch)
2016-11-29 02:04:01.446819: step 1230, loss = 2.32 (526.9 examples/sec; 0.243 sec/batch)
2016-11-29 02:04:03.840227: step 1240, loss = 2.14 (551.4 examples/sec; 0.232 sec/batch)
2016-11-29 02:04:06.149757: step 1250, loss = 2.02 (532.9 examples/sec; 0.240 sec/batch)
2016-11-29 02:04:08.558241: step 1260, loss = 2.05 (526.4 examples/sec; 0.243 sec/batch)
2016-11-29 02:04:10.940848: step 1270, loss = 2.13 (551.3 examples/sec; 0.232 sec/batch)
2016-11-29 02:04:13.335276: step 1280, loss = 2.34 (545.8 examples/sec; 0.235 sec/batch)
2016-11-29 02:04:15.715332: step 1290, loss = 2.01 (543.1 examples/sec; 0.236 sec/batch)
2016-11-29 02:04:18.065887: step 1300, loss = 2.15 (583.5 examples/sec; 0.219 sec/batch)
2016-11-29 02:04:20.652887: step 1310, loss = 2.15 (550.2 examples/sec; 0.233 sec/batch)
2016-11-29 02:04:22.995957: step 1320, loss = 2.10 (582.3 examples/sec; 0.220 sec/batch)
2016-11-29 02:04:25.375625: step 1330, loss = 2.10 (523.0 examples/sec; 0.245 sec/batch)
2016-11-29 02:04:27.710889: step 1340, loss = 2.36 (549.5 examples/sec; 0.233 sec/batch)
2016-11-29 02:04:30.081869: step 1350, loss = 2.01 (525.2 examples/sec; 0.244 sec/batch)
2016-11-29 02:04:32.452252: step 1360, loss = 2.03 (572.1 examples/sec; 0.224 sec/batch)
2016-11-29 02:04:34.859392: step 1370, loss = 1.94 (519.2 examples/sec; 0.247 sec/batch)
2016-11-29 02:04:37.163376: step 1380, loss = 2.14 (564.0 examples/sec; 0.227 sec/batch)
2016-11-29 02:04:39.602125: step 1390, loss = 2.18 (491.2 examples/sec; 0.261 sec/batch)
2016-11-29 02:04:41.982413: step 1400, loss = 1.93 (460.2 examples/sec; 0.278 sec/batch)
2016-11-29 02:04:44.412451: step 1410, loss = 1.84 (540.0 examples/sec; 0.237 sec/batch)
2016-11-29 02:04:46.785107: step 1420, loss = 2.09 (567.5 examples/sec; 0.226 sec/batch)
2016-11-29 02:04:49.136501: step 1430, loss = 1.96 (525.8 examples/sec; 0.243 sec/batch)
2016-11-29 02:04:51.475429: step 1440, loss = 1.94 (530.1 examples/sec; 0.241 sec/batch)
2016-11-29 02:04:53.803748: step 1450, loss = 2.12 (521.9 examples/sec; 0.245 sec/batch)
2016-11-29 02:04:56.115384: step 1460, loss = 2.13 (551.1 examples/sec; 0.232 sec/batch)
2016-11-29 02:04:58.507361: step 1470, loss = 1.82 (532.1 examples/sec; 0.241 sec/batch)
2016-11-29 02:05:00.913295: step 1480, loss = 1.97 (525.7 examples/sec; 0.244 sec/batch)
2016-11-29 02:05:03.336247: step 1490, loss = 2.06 (517.1 examples/sec; 0.248 sec/batch)
2016-11-29 02:05:05.621050: step 1500, loss = 1.87 (544.7 examples/sec; 0.235 sec/batch)
2016-11-29 02:05:08.201256: step 1510, loss = 1.91 (543.3 examples/sec; 0.236 sec/batch)
2016-11-29 02:05:10.571006: step 1520, loss = 2.13 (545.8 examples/sec; 0.235 sec/batch)
2016-11-29 02:05:12.941104: step 1530, loss = 1.87 (640.2 examples/sec; 0.200 sec/batch)
2016-11-29 02:05:15.324994: step 1540, loss = 1.98 (543.2 examples/sec; 0.236 sec/batch)
2016-11-29 02:05:17.676544: step 1550, loss = 1.99 (528.7 examples/sec; 0.242 sec/batch)
2016-11-29 02:05:20.013121: step 1560, loss = 1.85 (637.8 examples/sec; 0.201 sec/batch)
2016-11-29 02:05:22.414268: step 1570, loss = 1.95 (518.4 examples/sec; 0.247 sec/batch)
2016-11-29 02:05:24.777419: step 1580, loss = 1.74 (586.8 examples/sec; 0.218 sec/batch)
2016-11-29 02:05:27.154487: step 1590, loss = 1.99 (531.0 examples/sec; 0.241 sec/batch)
2016-11-29 02:05:29.540092: step 1600, loss = 1.77 (529.6 examples/sec; 0.242 sec/batch)
2016-11-29 02:05:32.076714: step 1610, loss = 1.86 (550.2 examples/sec; 0.233 sec/batch)
2016-11-29 02:05:34.465883: step 1620, loss = 1.76 (521.9 examples/sec; 0.245 sec/batch)
2016-11-29 02:05:36.876262: step 1630, loss = 1.93 (523.9 examples/sec; 0.244 sec/batch)
2016-11-29 02:05:39.257096: step 1640, loss = 1.94 (538.6 examples/sec; 0.238 sec/batch)
2016-11-29 02:05:41.604846: step 1650, loss = 1.75 (533.6 examples/sec; 0.240 sec/batch)
2016-11-29 02:05:43.981572: step 1660, loss = 1.68 (532.3 examples/sec; 0.240 sec/batch)
2016-11-29 02:05:46.376996: step 1670, loss = 1.93 (609.7 examples/sec; 0.210 sec/batch)
2016-11-29 02:05:48.723903: step 1680, loss = 1.84 (480.4 examples/sec; 0.266 sec/batch)
2016-11-29 02:05:51.041923: step 1690, loss = 1.96 (533.1 examples/sec; 0.240 sec/batch)
2016-11-29 02:05:53.345682: step 1700, loss = 1.76 (477.8 examples/sec; 0.268 sec/batch)
2016-11-29 02:05:55.970575: step 1710, loss = 1.77 (514.4 examples/sec; 0.249 sec/batch)
2016-11-29 02:05:58.347351: step 1720, loss = 1.73 (518.9 examples/sec; 0.247 sec/batch)
2016-11-29 02:06:00.751598: step 1730, loss = 1.73 (527.2 examples/sec; 0.243 sec/batch)