I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.7.5 locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:0f.0
Total memory: 11.25GiB
Free memory: 11.13GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3563df0
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:10.0
Total memory: 11.25GiB
Free memory: 11.13GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3973b70
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 2 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:11.0
Total memory: 11.25GiB
Free memory: 11.13GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3d82f40
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 3 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:12.0
Total memory: 11.25GiB
Free memory: 11.13GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4195f20
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 4 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:13.0
Total memory: 11.25GiB
Free memory: 11.13GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x45acb00
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 5 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:14.0
Total memory: 11.25GiB
Free memory: 11.13GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x49c70f0
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 6 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:15.0
Total memory: 11.25GiB
Free memory: 11.13GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4de5820
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 7 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:16.0
Total memory: 11.25GiB
Free memory: 11.13GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 1 2 3 4 5 6 7 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y Y Y Y Y Y Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 1:   Y Y Y Y Y Y Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 2:   Y Y Y Y Y Y Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 3:   Y Y Y Y Y Y Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 4:   Y Y Y Y Y Y Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 5:   Y Y Y Y Y Y Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 6:   Y Y Y Y Y Y Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 7:   Y Y Y Y Y Y Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:0f.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:00:10.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K80, pci bus id: 0000:00:11.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K80, pci bus id: 0000:00:12.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:4) -> (device: 4, name: Tesla K80, pci bus id: 0000:00:13.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:5) -> (device: 5, name: Tesla K80, pci bus id: 0000:00:14.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:6) -> (device: 6, name: Tesla K80, pci bus id: 0000:00:15.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:7) -> (device: 7, name: Tesla K80, pci bus id: 0000:00:16.0)
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2016-11-29 00:02:52.352607: step 0, loss = 4.68 (6.7 examples/sec; 19.131 sec/batch)
2016-11-29 00:03:00.039430: step 10, loss = 4.60 (2146.5 examples/sec; 0.060 sec/batch)
2016-11-29 00:03:05.024682: step 20, loss = 4.44 (2125.4 examples/sec; 0.060 sec/batch)
2016-11-29 00:03:10.094651: step 30, loss = 4.34 (2011.2 examples/sec; 0.064 sec/batch)
2016-11-29 00:03:15.184014: step 40, loss = 4.30 (2108.4 examples/sec; 0.061 sec/batch)
2016-11-29 00:03:20.057074: step 50, loss = 4.22 (2065.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:03:25.135353: step 60, loss = 4.22 (2105.6 examples/sec; 0.061 sec/batch)
2016-11-29 00:03:30.175953: step 70, loss = 4.10 (2093.1 examples/sec; 0.061 sec/batch)
2016-11-29 00:03:35.173838: step 80, loss = 4.06 (2110.9 examples/sec; 0.061 sec/batch)
2016-11-29 00:03:40.211371: step 90, loss = 4.00 (2034.9 examples/sec; 0.063 sec/batch)
2016-11-29 00:03:45.255882: step 100, loss = 4.10 (2332.4 examples/sec; 0.055 sec/batch)
2016-11-29 00:03:50.797127: step 110, loss = 3.96 (1782.3 examples/sec; 0.072 sec/batch)
2016-11-29 00:03:55.766440: step 120, loss = 4.10 (2228.3 examples/sec; 0.057 sec/batch)
2016-11-29 00:04:00.895633: step 130, loss = 4.15 (2018.5 examples/sec; 0.063 sec/batch)
2016-11-29 00:04:05.976139: step 140, loss = 4.02 (2072.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:04:11.001043: step 150, loss = 3.94 (2076.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:04:16.049019: step 160, loss = 3.84 (1995.4 examples/sec; 0.064 sec/batch)
2016-11-29 00:04:21.078962: step 170, loss = 3.91 (1730.8 examples/sec; 0.074 sec/batch)
2016-11-29 00:04:26.123655: step 180, loss = 4.02 (1988.7 examples/sec; 0.064 sec/batch)
2016-11-29 00:04:31.208697: step 190, loss = 3.72 (2025.3 examples/sec; 0.063 sec/batch)
2016-11-29 00:04:36.306456: step 200, loss = 3.67 (2036.1 examples/sec; 0.063 sec/batch)
2016-11-29 00:04:41.785474: step 210, loss = 3.83 (2129.5 examples/sec; 0.060 sec/batch)
2016-11-29 00:04:46.821025: step 220, loss = 3.54 (2028.4 examples/sec; 0.063 sec/batch)
2016-11-29 00:04:51.910501: step 230, loss = 3.69 (2073.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:04:56.809856: step 240, loss = 3.68 (2249.6 examples/sec; 0.057 sec/batch)
2016-11-29 00:05:01.840835: step 250, loss = 3.92 (1967.5 examples/sec; 0.065 sec/batch)
2016-11-29 00:05:06.896623: step 260, loss = 3.49 (1999.3 examples/sec; 0.064 sec/batch)
2016-11-29 00:05:11.895221: step 270, loss = 3.61 (2052.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:05:16.978711: step 280, loss = 3.54 (2069.3 examples/sec; 0.062 sec/batch)
2016-11-29 00:05:22.124054: step 290, loss = 3.55 (2220.2 examples/sec; 0.058 sec/batch)
2016-11-29 00:05:27.048273: step 300, loss = 3.46 (2123.6 examples/sec; 0.060 sec/batch)
2016-11-29 00:05:32.713342: step 310, loss = 3.45 (2233.1 examples/sec; 0.057 sec/batch)
2016-11-29 00:05:37.772453: step 320, loss = 3.63 (2088.5 examples/sec; 0.061 sec/batch)
2016-11-29 00:05:42.788859: step 330, loss = 3.44 (2108.5 examples/sec; 0.061 sec/batch)
2016-11-29 00:05:47.910265: step 340, loss = 3.46 (1908.6 examples/sec; 0.067 sec/batch)
2016-11-29 00:05:53.200807: step 350, loss = 3.42 (1929.7 examples/sec; 0.066 sec/batch)
2016-11-29 00:05:58.175726: step 360, loss = 3.18 (2165.1 examples/sec; 0.059 sec/batch)
2016-11-29 00:06:03.223138: step 370, loss = 3.31 (2072.9 examples/sec; 0.062 sec/batch)
2016-11-29 00:06:08.451148: step 380, loss = 3.50 (2195.3 examples/sec; 0.058 sec/batch)
2016-11-29 00:06:13.396589: step 390, loss = 3.27 (2043.5 examples/sec; 0.063 sec/batch)
2016-11-29 00:06:18.380108: step 400, loss = 3.30 (2060.3 examples/sec; 0.062 sec/batch)
2016-11-29 00:06:24.020824: step 410, loss = 3.13 (1914.6 examples/sec; 0.067 sec/batch)
2016-11-29 00:06:29.032969: step 420, loss = 3.36 (1993.2 examples/sec; 0.064 sec/batch)
2016-11-29 00:06:34.112876: step 430, loss = 3.28 (1992.4 examples/sec; 0.064 sec/batch)
2016-11-29 00:06:39.248905: step 440, loss = 3.29 (1972.6 examples/sec; 0.065 sec/batch)
2016-11-29 00:06:44.186960: step 450, loss = 3.00 (2092.5 examples/sec; 0.061 sec/batch)
2016-11-29 00:06:49.289817: step 460, loss = 3.09 (1956.5 examples/sec; 0.065 sec/batch)
2016-11-29 00:06:54.397528: step 470, loss = 3.13 (2052.7 examples/sec; 0.062 sec/batch)
2016-11-29 00:06:59.251034: step 480, loss = 3.04 (2138.0 examples/sec; 0.060 sec/batch)
2016-11-29 00:07:04.430893: step 490, loss = 3.00 (1877.7 examples/sec; 0.068 sec/batch)
2016-11-29 00:07:09.423494: step 500, loss = 3.17 (2147.6 examples/sec; 0.060 sec/batch)
2016-11-29 00:07:14.960616: step 510, loss = 3.10 (1947.2 examples/sec; 0.066 sec/batch)
2016-11-29 00:07:20.032546: step 520, loss = 3.03 (1963.5 examples/sec; 0.065 sec/batch)
2016-11-29 00:07:25.198051: step 530, loss = 3.04 (2074.6 examples/sec; 0.062 sec/batch)
2016-11-29 00:07:30.181432: step 540, loss = 2.96 (2483.4 examples/sec; 0.052 sec/batch)
2016-11-29 00:07:35.437248: step 550, loss = 3.06 (1929.7 examples/sec; 0.066 sec/batch)
2016-11-29 00:07:40.443603: step 560, loss = 3.04 (2095.0 examples/sec; 0.061 sec/batch)
2016-11-29 00:07:45.566778: step 570, loss = 2.94 (1636.3 examples/sec; 0.078 sec/batch)
2016-11-29 00:07:50.535636: step 580, loss = 3.10 (2139.8 examples/sec; 0.060 sec/batch)
2016-11-29 00:07:55.598317: step 590, loss = 2.91 (2201.3 examples/sec; 0.058 sec/batch)
2016-11-29 00:08:00.629982: step 600, loss = 2.85 (1906.8 examples/sec; 0.067 sec/batch)
2016-11-29 00:08:06.219333: step 610, loss = 2.94 (2065.9 examples/sec; 0.062 sec/batch)
2016-11-29 00:08:11.316796: step 620, loss = 2.88 (1952.6 examples/sec; 0.066 sec/batch)
2016-11-29 00:08:16.139611: step 630, loss = 3.11 (1960.9 examples/sec; 0.065 sec/batch)
2016-11-29 00:08:21.148072: step 640, loss = 2.88 (2122.6 examples/sec; 0.060 sec/batch)
2016-11-29 00:08:26.286903: step 650, loss = 2.61 (2170.6 examples/sec; 0.059 sec/batch)
2016-11-29 00:08:31.304440: step 660, loss = 3.18 (2135.6 examples/sec; 0.060 sec/batch)
2016-11-29 00:08:36.417397: step 670, loss = 2.85 (1904.0 examples/sec; 0.067 sec/batch)
2016-11-29 00:08:41.520663: step 680, loss = 2.81 (2185.9 examples/sec; 0.059 sec/batch)
2016-11-29 00:08:46.522066: step 690, loss = 2.87 (1811.2 examples/sec; 0.071 sec/batch)
2016-11-29 00:08:51.587819: step 700, loss = 2.75 (2013.8 examples/sec; 0.064 sec/batch)
2016-11-29 00:08:57.181771: step 710, loss = 2.73 (2084.2 examples/sec; 0.061 sec/batch)
2016-11-29 00:09:02.316828: step 720, loss = 2.56 (1565.1 examples/sec; 0.082 sec/batch)
2016-11-29 00:09:07.330908: step 730, loss = 2.90 (2034.0 examples/sec; 0.063 sec/batch)
2016-11-29 00:09:12.473759: step 740, loss = 2.52 (2027.8 examples/sec; 0.063 sec/batch)
2016-11-29 00:09:17.539663: step 750, loss = 2.68 (2018.6 examples/sec; 0.063 sec/batch)
2016-11-29 00:09:22.451468: step 760, loss = 2.51 (2142.0 examples/sec; 0.060 sec/batch)
2016-11-29 00:09:27.574457: step 770, loss = 2.60 (2288.7 examples/sec; 0.056 sec/batch)
2016-11-29 00:09:32.643730: step 780, loss = 2.63 (2281.7 examples/sec; 0.056 sec/batch)
2016-11-29 00:09:37.579685: step 790, loss = 2.53 (2146.5 examples/sec; 0.060 sec/batch)
2016-11-29 00:09:42.742659: step 800, loss = 2.69 (1989.4 examples/sec; 0.064 sec/batch)
2016-11-29 00:09:48.420059: step 810, loss = 2.65 (1986.3 examples/sec; 0.064 sec/batch)
2016-11-29 00:09:53.366362: step 820, loss = 2.50 (2103.4 examples/sec; 0.061 sec/batch)
2016-11-29 00:09:58.526450: step 830, loss = 2.39 (2091.3 examples/sec; 0.061 sec/batch)
2016-11-29 00:10:03.617730: step 840, loss = 2.52 (2284.8 examples/sec; 0.056 sec/batch)
2016-11-29 00:10:08.625434: step 850, loss = 2.46 (2031.0 examples/sec; 0.063 sec/batch)
2016-11-29 00:10:13.691615: step 860, loss = 2.48 (2204.7 examples/sec; 0.058 sec/batch)
2016-11-29 00:10:18.791909: step 870, loss = 2.31 (1960.8 examples/sec; 0.065 sec/batch)
2016-11-29 00:10:23.659461: step 880, loss = 2.41 (2090.5 examples/sec; 0.061 sec/batch)
2016-11-29 00:10:28.693676: step 890, loss = 2.39 (2139.4 examples/sec; 0.060 sec/batch)
2016-11-29 00:10:33.849808: step 900, loss = 2.57 (2018.8 examples/sec; 0.063 sec/batch)
2016-11-29 00:10:39.310625: step 910, loss = 2.37 (2042.7 examples/sec; 0.063 sec/batch)
2016-11-29 00:10:44.343726: step 920, loss = 2.39 (2064.3 examples/sec; 0.062 sec/batch)
2016-11-29 00:10:49.438263: step 930, loss = 2.45 (1945.8 examples/sec; 0.066 sec/batch)
2016-11-29 00:10:54.354769: step 940, loss = 2.42 (1867.8 examples/sec; 0.069 sec/batch)
2016-11-29 00:10:59.473934: step 950, loss = 2.29 (2042.9 examples/sec; 0.063 sec/batch)
2016-11-29 00:11:04.535124: step 960, loss = 2.30 (2065.5 examples/sec; 0.062 sec/batch)
2016-11-29 00:11:09.598384: step 970, loss = 2.16 (1964.1 examples/sec; 0.065 sec/batch)
2016-11-29 00:11:14.541083: step 980, loss = 2.37 (2152.6 examples/sec; 0.059 sec/batch)
2016-11-29 00:11:19.663301: step 990, loss = 2.27 (2077.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:11:24.699772: step 1000, loss = 2.27 (2178.7 examples/sec; 0.059 sec/batch)
2016-11-29 00:11:32.951518: step 1010, loss = 2.32 (1559.5 examples/sec; 0.082 sec/batch)
2016-11-29 00:11:37.896682: step 1020, loss = 2.33 (1923.1 examples/sec; 0.067 sec/batch)
2016-11-29 00:11:43.111848: step 1030, loss = 2.39 (1992.6 examples/sec; 0.064 sec/batch)
2016-11-29 00:11:48.073956: step 1040, loss = 2.53 (2130.4 examples/sec; 0.060 sec/batch)
2016-11-29 00:11:53.165648: step 1050, loss = 2.31 (2057.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:11:58.180193: step 1060, loss = 2.20 (2136.4 examples/sec; 0.060 sec/batch)
2016-11-29 00:12:03.213233: step 1070, loss = 2.31 (1928.3 examples/sec; 0.066 sec/batch)
2016-11-29 00:12:08.356094: step 1080, loss = 2.07 (2073.8 examples/sec; 0.062 sec/batch)
2016-11-29 00:12:13.422174: step 1090, loss = 2.11 (2071.3 examples/sec; 0.062 sec/batch)
2016-11-29 00:12:18.660244: step 1100, loss = 2.16 (1523.1 examples/sec; 0.084 sec/batch)
2016-11-29 00:12:24.146577: step 1110, loss = 2.09 (2033.5 examples/sec; 0.063 sec/batch)
2016-11-29 00:12:29.303683: step 1120, loss = 2.06 (2026.3 examples/sec; 0.063 sec/batch)
2016-11-29 00:12:34.550135: step 1130, loss = 2.09 (1920.4 examples/sec; 0.067 sec/batch)
2016-11-29 00:12:39.525415: step 1140, loss = 2.28 (1968.9 examples/sec; 0.065 sec/batch)
2016-11-29 00:12:44.588072: step 1150, loss = 2.15 (2095.8 examples/sec; 0.061 sec/batch)
2016-11-29 00:12:49.744188: step 1160, loss = 2.10 (2046.7 examples/sec; 0.063 sec/batch)
2016-11-29 00:12:54.787879: step 1170, loss = 2.01 (2037.4 examples/sec; 0.063 sec/batch)
2016-11-29 00:12:59.838743: step 1180, loss = 2.04 (2008.1 examples/sec; 0.064 sec/batch)
2016-11-29 00:13:04.892611: step 1190, loss = 2.21 (2061.4 examples/sec; 0.062 sec/batch)
2016-11-29 00:13:09.927252: step 1200, loss = 2.13 (1955.0 examples/sec; 0.065 sec/batch)
2016-11-29 00:13:15.593934: step 1210, loss = 2.11 (2096.9 examples/sec; 0.061 sec/batch)
2016-11-29 00:13:20.775132: step 1220, loss = 1.94 (2084.4 examples/sec; 0.061 sec/batch)
2016-11-29 00:13:25.858960: step 1230, loss = 1.95 (1949.9 examples/sec; 0.066 sec/batch)
2016-11-29 00:13:30.986771: step 1240, loss = 1.96 (1972.3 examples/sec; 0.065 sec/batch)
2016-11-29 00:13:36.062611: step 1250, loss = 2.14 (2111.2 examples/sec; 0.061 sec/batch)
2016-11-29 00:13:41.355031: step 1260, loss = 1.77 (1526.4 examples/sec; 0.084 sec/batch)
2016-11-29 00:13:46.329323: step 1270, loss = 2.07 (2003.5 examples/sec; 0.064 sec/batch)
2016-11-29 00:13:51.436473: step 1280, loss = 2.06 (2125.7 examples/sec; 0.060 sec/batch)
2016-11-29 00:13:56.442642: step 1290, loss = 1.92 (2038.1 examples/sec; 0.063 sec/batch)
2016-11-29 00:14:01.657994: step 1300, loss = 2.25 (2216.9 examples/sec; 0.058 sec/batch)
2016-11-29 00:14:07.428791: step 1310, loss = 2.11 (2003.7 examples/sec; 0.064 sec/batch)
2016-11-29 00:14:12.427815: step 1320, loss = 1.93 (1927.9 examples/sec; 0.066 sec/batch)
2016-11-29 00:14:17.592284: step 1330, loss = 1.87 (1973.7 examples/sec; 0.065 sec/batch)
2016-11-29 00:14:22.734658: step 1340, loss = 1.99 (1911.9 examples/sec; 0.067 sec/batch)
2016-11-29 00:14:27.853764: step 1350, loss = 2.31 (1714.9 examples/sec; 0.075 sec/batch)
2016-11-29 00:14:32.937577: step 1360, loss = 1.90 (2037.6 examples/sec; 0.063 sec/batch)
2016-11-29 00:14:38.122972: step 1370, loss = 1.83 (1995.9 examples/sec; 0.064 sec/batch)
2016-11-29 00:14:43.327200: step 1380, loss = 1.90 (1613.7 examples/sec; 0.079 sec/batch)
2016-11-29 00:14:48.333166: step 1390, loss = 1.73 (2181.6 examples/sec; 0.059 sec/batch)
2016-11-29 00:14:53.481452: step 1400, loss = 1.84 (1977.9 examples/sec; 0.065 sec/batch)
2016-11-29 00:14:59.134047: step 1410, loss = 1.85 (2067.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:15:04.144816: step 1420, loss = 1.93 (2381.6 examples/sec; 0.054 sec/batch)
2016-11-29 00:15:09.263892: step 1430, loss = 2.09 (2014.4 examples/sec; 0.064 sec/batch)
2016-11-29 00:15:14.498761: step 1440, loss = 1.96 (2050.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:15:19.494305: step 1450, loss = 1.81 (1985.4 examples/sec; 0.064 sec/batch)
2016-11-29 00:15:24.618753: step 1460, loss = 2.04 (1974.1 examples/sec; 0.065 sec/batch)
2016-11-29 00:15:29.680408: step 1470, loss = 1.77 (1960.3 examples/sec; 0.065 sec/batch)
2016-11-29 00:15:34.682144: step 1480, loss = 1.83 (2304.1 examples/sec; 0.056 sec/batch)
2016-11-29 00:15:39.842991: step 1490, loss = 1.87 (2122.9 examples/sec; 0.060 sec/batch)
2016-11-29 00:15:44.897519: step 1500, loss = 1.66 (1993.2 examples/sec; 0.064 sec/batch)
2016-11-29 00:15:50.405976: step 1510, loss = 1.80 (1934.5 examples/sec; 0.066 sec/batch)
2016-11-29 00:15:55.582144: step 1520, loss = 1.89 (2153.4 examples/sec; 0.059 sec/batch)
2016-11-29 00:16:00.719076: step 1530, loss = 1.69 (1459.9 examples/sec; 0.088 sec/batch)
2016-11-29 00:16:05.805168: step 1540, loss = 1.64 (1939.3 examples/sec; 0.066 sec/batch)
2016-11-29 00:16:10.863679: step 1550, loss = 1.66 (2078.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:16:15.954052: step 1560, loss = 1.79 (2276.0 examples/sec; 0.056 sec/batch)
2016-11-29 00:16:20.942855: step 1570, loss = 1.82 (1949.5 examples/sec; 0.066 sec/batch)
2016-11-29 00:16:26.045536: step 1580, loss = 1.80 (2214.3 examples/sec; 0.058 sec/batch)
2016-11-29 00:16:31.210385: step 1590, loss = 1.55 (1946.8 examples/sec; 0.066 sec/batch)
2016-11-29 00:16:36.172462: step 1600, loss = 1.76 (2211.7 examples/sec; 0.058 sec/batch)
2016-11-29 00:16:41.923078: step 1610, loss = 1.64 (2071.7 examples/sec; 0.062 sec/batch)
2016-11-29 00:16:47.054040: step 1620, loss = 1.78 (2142.0 examples/sec; 0.060 sec/batch)
2016-11-29 00:16:51.984244: step 1630, loss = 1.57 (2158.4 examples/sec; 0.059 sec/batch)
2016-11-29 00:16:57.023131: step 1640, loss = 1.52 (2291.8 examples/sec; 0.056 sec/batch)
2016-11-29 00:17:02.270714: step 1650, loss = 1.66 (1909.3 examples/sec; 0.067 sec/batch)
2016-11-29 00:17:07.287984: step 1660, loss = 1.69 (2250.1 examples/sec; 0.057 sec/batch)
2016-11-29 00:17:12.456090: step 1670, loss = 1.75 (1987.9 examples/sec; 0.064 sec/batch)
2016-11-29 00:17:17.507410: step 1680, loss = 1.56 (2093.0 examples/sec; 0.061 sec/batch)
2016-11-29 00:17:22.515157: step 1690, loss = 1.48 (2059.5 examples/sec; 0.062 sec/batch)
2016-11-29 00:17:27.677988: step 1700, loss = 1.51 (2060.4 examples/sec; 0.062 sec/batch)
2016-11-29 00:17:33.429500: step 1710, loss = 1.89 (2184.3 examples/sec; 0.059 sec/batch)
2016-11-29 00:17:38.437740: step 1720, loss = 1.74 (2192.9 examples/sec; 0.058 sec/batch)
2016-11-29 00:17:43.547305: step 1730, loss = 1.50 (2067.8 examples/sec; 0.062 sec/batch)
2016-11-29 00:17:48.657028: step 1740, loss = 1.67 (2119.1 examples/sec; 0.060 sec/batch)
2016-11-29 00:17:53.659339: step 1750, loss = 1.55 (2214.9 examples/sec; 0.058 sec/batch)
2016-11-29 00:17:58.783331: step 1760, loss = 1.58 (1879.5 examples/sec; 0.068 sec/batch)
2016-11-29 00:18:03.832409: step 1770, loss = 1.62 (2138.0 examples/sec; 0.060 sec/batch)
2016-11-29 00:18:08.766391: step 1780, loss = 1.44 (2021.9 examples/sec; 0.063 sec/batch)
2016-11-29 00:18:13.846536: step 1790, loss = 1.92 (1969.1 examples/sec; 0.065 sec/batch)
2016-11-29 00:18:18.984535: step 1800, loss = 1.58 (1987.3 examples/sec; 0.064 sec/batch)
2016-11-29 00:18:24.434575: step 1810, loss = 1.47 (1981.5 examples/sec; 0.065 sec/batch)
2016-11-29 00:18:29.598240: step 1820, loss = 1.56 (2164.5 examples/sec; 0.059 sec/batch)
2016-11-29 00:18:34.753125: step 1830, loss = 1.49 (2069.6 examples/sec; 0.062 sec/batch)
2016-11-29 00:18:39.755580: step 1840, loss = 1.58 (1992.9 examples/sec; 0.064 sec/batch)
2016-11-29 00:18:44.816463: step 1850, loss = 1.43 (2077.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:18:49.976910: step 1860, loss = 1.49 (1928.6 examples/sec; 0.066 sec/batch)
2016-11-29 00:18:54.979852: step 1870, loss = 1.49 (2086.3 examples/sec; 0.061 sec/batch)
2016-11-29 00:19:00.031897: step 1880, loss = 1.52 (2152.7 examples/sec; 0.059 sec/batch)
2016-11-29 00:19:05.159835: step 1890, loss = 1.56 (1965.2 examples/sec; 0.065 sec/batch)
2016-11-29 00:19:10.182859: step 1900, loss = 1.54 (1983.0 examples/sec; 0.065 sec/batch)
2016-11-29 00:19:15.740391: step 1910, loss = 1.47 (2192.4 examples/sec; 0.058 sec/batch)
2016-11-29 00:19:20.739160: step 1920, loss = 1.55 (2035.0 examples/sec; 0.063 sec/batch)
2016-11-29 00:19:25.807970: step 1930, loss = 1.51 (1656.4 examples/sec; 0.077 sec/batch)
2016-11-29 00:19:30.739441: step 1940, loss = 1.64 (1997.4 examples/sec; 0.064 sec/batch)
2016-11-29 00:19:35.893562: step 1950, loss = 1.67 (1953.8 examples/sec; 0.066 sec/batch)
2016-11-29 00:19:40.937667: step 1960, loss = 1.36 (1912.8 examples/sec; 0.067 sec/batch)
2016-11-29 00:19:46.087234: step 1970, loss = 1.68 (2185.3 examples/sec; 0.059 sec/batch)
2016-11-29 00:19:51.120541: step 1980, loss = 1.56 (1949.0 examples/sec; 0.066 sec/batch)
2016-11-29 00:19:56.345167: step 1990, loss = 1.57 (1517.1 examples/sec; 0.084 sec/batch)
2016-11-29 00:20:01.265757: step 2000, loss = 1.60 (2061.2 examples/sec; 0.062 sec/batch)
2016-11-29 00:20:09.483060: step 2010, loss = 1.37 (2023.1 examples/sec; 0.063 sec/batch)
2016-11-29 00:20:14.609761: step 2020, loss = 1.51 (1935.9 examples/sec; 0.066 sec/batch)
2016-11-29 00:20:19.573660: step 2030, loss = 1.50 (2107.1 examples/sec; 0.061 sec/batch)
2016-11-29 00:20:24.748567: step 2040, loss = 1.48 (2119.1 examples/sec; 0.060 sec/batch)
2016-11-29 00:20:29.859213: step 2050, loss = 1.32 (1997.6 examples/sec; 0.064 sec/batch)
2016-11-29 00:20:34.824770: step 2060, loss = 1.30 (2047.4 examples/sec; 0.063 sec/batch)
2016-11-29 00:20:39.906551: step 2070, loss = 1.52 (2054.5 examples/sec; 0.062 sec/batch)
2016-11-29 00:20:45.060621: step 2080, loss = 1.52 (1921.8 examples/sec; 0.067 sec/batch)
2016-11-29 00:20:50.111118: step 2090, loss = 1.39 (1898.5 examples/sec; 0.067 sec/batch)
2016-11-29 00:20:55.134467: step 2100, loss = 1.55 (1964.0 examples/sec; 0.065 sec/batch)
2016-11-29 00:21:00.718032: step 2110, loss = 1.55 (2032.0 examples/sec; 0.063 sec/batch)
2016-11-29 00:21:05.785619: step 2120, loss = 1.36 (2079.4 examples/sec; 0.062 sec/batch)
2016-11-29 00:21:10.809897: step 2130, loss = 1.23 (2018.8 examples/sec; 0.063 sec/batch)
2016-11-29 00:21:15.896820: step 2140, loss = 1.46 (1770.0 examples/sec; 0.072 sec/batch)
2016-11-29 00:21:20.982668: step 2150, loss = 1.40 (1885.7 examples/sec; 0.068 sec/batch)
2016-11-29 00:21:25.856262: step 2160, loss = 1.41 (2116.1 examples/sec; 0.060 sec/batch)
2016-11-29 00:21:30.965387: step 2170, loss = 1.48 (2171.3 examples/sec; 0.059 sec/batch)
2016-11-29 00:21:36.015557: step 2180, loss = 1.47 (2234.6 examples/sec; 0.057 sec/batch)
2016-11-29 00:21:41.064857: step 2190, loss = 1.29 (1947.2 examples/sec; 0.066 sec/batch)
2016-11-29 00:21:46.364757: step 2200, loss = 1.30 (1914.6 examples/sec; 0.067 sec/batch)
2016-11-29 00:21:52.086318: step 2210, loss = 1.43 (1948.4 examples/sec; 0.066 sec/batch)
2016-11-29 00:21:57.435470: step 2220, loss = 1.33 (1484.1 examples/sec; 0.086 sec/batch)
2016-11-29 00:22:02.525468: step 2230, loss = 1.39 (1898.8 examples/sec; 0.067 sec/batch)
2016-11-29 00:22:07.707017: step 2240, loss = 1.31 (1889.7 examples/sec; 0.068 sec/batch)
2016-11-29 00:22:12.812481: step 2250, loss = 1.31 (2070.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:22:17.865393: step 2260, loss = 1.24 (1981.6 examples/sec; 0.065 sec/batch)
2016-11-29 00:22:23.004648: step 2270, loss = 1.20 (1959.8 examples/sec; 0.065 sec/batch)
2016-11-29 00:22:28.205180: step 2280, loss = 1.34 (1928.5 examples/sec; 0.066 sec/batch)
2016-11-29 00:22:33.384554: step 2290, loss = 1.37 (2151.3 examples/sec; 0.059 sec/batch)
2016-11-29 00:22:38.529566: step 2300, loss = 1.41 (1930.1 examples/sec; 0.066 sec/batch)
2016-11-29 00:22:44.228019: step 2310, loss = 1.33 (1855.6 examples/sec; 0.069 sec/batch)
2016-11-29 00:22:49.186583: step 2320, loss = 1.46 (2158.0 examples/sec; 0.059 sec/batch)
2016-11-29 00:22:54.266779: step 2330, loss = 1.28 (2054.6 examples/sec; 0.062 sec/batch)
2016-11-29 00:22:59.236038: step 2340, loss = 1.27 (2127.2 examples/sec; 0.060 sec/batch)
2016-11-29 00:23:04.321746: step 2350, loss = 1.32 (2055.3 examples/sec; 0.062 sec/batch)
2016-11-29 00:23:09.480079: step 2360, loss = 1.32 (1997.3 examples/sec; 0.064 sec/batch)
2016-11-29 00:23:14.665261: step 2370, loss = 1.23 (2160.6 examples/sec; 0.059 sec/batch)
2016-11-29 00:23:19.680175: step 2380, loss = 1.11 (1979.5 examples/sec; 0.065 sec/batch)
2016-11-29 00:23:24.783394: step 2390, loss = 1.24 (2042.0 examples/sec; 0.063 sec/batch)
2016-11-29 00:23:29.828111: step 2400, loss = 1.22 (2086.1 examples/sec; 0.061 sec/batch)
2016-11-29 00:23:35.393501: step 2410, loss = 1.25 (1997.5 examples/sec; 0.064 sec/batch)
2016-11-29 00:23:40.529501: step 2420, loss = 1.26 (2017.9 examples/sec; 0.063 sec/batch)
2016-11-29 00:23:45.651240: step 2430, loss = 1.38 (2056.2 examples/sec; 0.062 sec/batch)
2016-11-29 00:23:50.696082: step 2440, loss = 1.30 (2232.1 examples/sec; 0.057 sec/batch)
2016-11-29 00:23:55.854064: step 2450, loss = 1.35 (2225.8 examples/sec; 0.058 sec/batch)
2016-11-29 00:24:00.941408: step 2460, loss = 1.25 (1972.0 examples/sec; 0.065 sec/batch)
2016-11-29 00:24:05.906184: step 2470, loss = 1.27 (1982.4 examples/sec; 0.065 sec/batch)
2016-11-29 00:24:11.096796: step 2480, loss = 1.08 (1998.4 examples/sec; 0.064 sec/batch)
2016-11-29 00:24:16.272889: step 2490, loss = 1.14 (2207.8 examples/sec; 0.058 sec/batch)
2016-11-29 00:24:21.425500: step 2500, loss = 1.18 (2111.2 examples/sec; 0.061 sec/batch)
2016-11-29 00:24:26.935620: step 2510, loss = 1.33 (1969.8 examples/sec; 0.065 sec/batch)
2016-11-29 00:24:32.029206: step 2520, loss = 1.25 (2014.7 examples/sec; 0.064 sec/batch)
2016-11-29 00:24:37.283160: step 2530, loss = 1.30 (2004.9 examples/sec; 0.064 sec/batch)
2016-11-29 00:24:42.344106: step 2540, loss = 1.11 (1935.9 examples/sec; 0.066 sec/batch)
2016-11-29 00:24:47.410875: step 2550, loss = 1.25 (1991.8 examples/sec; 0.064 sec/batch)
2016-11-29 00:24:52.520792: step 2560, loss = 1.19 (1877.0 examples/sec; 0.068 sec/batch)
2016-11-29 00:24:57.639345: step 2570, loss = 1.17 (2033.4 examples/sec; 0.063 sec/batch)
2016-11-29 00:25:02.660895: step 2580, loss = 1.29 (2149.9 examples/sec; 0.060 sec/batch)
2016-11-29 00:25:07.764894: step 2590, loss = 1.24 (2227.2 examples/sec; 0.057 sec/batch)
2016-11-29 00:25:12.918205: step 2600, loss = 1.38 (1918.3 examples/sec; 0.067 sec/batch)
2016-11-29 00:25:18.612967: step 2610, loss = 1.18 (2054.7 examples/sec; 0.062 sec/batch)
2016-11-29 00:25:23.672279: step 2620, loss = 1.24 (2076.1 examples/sec; 0.062 sec/batch)
2016-11-29 00:25:28.672209: step 2630, loss = 1.12 (2207.6 examples/sec; 0.058 sec/batch)
2016-11-29 00:25:33.779246: step 2640, loss = 1.22 (1994.2 examples/sec; 0.064 sec/batch)
2016-11-29 00:25:38.939504: step 2650, loss = 1.23 (1959.8 examples/sec; 0.065 sec/batch)
2016-11-29 00:25:43.916943: step 2660, loss = 1.16 (1994.6 examples/sec; 0.064 sec/batch)
2016-11-29 00:25:48.981917: step 2670, loss = 1.12 (2048.8 examples/sec; 0.062 sec/batch)
2016-11-29 00:25:54.193517: step 2680, loss = 1.23 (2308.8 examples/sec; 0.055 sec/batch)
2016-11-29 00:25:59.223668: step 2690, loss = 1.35 (2127.9 examples/sec; 0.060 sec/batch)
2016-11-29 00:26:04.290686: step 2700, loss = 1.10 (2184.9 examples/sec; 0.059 sec/batch)
2016-11-29 00:26:09.985533: step 2710, loss = 1.26 (2092.7 examples/sec; 0.061 sec/batch)
2016-11-29 00:26:15.078704: step 2720, loss = 1.13 (1895.8 examples/sec; 0.068 sec/batch)
2016-11-29 00:26:20.273281: step 2730, loss = 1.27 (1898.7 examples/sec; 0.067 sec/batch)
2016-11-29 00:26:25.351376: step 2740, loss = 0.92 (2026.7 examples/sec; 0.063 sec/batch)
2016-11-29 00:26:30.566832: step 2750, loss = 1.20 (1913.9 examples/sec; 0.067 sec/batch)
2016-11-29 00:26:35.566805: step 2760, loss = 1.10 (2084.1 examples/sec; 0.061 sec/batch)
2016-11-29 00:26:40.770764: step 2770, loss = 1.34 (2108.6 examples/sec; 0.061 sec/batch)
2016-11-29 00:26:45.911779: step 2780, loss = 1.15 (2132.8 examples/sec; 0.060 sec/batch)
2016-11-29 00:26:50.877448: step 2790, loss = 1.19 (2047.4 examples/sec; 0.063 sec/batch)
2016-11-29 00:26:55.984508: step 2800, loss = 1.11 (1833.5 examples/sec; 0.070 sec/batch)
2016-11-29 00:27:01.583071: step 2810, loss = 1.10 (1906.4 examples/sec; 0.067 sec/batch)
2016-11-29 00:27:06.679931: step 2820, loss = 1.21 (1968.5 examples/sec; 0.065 sec/batch)
2016-11-29 00:27:11.747353: step 2830, loss = 1.13 (2244.5 examples/sec; 0.057 sec/batch)
2016-11-29 00:27:16.829836: step 2840, loss = 1.18 (1709.3 examples/sec; 0.075 sec/batch)
2016-11-29 00:27:21.842748: step 2850, loss = 1.08 (1947.2 examples/sec; 0.066 sec/batch)
2016-11-29 00:27:26.971879: step 2860, loss = 1.00 (2204.0 examples/sec; 0.058 sec/batch)
2016-11-29 00:27:32.192947: step 2870, loss = 1.17 (1662.6 examples/sec; 0.077 sec/batch)
2016-11-29 00:27:37.278739: step 2880, loss = 1.12 (2002.6 examples/sec; 0.064 sec/batch)
2016-11-29 00:27:42.478025: step 2890, loss = 1.16 (1814.6 examples/sec; 0.071 sec/batch)
2016-11-29 00:27:47.657670: step 2900, loss = 1.10 (2183.3 examples/sec; 0.059 sec/batch)
2016-11-29 00:27:53.281258: step 2910, loss = 1.19 (1931.7 examples/sec; 0.066 sec/batch)
2016-11-29 00:27:58.391433: step 2920, loss = 1.04 (1965.6 examples/sec; 0.065 sec/batch)
2016-11-29 00:28:03.608837: step 2930, loss = 1.04 (2008.1 examples/sec; 0.064 sec/batch)
2016-11-29 00:28:08.641357: step 2940, loss = 1.10 (2221.0 examples/sec; 0.058 sec/batch)
2016-11-29 00:28:13.772886: step 2950, loss = 1.14 (1904.0 examples/sec; 0.067 sec/batch)
2016-11-29 00:28:18.793466: step 2960, loss = 1.05 (2187.3 examples/sec; 0.059 sec/batch)
2016-11-29 00:28:23.829072: step 2970, loss = 1.09 (2030.7 examples/sec; 0.063 sec/batch)
2016-11-29 00:28:28.795091: step 2980, loss = 1.22 (2018.3 examples/sec; 0.063 sec/batch)
2016-11-29 00:28:33.914170: step 2990, loss = 1.20 (2156.2 examples/sec; 0.059 sec/batch)
2016-11-29 00:28:38.844906: step 3000, loss = 1.07 (2052.4 examples/sec; 0.062 sec/batch)
2016-11-29 00:28:48.079946: step 3010, loss = 1.25 (1588.8 examples/sec; 0.081 sec/batch)
2016-11-29 00:28:53.175573: step 3020, loss = 1.12 (1963.1 examples/sec; 0.065 sec/batch)
2016-11-29 00:28:58.295341: step 3030, loss = 1.21 (2260.0 examples/sec; 0.057 sec/batch)
2016-11-29 00:29:03.391806: step 3040, loss = 1.12 (1730.9 examples/sec; 0.074 sec/batch)
2016-11-29 00:29:08.432143: step 3050, loss = 1.01 (2104.0 examples/sec; 0.061 sec/batch)
2016-11-29 00:29:13.556198: step 3060, loss = 1.05 (2235.1 examples/sec; 0.057 sec/batch)
2016-11-29 00:29:18.709995: step 3070, loss = 0.88 (1508.0 examples/sec; 0.085 sec/batch)
2016-11-29 00:29:23.662720: step 3080, loss = 0.99 (2029.4 examples/sec; 0.063 sec/batch)
2016-11-29 00:29:28.811712: step 3090, loss = 0.98 (2081.3 examples/sec; 0.061 sec/batch)
2016-11-29 00:29:33.818570: step 3100, loss = 0.93 (2266.4 examples/sec; 0.056 sec/batch)
2016-11-29 00:29:39.530897: step 3110, loss = 0.95 (2075.5 examples/sec; 0.062 sec/batch)
2016-11-29 00:29:44.806242: step 3120, loss = 0.97 (1812.7 examples/sec; 0.071 sec/batch)
2016-11-29 00:29:49.950171: step 3130, loss = 1.05 (1939.5 examples/sec; 0.066 sec/batch)
2016-11-29 00:29:55.106445: step 3140, loss = 1.05 (2165.8 examples/sec; 0.059 sec/batch)
2016-11-29 00:30:00.206899: step 3150, loss = 1.08 (2003.1 examples/sec; 0.064 sec/batch)
2016-11-29 00:30:05.345745: step 3160, loss = 1.16 (2019.9 examples/sec; 0.063 sec/batch)
2016-11-29 00:30:10.366738: step 3170, loss = 0.94 (2148.0 examples/sec; 0.060 sec/batch)
2016-11-29 00:30:15.452289: step 3180, loss = 0.92 (1995.0 examples/sec; 0.064 sec/batch)
2016-11-29 00:30:20.502507: step 3190, loss = 1.11 (1846.4 examples/sec; 0.069 sec/batch)
2016-11-29 00:30:25.574265: step 3200, loss = 1.11 (2146.4 examples/sec; 0.060 sec/batch)
2016-11-29 00:30:31.177396: step 3210, loss = 1.09 (2050.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:30:36.321240: step 3220, loss = 1.12 (2116.4 examples/sec; 0.060 sec/batch)
2016-11-29 00:30:41.404926: step 3230, loss = 0.97 (2015.7 examples/sec; 0.064 sec/batch)
2016-11-29 00:30:46.460679: step 3240, loss = 1.19 (2033.1 examples/sec; 0.063 sec/batch)
2016-11-29 00:30:51.705320: step 3250, loss = 1.04 (1883.1 examples/sec; 0.068 sec/batch)
2016-11-29 00:30:56.762599: step 3260, loss = 1.04 (2052.7 examples/sec; 0.062 sec/batch)
2016-11-29 00:31:01.805367: step 3270, loss = 0.92 (2058.5 examples/sec; 0.062 sec/batch)
2016-11-29 00:31:06.807917: step 3280, loss = 0.97 (1706.6 examples/sec; 0.075 sec/batch)
2016-11-29 00:31:11.710894: step 3290, loss = 1.16 (2174.9 examples/sec; 0.059 sec/batch)
2016-11-29 00:31:16.823601: step 3300, loss = 0.89 (2099.0 examples/sec; 0.061 sec/batch)
2016-11-29 00:31:22.555085: step 3310, loss = 0.91 (1600.2 examples/sec; 0.080 sec/batch)
2016-11-29 00:31:27.535395: step 3320, loss = 0.85 (2205.7 examples/sec; 0.058 sec/batch)
2016-11-29 00:31:32.610819: step 3330, loss = 0.99 (2127.8 examples/sec; 0.060 sec/batch)
2016-11-29 00:31:37.753717: step 3340, loss = 0.97 (1628.0 examples/sec; 0.079 sec/batch)
2016-11-29 00:31:42.759160: step 3350, loss = 0.94 (2275.4 examples/sec; 0.056 sec/batch)
2016-11-29 00:31:47.883942: step 3360, loss = 1.02 (2046.0 examples/sec; 0.063 sec/batch)
2016-11-29 00:31:53.056702: step 3370, loss = 0.99 (1881.9 examples/sec; 0.068 sec/batch)
2016-11-29 00:31:58.135447: step 3380, loss = 0.85 (1984.9 examples/sec; 0.064 sec/batch)
2016-11-29 00:32:03.222501: step 3390, loss = 1.07 (2056.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:32:08.459758: step 3400, loss = 0.96 (1569.8 examples/sec; 0.082 sec/batch)
2016-11-29 00:32:14.151250: step 3410, loss = 1.17 (1870.9 examples/sec; 0.068 sec/batch)
2016-11-29 00:32:19.211915: step 3420, loss = 1.19 (2140.3 examples/sec; 0.060 sec/batch)
2016-11-29 00:32:24.372299: step 3430, loss = 1.15 (2036.2 examples/sec; 0.063 sec/batch)
2016-11-29 00:32:29.448943: step 3440, loss = 0.94 (1974.9 examples/sec; 0.065 sec/batch)
2016-11-29 00:32:34.695481: step 3450, loss = 0.81 (2202.6 examples/sec; 0.058 sec/batch)
2016-11-29 00:32:39.874698: step 3460, loss = 0.93 (1958.2 examples/sec; 0.065 sec/batch)
2016-11-29 00:32:44.913012: step 3470, loss = 1.02 (1978.7 examples/sec; 0.065 sec/batch)
2016-11-29 00:32:50.050475: step 3480, loss = 1.04 (2160.4 examples/sec; 0.059 sec/batch)
2016-11-29 00:32:55.137163: step 3490, loss = 1.04 (2063.6 examples/sec; 0.062 sec/batch)
2016-11-29 00:33:00.262243: step 3500, loss = 1.15 (1902.4 examples/sec; 0.067 sec/batch)
2016-11-29 00:33:05.865917: step 3510, loss = 0.82 (2019.6 examples/sec; 0.063 sec/batch)
2016-11-29 00:33:11.013948: step 3520, loss = 0.97 (1873.9 examples/sec; 0.068 sec/batch)
2016-11-29 00:33:15.978261: step 3530, loss = 0.88 (2243.3 examples/sec; 0.057 sec/batch)
2016-11-29 00:33:21.138946: step 3540, loss = 0.80 (2130.1 examples/sec; 0.060 sec/batch)
2016-11-29 00:33:26.297605: step 3550, loss = 1.00 (2052.9 examples/sec; 0.062 sec/batch)
2016-11-29 00:33:31.284675: step 3560, loss = 0.96 (1745.7 examples/sec; 0.073 sec/batch)
2016-11-29 00:33:36.304718: step 3570, loss = 1.03 (2043.0 examples/sec; 0.063 sec/batch)
2016-11-29 00:33:41.511287: step 3580, loss = 1.19 (1816.3 examples/sec; 0.070 sec/batch)
2016-11-29 00:33:46.607135: step 3590, loss = 0.88 (1795.6 examples/sec; 0.071 sec/batch)
2016-11-29 00:33:51.551820: step 3600, loss = 0.85 (2125.1 examples/sec; 0.060 sec/batch)
2016-11-29 00:33:57.104244: step 3610, loss = 0.85 (1995.7 examples/sec; 0.064 sec/batch)
2016-11-29 00:34:02.164023: step 3620, loss = 0.94 (2000.9 examples/sec; 0.064 sec/batch)
2016-11-29 00:34:07.195411: step 3630, loss = 0.94 (2093.6 examples/sec; 0.061 sec/batch)
2016-11-29 00:34:12.281332: step 3640, loss = 0.81 (1885.1 examples/sec; 0.068 sec/batch)
2016-11-29 00:34:17.376255: step 3650, loss = 0.91 (2060.8 examples/sec; 0.062 sec/batch)
2016-11-29 00:34:22.499688: step 3660, loss = 0.95 (1767.6 examples/sec; 0.072 sec/batch)
2016-11-29 00:34:27.797634: step 3670, loss = 0.95 (1934.7 examples/sec; 0.066 sec/batch)
2016-11-29 00:34:32.961754: step 3680, loss = 1.00 (1875.9 examples/sec; 0.068 sec/batch)
2016-11-29 00:34:37.932774: step 3690, loss = 0.94 (1987.1 examples/sec; 0.064 sec/batch)
2016-11-29 00:34:43.055039: step 3700, loss = 1.01 (1965.2 examples/sec; 0.065 sec/batch)
2016-11-29 00:34:48.625387: step 3710, loss = 0.74 (2183.2 examples/sec; 0.059 sec/batch)
2016-11-29 00:34:53.816751: step 3720, loss = 0.90 (1669.5 examples/sec; 0.077 sec/batch)
2016-11-29 00:34:58.769346: step 3730, loss = 0.91 (1967.1 examples/sec; 0.065 sec/batch)
2016-11-29 00:35:03.870691: step 3740, loss = 0.98 (1906.3 examples/sec; 0.067 sec/batch)
2016-11-29 00:35:08.843057: step 3750, loss = 1.07 (1760.9 examples/sec; 0.073 sec/batch)
2016-11-29 00:35:14.027919: step 3760, loss = 0.89 (1954.5 examples/sec; 0.065 sec/batch)
2016-11-29 00:35:19.134595: step 3770, loss = 0.79 (2055.1 examples/sec; 0.062 sec/batch)
2016-11-29 00:35:24.179937: step 3780, loss = 0.82 (1699.5 examples/sec; 0.075 sec/batch)
2016-11-29 00:35:29.165201: step 3790, loss = 0.87 (2013.1 examples/sec; 0.064 sec/batch)
2016-11-29 00:35:34.206210: step 3800, loss = 0.92 (2113.6 examples/sec; 0.061 sec/batch)
2016-11-29 00:35:39.880240: step 3810, loss = 0.89 (1876.5 examples/sec; 0.068 sec/batch)
2016-11-29 00:35:44.971965: step 3820, loss = 0.90 (2060.1 examples/sec; 0.062 sec/batch)
2016-11-29 00:35:50.069968: step 3830, loss = 0.90 (1961.3 examples/sec; 0.065 sec/batch)
2016-11-29 00:35:55.112783: step 3840, loss = 1.14 (1926.1 examples/sec; 0.066 sec/batch)
2016-11-29 00:36:00.346397: step 3850, loss = 0.87 (1831.4 examples/sec; 0.070 sec/batch)
2016-11-29 00:36:05.528374: step 3860, loss = 0.95 (2076.9 examples/sec; 0.062 sec/batch)
2016-11-29 00:36:10.671499: step 3870, loss = 0.95 (1690.4 examples/sec; 0.076 sec/batch)
2016-11-29 00:36:15.729862: step 3880, loss = 0.83 (1861.8 examples/sec; 0.069 sec/batch)
2016-11-29 00:36:20.795628: step 3890, loss = 1.07 (2083.6 examples/sec; 0.061 sec/batch)
2016-11-29 00:36:25.955441: step 3900, loss = 0.86 (1623.8 examples/sec; 0.079 sec/batch)
2016-11-29 00:36:31.630307: step 3910, loss = 0.95 (1889.3 examples/sec; 0.068 sec/batch)
2016-11-29 00:36:36.799889: step 3920, loss = 0.69 (1939.1 examples/sec; 0.066 sec/batch)
2016-11-29 00:36:41.992261: step 3930, loss = 0.92 (2074.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:36:47.056044: step 3940, loss = 0.96 (2103.5 examples/sec; 0.061 sec/batch)
2016-11-29 00:36:52.234580: step 3950, loss = 0.99 (1962.4 examples/sec; 0.065 sec/batch)
2016-11-29 00:36:57.315854: step 3960, loss = 1.00 (2004.5 examples/sec; 0.064 sec/batch)
2016-11-29 00:37:02.449909: step 3970, loss = 0.91 (2020.0 examples/sec; 0.063 sec/batch)
2016-11-29 00:37:07.672173: step 3980, loss = 1.05 (2018.9 examples/sec; 0.063 sec/batch)
2016-11-29 00:37:12.958270: step 3990, loss = 0.85 (1889.4 examples/sec; 0.068 sec/batch)
2016-11-29 00:37:18.088939: step 4000, loss = 0.80 (2042.9 examples/sec; 0.063 sec/batch)
2016-11-29 00:37:26.473603: step 4010, loss = 0.90 (2059.9 examples/sec; 0.062 sec/batch)
2016-11-29 00:37:31.662820: step 4020, loss = 1.00 (1908.8 examples/sec; 0.067 sec/batch)
2016-11-29 00:37:36.783781: step 4030, loss = 0.84 (2207.6 examples/sec; 0.058 sec/batch)
2016-11-29 00:37:41.833562: step 4040, loss = 1.07 (2155.1 examples/sec; 0.059 sec/batch)
2016-11-29 00:37:46.824077: step 4050, loss = 1.07 (2210.0 examples/sec; 0.058 sec/batch)
2016-11-29 00:37:52.033096: step 4060, loss = 0.94 (1990.8 examples/sec; 0.064 sec/batch)
2016-11-29 00:37:57.229463: step 4070, loss = 0.96 (1755.8 examples/sec; 0.073 sec/batch)
2016-11-29 00:38:02.286499: step 4080, loss = 0.76 (2124.4 examples/sec; 0.060 sec/batch)
2016-11-29 00:38:07.463454: step 4090, loss = 1.03 (1899.7 examples/sec; 0.067 sec/batch)
2016-11-29 00:38:12.527979: step 4100, loss = 0.86 (2067.4 examples/sec; 0.062 sec/batch)
2016-11-29 00:38:18.203931: step 4110, loss = 0.89 (1751.5 examples/sec; 0.073 sec/batch)
2016-11-29 00:38:23.303394: step 4120, loss = 0.76 (2062.8 examples/sec; 0.062 sec/batch)
2016-11-29 00:38:28.414814: step 4130, loss = 1.04 (2344.0 examples/sec; 0.055 sec/batch)
2016-11-29 00:38:33.478320: step 4140, loss = 0.87 (2002.4 examples/sec; 0.064 sec/batch)
2016-11-29 00:38:38.660114: step 4150, loss = 0.93 (2113.4 examples/sec; 0.061 sec/batch)
2016-11-29 00:38:43.797478: step 4160, loss = 0.93 (1888.9 examples/sec; 0.068 sec/batch)
2016-11-29 00:38:48.794207: step 4170, loss = 0.92 (1946.2 examples/sec; 0.066 sec/batch)
2016-11-29 00:38:53.862996: step 4180, loss = 0.82 (1990.1 examples/sec; 0.064 sec/batch)
2016-11-29 00:38:58.925211: step 4190, loss = 0.79 (2162.0 examples/sec; 0.059 sec/batch)
2016-11-29 00:39:04.112567: step 4200, loss = 0.84 (1861.8 examples/sec; 0.069 sec/batch)
2016-11-29 00:39:09.761520: step 4210, loss = 0.92 (2042.0 examples/sec; 0.063 sec/batch)
2016-11-29 00:39:14.848878: step 4220, loss = 0.94 (2135.8 examples/sec; 0.060 sec/batch)
2016-11-29 00:39:19.900959: step 4230, loss = 0.64 (1854.6 examples/sec; 0.069 sec/batch)
2016-11-29 00:39:25.166365: step 4240, loss = 0.91 (1964.5 examples/sec; 0.065 sec/batch)
2016-11-29 00:39:30.283887: step 4250, loss = 0.87 (2121.5 examples/sec; 0.060 sec/batch)
2016-11-29 00:39:35.232697: step 4260, loss = 1.00 (2144.4 examples/sec; 0.060 sec/batch)
2016-11-29 00:39:40.357055: step 4270, loss = 0.88 (1984.0 examples/sec; 0.065 sec/batch)
2016-11-29 00:39:45.506603: step 4280, loss = 0.89 (2298.4 examples/sec; 0.056 sec/batch)
2016-11-29 00:39:50.543082: step 4290, loss = 0.88 (1812.5 examples/sec; 0.071 sec/batch)
2016-11-29 00:39:55.800835: step 4300, loss = 0.85 (2102.7 examples/sec; 0.061 sec/batch)
2016-11-29 00:40:01.482900: step 4310, loss = 1.02 (1940.9 examples/sec; 0.066 sec/batch)
2016-11-29 00:40:06.593724: step 4320, loss = 0.92 (1896.5 examples/sec; 0.067 sec/batch)
2016-11-29 00:40:11.800262: step 4330, loss = 0.73 (2060.6 examples/sec; 0.062 sec/batch)
2016-11-29 00:40:16.874999: step 4340, loss = 0.87 (1888.2 examples/sec; 0.068 sec/batch)
2016-11-29 00:40:22.010647: step 4350, loss = 0.78 (1674.7 examples/sec; 0.076 sec/batch)
2016-11-29 00:40:27.098227: step 4360, loss = 0.78 (2032.7 examples/sec; 0.063 sec/batch)
2016-11-29 00:40:32.212915: step 4370, loss = 0.80 (2021.4 examples/sec; 0.063 sec/batch)
2016-11-29 00:40:37.177475: step 4380, loss = 0.78 (1984.4 examples/sec; 0.065 sec/batch)
2016-11-29 00:40:42.322862: step 4390, loss = 0.84 (1988.9 examples/sec; 0.064 sec/batch)
2016-11-29 00:40:47.432609: step 4400, loss = 0.68 (1903.3 examples/sec; 0.067 sec/batch)
2016-11-29 00:40:53.110248: step 4410, loss = 0.74 (1624.5 examples/sec; 0.079 sec/batch)
2016-11-29 00:40:58.063406: step 4420, loss = 0.85 (1977.6 examples/sec; 0.065 sec/batch)
2016-11-29 00:41:03.337178: step 4430, loss = 1.02 (2064.2 examples/sec; 0.062 sec/batch)
2016-11-29 00:41:08.530659: step 4440, loss = 1.09 (1902.2 examples/sec; 0.067 sec/batch)
2016-11-29 00:41:13.527506: step 4450, loss = 0.94 (2127.2 examples/sec; 0.060 sec/batch)
2016-11-29 00:41:18.718607: step 4460, loss = 0.78 (1972.6 examples/sec; 0.065 sec/batch)
2016-11-29 00:41:23.803131: step 4470, loss = 0.78 (2167.5 examples/sec; 0.059 sec/batch)
2016-11-29 00:41:28.852795: step 4480, loss = 0.90 (2116.0 examples/sec; 0.060 sec/batch)
2016-11-29 00:41:34.019766: step 4490, loss = 0.82 (2059.3 examples/sec; 0.062 sec/batch)
2016-11-29 00:41:39.183986: step 4500, loss = 0.74 (2225.0 examples/sec; 0.058 sec/batch)
2016-11-29 00:41:44.780143: step 4510, loss = 0.81 (1976.0 examples/sec; 0.065 sec/batch)
2016-11-29 00:41:49.756620: step 4520, loss = 0.80 (1989.4 examples/sec; 0.064 sec/batch)
2016-11-29 00:41:54.926531: step 4530, loss = 0.92 (1645.2 examples/sec; 0.078 sec/batch)
2016-11-29 00:41:59.990129: step 4540, loss = 0.76 (2017.0 examples/sec; 0.063 sec/batch)
2016-11-29 00:42:05.018609: step 4550, loss = 0.72 (2180.7 examples/sec; 0.059 sec/batch)
2016-11-29 00:42:10.296367: step 4560, loss = 0.88 (1622.6 examples/sec; 0.079 sec/batch)
2016-11-29 00:42:15.367659: step 4570, loss = 0.76 (2135.0 examples/sec; 0.060 sec/batch)
2016-11-29 00:42:20.525321: step 4580, loss = 0.96 (2098.9 examples/sec; 0.061 sec/batch)
2016-11-29 00:42:25.609321: step 4590, loss = 0.84 (1933.1 examples/sec; 0.066 sec/batch)
2016-11-29 00:42:30.694510: step 4600, loss = 0.69 (1988.3 examples/sec; 0.064 sec/batch)
2016-11-29 00:42:36.426499: step 4610, loss = 0.72 (1890.2 examples/sec; 0.068 sec/batch)
2016-11-29 00:42:41.614871: step 4620, loss = 0.78 (2146.0 examples/sec; 0.060 sec/batch)
2016-11-29 00:42:46.690604: step 4630, loss = 0.86 (1930.9 examples/sec; 0.066 sec/batch)
2016-11-29 00:42:51.802572: step 4640, loss = 0.74 (1961.4 examples/sec; 0.065 sec/batch)
2016-11-29 00:42:57.078722: step 4650, loss = 0.84 (2019.0 examples/sec; 0.063 sec/batch)
2016-11-29 00:43:02.061654: step 4660, loss = 0.71 (2166.1 examples/sec; 0.059 sec/batch)
2016-11-29 00:43:07.239548: step 4670, loss = 0.79 (2005.4 examples/sec; 0.064 sec/batch)
2016-11-29 00:43:12.337608: step 4680, loss = 0.80 (2007.5 examples/sec; 0.064 sec/batch)
2016-11-29 00:43:17.435979: step 4690, loss = 0.84 (1989.4 examples/sec; 0.064 sec/batch)
2016-11-29 00:43:22.565413: step 4700, loss = 0.89 (2163.9 examples/sec; 0.059 sec/batch)
2016-11-29 00:43:28.212058: step 4710, loss = 0.81 (2100.1 examples/sec; 0.061 sec/batch)
2016-11-29 00:43:33.301929: step 4720, loss = 0.74 (2068.1 examples/sec; 0.062 sec/batch)
2016-11-29 00:43:38.467947: step 4730, loss = 0.69 (2005.1 examples/sec; 0.064 sec/batch)
2016-11-29 00:43:43.554494: step 4740, loss = 0.88 (2147.6 examples/sec; 0.060 sec/batch)
2016-11-29 00:43:48.628025: step 4750, loss = 0.77 (2072.9 examples/sec; 0.062 sec/batch)
2016-11-29 00:43:53.813015: step 4760, loss = 0.77 (2204.5 examples/sec; 0.058 sec/batch)
2016-11-29 00:43:58.999577: step 4770, loss = 0.63 (1979.5 examples/sec; 0.065 sec/batch)
2016-11-29 00:44:04.040747: step 4780, loss = 0.78 (1916.8 examples/sec; 0.067 sec/batch)
2016-11-29 00:44:09.145648: step 4790, loss = 0.72 (2004.0 examples/sec; 0.064 sec/batch)
2016-11-29 00:44:14.342177: step 4800, loss = 0.78 (2004.7 examples/sec; 0.064 sec/batch)
2016-11-29 00:44:20.029379: step 4810, loss = 0.90 (2182.4 examples/sec; 0.059 sec/batch)
2016-11-29 00:44:25.045522: step 4820, loss = 0.73 (1910.3 examples/sec; 0.067 sec/batch)
2016-11-29 00:44:30.140029: step 4830, loss = 0.84 (2059.1 examples/sec; 0.062 sec/batch)
2016-11-29 00:44:35.306567: step 4840, loss = 0.79 (2024.0 examples/sec; 0.063 sec/batch)
2016-11-29 00:44:40.329824: step 4850, loss = 0.82 (1965.6 examples/sec; 0.065 sec/batch)
2016-11-29 00:44:45.456075: step 4860, loss = 0.75 (2047.7 examples/sec; 0.063 sec/batch)
2016-11-29 00:44:50.553863: step 4870, loss = 0.74 (2423.9 examples/sec; 0.053 sec/batch)
2016-11-29 00:44:55.496757: step 4880, loss = 0.64 (2055.4 examples/sec; 0.062 sec/batch)
2016-11-29 00:45:00.597430: step 4890, loss = 0.80 (2256.5 examples/sec; 0.057 sec/batch)
2016-11-29 00:45:05.728475: step 4900, loss = 0.75 (2079.6 examples/sec; 0.062 sec/batch)
2016-11-29 00:45:11.255546: step 4910, loss = 0.82 (2103.2 examples/sec; 0.061 sec/batch)
2016-11-29 00:45:16.349412: step 4920, loss = 1.06 (2119.1 examples/sec; 0.060 sec/batch)
2016-11-29 00:45:21.409685: step 4930, loss = 0.64 (2118.4 examples/sec; 0.060 sec/batch)
2016-11-29 00:45:26.363628: step 4940, loss = 0.76 (1892.7 examples/sec; 0.068 sec/batch)
2016-11-29 00:45:31.480659: step 4950, loss = 0.82 (2081.7 examples/sec; 0.061 sec/batch)
2016-11-29 00:45:36.569296: step 4960, loss = 0.97 (2080.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:45:41.569812: step 4970, loss = 0.74 (2325.1 examples/sec; 0.055 sec/batch)
2016-11-29 00:45:46.712282: step 4980, loss = 0.93 (1780.5 examples/sec; 0.072 sec/batch)
2016-11-29 00:45:51.833917: step 4990, loss = 0.83 (2154.0 examples/sec; 0.059 sec/batch)
2016-11-29 00:45:56.988167: step 5000, loss = 0.75 (2082.0 examples/sec; 0.061 sec/batch)
2016-11-29 00:46:05.565908: step 5010, loss = 0.90 (2154.7 examples/sec; 0.059 sec/batch)
2016-11-29 00:46:10.609398: step 5020, loss = 0.83 (1970.5 examples/sec; 0.065 sec/batch)
2016-11-29 00:46:15.644697: step 5030, loss = 0.66 (2154.8 examples/sec; 0.059 sec/batch)
2016-11-29 00:46:20.681690: step 5040, loss = 0.64 (1871.1 examples/sec; 0.068 sec/batch)
2016-11-29 00:46:25.914502: step 5050, loss = 0.69 (1988.7 examples/sec; 0.064 sec/batch)
2016-11-29 00:46:30.969103: step 5060, loss = 0.92 (1959.3 examples/sec; 0.065 sec/batch)
2016-11-29 00:46:36.208982: step 5070, loss = 0.82 (1924.0 examples/sec; 0.067 sec/batch)
2016-11-29 00:46:41.379542: step 5080, loss = 0.80 (1889.8 examples/sec; 0.068 sec/batch)
2016-11-29 00:46:46.477317: step 5090, loss = 0.70 (2027.0 examples/sec; 0.063 sec/batch)
2016-11-29 00:46:51.573091: step 5100, loss = 0.85 (2057.7 examples/sec; 0.062 sec/batch)
2016-11-29 00:46:57.135461: step 5110, loss = 0.87 (1910.4 examples/sec; 0.067 sec/batch)
2016-11-29 00:47:02.192521: step 5120, loss = 0.69 (1914.0 examples/sec; 0.067 sec/batch)
2016-11-29 00:47:07.358679: step 5130, loss = 0.83 (2066.9 examples/sec; 0.062 sec/batch)
2016-11-29 00:47:12.441418: step 5140, loss = 0.97 (2057.7 examples/sec; 0.062 sec/batch)
2016-11-29 00:47:17.437053: step 5150, loss = 0.91 (1977.4 examples/sec; 0.065 sec/batch)
2016-11-29 00:47:22.529949: step 5160, loss = 0.79 (2075.6 examples/sec; 0.062 sec/batch)
2016-11-29 00:47:27.685654: step 5170, loss = 0.78 (2081.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:47:32.861044: step 5180, loss = 0.65 (2138.6 examples/sec; 0.060 sec/batch)
2016-11-29 00:47:38.089949: step 5190, loss = 0.73 (2222.5 examples/sec; 0.058 sec/batch)
2016-11-29 00:47:43.303505: step 5200, loss = 0.77 (2061.9 examples/sec; 0.062 sec/batch)
2016-11-29 00:47:48.833566: step 5210, loss = 0.74 (1988.5 examples/sec; 0.064 sec/batch)
2016-11-29 00:47:53.965653: step 5220, loss = 0.76 (1905.0 examples/sec; 0.067 sec/batch)
2016-11-29 00:47:59.270186: step 5230, loss = 0.66 (2176.9 examples/sec; 0.059 sec/batch)
2016-11-29 00:48:04.327155: step 5240, loss = 0.76 (2058.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:48:09.334738: step 5250, loss = 0.86 (2051.9 examples/sec; 0.062 sec/batch)
2016-11-29 00:48:14.499826: step 5260, loss = 0.79 (2005.1 examples/sec; 0.064 sec/batch)
2016-11-29 00:48:19.611910: step 5270, loss = 0.73 (2269.9 examples/sec; 0.056 sec/batch)
2016-11-29 00:48:24.739726: step 5280, loss = 0.80 (2046.6 examples/sec; 0.063 sec/batch)
2016-11-29 00:48:29.928820: step 5290, loss = 0.70 (1922.0 examples/sec; 0.067 sec/batch)
2016-11-29 00:48:35.008533: step 5300, loss = 0.84 (1957.0 examples/sec; 0.065 sec/batch)
2016-11-29 00:48:40.628108: step 5310, loss = 0.66 (1925.4 examples/sec; 0.066 sec/batch)
2016-11-29 00:48:45.739723: step 5320, loss = 0.88 (1929.2 examples/sec; 0.066 sec/batch)
2016-11-29 00:48:50.841513: step 5330, loss = 0.92 (2103.9 examples/sec; 0.061 sec/batch)
2016-11-29 00:48:55.834769: step 5340, loss = 0.60 (2135.3 examples/sec; 0.060 sec/batch)
2016-11-29 00:49:01.046830: step 5350, loss = 0.74 (1955.4 examples/sec; 0.065 sec/batch)
2016-11-29 00:49:06.141172: step 5360, loss = 0.82 (2102.4 examples/sec; 0.061 sec/batch)
2016-11-29 00:49:11.259859: step 5370, loss = 0.92 (2030.8 examples/sec; 0.063 sec/batch)
2016-11-29 00:49:16.402660: step 5380, loss = 0.66 (2041.5 examples/sec; 0.063 sec/batch)
2016-11-29 00:49:21.399387: step 5390, loss = 0.96 (2201.4 examples/sec; 0.058 sec/batch)
2016-11-29 00:49:26.454391: step 5400, loss = 0.70 (2094.3 examples/sec; 0.061 sec/batch)
2016-11-29 00:49:32.120200: step 5410, loss = 0.87 (2097.5 examples/sec; 0.061 sec/batch)
2016-11-29 00:49:37.274259: step 5420, loss = 0.73 (1948.9 examples/sec; 0.066 sec/batch)
2016-11-29 00:49:42.572509: step 5430, loss = 0.87 (1647.9 examples/sec; 0.078 sec/batch)
2016-11-29 00:49:47.630492: step 5440, loss = 0.80 (2110.2 examples/sec; 0.061 sec/batch)
2016-11-29 00:49:52.768415: step 5450, loss = 0.66 (2079.4 examples/sec; 0.062 sec/batch)
2016-11-29 00:49:57.951661: step 5460, loss = 0.66 (2006.0 examples/sec; 0.064 sec/batch)
2016-11-29 00:50:02.980553: step 5470, loss = 0.81 (2250.2 examples/sec; 0.057 sec/batch)
2016-11-29 00:50:08.036904: step 5480, loss = 0.90 (2240.5 examples/sec; 0.057 sec/batch)
2016-11-29 00:50:13.030417: step 5490, loss = 1.02 (2089.1 examples/sec; 0.061 sec/batch)
2016-11-29 00:50:18.100408: step 5500, loss = 0.81 (1949.5 examples/sec; 0.066 sec/batch)
2016-11-29 00:50:23.615252: step 5510, loss = 0.87 (2257.0 examples/sec; 0.057 sec/batch)
2016-11-29 00:50:28.726320: step 5520, loss = 0.66 (1984.8 examples/sec; 0.064 sec/batch)
2016-11-29 00:50:33.666604: step 5530, loss = 1.25 (1913.5 examples/sec; 0.067 sec/batch)
2016-11-29 00:50:38.785924: step 5540, loss = 0.74 (2119.5 examples/sec; 0.060 sec/batch)
2016-11-29 00:50:43.872235: step 5550, loss = 0.61 (2238.9 examples/sec; 0.057 sec/batch)
2016-11-29 00:50:48.826179: step 5560, loss = 0.89 (2211.0 examples/sec; 0.058 sec/batch)
2016-11-29 00:50:54.017481: step 5570, loss = 1.01 (1997.5 examples/sec; 0.064 sec/batch)
2016-11-29 00:50:59.229503: step 5580, loss = 0.66 (2152.4 examples/sec; 0.059 sec/batch)
2016-11-29 00:51:04.283919: step 5590, loss = 0.60 (1913.6 examples/sec; 0.067 sec/batch)
2016-11-29 00:51:09.471184: step 5600, loss = 0.71 (2148.1 examples/sec; 0.060 sec/batch)
2016-11-29 00:51:15.065537: step 5610, loss = 0.69 (2000.2 examples/sec; 0.064 sec/batch)
2016-11-29 00:51:20.224778: step 5620, loss = 0.81 (1957.5 examples/sec; 0.065 sec/batch)
2016-11-29 00:51:25.163331: step 5630, loss = 0.84 (2022.8 examples/sec; 0.063 sec/batch)
2016-11-29 00:51:30.207505: step 5640, loss = 0.80 (1838.2 examples/sec; 0.070 sec/batch)
2016-11-29 00:51:35.291285: step 5650, loss = 0.73 (1872.4 examples/sec; 0.068 sec/batch)
2016-11-29 00:51:40.357555: step 5660, loss = 0.81 (1881.2 examples/sec; 0.068 sec/batch)
2016-11-29 00:51:45.541688: step 5670, loss = 0.89 (1921.4 examples/sec; 0.067 sec/batch)
2016-11-29 00:51:50.743951: step 5680, loss = 0.83 (1964.9 examples/sec; 0.065 sec/batch)
2016-11-29 00:51:55.853282: step 5690, loss = 0.60 (2103.3 examples/sec; 0.061 sec/batch)
2016-11-29 00:52:00.807455: step 5700, loss = 0.75 (2152.8 examples/sec; 0.059 sec/batch)
2016-11-29 00:52:06.391693: step 5710, loss = 0.79 (2154.9 examples/sec; 0.059 sec/batch)
2016-11-29 00:52:11.594169: step 5720, loss = 0.78 (1917.1 examples/sec; 0.067 sec/batch)
2016-11-29 00:52:16.562793: step 5730, loss = 0.86 (2247.3 examples/sec; 0.057 sec/batch)
2016-11-29 00:52:21.617052: step 5740, loss = 0.61 (1998.1 examples/sec; 0.064 sec/batch)
2016-11-29 00:52:26.714935: step 5750, loss = 0.80 (1957.1 examples/sec; 0.065 sec/batch)
2016-11-29 00:52:31.877138: step 5760, loss = 0.75 (2111.2 examples/sec; 0.061 sec/batch)
2016-11-29 00:52:37.063687: step 5770, loss = 0.74 (2119.2 examples/sec; 0.060 sec/batch)
2016-11-29 00:52:42.266341: step 5780, loss = 0.79 (1981.8 examples/sec; 0.065 sec/batch)
2016-11-29 00:52:47.133065: step 5790, loss = 0.68 (2346.8 examples/sec; 0.055 sec/batch)
2016-11-29 00:52:52.218550: step 5800, loss = 0.64 (2107.2 examples/sec; 0.061 sec/batch)
2016-11-29 00:52:57.945305: step 5810, loss = 0.75 (2071.7 examples/sec; 0.062 sec/batch)
2016-11-29 00:53:02.836708: step 5820, loss = 0.73 (2041.2 examples/sec; 0.063 sec/batch)
2016-11-29 00:53:08.018599: step 5830, loss = 0.73 (1988.0 examples/sec; 0.064 sec/batch)
2016-11-29 00:53:13.064628: step 5840, loss = 0.65 (2234.7 examples/sec; 0.057 sec/batch)
2016-11-29 00:53:18.148850: step 5850, loss = 0.66 (2050.0 examples/sec; 0.062 sec/batch)
2016-11-29 00:53:23.142166: step 5860, loss = 0.76 (2101.7 examples/sec; 0.061 sec/batch)
2016-11-29 00:53:28.264693: step 5870, loss = 0.72 (2021.1 examples/sec; 0.063 sec/batch)
2016-11-29 00:53:33.371952: step 5880, loss = 0.68 (2237.1 examples/sec; 0.057 sec/batch)
2016-11-29 00:53:38.418209: step 5890, loss = 0.66 (2063.7 examples/sec; 0.062 sec/batch)
2016-11-29 00:53:43.528528: step 5900, loss = 0.83 (1907.3 examples/sec; 0.067 sec/batch)
2016-11-29 00:53:49.183011: step 5910, loss = 0.93 (2011.0 examples/sec; 0.064 sec/batch)
2016-11-29 00:53:54.247890: step 5920, loss = 0.79 (1959.3 examples/sec; 0.065 sec/batch)
2016-11-29 00:53:59.275469: step 5930, loss = 0.81 (1979.9 examples/sec; 0.065 sec/batch)
2016-11-29 00:54:04.358034: step 5940, loss = 0.76 (2031.9 examples/sec; 0.063 sec/batch)
2016-11-29 00:54:09.375171: step 5950, loss = 1.13 (2018.1 examples/sec; 0.063 sec/batch)
2016-11-29 00:54:14.525837: step 5960, loss = 0.72 (2081.6 examples/sec; 0.061 sec/batch)
2016-11-29 00:54:19.672109: step 5970, loss = 0.89 (1423.3 examples/sec; 0.090 sec/batch)
2016-11-29 00:54:24.669827: step 5980, loss = 0.76 (2092.1 examples/sec; 0.061 sec/batch)
2016-11-29 00:54:29.888576: step 5990, loss = 0.88 (1766.7 examples/sec; 0.072 sec/batch)
2016-11-29 00:54:35.005590: step 6000, loss = 0.70 (2109.4 examples/sec; 0.061 sec/batch)
2016-11-29 00:54:43.382708: step 6010, loss = 0.69 (2050.5 examples/sec; 0.062 sec/batch)
2016-11-29 00:54:48.344046: step 6020, loss = 0.80 (2162.4 examples/sec; 0.059 sec/batch)
2016-11-29 00:54:53.373353: step 6030, loss = 0.82 (2150.2 examples/sec; 0.060 sec/batch)
2016-11-29 00:54:58.485771: step 6040, loss = 0.77 (2188.1 examples/sec; 0.058 sec/batch)
2016-11-29 00:55:03.501889: step 6050, loss = 0.76 (2062.9 examples/sec; 0.062 sec/batch)
2016-11-29 00:55:08.707623: step 6060, loss = 0.68 (1899.0 examples/sec; 0.067 sec/batch)
2016-11-29 00:55:13.842020: step 6070, loss = 0.71 (2170.4 examples/sec; 0.059 sec/batch)
2016-11-29 00:55:18.813245: step 6080, loss = 0.86 (2039.3 examples/sec; 0.063 sec/batch)
2016-11-29 00:55:23.978375: step 6090, loss = 0.80 (2056.1 examples/sec; 0.062 sec/batch)
2016-11-29 00:55:29.087257: step 6100, loss = 0.70 (2205.6 examples/sec; 0.058 sec/batch)
2016-11-29 00:55:34.656060: step 6110, loss = 0.64 (1969.8 examples/sec; 0.065 sec/batch)
2016-11-29 00:55:39.849748: step 6120, loss = 0.60 (2039.8 examples/sec; 0.063 sec/batch)
2016-11-29 00:55:45.012254: step 6130, loss = 0.49 (2036.6 examples/sec; 0.063 sec/batch)
2016-11-29 00:55:50.056989: step 6140, loss = 0.78 (1663.9 examples/sec; 0.077 sec/batch)
2016-11-29 00:55:55.085734: step 6150, loss = 0.76 (1996.7 examples/sec; 0.064 sec/batch)
2016-11-29 00:56:00.179516: step 6160, loss = 0.86 (2088.4 examples/sec; 0.061 sec/batch)
2016-11-29 00:56:05.242160: step 6170, loss = 0.57 (1806.1 examples/sec; 0.071 sec/batch)
2016-11-29 00:56:10.272510: step 6180, loss = 0.80 (2033.8 examples/sec; 0.063 sec/batch)
2016-11-29 00:56:15.381645: step 6190, loss = 0.73 (1955.5 examples/sec; 0.065 sec/batch)
2016-11-29 00:56:20.576299: step 6200, loss = 0.60 (1586.2 examples/sec; 0.081 sec/batch)
2016-11-29 00:56:26.088591: step 6210, loss = 0.64 (2148.3 examples/sec; 0.060 sec/batch)
2016-11-29 00:56:31.165871: step 6220, loss = 0.71 (2129.3 examples/sec; 0.060 sec/batch)
2016-11-29 00:56:36.357040: step 6230, loss = 0.75 (1529.5 examples/sec; 0.084 sec/batch)
2016-11-29 00:56:41.330902: step 6240, loss = 0.67 (2059.6 examples/sec; 0.062 sec/batch)
2016-11-29 00:56:46.402423: step 6250, loss = 0.61 (2135.5 examples/sec; 0.060 sec/batch)
2016-11-29 00:56:51.573204: step 6260, loss = 0.93 (1687.9 examples/sec; 0.076 sec/batch)
2016-11-29 00:56:56.509104: step 6270, loss = 0.68 (2201.5 examples/sec; 0.058 sec/batch)
2016-11-29 00:57:01.708665: step 6280, loss = 0.76 (1934.7 examples/sec; 0.066 sec/batch)
2016-11-29 00:57:06.957483: step 6290, loss = 0.79 (1438.0 examples/sec; 0.089 sec/batch)
2016-11-29 00:57:11.967210: step 6300, loss = 0.73 (2060.5 examples/sec; 0.062 sec/batch)
2016-11-29 00:57:17.613509: step 6310, loss = 0.66 (2079.3 examples/sec; 0.062 sec/batch)
2016-11-29 00:57:22.755397: step 6320, loss = 0.61 (1967.2 examples/sec; 0.065 sec/batch)
2016-11-29 00:57:27.772013: step 6330, loss = 0.68 (1972.4 examples/sec; 0.065 sec/batch)
2016-11-29 00:57:33.035600: step 6340, loss = 0.60 (1991.2 examples/sec; 0.064 sec/batch)
2016-11-29 00:57:38.150974: step 6350, loss = 0.73 (2356.5 examples/sec; 0.054 sec/batch)
2016-11-29 00:57:43.210934: step 6360, loss = 0.70 (2114.1 examples/sec; 0.061 sec/batch)
2016-11-29 00:57:48.350179: step 6370, loss = 0.67 (1929.7 examples/sec; 0.066 sec/batch)
2016-11-29 00:57:53.367054: step 6380, loss = 0.74 (2020.2 examples/sec; 0.063 sec/batch)
2016-11-29 00:57:58.344373: step 6390, loss = 0.66 (1953.4 examples/sec; 0.066 sec/batch)
2016-11-29 00:58:03.462456: step 6400, loss = 0.64 (2100.3 examples/sec; 0.061 sec/batch)
2016-11-29 00:58:09.097579: step 6410, loss = 0.75 (2069.3 examples/sec; 0.062 sec/batch)
2016-11-29 00:58:14.119570: step 6420, loss = 0.67 (2041.8 examples/sec; 0.063 sec/batch)
2016-11-29 00:58:19.288091: step 6430, loss = 0.85 (1988.2 examples/sec; 0.064 sec/batch)
2016-11-29 00:58:24.389766: step 6440, loss = 0.68 (2053.9 examples/sec; 0.062 sec/batch)
2016-11-29 00:58:29.453820: step 6450, loss = 0.58 (1972.0 examples/sec; 0.065 sec/batch)
2016-11-29 00:58:34.681075: step 6460, loss = 0.72 (2144.7 examples/sec; 0.060 sec/batch)
2016-11-29 00:58:39.851795: step 6470, loss = 0.61 (1897.9 examples/sec; 0.067 sec/batch)
2016-11-29 00:58:44.943670: step 6480, loss = 0.60 (2143.2 examples/sec; 0.060 sec/batch)
2016-11-29 00:58:50.225448: step 6490, loss = 0.66 (1972.2 examples/sec; 0.065 sec/batch)
2016-11-29 00:58:55.325190: step 6500, loss = 0.82 (2101.4 examples/sec; 0.061 sec/batch)
2016-11-29 00:59:00.876046: step 6510, loss = 0.66 (2090.1 examples/sec; 0.061 sec/batch)
2016-11-29 00:59:06.087673: step 6520, loss = 0.50 (1966.5 examples/sec; 0.065 sec/batch)
2016-11-29 00:59:11.275388: step 6530, loss = 0.66 (2124.9 examples/sec; 0.060 sec/batch)
2016-11-29 00:59:16.336015: step 6540, loss = 0.81 (2055.1 examples/sec; 0.062 sec/batch)
2016-11-29 00:59:21.482350: step 6550, loss = 0.86 (2133.4 examples/sec; 0.060 sec/batch)
2016-11-29 00:59:26.636220: step 6560, loss = 0.74 (2180.9 examples/sec; 0.059 sec/batch)
2016-11-29 00:59:31.712437: step 6570, loss = 0.72 (2116.9 examples/sec; 0.060 sec/batch)
2016-11-29 00:59:36.908012: step 6580, loss = 0.48 (2028.4 examples/sec; 0.063 sec/batch)
2016-11-29 00:59:41.999649: step 6590, loss = 0.57 (2010.4 examples/sec; 0.064 sec/batch)
2016-11-29 00:59:47.209323: step 6600, loss = 0.83 (1912.3 examples/sec; 0.067 sec/batch)
2016-11-29 00:59:52.694032: step 6610, loss = 0.71 (2099.4 examples/sec; 0.061 sec/batch)
2016-11-29 00:59:57.825684: step 6620, loss = 0.77 (1992.7 examples/sec; 0.064 sec/batch)
2016-11-29 01:00:02.974682: step 6630, loss = 0.77 (2077.3 examples/sec; 0.062 sec/batch)
2016-11-29 01:00:08.085634: step 6640, loss = 0.55 (2108.7 examples/sec; 0.061 sec/batch)
2016-11-29 01:00:13.273117: step 6650, loss = 0.71 (1901.9 examples/sec; 0.067 sec/batch)
2016-11-29 01:00:18.402054: step 6660, loss = 0.71 (2091.2 examples/sec; 0.061 sec/batch)
2016-11-29 01:00:23.370712: step 6670, loss = 0.60 (2081.7 examples/sec; 0.061 sec/batch)
2016-11-29 01:00:28.630049: step 6680, loss = 0.64 (1856.2 examples/sec; 0.069 sec/batch)
2016-11-29 01:00:33.750165: step 6690, loss = 0.94 (2181.1 examples/sec; 0.059 sec/batch)
2016-11-29 01:00:38.779708: step 6700, loss = 0.62 (2125.2 examples/sec; 0.060 sec/batch)
2016-11-29 01:00:44.512411: step 6710, loss = 0.73 (2030.2 examples/sec; 0.063 sec/batch)
2016-11-29 01:00:49.775457: step 6720, loss = 0.59 (1995.3 examples/sec; 0.064 sec/batch)
2016-11-29 01:00:55.010987: step 6730, loss = 0.57 (1790.9 examples/sec; 0.071 sec/batch)
2016-11-29 01:00:59.954449: step 6740, loss = 0.76 (1954.3 examples/sec; 0.065 sec/batch)
2016-11-29 01:01:05.131763: step 6750, loss = 0.72 (1948.0 examples/sec; 0.066 sec/batch)
2016-11-29 01:01:10.282368: step 6760, loss = 0.78 (2082.1 examples/sec; 0.061 sec/batch)
2016-11-29 01:01:15.313716: step 6770, loss = 0.77 (1976.6 examples/sec; 0.065 sec/batch)
2016-11-29 01:01:20.516007: step 6780, loss = 0.59 (1836.9 examples/sec; 0.070 sec/batch)
2016-11-29 01:01:25.668415: step 6790, loss = 0.70 (1915.5 examples/sec; 0.067 sec/batch)
2016-11-29 01:01:30.597175: step 6800, loss = 0.88 (2216.2 examples/sec; 0.058 sec/batch)
2016-11-29 01:01:36.296462: step 6810, loss = 0.66 (2059.8 examples/sec; 0.062 sec/batch)
2016-11-29 01:01:41.462167: step 6820, loss = 0.74 (1863.0 examples/sec; 0.069 sec/batch)
2016-11-29 01:01:46.471259: step 6830, loss = 0.61 (1965.1 examples/sec; 0.065 sec/batch)
2016-11-29 01:01:51.498780: step 6840, loss = 0.65 (2370.7 examples/sec; 0.054 sec/batch)
2016-11-29 01:01:56.708159: step 6850, loss = 0.69 (1963.2 examples/sec; 0.065 sec/batch)
2016-11-29 01:02:01.777438: step 6860, loss = 0.67 (2059.4 examples/sec; 0.062 sec/batch)
2016-11-29 01:02:06.995365: step 6870, loss = 0.62 (2043.9 examples/sec; 0.063 sec/batch)
2016-11-29 01:02:12.132419: step 6880, loss = 0.54 (2021.8 examples/sec; 0.063 sec/batch)
2016-11-29 01:02:17.139765: step 6890, loss = 0.75 (2073.9 examples/sec; 0.062 sec/batch)
2016-11-29 01:02:22.284351: step 6900, loss = 0.73 (1939.7 examples/sec; 0.066 sec/batch)
2016-11-29 01:02:27.983628: step 6910, loss = 0.82 (2021.7 examples/sec; 0.063 sec/batch)
2016-11-29 01:02:33.219692: step 6920, loss = 0.78 (2027.0 examples/sec; 0.063 sec/batch)
2016-11-29 01:02:38.284682: step 6930, loss = 0.79 (2025.6 examples/sec; 0.063 sec/batch)
2016-11-29 01:02:43.515131: step 6940, loss = 0.69 (1985.3 examples/sec; 0.064 sec/batch)
2016-11-29 01:02:48.489093: step 6950, loss = 0.48 (1978.6 examples/sec; 0.065 sec/batch)
2016-11-29 01:02:53.677217: step 6960, loss = 0.57 (1886.1 examples/sec; 0.068 sec/batch)
2016-11-29 01:02:58.799143: step 6970, loss = 0.62 (2027.5 examples/sec; 0.063 sec/batch)
2016-11-29 01:03:03.715416: step 6980, loss = 0.59 (2182.4 examples/sec; 0.059 sec/batch)
2016-11-29 01:03:08.950806: step 6990, loss = 0.71 (1911.6 examples/sec; 0.067 sec/batch)
2016-11-29 01:03:14.002310: step 7000, loss = 0.81 (2132.6 examples/sec; 0.060 sec/batch)
2016-11-29 01:03:22.488596: step 7010, loss = 0.79 (1897.4 examples/sec; 0.067 sec/batch)
2016-11-29 01:03:27.659207: step 7020, loss = 0.65 (2019.4 examples/sec; 0.063 sec/batch)
2016-11-29 01:03:32.696618: step 7030, loss = 0.78 (2106.9 examples/sec; 0.061 sec/batch)
2016-11-29 01:03:37.818594: step 7040, loss = 0.59 (2113.2 examples/sec; 0.061 sec/batch)
2016-11-29 01:03:42.997350: step 7050, loss = 0.58 (1875.4 examples/sec; 0.068 sec/batch)
2016-11-29 01:03:48.031491: step 7060, loss = 0.59 (1978.9 examples/sec; 0.065 sec/batch)
2016-11-29 01:03:53.265729: step 7070, loss = 0.62 (1977.5 examples/sec; 0.065 sec/batch)
2016-11-29 01:03:58.386420: step 7080, loss = 0.69 (2016.0 examples/sec; 0.063 sec/batch)
2016-11-29 01:04:03.544009: step 7090, loss = 0.55 (1497.4 examples/sec; 0.085 sec/batch)
2016-11-29 01:04:08.583347: step 7100, loss = 0.62 (1946.6 examples/sec; 0.066 sec/batch)
2016-11-29 01:04:14.374695: step 7110, loss = 0.77 (1981.0 examples/sec; 0.065 sec/batch)
2016-11-29 01:04:19.451036: step 7120, loss = 0.64 (1995.6 examples/sec; 0.064 sec/batch)
2016-11-29 01:04:24.480385: step 7130, loss = 0.68 (1927.6 examples/sec; 0.066 sec/batch)
2016-11-29 01:04:29.646266: step 7140, loss = 0.72 (2006.9 examples/sec; 0.064 sec/batch)
2016-11-29 01:04:34.870769: step 7150, loss = 0.68 (2007.2 examples/sec; 0.064 sec/batch)
2016-11-29 01:04:39.955761: step 7160, loss = 0.78 (2047.7 examples/sec; 0.063 sec/batch)
2016-11-29 01:04:45.048019: step 7170, loss = 0.63 (2287.8 examples/sec; 0.056 sec/batch)
2016-11-29 01:04:50.143314: step 7180, loss = 0.71 (2297.0 examples/sec; 0.056 sec/batch)
2016-11-29 01:04:55.200279: step 7190, loss = 0.78 (2059.3 examples/sec; 0.062 sec/batch)
2016-11-29 01:05:00.294852: step 7200, loss = 0.59 (2143.1 examples/sec; 0.060 sec/batch)
2016-11-29 01:05:06.045993: step 7210, loss = 0.60 (2084.7 examples/sec; 0.061 sec/batch)
2016-11-29 01:05:11.056933: step 7220, loss = 0.63 (2095.9 examples/sec; 0.061 sec/batch)
2016-11-29 01:05:16.264243: step 7230, loss = 0.75 (2002.3 examples/sec; 0.064 sec/batch)
2016-11-29 01:05:21.351608: step 7240, loss = 0.64 (2154.5 examples/sec; 0.059 sec/batch)
2016-11-29 01:05:26.358969: step 7250, loss = 0.83 (1999.1 examples/sec; 0.064 sec/batch)
2016-11-29 01:05:31.535699: step 7260, loss = 0.56 (2103.7 examples/sec; 0.061 sec/batch)
2016-11-29 01:05:36.779044: step 7270, loss = 0.64 (2000.6 examples/sec; 0.064 sec/batch)
2016-11-29 01:05:41.825534: step 7280, loss = 0.66 (1651.9 examples/sec; 0.077 sec/batch)
2016-11-29 01:05:46.916624: step 7290, loss = 0.67 (1925.6 examples/sec; 0.066 sec/batch)
2016-11-29 01:05:52.079847: step 7300, loss = 0.78 (2117.3 examples/sec; 0.060 sec/batch)
2016-11-29 01:05:57.670785: step 7310, loss = 0.71 (1923.0 examples/sec; 0.067 sec/batch)
2016-11-29 01:06:02.838256: step 7320, loss = 0.67 (1989.4 examples/sec; 0.064 sec/batch)
2016-11-29 01:06:08.015360: step 7330, loss = 0.56 (2041.4 examples/sec; 0.063 sec/batch)
2016-11-29 01:06:13.051262: step 7340, loss = 0.58 (2241.9 examples/sec; 0.057 sec/batch)
2016-11-29 01:06:18.160276: step 7350, loss = 0.56 (2220.8 examples/sec; 0.058 sec/batch)
2016-11-29 01:06:23.370460: step 7360, loss = 0.56 (1941.2 examples/sec; 0.066 sec/batch)
2016-11-29 01:06:28.438761: step 7370, loss = 0.56 (2064.6 examples/sec; 0.062 sec/batch)
2016-11-29 01:06:33.761280: step 7380, loss = 0.82 (1958.2 examples/sec; 0.065 sec/batch)
2016-11-29 01:06:39.026389: step 7390, loss = 0.55 (2157.7 examples/sec; 0.059 sec/batch)
2016-11-29 01:06:44.021939: step 7400, loss = 0.64 (1930.2 examples/sec; 0.066 sec/batch)
2016-11-29 01:06:49.735992: step 7410, loss = 0.61 (1861.7 examples/sec; 0.069 sec/batch)
2016-11-29 01:06:54.878873: step 7420, loss = 0.69 (2061.2 examples/sec; 0.062 sec/batch)
2016-11-29 01:06:59.879446: step 7430, loss = 0.69 (1982.7 examples/sec; 0.065 sec/batch)
2016-11-29 01:07:04.861044: step 7440, loss = 0.75 (2252.2 examples/sec; 0.057 sec/batch)
2016-11-29 01:07:09.962352: step 7450, loss = 0.63 (2127.7 examples/sec; 0.060 sec/batch)
2016-11-29 01:07:15.055582: step 7460, loss = 0.69 (1766.6 examples/sec; 0.072 sec/batch)
2016-11-29 01:07:20.046628: step 7470, loss = 0.72 (1950.7 examples/sec; 0.066 sec/batch)
2016-11-29 01:07:25.213962: step 7480, loss = 0.68 (2274.6 examples/sec; 0.056 sec/batch)
2016-11-29 01:07:30.414387: step 7490, loss = 0.79 (1995.2 examples/sec; 0.064 sec/batch)
2016-11-29 01:07:35.672422: step 7500, loss = 0.65 (2048.8 examples/sec; 0.062 sec/batch)
2016-11-29 01:07:41.466839: step 7510, loss = 0.66 (2025.6 examples/sec; 0.063 sec/batch)
2016-11-29 01:07:46.628497: step 7520, loss = 0.71 (1460.5 examples/sec; 0.088 sec/batch)
2016-11-29 01:07:51.536565: step 7530, loss = 0.64 (2088.5 examples/sec; 0.061 sec/batch)
2016-11-29 01:07:56.639266: step 7540, loss = 0.64 (1968.7 examples/sec; 0.065 sec/batch)
2016-11-29 01:08:01.787624: step 7550, loss = 0.69 (1501.9 examples/sec; 0.085 sec/batch)
2016-11-29 01:08:06.842273: step 7560, loss = 0.63 (2275.7 examples/sec; 0.056 sec/batch)
2016-11-29 01:08:11.964864: step 7570, loss = 0.65 (2064.1 examples/sec; 0.062 sec/batch)
2016-11-29 01:08:17.110931: step 7580, loss = 0.74 (1962.4 examples/sec; 0.065 sec/batch)
2016-11-29 01:08:22.171428: step 7590, loss = 0.66 (1919.0 examples/sec; 0.067 sec/batch)
2016-11-29 01:08:27.359425: step 7600, loss = 0.54 (2083.9 examples/sec; 0.061 sec/batch)
2016-11-29 01:08:33.094581: step 7610, loss = 0.65 (2105.9 examples/sec; 0.061 sec/batch)
2016-11-29 01:08:38.328681: step 7620, loss = 0.50 (2064.2 examples/sec; 0.062 sec/batch)
2016-11-29 01:08:43.267410: step 7630, loss = 0.64 (2075.3 examples/sec; 0.062 sec/batch)
2016-11-29 01:08:48.474876: step 7640, loss = 0.85 (2187.6 examples/sec; 0.059 sec/batch)
2016-11-29 01:08:53.626978: step 7650, loss = 0.72 (2050.7 examples/sec; 0.062 sec/batch)
2016-11-29 01:08:58.625795: step 7660, loss = 0.60 (2008.7 examples/sec; 0.064 sec/batch)
2016-11-29 01:09:03.795327: step 7670, loss = 0.56 (2086.5 examples/sec; 0.061 sec/batch)
2016-11-29 01:09:08.802232: step 7680, loss = 0.63 (2047.2 examples/sec; 0.063 sec/batch)
2016-11-29 01:09:13.918612: step 7690, loss = 0.66 (1989.7 examples/sec; 0.064 sec/batch)
2016-11-29 01:09:19.119807: step 7700, loss = 0.65 (1939.2 examples/sec; 0.066 sec/batch)
2016-11-29 01:09:24.678222: step 7710, loss = 0.60 (2178.5 examples/sec; 0.059 sec/batch)
2016-11-29 01:09:29.782443: step 7720, loss = 0.76 (2170.9 examples/sec; 0.059 sec/batch)
2016-11-29 01:09:34.886645: step 7730, loss = 0.80 (1910.6 examples/sec; 0.067 sec/batch)
2016-11-29 01:09:40.024252: step 7740, loss = 0.69 (1705.1 examples/sec; 0.075 sec/batch)
2016-11-29 01:09:45.062321: step 7750, loss = 0.66 (1942.1 examples/sec; 0.066 sec/batch)
2016-11-29 01:09:50.160529: step 7760, loss = 0.65 (1964.0 examples/sec; 0.065 sec/batch)
2016-11-29 01:09:55.360015: step 7770, loss = 0.50 (1551.6 examples/sec; 0.082 sec/batch)
2016-11-29 01:10:00.413460: step 7780, loss = 0.54 (2058.9 examples/sec; 0.062 sec/batch)
2016-11-29 01:10:05.512829: step 7790, loss = 0.61 (2175.5 examples/sec; 0.059 sec/batch)
2016-11-29 01:10:10.699036: step 7800, loss = 0.60 (1712.2 examples/sec; 0.075 sec/batch)
2016-11-29 01:10:16.211691: step 7810, loss = 0.65 (2291.3 examples/sec; 0.056 sec/batch)
2016-11-29 01:10:21.454130: step 7820, loss = 0.46 (1897.8 examples/sec; 0.067 sec/batch)
2016-11-29 01:10:26.570615: step 7830, loss = 0.56 (1887.2 examples/sec; 0.068 sec/batch)
2016-11-29 01:10:31.552157: step 7840, loss = 0.50 (2020.4 examples/sec; 0.063 sec/batch)
2016-11-29 01:10:36.669601: step 7850, loss = 1.00 (2130.7 examples/sec; 0.060 sec/batch)
2016-11-29 01:10:41.832443: step 7860, loss = 0.68 (2009.6 examples/sec; 0.064 sec/batch)
2016-11-29 01:10:46.833381: step 7870, loss = 0.69 (1911.5 examples/sec; 0.067 sec/batch)
2016-11-29 01:10:51.987366: step 7880, loss = 0.74 (2128.8 examples/sec; 0.060 sec/batch)
2016-11-29 01:10:57.090243: step 7890, loss = 0.64 (1974.9 examples/sec; 0.065 sec/batch)
2016-11-29 01:11:02.111675: step 7900, loss = 0.64 (1992.7 examples/sec; 0.064 sec/batch)
2016-11-29 01:11:07.781442: step 7910, loss = 0.64 (1939.4 examples/sec; 0.066 sec/batch)
2016-11-29 01:11:12.841227: step 7920, loss = 0.62 (1978.5 examples/sec; 0.065 sec/batch)
2016-11-29 01:11:18.029766: step 7930, loss = 0.65 (1549.2 examples/sec; 0.083 sec/batch)
2016-11-29 01:11:22.972582: step 7940, loss = 0.57 (1931.3 examples/sec; 0.066 sec/batch)
2016-11-29 01:11:28.072468: step 7950, loss = 0.62 (2154.6 examples/sec; 0.059 sec/batch)
2016-11-29 01:11:32.928524: step 7960, loss = 0.65 (2225.3 examples/sec; 0.058 sec/batch)
2016-11-29 01:11:38.055612: step 7970, loss = 0.70 (2001.8 examples/sec; 0.064 sec/batch)
2016-11-29 01:11:43.162415: step 7980, loss = 0.56 (1929.1 examples/sec; 0.066 sec/batch)
2016-11-29 01:11:48.133554: step 7990, loss = 0.58 (1917.5 examples/sec; 0.067 sec/batch)
2016-11-29 01:11:53.315922: step 8000, loss = 0.64 (2046.9 examples/sec; 0.063 sec/batch)
2016-11-29 01:12:01.926689: step 8010, loss = 0.68 (2239.1 examples/sec; 0.057 sec/batch)
2016-11-29 01:12:07.048802: step 8020, loss = 0.74 (1975.8 examples/sec; 0.065 sec/batch)
2016-11-29 01:12:12.103765: step 8030, loss = 0.55 (2093.1 examples/sec; 0.061 sec/batch)
2016-11-29 01:12:17.097671: step 8040, loss = 0.51 (1982.9 examples/sec; 0.065 sec/batch)
2016-11-29 01:12:22.144926: step 8050, loss = 0.62 (1974.0 examples/sec; 0.065 sec/batch)
2016-11-29 01:12:27.257976: step 8060, loss = 0.57 (2129.5 examples/sec; 0.060 sec/batch)
2016-11-29 01:12:32.333400: step 8070, loss = 0.54 (2096.1 examples/sec; 0.061 sec/batch)
2016-11-29 01:12:37.382317: step 8080, loss = 0.75 (2134.1 examples/sec; 0.060 sec/batch)
2016-11-29 01:12:42.367816: step 8090, loss = 0.73 (1807.6 examples/sec; 0.071 sec/batch)
2016-11-29 01:12:47.305827: step 8100, loss = 0.61 (2135.7 examples/sec; 0.060 sec/batch)
2016-11-29 01:12:52.940238: step 8110, loss = 0.78 (2067.6 examples/sec; 0.062 sec/batch)
2016-11-29 01:12:57.971092: step 8120, loss = 0.60 (2200.9 examples/sec; 0.058 sec/batch)
2016-11-29 01:13:03.016223: step 8130, loss = 0.71 (1935.8 examples/sec; 0.066 sec/batch)
2016-11-29 01:13:08.168992: step 8140, loss = 0.71 (1797.1 examples/sec; 0.071 sec/batch)
2016-11-29 01:13:13.245155: step 8150, loss = 0.71 (1620.0 examples/sec; 0.079 sec/batch)
2016-11-29 01:13:18.233127: step 8160, loss = 0.66 (1977.5 examples/sec; 0.065 sec/batch)
2016-11-29 01:13:23.523601: step 8170, loss = 0.67 (2007.8 examples/sec; 0.064 sec/batch)
2016-11-29 01:13:28.482600: step 8180, loss = 0.62 (1892.5 examples/sec; 0.068 sec/batch)
2016-11-29 01:13:33.725917: step 8190, loss = 0.59 (2018.5 examples/sec; 0.063 sec/batch)
2016-11-29 01:13:38.789134: step 8200, loss = 0.63 (2060.7 examples/sec; 0.062 sec/batch)
2016-11-29 01:13:44.492131: step 8210, loss = 0.51 (1595.1 examples/sec; 0.080 sec/batch)
2016-11-29 01:13:49.411723: step 8220, loss = 0.72 (2202.3 examples/sec; 0.058 sec/batch)
2016-11-29 01:13:54.617888: step 8230, loss = 0.90 (2037.0 examples/sec; 0.063 sec/batch)
2016-11-29 01:13:59.709391: step 8240, loss = 0.67 (2085.2 examples/sec; 0.061 sec/batch)
2016-11-29 01:14:04.601688: step 8250, loss = 0.62 (2010.4 examples/sec; 0.064 sec/batch)
2016-11-29 01:14:09.751957: step 8260, loss = 0.64 (1966.8 examples/sec; 0.065 sec/batch)
2016-11-29 01:14:14.866286: step 8270, loss = 0.67 (1725.2 examples/sec; 0.074 sec/batch)
2016-11-29 01:14:20.005692: step 8280, loss = 0.58 (2079.2 examples/sec; 0.062 sec/batch)
2016-11-29 01:14:25.157347: step 8290, loss = 0.63 (2268.8 examples/sec; 0.056 sec/batch)
2016-11-29 01:14:30.154873: step 8300, loss = 0.61 (2072.0 examples/sec; 0.062 sec/batch)
2016-11-29 01:14:35.668424: step 8310, loss = 0.59 (2108.4 examples/sec; 0.061 sec/batch)
2016-11-29 01:14:40.700645: step 8320, loss = 0.64 (2156.5 examples/sec; 0.059 sec/batch)
2016-11-29 01:14:45.733415: step 8330, loss = 0.67 (2206.8 examples/sec; 0.058 sec/batch)
2016-11-29 01:14:50.810504: step 8340, loss = 0.57 (1585.4 examples/sec; 0.081 sec/batch)
2016-11-29 01:14:55.925207: step 8350, loss = 0.53 (1878.1 examples/sec; 0.068 sec/batch)
2016-11-29 01:15:00.997428: step 8360, loss = 0.66 (2115.4 examples/sec; 0.061 sec/batch)
2016-11-29 01:15:06.125385: step 8370, loss = 0.85 (1774.1 examples/sec; 0.072 sec/batch)
2016-11-29 01:15:11.132077: step 8380, loss = 0.69 (2010.0 examples/sec; 0.064 sec/batch)
2016-11-29 01:15:16.182170: step 8390, loss = 0.76 (2138.4 examples/sec; 0.060 sec/batch)
2016-11-29 01:15:21.295421: step 8400, loss = 0.57 (1607.3 examples/sec; 0.080 sec/batch)
2016-11-29 01:15:26.806149: step 8410, loss = 0.66 (1873.2 examples/sec; 0.068 sec/batch)
2016-11-29 01:15:31.894461: step 8420, loss = 0.62 (2254.1 examples/sec; 0.057 sec/batch)
2016-11-29 01:15:36.936944: step 8430, loss = 0.57 (1999.3 examples/sec; 0.064 sec/batch)
2016-11-29 01:15:41.989853: step 8440, loss = 0.73 (1972.3 examples/sec; 0.065 sec/batch)
2016-11-29 01:15:47.137299: step 8450, loss = 0.59 (1996.2 examples/sec; 0.064 sec/batch)
2016-11-29 01:15:52.220856: step 8460, loss = 0.54 (2124.2 examples/sec; 0.060 sec/batch)
2016-11-29 01:15:57.224490: step 8470, loss = 0.85 (1845.6 examples/sec; 0.069 sec/batch)
2016-11-29 01:16:02.315185: step 8480, loss = 0.67 (2106.5 examples/sec; 0.061 sec/batch)
2016-11-29 01:16:07.448095: step 8490, loss = 0.59 (2085.7 examples/sec; 0.061 sec/batch)
2016-11-29 01:16:12.382721: step 8500, loss = 0.53 (2108.8 examples/sec; 0.061 sec/batch)
2016-11-29 01:16:18.030114: step 8510, loss = 0.63 (2135.4 examples/sec; 0.060 sec/batch)
2016-11-29 01:16:23.198649: step 8520, loss = 0.61 (2068.4 examples/sec; 0.062 sec/batch)
2016-11-29 01:16:28.200895: step 8530, loss = 0.59 (1940.4 examples/sec; 0.066 sec/batch)
2016-11-29 01:16:33.305617: step 8540, loss = 0.61 (1967.3 examples/sec; 0.065 sec/batch)
2016-11-29 01:16:38.402784: step 8550, loss = 0.53 (2077.0 examples/sec; 0.062 sec/batch)
2016-11-29 01:16:43.451183: step 8560, loss = 0.71 (2054.4 examples/sec; 0.062 sec/batch)
2016-11-29 01:16:48.456171: step 8570, loss = 0.57 (2355.8 examples/sec; 0.054 sec/batch)
2016-11-29 01:16:53.644446: step 8580, loss = 0.62 (2082.8 examples/sec; 0.061 sec/batch)
2016-11-29 01:16:58.579415: step 8590, loss = 0.81 (2121.4 examples/sec; 0.060 sec/batch)
2016-11-29 01:17:03.582980: step 8600, loss = 0.53 (1730.1 examples/sec; 0.074 sec/batch)
2016-11-29 01:17:09.130402: step 8610, loss = 0.59 (2073.9 examples/sec; 0.062 sec/batch)
2016-11-29 01:17:14.165479: step 8620, loss = 0.53 (1911.7 examples/sec; 0.067 sec/batch)
2016-11-29 01:17:19.330946: step 8630, loss = 0.59 (1995.6 examples/sec; 0.064 sec/batch)
2016-11-29 01:17:24.360274: step 8640, loss = 0.67 (1947.5 examples/sec; 0.066 sec/batch)
2016-11-29 01:17:29.465050: step 8650, loss = 0.80 (1668.9 examples/sec; 0.077 sec/batch)
2016-11-29 01:17:34.561877: step 8660, loss = 0.67 (1944.5 examples/sec; 0.066 sec/batch)
2016-11-29 01:17:39.756246: step 8670, loss = 0.65 (1825.8 examples/sec; 0.070 sec/batch)
2016-11-29 01:17:44.774266: step 8680, loss = 0.74 (2105.7 examples/sec; 0.061 sec/batch)
2016-11-29 01:17:49.693491: step 8690, loss = 0.71 (1980.1 examples/sec; 0.065 sec/batch)
2016-11-29 01:17:54.798954: step 8700, loss = 0.43 (1957.0 examples/sec; 0.065 sec/batch)
2016-11-29 01:18:00.407261: step 8710, loss = 0.67 (1961.1 examples/sec; 0.065 sec/batch)
2016-11-29 01:18:05.280840: step 8720, loss = 0.63 (2126.6 examples/sec; 0.060 sec/batch)
2016-11-29 01:18:10.361782: step 8730, loss = 0.69 (2265.1 examples/sec; 0.057 sec/batch)
2016-11-29 01:18:15.372226: step 8740, loss = 0.76 (2108.0 examples/sec; 0.061 sec/batch)
2016-11-29 01:18:20.473070: step 8750, loss = 0.78 (1940.2 examples/sec; 0.066 sec/batch)
2016-11-29 01:18:25.434442: step 8760, loss = 0.54 (2253.6 examples/sec; 0.057 sec/batch)
2016-11-29 01:18:30.612482: step 8770, loss = 0.55 (1519.9 examples/sec; 0.084 sec/batch)
2016-11-29 01:18:35.600925: step 8780, loss = 0.67 (1881.6 examples/sec; 0.068 sec/batch)
2016-11-29 01:18:40.633168: step 8790, loss = 0.65 (1916.5 examples/sec; 0.067 sec/batch)
2016-11-29 01:18:45.651794: step 8800, loss = 0.61 (1918.2 examples/sec; 0.067 sec/batch)
2016-11-29 01:18:51.194741: step 8810, loss = 0.68 (2057.6 examples/sec; 0.062 sec/batch)
2016-11-29 01:18:56.250179: step 8820, loss = 0.60 (2073.0 examples/sec; 0.062 sec/batch)
2016-11-29 01:19:01.360139: step 8830, loss = 0.82 (1988.7 examples/sec; 0.064 sec/batch)
2016-11-29 01:19:06.254286: step 8840, loss = 0.57 (1997.9 examples/sec; 0.064 sec/batch)
2016-11-29 01:19:11.330804: step 8850, loss = 0.60 (1982.6 examples/sec; 0.065 sec/batch)
2016-11-29 01:19:16.377006: step 8860, loss = 0.77 (1881.3 examples/sec; 0.068 sec/batch)
2016-11-29 01:19:21.361205: step 8870, loss = 0.52 (1918.4 examples/sec; 0.067 sec/batch)
2016-11-29 01:19:26.496772: step 8880, loss = 0.70 (2093.6 examples/sec; 0.061 sec/batch)
2016-11-29 01:19:31.611461: step 8890, loss = 0.51 (2377.3 examples/sec; 0.054 sec/batch)
2016-11-29 01:19:36.629630: step 8900, loss = 0.67 (2115.0 examples/sec; 0.061 sec/batch)
2016-11-29 01:19:42.167479: step 8910, loss = 0.58 (2173.2 examples/sec; 0.059 sec/batch)
2016-11-29 01:19:47.145924: step 8920, loss = 0.50 (2249.9 examples/sec; 0.057 sec/batch)
2016-11-29 01:19:52.125833: step 8930, loss = 0.61 (2160.3 examples/sec; 0.059 sec/batch)
2016-11-29 01:19:57.266726: step 8940, loss = 0.70 (1954.1 examples/sec; 0.066 sec/batch)
2016-11-29 01:20:02.479208: step 8950, loss = 0.73 (1930.6 examples/sec; 0.066 sec/batch)
2016-11-29 01:20:07.408117: step 8960, loss = 0.65 (2052.8 examples/sec; 0.062 sec/batch)
2016-11-29 01:20:12.479940: step 8970, loss = 0.55 (1915.5 examples/sec; 0.067 sec/batch)
2016-11-29 01:20:17.624547: step 8980, loss = 0.51 (1635.5 examples/sec; 0.078 sec/batch)
2016-11-29 01:20:22.558237: step 8990, loss = 0.62 (2021.6 examples/sec; 0.063 sec/batch)
2016-11-29 01:20:27.645166: step 9000, loss = 0.61 (2041.7 examples/sec; 0.063 sec/batch)
2016-11-29 01:20:36.114285: step 9010, loss = 0.50 (1863.6 examples/sec; 0.069 sec/batch)
2016-11-29 01:20:40.975079: step 9020, loss = 0.66 (1990.4 examples/sec; 0.064 sec/batch)
2016-11-29 01:20:46.095501: step 9030, loss = 0.67 (2063.1 examples/sec; 0.062 sec/batch)
2016-11-29 01:20:51.163050: step 9040, loss = 0.69 (1952.0 examples/sec; 0.066 sec/batch)
2016-11-29 01:20:56.260202: step 9050, loss = 0.62 (2093.7 examples/sec; 0.061 sec/batch)
2016-11-29 01:21:01.358404: step 9060, loss = 0.51 (2044.3 examples/sec; 0.063 sec/batch)
2016-11-29 01:21:06.327833: step 9070, loss = 0.68 (1910.4 examples/sec; 0.067 sec/batch)
2016-11-29 01:21:11.485327: step 9080, loss = 0.62 (1972.2 examples/sec; 0.065 sec/batch)
2016-11-29 01:21:16.613549: step 9090, loss = 0.68 (1900.8 examples/sec; 0.067 sec/batch)
2016-11-29 01:21:21.543081: step 9100, loss = 0.61 (2139.6 examples/sec; 0.060 sec/batch)
2016-11-29 01:21:27.148699: step 9110, loss = 0.68 (2238.8 examples/sec; 0.057 sec/batch)
2016-11-29 01:21:32.261453: step 9120, loss = 0.72 (2168.3 examples/sec; 0.059 sec/batch)
2016-11-29 01:21:37.191821: step 9130, loss = 0.69 (1988.9 examples/sec; 0.064 sec/batch)
2016-11-29 01:21:42.257255: step 9140, loss = 0.68 (2144.3 examples/sec; 0.060 sec/batch)
2016-11-29 01:21:47.409014: step 9150, loss = 0.54 (1952.4 examples/sec; 0.066 sec/batch)
2016-11-29 01:21:52.410698: step 9160, loss = 0.63 (2049.4 examples/sec; 0.062 sec/batch)
2016-11-29 01:21:57.543496: step 9170, loss = 0.54 (2015.4 examples/sec; 0.064 sec/batch)
2016-11-29 01:22:02.492467: step 9180, loss = 0.62 (2031.5 examples/sec; 0.063 sec/batch)
2016-11-29 01:22:07.544698: step 9190, loss = 0.54 (2088.3 examples/sec; 0.061 sec/batch)
2016-11-29 01:22:12.645126: step 9200, loss = 0.59 (2152.6 examples/sec; 0.059 sec/batch)
2016-11-29 01:22:18.314576: step 9210, loss = 0.76 (1617.8 examples/sec; 0.079 sec/batch)
2016-11-29 01:22:23.252172: step 9220, loss = 0.59 (2035.7 examples/sec; 0.063 sec/batch)
2016-11-29 01:22:28.415495: step 9230, loss = 0.62 (2050.1 examples/sec; 0.062 sec/batch)
2016-11-29 01:22:33.667695: step 9240, loss = 0.73 (1799.6 examples/sec; 0.071 sec/batch)
2016-11-29 01:22:38.729390: step 9250, loss = 0.56 (1940.8 examples/sec; 0.066 sec/batch)
2016-11-29 01:22:43.763665: step 9260, loss = 0.61 (2113.0 examples/sec; 0.061 sec/batch)
2016-11-29 01:22:48.845124: step 9270, loss = 0.65 (1824.5 examples/sec; 0.070 sec/batch)
2016-11-29 01:22:53.858219: step 9280, loss = 0.68 (1855.3 examples/sec; 0.069 sec/batch)
2016-11-29 01:22:58.842851: step 9290, loss = 0.71 (2108.1 examples/sec; 0.061 sec/batch)
2016-11-29 01:23:03.780539: step 9300, loss = 0.63 (2174.9 examples/sec; 0.059 sec/batch)
2016-11-29 01:23:09.402320: step 9310, loss = 0.66 (2051.5 examples/sec; 0.062 sec/batch)
2016-11-29 01:23:14.430401: step 9320, loss = 0.64 (2002.5 examples/sec; 0.064 sec/batch)
2016-11-29 01:23:19.414215: step 9330, loss = 0.60 (2002.7 examples/sec; 0.064 sec/batch)
2016-11-29 01:23:24.505690: step 9340, loss = 0.61 (2221.0 examples/sec; 0.058 sec/batch)
2016-11-29 01:23:29.629791: step 9350, loss = 0.63 (1993.6 examples/sec; 0.064 sec/batch)
2016-11-29 01:23:34.564619: step 9360, loss = 0.70 (1961.8 examples/sec; 0.065 sec/batch)
2016-11-29 01:23:39.672102: step 9370, loss = 0.53 (2078.2 examples/sec; 0.062 sec/batch)
2016-11-29 01:23:44.765647: step 9380, loss = 0.55 (2033.4 examples/sec; 0.063 sec/batch)
2016-11-29 01:23:49.749550: step 9390, loss = 0.62 (1704.1 examples/sec; 0.075 sec/batch)
2016-11-29 01:23:54.702876: step 9400, loss = 0.67 (2124.8 examples/sec; 0.060 sec/batch)
2016-11-29 01:24:00.341842: step 9410, loss = 0.52 (1956.5 examples/sec; 0.065 sec/batch)
2016-11-29 01:24:05.386808: step 9420, loss = 0.60 (2016.3 examples/sec; 0.063 sec/batch)
2016-11-29 01:24:10.321984: step 9430, loss = 0.62 (2201.9 examples/sec; 0.058 sec/batch)
2016-11-29 01:24:15.472272: step 9440, loss = 0.59 (1990.9 examples/sec; 0.064 sec/batch)
2016-11-29 01:24:20.467351: step 9450, loss = 0.51 (2005.6 examples/sec; 0.064 sec/batch)
2016-11-29 01:24:25.385480: step 9460, loss = 0.66 (2092.5 examples/sec; 0.061 sec/batch)
2016-11-29 01:24:30.444520: step 9470, loss = 0.50 (2028.9 examples/sec; 0.063 sec/batch)
2016-11-29 01:24:35.473091: step 9480, loss = 0.62 (2066.5 examples/sec; 0.062 sec/batch)
2016-11-29 01:24:40.443251: step 9490, loss = 0.65 (2097.0 examples/sec; 0.061 sec/batch)
2016-11-29 01:24:45.515928: step 9500, loss = 0.62 (1906.5 examples/sec; 0.067 sec/batch)
2016-11-29 01:24:51.029125: step 9510, loss = 0.71 (2115.4 examples/sec; 0.061 sec/batch)
2016-11-29 01:24:56.026657: step 9520, loss = 0.76 (2145.6 examples/sec; 0.060 sec/batch)
2016-11-29 01:25:01.102105: step 9530, loss = 0.54 (2047.4 examples/sec; 0.063 sec/batch)
2016-11-29 01:25:06.242943: step 9540, loss = 0.61 (2044.0 examples/sec; 0.063 sec/batch)
2016-11-29 01:25:11.226345: step 9550, loss = 0.55 (2008.1 examples/sec; 0.064 sec/batch)
2016-11-29 01:25:16.263345: step 9560, loss = 0.63 (2078.8 examples/sec; 0.062 sec/batch)
2016-11-29 01:25:21.220274: step 9570, loss = 0.57 (2225.9 examples/sec; 0.058 sec/batch)
2016-11-29 01:25:26.162372: step 9580, loss = 0.69 (2043.1 examples/sec; 0.063 sec/batch)
2016-11-29 01:25:31.305052: step 9590, loss = 0.61 (1896.6 examples/sec; 0.067 sec/batch)
2016-11-29 01:25:36.291749: step 9600, loss = 0.57 (2006.3 examples/sec; 0.064 sec/batch)
2016-11-29 01:25:41.751976: step 9610, loss = 0.62 (2191.6 examples/sec; 0.058 sec/batch)
2016-11-29 01:25:46.745991: step 9620, loss = 0.54 (2067.6 examples/sec; 0.062 sec/batch)
2016-11-29 01:25:51.755238: step 9630, loss = 0.56 (2142.2 examples/sec; 0.060 sec/batch)
2016-11-29 01:25:56.768206: step 9640, loss = 0.66 (1784.8 examples/sec; 0.072 sec/batch)
2016-11-29 01:26:01.670174: step 9650, loss = 0.48 (2226.4 examples/sec; 0.057 sec/batch)
2016-11-29 01:26:06.680872: step 9660, loss = 0.54 (1898.8 examples/sec; 0.067 sec/batch)
2016-11-29 01:26:11.469761: step 9670, loss = 0.51 (2050.5 examples/sec; 0.062 sec/batch)
2016-11-29 01:26:16.470843: step 9680, loss = 0.57 (2196.1 examples/sec; 0.058 sec/batch)
2016-11-29 01:26:21.432961: step 9690, loss = 0.59 (2171.8 examples/sec; 0.059 sec/batch)
2016-11-29 01:26:26.277138: step 9700, loss = 0.52 (2160.3 examples/sec; 0.059 sec/batch)
2016-11-29 01:26:31.854828: step 9710, loss = 0.60 (2133.4 examples/sec; 0.060 sec/batch)
2016-11-29 01:26:36.910064: step 9720, loss = 0.56 (1871.8 examples/sec; 0.068 sec/batch)
2016-11-29 01:26:41.865316: step 9730, loss = 0.63 (2098.3 examples/sec; 0.061 sec/batch)
2016-11-29 01:26:46.915782: step 9740, loss = 0.70 (1981.5 examples/sec; 0.065 sec/batch)
2016-11-29 01:26:51.926885: step 9750, loss = 0.54 (2415.3 examples/sec; 0.053 sec/batch)
2016-11-29 01:26:56.974044: step 9760, loss = 0.47 (1838.0 examples/sec; 0.070 sec/batch)
2016-11-29 01:27:01.979990: step 9770, loss = 0.54 (2007.5 examples/sec; 0.064 sec/batch)
2016-11-29 01:27:07.016044: step 9780, loss = 0.59 (2335.6 examples/sec; 0.055 sec/batch)
2016-11-29 01:27:11.901497: step 9790, loss = 0.52 (1979.7 examples/sec; 0.065 sec/batch)
2016-11-29 01:27:16.987594: step 9800, loss = 0.73 (1985.0 examples/sec; 0.064 sec/batch)
2016-11-29 01:27:22.621129: step 9810, loss = 0.64 (1946.3 examples/sec; 0.066 sec/batch)
2016-11-29 01:27:27.582796: step 9820, loss = 0.61 (2062.1 examples/sec; 0.062 sec/batch)
2016-11-29 01:27:32.854607: step 9830, loss = 0.56 (2214.3 examples/sec; 0.058 sec/batch)
2016-11-29 01:27:37.803601: step 9840, loss = 0.66 (2044.3 examples/sec; 0.063 sec/batch)
2016-11-29 01:27:42.679219: step 9850, loss = 0.61 (2224.6 examples/sec; 0.058 sec/batch)
2016-11-29 01:27:47.752339: step 9860, loss = 0.51 (2010.0 examples/sec; 0.064 sec/batch)
2016-11-29 01:27:52.779507: step 9870, loss = 0.58 (2042.5 examples/sec; 0.063 sec/batch)
2016-11-29 01:27:57.716198: step 9880, loss = 0.61 (2284.4 examples/sec; 0.056 sec/batch)
2016-11-29 01:28:02.818982: step 9890, loss = 0.64 (2086.9 examples/sec; 0.061 sec/batch)
2016-11-29 01:28:07.842770: step 9900, loss = 0.77 (2422.3 examples/sec; 0.053 sec/batch)
2016-11-29 01:28:13.585128: step 9910, loss = 0.63 (1491.2 examples/sec; 0.086 sec/batch)
2016-11-29 01:28:18.505910: step 9920, loss = 0.63 (2240.3 examples/sec; 0.057 sec/batch)
2016-11-29 01:28:23.486893: step 9930, loss = 0.62 (2005.4 examples/sec; 0.064 sec/batch)
2016-11-29 01:28:28.553612: step 9940, loss = 0.60 (1573.2 examples/sec; 0.081 sec/batch)
2016-11-29 01:28:33.513938: step 9950, loss = 0.58 (1896.0 examples/sec; 0.068 sec/batch)
2016-11-29 01:28:38.574136: step 9960, loss = 0.58 (1917.2 examples/sec; 0.067 sec/batch)
2016-11-29 01:28:43.646295: step 9970, loss = 0.55 (1660.0 examples/sec; 0.077 sec/batch)
2016-11-29 01:28:48.585435: step 9980, loss = 0.70 (1899.4 examples/sec; 0.067 sec/batch)
2016-11-29 01:28:53.697738: step 9990, loss = 0.56 (2070.5 examples/sec; 0.062 sec/batch)
2016-11-29 01:28:58.754180: step 10000, loss = 0.62 (1709.8 examples/sec; 0.075 sec/batch)
2016-11-29 01:29:07.134097: step 10010, loss = 0.59 (2068.3 examples/sec; 0.062 sec/batch)
2016-11-29 01:29:12.039458: step 10020, loss = 0.51 (1999.6 examples/sec; 0.064 sec/batch)
2016-11-29 01:29:17.131585: step 10030, loss = 0.69 (1922.5 examples/sec; 0.067 sec/batch)
2016-11-29 01:29:22.243520: step 10040, loss = 0.56 (2228.6 examples/sec; 0.057 sec/batch)
2016-11-29 01:29:27.259634: step 10050, loss = 0.74 (2183.3 examples/sec; 0.059 sec/batch)
2016-11-29 01:29:32.230424: step 10060, loss = 0.62 (2208.3 examples/sec; 0.058 sec/batch)
2016-11-29 01:29:37.273693: step 10070, loss = 0.71 (2015.4 examples/sec; 0.064 sec/batch)
2016-11-29 01:29:42.327357: step 10080, loss = 0.68 (1741.7 examples/sec; 0.073 sec/batch)
2016-11-29 01:29:47.261384: step 10090, loss = 0.67 (1951.9 examples/sec; 0.066 sec/batch)
2016-11-29 01:29:52.334928: step 10100, loss = 0.58 (1994.8 examples/sec; 0.064 sec/batch)
2016-11-29 01:29:57.914323: step 10110, loss = 0.62 (1961.9 examples/sec; 0.065 sec/batch)
2016-11-29 01:30:02.856417: step 10120, loss = 0.65 (2307.6 examples/sec; 0.055 sec/batch)
2016-11-29 01:30:07.882359: step 10130, loss = 0.55 (2079.0 examples/sec; 0.062 sec/batch)
2016-11-29 01:30:12.910696: step 10140, loss = 0.51 (2041.5 examples/sec; 0.063 sec/batch)
2016-11-29 01:30:17.955310: step 10150, loss = 0.51 (1934.2 examples/sec; 0.066 sec/batch)
2016-11-29 01:30:22.950495: step 10160, loss = 0.70 (2055.8 examples/sec; 0.062 sec/batch)
2016-11-29 01:30:28.003494: step 10170, loss = 0.55 (2198.6 examples/sec; 0.058 sec/batch)
2016-11-29 01:30:32.945032: step 10180, loss = 0.59 (1936.3 examples/sec; 0.066 sec/batch)
2016-11-29 01:30:38.077972: step 10190, loss = 0.68 (2199.2 examples/sec; 0.058 sec/batch)
2016-11-29 01:30:43.140067: step 10200, loss = 0.58 (2021.9 examples/sec; 0.063 sec/batch)
2016-11-29 01:30:48.801685: step 10210, loss = 0.66 (2082.9 examples/sec; 0.061 sec/batch)
2016-11-29 01:30:53.880753: step 10220, loss = 0.72 (1995.5 examples/sec; 0.064 sec/batch)
2016-11-29 01:30:58.837273: step 10230, loss = 0.67 (1768.5 examples/sec; 0.072 sec/batch)
2016-11-29 01:31:03.744972: step 10240, loss = 0.49 (2011.2 examples/sec; 0.064 sec/batch)
2016-11-29 01:31:08.868745: step 10250, loss = 0.78 (2043.0 examples/sec; 0.063 sec/batch)
2016-11-29 01:31:13.785009: step 10260, loss = 0.63 (2087.7 examples/sec; 0.061 sec/batch)
2016-11-29 01:31:18.859137: step 10270, loss = 0.57 (1955.5 examples/sec; 0.065 sec/batch)
2016-11-29 01:31:23.921013: step 10280, loss = 0.53 (2099.4 examples/sec; 0.061 sec/batch)
2016-11-29 01:31:28.919035: step 10290, loss = 0.56 (2027.5 examples/sec; 0.063 sec/batch)
2016-11-29 01:31:33.981781: step 10300, loss = 0.60 (2165.1 examples/sec; 0.059 sec/batch)
2016-11-29 01:31:39.603415: step 10310, loss = 0.56 (1998.4 examples/sec; 0.064 sec/batch)
2016-11-29 01:31:44.541403: step 10320, loss = 0.47 (2268.1 examples/sec; 0.056 sec/batch)
2016-11-29 01:31:49.537299: step 10330, loss = 0.53 (2061.1 examples/sec; 0.062 sec/batch)
2016-11-29 01:31:54.609780: step 10340, loss = 0.40 (2127.5 examples/sec; 0.060 sec/batch)
2016-11-29 01:31:59.707049: step 10350, loss = 0.63 (1554.3 examples/sec; 0.082 sec/batch)
2016-11-29 01:32:04.646983: step 10360, loss = 0.55 (2104.3 examples/sec; 0.061 sec/batch)
2016-11-29 01:32:09.709123: step 10370, loss = 0.46 (1970.6 examples/sec; 0.065 sec/batch)
2016-11-29 01:32:14.684407: step 10380, loss = 0.56 (1994.3 examples/sec; 0.064 sec/batch)
2016-11-29 01:32:19.