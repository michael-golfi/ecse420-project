I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.7.5 locally
E tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:140] kernel driver does not appear to be running on this host (ip-172-31-21-34): /proc/driver/nvidia/version does not exist
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2016-11-29 01:38:52.116491: step 0, loss = 4.68 (11.1 examples/sec; 11.494 sec/batch)
2016-11-29 01:39:10.615370: step 10, loss = 4.57 (80.2 examples/sec; 1.597 sec/batch)
2016-11-29 01:39:26.649399: step 20, loss = 4.50 (80.0 examples/sec; 1.599 sec/batch)
2016-11-29 01:39:42.645101: step 30, loss = 4.43 (80.0 examples/sec; 1.599 sec/batch)
2016-11-29 01:39:58.620897: step 40, loss = 4.54 (80.2 examples/sec; 1.596 sec/batch)
2016-11-29 01:40:14.612304: step 50, loss = 4.29 (79.7 examples/sec; 1.606 sec/batch)
2016-11-29 01:40:30.596732: step 60, loss = 4.29 (79.7 examples/sec; 1.607 sec/batch)
2016-11-29 01:40:46.604636: step 70, loss = 4.25 (79.9 examples/sec; 1.601 sec/batch)
2016-11-29 01:41:02.652959: step 80, loss = 4.24 (80.8 examples/sec; 1.585 sec/batch)
2016-11-29 01:41:18.744620: step 90, loss = 4.10 (74.9 examples/sec; 1.709 sec/batch)
2016-11-29 01:41:34.848698: step 100, loss = 4.02 (80.4 examples/sec; 1.593 sec/batch)
2016-11-29 01:41:52.660335: step 110, loss = 4.06 (81.4 examples/sec; 1.572 sec/batch)
2016-11-29 01:42:08.437921: step 120, loss = 4.03 (81.0 examples/sec; 1.580 sec/batch)
2016-11-29 01:42:24.382258: step 130, loss = 3.91 (80.6 examples/sec; 1.588 sec/batch)
2016-11-29 01:42:40.269162: step 140, loss = 3.96 (80.9 examples/sec; 1.581 sec/batch)
2016-11-29 01:42:56.198060: step 150, loss = 3.83 (80.2 examples/sec; 1.595 sec/batch)
2016-11-29 01:43:12.065875: step 160, loss = 3.84 (80.8 examples/sec; 1.585 sec/batch)
2016-11-29 01:43:27.962361: step 170, loss = 3.68 (81.1 examples/sec; 1.579 sec/batch)
2016-11-29 01:43:43.795045: step 180, loss = 3.78 (80.8 examples/sec; 1.584 sec/batch)
2016-11-29 01:43:59.613132: step 190, loss = 3.83 (81.3 examples/sec; 1.574 sec/batch)
2016-11-29 01:44:15.470937: step 200, loss = 3.78 (79.4 examples/sec; 1.611 sec/batch)
2016-11-29 01:44:33.256975: step 210, loss = 3.76 (80.9 examples/sec; 1.582 sec/batch)
2016-11-29 01:44:49.094914: step 220, loss = 4.18 (80.6 examples/sec; 1.588 sec/batch)
2016-11-29 01:45:05.176810: step 230, loss = 3.66 (80.6 examples/sec; 1.588 sec/batch)
2016-11-29 01:45:21.060782: step 240, loss = 3.68 (80.6 examples/sec; 1.588 sec/batch)
2016-11-29 01:45:36.967574: step 250, loss = 4.12 (81.1 examples/sec; 1.579 sec/batch)
2016-11-29 01:45:52.748490: step 260, loss = 3.55 (81.3 examples/sec; 1.575 sec/batch)
2016-11-29 01:46:08.572673: step 270, loss = 3.76 (81.4 examples/sec; 1.573 sec/batch)
2016-11-29 01:46:24.391744: step 280, loss = 3.55 (80.8 examples/sec; 1.583 sec/batch)
2016-11-29 01:46:40.264438: step 290, loss = 3.51 (80.4 examples/sec; 1.591 sec/batch)
2016-11-29 01:46:56.156838: step 300, loss = 3.67 (80.1 examples/sec; 1.599 sec/batch)
2016-11-29 01:47:13.946319: step 310, loss = 3.48 (81.2 examples/sec; 1.576 sec/batch)
2016-11-29 01:47:29.739127: step 320, loss = 3.50 (80.6 examples/sec; 1.588 sec/batch)
2016-11-29 01:47:45.588572: step 330, loss = 3.43 (80.9 examples/sec; 1.582 sec/batch)
2016-11-29 01:48:01.440151: step 340, loss = 3.59 (81.2 examples/sec; 1.576 sec/batch)
2016-11-29 01:48:17.206619: step 350, loss = 3.58 (80.4 examples/sec; 1.593 sec/batch)
2016-11-29 01:48:32.976782: step 360, loss = 3.48 (80.2 examples/sec; 1.596 sec/batch)
2016-11-29 01:48:48.750218: step 370, loss = 3.34 (80.5 examples/sec; 1.590 sec/batch)
2016-11-29 01:49:04.494341: step 380, loss = 3.39 (82.0 examples/sec; 1.562 sec/batch)
2016-11-29 01:49:20.266476: step 390, loss = 3.30 (80.6 examples/sec; 1.589 sec/batch)
2016-11-29 01:49:36.091446: step 400, loss = 3.37 (80.9 examples/sec; 1.583 sec/batch)
2016-11-29 01:49:53.848122: step 410, loss = 3.55 (81.0 examples/sec; 1.581 sec/batch)
2016-11-29 01:50:09.624877: step 420, loss = 3.23 (81.1 examples/sec; 1.578 sec/batch)
2016-11-29 01:50:25.397152: step 430, loss = 3.37 (81.0 examples/sec; 1.580 sec/batch)
2016-11-29 01:50:41.153758: step 440, loss = 3.23 (81.1 examples/sec; 1.579 sec/batch)
2016-11-29 01:50:56.951970: step 450, loss = 3.31 (81.0 examples/sec; 1.581 sec/batch)
2016-11-29